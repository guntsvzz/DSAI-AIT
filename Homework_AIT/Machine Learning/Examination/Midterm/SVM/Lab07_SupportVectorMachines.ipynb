{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 07: Support Vector Machines\n",
    "\n",
    "Today we'll look at the SVM maximum margin classification problem and how we can implement the optimization\n",
    "in Python.\n",
    "\n",
    "We'll use the cvxopt quadratic programming optimizer in Python.\n",
    "\n",
    "Later in the lectures, we'll see that more specialized algorithms such as Sequential Minimal Optimization\n",
    "implemented by the machine learning libraries are more effective for large SVM problems.\n",
    "\n",
    "## Linearly separable case: direct solution using quadratic programming\n",
    "\n",
    "If we assume that the data are linearly separable, we can use the following setup for the optimization:\n",
    "- The data are pairs $(\\mathbf{x}^{(i)},y^{(i)})$ with $\\mathbf{x}^{(i)} \\in \\mathbb{R}^n$ and $y^{(i)} \\in \\{-1,1\\}$.\n",
    "- The hypothesis is\n",
    "  $$h_{\\mathbf{w},b}(\\mathbf{x}) = \\begin{cases} 1 & \\text{if} \\; \\mathbf{w}^\\top \\mathbf{x} + b > 0 \\\\ -1 & \\text{otherwise} \\end{cases}$$\n",
    "- The objective function is\n",
    "  $$\\mathbf{w}^*,b^* = \\mathrm{argmax}_{\\mathbf{w},b} \\gamma,$$\n",
    "  where $\\gamma$ is the minimum geometric margin for the training data:\n",
    "  $$\\gamma = \\min_i \\gamma^{(i)}$$\n",
    "  and $\\gamma^{(i)}$ is the geometric margin for training example $i$, i.e., the signed distance of $\\mathbf{x}^{(i)}$ from the decision boundary,\n",
    "  with positive distances indicating that the point is on the correct side of the boundary and negative distances indicating that the point is on the\n",
    "  incorrect side of the boundary:\n",
    "  $$\\gamma^{(i)} = y^{(i)}\\left( \\left( \\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|} \\right)^\\top \\mathbf{x}^{(i)} + \\frac{b}{\\| \\mathbf{w}\\|} \\right). $$\n",
    "- To find the optimal $\\mathbf{w},b$ according to the objective function above, we can solve the constrained\n",
    "  optimization problem\n",
    "  $$ \\begin{array}{rl} \\min_{\\mathbf{w},b} & \\|\\mathbf{w}\\| \\\\\n",
    "                     \\text{subject to} & y^{(i)}(\\mathbf{w}^\\top\\mathbf{x}^{(i)}+b)\\ge 1, i \\in 1..m\n",
    "   \\end{array}\n",
    "  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (in lab): linearly separable data\n",
    "\n",
    "Take the example data and SVM optimization code using cvxopt from the exercise in lecture. Verify that you can find the decision boundary for such \"easy\" cases.\n",
    "Show your results in your lab report.\n",
    "\n",
    "## Exercise 2 (in lab): non-separable data\n",
    "\n",
    "Take the example of the annulus from the logistic regression lab. Verify that cvxopt cannot find a decision boundary for this case. Show your results in your\n",
    "lab report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Lagrangian optimization for SMVs\n",
    "\n",
    "Now we consider the generalized Lagrangian for the SVM. This technique is suitable for solving problems of the form\n",
    "  $$ \\begin{array}{rl}\n",
    "    \\min_{\\mathbf{w}} & f(\\mathbf{w}) \\\\\n",
    "    \\text{subject to} & g_i(\\mathbf{w}) \\le 0, i \\in 1..k \\\\\n",
    "                      & h_i(\\mathbf{w}) = 0, i \\in 1..l\n",
    "   \\end{array}$$\n",
    "\n",
    "The generalized Lagrangian is\n",
    "$$\\cal{L}(\\mathbf{w},\\mathbf{\\alpha},\\mathbf{\\beta}) = f(\\mathbf{w}) + \\sum_{i=1}^k \\alpha_i g_i(\\mathbf{w}) + \\sum_{i=1}^l \\beta_i h_i(\\mathbf{w}),$$\n",
    "which has been cleverly arranged to be equal to $f(\\mathbf{w})$ whenever $\\mathbf{w}$ satisfies the constraints and $\\infty$ otherwise.\n",
    "\n",
    "### Primal and dual Lagrangian problems\n",
    "\n",
    "The primal problem is to find\n",
    "$$p^* = \\min_{\\mathbf{w}}\\theta_{\\cal P}(\\mathbf{w}) = \\min_\\mathbf{w}\\max_{\\mathbf{\\alpha},\\mathbf{\\beta},\\alpha_i \\ge 0} {\\cal L}(\\mathbf{w},\\mathbf{\\alpha},\\mathbf{\\beta})$$\n",
    "and the dual problem is to find\n",
    "$$d^* = \\max_{\\mathbf{\\alpha},\\mathbf{\\beta},\\alpha_i \\ge 0} \\theta_{\\cal D}(\\mathbf{\\alpha},\\mathbf{\\beta}) =  \\max_{\\mathbf{\\alpha},\\mathbf{\\beta},\\alpha_i \\ge 0} \\min_\\mathbf{w}{\\cal L}(\\mathbf{w},\\mathbf{\\alpha},\\mathbf{\\beta}).$$\n",
    "If $f$ is convex, the $g_i$'s are affine, the $h_i$'s are convex, and the $g_i$'s are strictly\n",
    "feasible, it turns out that the solutions to the primal and dual problem are the same, and the KKT conditions hold:\n",
    "$$ \\begin{array}{rcl}\n",
    "  \\frac{\\partial}{\\partial w_i}{\\cal L}(\\mathbf{w}^*,\\mathbf{\\alpha}^*,\\mathbf{\\beta}^*) & = & 0, i \\in 1..n \\\\\n",
    "  \\frac{\\partial}{\\partial \\beta_i}{\\cal L}(\\mathbf{w}^*,\\mathbf{\\alpha}^*,\\mathbf{\\beta}^*) & = & 0, i \\in 1..l \\\\\n",
    "  \\alpha_i^*g_i(\\mathbf{w}^*) & = & 0, i \\in 1..k \\\\\n",
    "  g_i(\\mathbf{w}^*) & \\le & 0, i \\in 1..k \\\\\n",
    "  \\alpha_i^* & \\ge & 0, i \\in 1..k\n",
    "  \\end{array} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the dual Lagrangian problem\n",
    "\n",
    "The dual problem turns out to be easiest to solve.\n",
    "\n",
    "We first solve for $\\mathbf{w}$ assuming fixed $\\mathbf{\\alpha}$ and $\\mathbf{\\beta}$ (we don't have equality constraints though, so no need for $\\mathbf{\\beta}$).\n",
    "\n",
    "We need to rewrite the SVM constraints in the necessary form with $g_i(\\mathbf{w})=0$.obtain, for the SVM, constraints\n",
    "$$g_i(\\mathbf{w},b) = -y^{(i)}(\\mathbf{w}^\\top\\mathbf{x}^{(i)} + b) + 1 \\ge 0.$$\n",
    "Using that definition of $g_i(\\mathbf{w},b)$, we obtain the Lagrangian\n",
    "$${\\cal L}(\\mathbf{w},b,\\mathbf{\\alpha}) = \\frac{1}{2}\\|\\mathbf{w}\\|^2 - \\sum_{i=1}^m\\alpha_i\\left[ y^{(i)}(\\mathbf{w}^\\top\\mathbf{x}^{(i)} + b) -1 \\right] $$\n",
    "Taking the gradient of ${\\cal L}$ with respect to $\\mathbf{w}$ and setting it to 0, we obtain\n",
    "\n",
    "$$\\nabla_\\mathbf{w}{\\cal L}(\\mathbf{w},b,\\mathbf{\\alpha}) = \\mathbf{w} - \\sum_{i=1}^m\\alpha_i y^{(i)} \\mathbf{x}^{(i)} = 0,$$\n",
    "which gives us\n",
    "\n",
    "$$\\mathbf{w}=\\sum_{i=1}^m\\alpha_i y^{(i)} \\mathbf{x}^{(i)}.$$\n",
    "\n",
    "From $\\frac{\\partial {\\cal L}}{\\partial b} = 0$, we obtain\n",
    "$$\\sum_{i=1}^m \\alpha_i y^{(i)} = 0,$$\n",
    "which is interesting (think about what it means also considering that $\\alpha_i > 0$ only for examples on the margin.\n",
    "Unfortunately it doesn't help us find $b$! In any case, we plug this definition for the optimal $\\mathbf{w}$ into the original Lagrangian,\n",
    "to obtain\n",
    "$${\\cal L}(\\mathbf{w},b,\\mathbf{\\alpha}) = \\sum_{i=1}^m \\alpha_i - \\frac{1}{2}\\sum_{i,j=1}^m y^{(i)} y^{(j)} \\alpha_i \\alpha_j (\\mathbf{x}^{(i)})^\\top\\mathbf{x}^{(j)} - b\\sum_{i=1}^m \\alpha_i y^{(i)}. $$\n",
    "We already know that the last term is 0, so we get\n",
    "$${\\cal L}(\\mathbf{w},b,\\mathbf{\\alpha}) = \\sum_{i=1}^m \\alpha_i - \\frac{1}{2}\\sum_{i,j=1}^m y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\left< \\mathbf{x}^{(i)},\\mathbf{x}^{(j)} \\right> . $$\n",
    "OK! We've eliminated $\\mathbf{w}$ and $b$ from the optimization. Now we just need to maximize ${\\cal L}$ with respect to $\\mathbf{\\alpha}$.\n",
    "This gives us the final (dual) optimization problem\n",
    "$$ \\begin{array}{rl}\n",
    "  \\max_{\\mathbf{\\alpha}} & W(\\mathbf{\\alpha}) = \\sum_{i=1}^m \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^m y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\left< \\mathbf{x}^{(i)}, \\mathbf{x}^{(j)} \\right> \\\\\n",
    "  \\text{such that} & \\alpha_i \\ge 0, i \\in 1..m \\\\\n",
    "                   & \\sum_{i=1}^m \\alpha_i y^{(i)} = 0\n",
    "\\end{array} $$\n",
    "This turns out to be QP again!\n",
    "\n",
    "Aside: once we solve for $\\mathbf{\\alpha}$, we obtain $\\mathbf{w}$ according to the equation above, then it turns out that the optimal $b$ can be obtained as in\n",
    "the lecture notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QP solution to dual problem\n",
    "\n",
    "We need to negate our objective function to turn the max (SVM formulation) into a min (QP formalation).\n",
    "\n",
    "For the second term of $W(\\mathbf{\\alpha})$, first let $\\mathtt{K}$ be the kernel matrix with $\\mathtt{K}_{ij} = \\left< \\mathbf{x}^{(i)}, \\mathbf{x}^{(j)} \\right>$. Then $\\mathbf{\\alpha}^\\top \\text{diag}(\\mathbf{y}) \\mathtt{K} \\text{diag}(\\mathbf{y}) \\mathbf{\\alpha}$ gives us the summation in the second term ($\\text{diag}(\\mathbf{y})$ is just the square diagonal matrix with $\\mathbf{y}$ as its diagonal).\n",
    "\n",
    "The (negated) first term of $W(\\mathbf{\\alpha})$ can be written in QP form with $\\mathbf{c} = \\begin{bmatrix} -1 & -1 & \\ldots \\end{bmatrix}^\\top$.\n",
    "\n",
    "So that gives us our QP setup:\n",
    "$$ \\mathtt{Q} = \\text{diag}(\\mathbf{y}) \\mathtt{K} \\text{diag}(\\mathbf{y}) \\; \\; \\;\n",
    "   \\mathbf{c} = \\begin{bmatrix} -1 \\\\ -1 \\\\ \\vdots \\end{bmatrix} $$\n",
    "\n",
    "$$\\mathtt{A} = -\\mathtt{I}_{m\\times m} \\;\\;\\;\n",
    "  \\mathbf{b} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\end{bmatrix} $$\n",
    "\n",
    "$$\\mathtt{E} = \\mathbf{y}^\\top \\;\\;\\;\n",
    "  \\mathbf{d} = \\begin{bmatrix} 0 \\end{bmatrix}. $$\n",
    "\n",
    "OK, now the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X.shape[0];\n",
    "n = X.shape[1];\n",
    "\n",
    "# Transform data set so that each attribute has a\n",
    "# mean of 0 and a standard deviation of 1\n",
    "\n",
    "def preprocess(X):\n",
    "    means = X.mean(0);\n",
    "    scales = 1/np.std(X,0);\n",
    "    Xh = np.concatenate([X.T,np.ones([1,20])],0);\n",
    "    Tm = np.matrix(np.eye(3));\n",
    "    Tm[0:2,2:3] = -X.mean(0).T;\n",
    "    Ts = np.matrix(np.eye(3));\n",
    "    Ts[0:2,0:2] = np.diagflat(scales);\n",
    "    T = Ts*Tm;\n",
    "    XX = (T * Xh);\n",
    "    XX = XX[0:2,:].T;\n",
    "    return XX, T;\n",
    "\n",
    "# RBF/Gaussian kernel\n",
    "\n",
    "def gauss_kernel(X):\n",
    "    sigma = 0.2\n",
    "    m = X.shape[0];\n",
    "    K = np.matrix(np.zeros([m,m]));\n",
    "    for i in range(0,m):\n",
    "        for j in range(0,m):\n",
    "            K[i,j] = (X[i,:] - X[j,:]) * (X[i,:] - X[j,:]).T\n",
    "    K = np.exp(-K/(2*sigma*sigma))      \n",
    "    return K;\n",
    "\n",
    "def linear_kernel(X):\n",
    "    m = X.shape[0];\n",
    "    K = np.matrix(np.zeros([m,m]));\n",
    "    for i in range(0,m):\n",
    "        for j in range(0,m):\n",
    "            K[i,j] = X[i,:]*(X[j,:].T)\n",
    "    return K;\n",
    "\n",
    "# Linear kernel below. We might also try\n",
    "# XX, T = preprocess(X)\n",
    "# K = gauss_kernel(XX)\n",
    "\n",
    "K = linear_kernel(X);\n",
    "\n",
    "Q = np.multiply(y * y.T, K)\n",
    "print('Q rank: %d' % np.linalg.matrix_rank(Q))\n",
    "c = -np.ones([m]);\n",
    "A = -np.eye(m);\n",
    "b = np.zeros([m]);\n",
    "E = y.T;\n",
    "d = np.zeros(1);\n",
    "alpha_star = cvxopt_solve_qp(Q, c, A, b, E, d);\n",
    "print(\"Optimal alpha:\")\n",
    "print(alpha_star)\n",
    "\n",
    "def get_wb(X, y, alpha):\n",
    "    # Find the support vectors\n",
    "    S = alpha > 1e-6\n",
    "    XS = X[S,:]\n",
    "    yS = y[S]\n",
    "    alphaS = alpha[S]\n",
    "    alphaSyS = np.tile(np.multiply(yS.T, alphaS).T, n)\n",
    "    w = sum(np.multiply(alphaSyS, XS)).T\n",
    "    # Find b\n",
    "    KS = K[S,:][:,S]\n",
    "    NS = yS.shape[0]\n",
    "    b = (np.sum(yS) - np.sum(np.multiply(alphaS,yS.T)*KS))/NS\n",
    "    # Normalize w,b\n",
    "    scalef = np.linalg.norm(w)\n",
    "    w = w / scalef\n",
    "    b = b / scalef\n",
    "    return w,b\n",
    "\n",
    "w,b = get_wb(X, y, alpha_star)\n",
    "\n",
    "print(\"Optimal w: [%f,%f] b: %f\" % (w[0],w[1],b))\n",
    "plot_mf(Xf,Xm)\n",
    "\n",
    "def plot_w(w,b):\n",
    "    ylim = plt.axes().get_ylim()\n",
    "    xlim = plt.axes().get_xlim()\n",
    "    p1 = (xlim[0], - (w[0,0] * xlim[0] + b) / w[1,0])\n",
    "    p2 = (xlim[1], - (w[0,0] * xlim[1] + b) / w[1,0])\n",
    "    plt.plot((p1[0],p2[0]), (p1[1],p2[1]), 'r-')\n",
    "\n",
    "plot_w(w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (in lab): linearly separable data\n",
    "\n",
    "Take the example data from the exercise in lecture. Verify that you can use the dual optmization to find the decision boundary for such \"easy\" cases.\n",
    "Show your results in your lab report.\n",
    "\n",
    "## Exercise 4 (in lab): non-separable data, linear kernel\n",
    "\n",
    "Again, take the example of the annulus from the logistic regression lab. Verify that the dual optimization with the linear kernel\n",
    "still cannot find a decision boundary for this case. Show your results in your\n",
    "lab report.\n",
    "\n",
    "## Exercise 5 (in lab): \"easy\" non-separable data, Gaussian (RBF) kernel with non-overlapping data\n",
    "\n",
    "Now use the Gaussian (radial basis function) kernel instead of the linear kernel implemented in the code above and verify that you can correctly\n",
    "solve the problem.\n",
    "\n",
    "## Exercise 6 (take home): more difficult non-separable data\n",
    "\n",
    "Now find or generate a dataset in which the decision boundary is nonlinear AND the data overlap along that nonlinear boundary.\n",
    "Show that the "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
