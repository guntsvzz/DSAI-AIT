{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## Further Study - Other Regressions\n",
    "\n",
    "### Readings: \n",
    "- https://scikit-learn.org/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other types of regression\n",
    "\n",
    "Linear regression is the most basic algorithm.  There are many mores.  Today, we shall briefly talked about other types, for the sake of completeness.   Then we shall revisit the diabetes dataset, and compare different regression algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "Limitation of simple linear regression comes when we have non-linear data.  We can simply use polynomial regression.  For example, a degree-1 polynomial fits a straight line to the data like this: \n",
    "\n",
    "$$y = ax + b$$  \n",
    "\n",
    "A degree-3 polynomial fits a cubic curve to the data \n",
    "\n",
    "$$y = ax^3 + bx^2 + cx + d$$\n",
    "\n",
    "In scikit learn, we can implement this using a polynomial preprocessor which translate data into its polynomials.\n",
    "\n",
    "For example, if our x is \n",
    "\n",
    "<code>x = np.array([1, 2, 3, 4, 5])</code>\n",
    "\n",
    "If we perform polynomial transformation like this:\n",
    "\n",
    "<code>poly_X = PolynomialFeatures(degree = 3).fit_transform(X)</code>\n",
    "    \n",
    "X2 will look like this:\n",
    "\n",
    "<code>[[ 1, 1, 1]\n",
    " [ 2, 4, 8]\n",
    " [ 3, 9, 27]\n",
    " [ 4, 16, 64]\n",
    " [ 5, 25, 125]]</code>\n",
    " \n",
    " Now our new feature_engineered X has one column representing x, second column representing $x^2$, and third column representing $x^3$.  Now the y becomes \n",
    " \n",
    " $$ y = ax^3 + bx^2 + cx $$ \n",
    " \n",
    "Now let's look at some example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Performing polynomial regression with deg:  1 ======\n",
      "MSE = 2723.98\n",
      "r^2 = 0.488\n",
      "adjusted $r^2$ = 0.476\n",
      "===Performing polynomial regression with deg:  3 ======\n",
      "MSE = 1447967.71\n",
      "r^2 = -271.051\n",
      "adjusted $r^2$ = -277.363\n",
      "===Performing polynomial regression with deg:  5 ======\n",
      "MSE = 1010991.62\n",
      "r^2 = -188.950\n",
      "adjusted $r^2$ = -193.357\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "#let's try to fit the model using 1, 3, 5 degrees....\n",
    "for ix, deg in enumerate([1, 3, 5]):\n",
    "    print(\"===Performing polynomial regression with deg: \", deg, \"======\")\n",
    "    model = make_pipeline(PolynomialFeatures(deg), LinearRegression())\n",
    "    model.fit(X_train, y_train)\n",
    "    y_hat = model.predict(X_test)\n",
    "    \n",
    "    #print(\"Coeff: \", model.named_steps['linearregression'].coef_)\n",
    "    \n",
    "    #$(1/n)sigma(y - f(x))^2) where SSE = sigma(y-f(x))^2\n",
    "    print(f\"MSE = {mean_squared_error(y_test, y_hat):.2f}\")\n",
    "    \n",
    "    #measures goodness of fit\n",
    "    #1 - SSE/TSS  where TSS = sigma(y-ymean)^2\n",
    "    #r^2 can be negative, when fit without an intercept\n",
    "    #We ALMOST never fit without the intercept unless\n",
    "    #you are sure your data comes through the origin (0, 0), e.g., height, width, but NOT house value!\n",
    "    #r^2 upper bound is 1, lower bound can be anything\n",
    "    print(f\"r^2 = {r2_score(y_test, y_hat):.3f}\")\n",
    "    \n",
    "    #calculate adjusted rsquare\n",
    "    #take IV into consideration, to balance out possible overfitting\n",
    "    #increases only if new predictor (x) enhances the model\n",
    "    n, p = X.shape[0], X.shape[1]\n",
    "    adjusted_rsqrt = 1-(1-r2_score(y_test, y_hat))*(n-1)/(n-p-1)\n",
    "    print(f\"adjusted $r^2$ = {adjusted_rsqrt:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression + Grid Search\n",
    "\n",
    "Let's apply grid search with polynomial regression\n",
    "\n",
    "Note: once again, we avoid doing nested cross-validation to save time...but it's recommended.  It helps you check whether the model or the search space really gives stable and generalized results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params:  {'linearregression__fit_intercept': False, 'polynomialfeatures__degree': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'polynomialfeatures__degree': np.arange(1, 10),\n",
    "              'linearregression__fit_intercept': [True, False]}\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(), LinearRegression())\n",
    "\n",
    "grid = GridSearchCV(model, param_grid, cv=3)\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best params: \", grid.best_params_)\n",
    "\n",
    "y_hat = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 2723.98\n",
      "r^2 = 0.488\n",
      "adjusted $r^2$ = 0.476\n"
     ]
    }
   ],
   "source": [
    "print(f\"MSE = {mean_squared_error(y_test, y_hat):.2f}\")\n",
    "print(f\"r^2 = {r2_score(y_test, y_hat):.3f}\")\n",
    "n, p = X.shape[0], X.shape[1]\n",
    "adjusted_rsqrt = 1-(1-r2_score(y_test, y_hat))*(n-1)/(n-p-1)\n",
    "print(f\"adjusted $r^2$ = {adjusted_rsqrt:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Regularization is a technique to alleviate overfitting problem by imposing some penalty to the loss function.\n",
    "\n",
    "### Ridge regression ($L_2$ regularization)\n",
    "\n",
    "Perhaps the most common form of regularization is known as *ridge regression* or $L_2$ *regularization*, sometimes also called *Tikhonov regularization*. This proceeds by penalizing the sum of squares (2-norms) of the model coefficients; in this case, the penalty on the model fit would be \n",
    "\n",
    "$$ J(\\theta) =  \\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda\\sum_{j=1}^n \\boldsymbol{\\theta}_j^2$$\n",
    "\n",
    "where $\\alpha$ is a free parameter that controls the strength of the penalty.\n",
    "This type of penalized model is built into Scikit-Learn with the ``Ridge`` estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "params_Ridge = {'polynomialfeatures__degree': np.arange(1, 10),\n",
    "                'ridge__alpha': np.logspace(-1, -4, 10)}\n",
    "\n",
    "##I put normalize=True to reach convergence faster, since it is giving me warnings...as my x value have wide range\n",
    "model = make_pipeline(PolynomialFeatures(), Ridge(normalize=True))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regression ($L_1$ regularization)\n",
    "\n",
    "Another very common type of regularization is known as lasso, and involves penalizing the sum of absolute values (1-norms) of regression coefficients:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2+ \\lambda\\sum_{j=1}^n |\\theta_j|$$\n",
    "\n",
    "Though this is conceptually very similar to ridge regression, the results can differ surprisingly: for example, due to geometric reasons lasso regression tends to favor *sparse models* where possible: that is, it preferentially sets model coefficients to exactly zero.\n",
    "\n",
    "We can see this behavior in duplicating the ridge regression figure, but using L1-normalized coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "params_Lasso = {'polynomialfeatures__degree': np.arange(1, 10),\n",
    "                'lasso__alpha': np.logspace(-1, -4, 10)}\n",
    "\n",
    "#put max_iter since it needs more time to reach convergence\n",
    "model = make_pipeline(PolynomialFeatures(), \n",
    "                      Lasso(normalize=True, tol=0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic net \n",
    "\n",
    "Linear regression with combined L1 and L2 regularizer\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{2}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\alpha \\sum_{j=1}^n |\\theta_k| + (1 - \\alpha) \\sum_{k=1}^n \\theta_j^2\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "#i set tol to be low since it is eating my pc....\n",
    "model = make_pipeline(PolynomialFeatures(), \n",
    "                      ElasticNet(normalize=True))\n",
    "\n",
    "#note that sklearn has two parameters, alpha and l1_ratio, for the complete equation, refer to the doc\n",
    "params_Elasticnet = {'polynomialfeatures__degree': np.arange(1, 10),\n",
    "                'elasticnet__alpha': np.logspace(-1, -4, 10),\n",
    "                \"elasticnet__l1_ratio\": np.linspace(0, 1, 5)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge or Lasso or Elastic net??\n",
    "\n",
    "Regularization should be ALMOST ALWAYS used, since these techniques reduces overfitting.\n",
    "\n",
    "How to choose is a little bit difficult. It is easier to understand the assumptions behind.\n",
    "1.  Ridge assumes that coefficients are normally distributed.   **Thus, if you don't want any feature to dominate too much, use Ridge.**\n",
    "2. Lasso assumes that coefficients are Laplace distributed (in layman sense, it mean some predictors are very useful while some are completely irrelevant).   Here, Lasso has the ability to shrink coefficient to zero thus eliminate predictors that are not useful to the output, thus automatic feature selection.  **In simple words, if you have only very few predictors with medium/large effect, use Lasso.**\n",
    "3.  Elastic basically is a compromise between the two, and thus take huge computation time to reach that compromise.  **If you have the resource to spare, you can use Elastic net**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet + Stochastic Gradient Descent\n",
    "\n",
    "Sklearn provides ElasticNet along with stochastic gradient descent, and they called <code>SGDRegressor()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(), \n",
    "                      SGDRegressor())\n",
    "\n",
    "params_SGD = {'polynomialfeatures__degree': np.arange(1, 10),\n",
    "                'sgdregressor__alpha': np.logspace(-1, -4, 10),\n",
    "                'sgdregressor__penalty': ['l2', 'l1', 'elasticnet'],\n",
    "                 'sgdregressor__l1_ratio': np.linspace(0, 1, 5),\n",
    "              'sgdregressor__learning_rate': ['constant', 'optimal',\n",
    "                                             'invscaling', 'adaptive']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many more....\n",
    "\n",
    "There are just too many to mention.  It may be nice to read here: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model.   Sklearn documentation usually writes very good manual when to use which algorithm.  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
