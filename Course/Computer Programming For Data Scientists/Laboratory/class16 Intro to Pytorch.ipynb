{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where are we now\n",
    "1. Python - general problem solving\n",
    "2. Data Science - Numpy, Pandas, Sklearn, Matplotlib\n",
    "3. ML from Scratch - Intuition (so for those who want to further advance ........)\n",
    "4. Signal Processing - Energy, Telecommunications, Biosignals, Time Series\n",
    "5. Deep Learning - PyTorch\n",
    "    1. One of the most popular DL framework (against TensorFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning vs. Machine Learning\n",
    "\n",
    "Good News\n",
    "- Deep Learning can automatically feature engineer / feature selection\n",
    "- Deep Learning can benefit from huge amount of data, while Machine Learning cannot\n",
    "    - 100 samples vs 1000 samples, ML will get the same accuracy\n",
    "    - But DL will see increased accuracy\n",
    "- Deep Learning is basically stacking a lot of linear regression together\n",
    "    - DL can learn very complex patterns\n",
    "    - DL is perfect for (1) images, (2) text, (3) time series / signal (very random)\n",
    "\n",
    "Bad News\n",
    "- Deep Learning sucks with small data (vs. Machine Learning) - 5000++ samples\n",
    "- For Tabular Data, Deep Learning will ALMOST LOSE TO gradient boosting (or its variants)\n",
    "    - Gradient Boosting is basically decision tree stacking after one another....\n",
    "    - For most competition, XGBoost and LIghtGBM are always the winner for tabular data\n",
    "    - If you work in a company, mostly they use tabular data, then you should look for gradient boosting types....\n",
    "- Deep Learing has NO feature importance; so it's mostly blackbox.... (Explanable AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install torch or pip3 install torch or conda install torch\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.22.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1+cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch Tensors\n",
    "\n",
    "Pytorch don't use Numpy, Instead, it has its own data structures, called Tensor, which support automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create torch Tensor from Numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a numpy array of 1 to 5\n",
    "arr = np.arange(1,6)\n",
    "arr\n",
    "\n",
    "#print the data type\n",
    "arr.dtype #int64\n",
    "\n",
    "#print the type()\n",
    "type(arr) #belongs to Python itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guntsv\\AppData\\Local\\Temp\\ipykernel_16772\\3626803662.py:4: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  torch_arr_from = torch.from_numpy(arr)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Guntsv\\Downloads\\AIT\\AT82.01 Computer Programming for Data Science and Artificial Intelligence (PDS)\\Coding-Along\\class16 Intro to Pytorch.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Downloads/AIT/AT82.01%20Computer%20Programming%20for%20Data%20Science%20and%20Artificial%20Intelligence%20%28PDS%29/Coding-Along/class16%20Intro%20to%20Pytorch.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#convert numpy to tensor\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Downloads/AIT/AT82.01%20Computer%20Programming%20for%20Data%20Science%20and%20Artificial%20Intelligence%20%28PDS%29/Coding-Along/class16%20Intro%20to%20Pytorch.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Downloads/AIT/AT82.01%20Computer%20Programming%20for%20Data%20Science%20and%20Artificial%20Intelligence%20%28PDS%29/Coding-Along/class16%20Intro%20to%20Pytorch.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#1. from_numpy (copy)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Downloads/AIT/AT82.01%20Computer%20Programming%20for%20Data%20Science%20and%20Artificial%20Intelligence%20%28PDS%29/Coding-Along/class16%20Intro%20to%20Pytorch.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m torch_arr_from \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mfrom_numpy(arr)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Downloads/AIT/AT82.01%20Computer%20Programming%20for%20Data%20Science%20and%20Artificial%20Intelligence%20%28PDS%29/Coding-Along/class16%20Intro%20to%20Pytorch.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m torch_arr_from\u001b[39m.\u001b[39mdtype  \u001b[39m#torch.int64\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Downloads/AIT/AT82.01%20Computer%20Programming%20for%20Data%20Science%20and%20Artificial%20Intelligence%20%28PDS%29/Coding-Along/class16%20Intro%20to%20Pytorch.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mtype\u001b[39m(torch_arr_from)  \u001b[39m#torch.Tensor\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "#convert numpy to tensor\n",
    "\n",
    "#1. from_numpy (copy)\n",
    "torch_arr_from = torch.from_numpy(arr)\n",
    "torch_arr_from.dtype  #torch.int64\n",
    "type(torch_arr_from)  #torch.Tensor\n",
    "torch_arr_from.type() #torch.LongTensor (int64); if torch.IntTensor (int32)\n",
    "                      #torch.FloatTensor (float32); if torch.DoubleTensor (float64)\n",
    "#from_numpy is a copy!!!  This is intended, for easy use between numpy and tensor...\n",
    "# arr[2] = 999\n",
    "# torch_arr_from\n",
    "\n",
    "#2. tensor (not a copy)\n",
    "torch_arr_tensor = torch.tensor(arr)  #everything is the same, except it's NOT a copy\n",
    "arr[2] = 9999999\n",
    "torch_arr_tensor\n",
    "\n",
    "#In our class, mostly we use torch.tensor; it won't fail us :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some API to create tensor\n",
    "\n",
    "'torch.empty(size)'\n",
    "\n",
    "'torch.oens(size)'\n",
    "\n",
    "'torch.zeros(size)'\n",
    "\n",
    "'torch.arange(start,stop(ex),step)'\n",
    "\n",
    "'torch.linspace(start,stop(ex),step)' - power of 10\n",
    "\n",
    "'torch.rand(size)' - [0,1)\n",
    "\n",
    "'torch.randn(size)' - std = 1 with uniform distribution \n",
    "\n",
    "'torch.randint(low, high ,size)' - [low, high)\n",
    "\n",
    "'torch.ones_like(input)' = 'torch.ones(input.shape)'\n",
    "'torch.zeros_like(input)'\n",
    "'torch.rand_like(input)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1])\n"
     ]
    }
   ],
   "source": [
    "#import some deep learning layer\n",
    "#you have to help me create the right shape to insert to this layer\n",
    "\n",
    "import torch.nn as nn  #nn contains a lot of useful deep learning layers\n",
    "\n",
    "linear_layer = nn.Linear(5, 1)  #basically you insert 5 features, output 1 number\n",
    "linear_layer.weight  #they treat this as theta, X @ theta^T\n",
    "linear_layer.bias\n",
    "#[0.1315, 0.3990, 0.0960, 0.0807, 0.2908]\n",
    "#weight - [5, 1]\n",
    "#X @ weight\n",
    "#(anything, 5) @ (5, 1)\n",
    "\n",
    "#can you guys help me generate any pytorch tensor of size (?, ?)\n",
    "data   = torch.rand(1000, 5)\n",
    "output = linear_layer(data)\n",
    "print(output.shape)  #output shape?? - 1000, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(9999)  #this will make sure your weight is always init the same thing\n",
    "#this seed is VERY IMPORTANT for research\n",
    "#you CANNOT FORGET THIS - setting 5 different seeds is basically doing cross validation\n",
    "#please create two linear layers of size (100, 5), (5, 1)\n",
    "layer1 = nn.Linear(100, 5)\n",
    "layer2 = nn.Linear(5,   1)\n",
    "\n",
    "#try some input that pass through these two layers\n",
    "sample_size = 1000\n",
    "_input = torch.rand(sample_size, 100)\n",
    "# _input = layer1(_input)\n",
    "# _input = layer2(_input)\n",
    "# _input.shape\n",
    "\n",
    "#try nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    layer1,\n",
    "    layer2\n",
    ")\n",
    "_input = model(_input)\n",
    "_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chaniging the type\n",
    "#format: .type()\n",
    "\n",
    "x = torch.arange(1,6)\n",
    "x.dtype\n",
    "\n",
    "x.type(torch.float64) #is NOT this in-place\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshape and view\n",
    "- they are very similar\n",
    "- view will create a copy, while reshape does not\n",
    "- view will create a contiguous array, while reshape does not!\n",
    "- contiguous array - share consectutive memory x001 x002\n",
    "- non-contiguous array - memory in different place x001 x140 x004\n",
    "- some algorihtms/model/cuda require your array to be contiguous\n",
    "    - in those case, use view or rreshape to fix it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(10)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x.view(2,5)\n",
    "y[0,0] = 9999\n",
    "y.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9999,    1,    2,    3,    4,    5,    6,    7,    8,    9])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#please help me check x, does it change?\n",
    "x #xand y shares memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  9999, 888833,      2,      3,      4,      5,      6,      7,      8,\n",
       "             9])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x.reshape(2,5) #can or cannot be copy # see the documentation\n",
    "z.is_contiguous()\n",
    "\n",
    "z[0,1] = 888833\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_transpose = z.transpose(1,0) #5,2\n",
    "z_transpose.shape\n",
    "z_transpose.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One strp back-propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of using derivates to learn the weights - **gradient descent**. But when in a deep learing form, because we can have many layers stacked together, we give a new name - **backpropagation**\n",
    "\n",
    "\n",
    "$$y = 2x^4 + x^3 +3x^2 +5x +1 $$\n",
    "$$ \\frac{dy}{dx} = y' = 8x^3 + 3x^2 +6x +5 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#why pytorch is so amazing!!!\n",
    "#because pytorch automically calculates this gradient, always available\n",
    "#that's wht pytoch is very nice for deep learning\n",
    "#requires_grad=True, make sure we gonna let pytorch always calculate the gradient/derivative\n",
    "#when requires_grad=True, nubmer MUST BE FLOAT\n",
    "#try remove the 2. --> 2 --> there will be error\n",
    "x = torch.tensor(2., requires_grad=True)\n",
    "print(x.grad) #no derivatives calculated yet *until we call. backward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(63., grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = 2*x ** 4 + x ** 3 + 3*x ** 2 + 5*x + 1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call backward, which gonna calculate all the gradient relevant to y\n",
    "y.backward() #is a inplace function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(93.)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#try now print x.grad #this wil be basically dy/dx at the pont of x = 2\n",
    "x.grad\n",
    "\n",
    "#can you check how I get 93????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y' = 8x^3 + 3x^2 +6x +5 $$\n",
    "\n",
    "- try out x = 2 here\n",
    "- this is the derivative/gradient/slop/rate of change at the point(2,63)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(93., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy = 8*x ** 3 + 3*x ** 2 + 6*x + 5\n",
    "dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple step back-propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y = 3x +2$$\n",
    "$$ z = 2y^2 $$\n",
    "$$ o = z/6 $$ \n",
    "- let's assume we have 6 elements\n",
    "\n",
    "$$ \\frac{\\partial o}{\\partial x} = \\frac{\\partial o}{\\partial z} * \\frac{\\partial z}{\\partial y} * \\frac{\\partial y}{\\partial x}$$\n",
    "\n",
    "$$\\frac{\\partial o}{\\partial z}= \\frac{1}{6} $$\n",
    "\n",
    "$$\\frac{\\partial z}{\\partial y}= 4y = 4(3x +2) $$\n",
    "\n",
    "$$\\frac{\\partial y}{\\partial x} = 3 $$\n",
    "\n",
    "$$ \\frac{\\partial o}{\\partial x} = 2(3x+2) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.,2,3],[3,2,1]],requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 3*x +2\n",
    "z = 2*y **2\n",
    "o = z.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 16., 22.],\n",
       "        [22., 16., 10.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad #try to find out how did we get 10,16,22!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise\n",
    "\n",
    "use the same x\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= 10x-9999 \\\\\n",
    "z &= 5 - y \\\\\n",
    "o &= 3z^2 \\\\  \n",
    "oo & = \\frac{o}{6} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Task1: Calculate all the gradients\n",
    "\n",
    "$$ \\frac{\\partial oo}{\\partial x} = \\frac{\\partial oo}{\\partial o} *\\frac{\\partial o}{\\partial z} * \\frac{\\partial z}{\\partial y} * \\frac{\\partial y}{\\partial x}$$\n",
    "\n",
    "$$ \\frac{\\partial oo}{\\partial o} = \\frac{1}{6} \\quad \\frac{\\partial o}{\\partial z}= 6z \\quad \\frac{\\partial z}{\\partial y}= -1 \\quad \\frac{\\partial y}{\\partial x} = 10 \\quad \\frac{\\partial o}{\\partial x} = -60z $$\n",
    "\n",
    "Task2: code and try whether it matchs yours\n",
    "\n",
    "Put on the chat if you are done; you can put on the chat for task 1 first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.,2,3],[3,2,1]],requires_grad=True)\n",
    "y = 10*x-9999\n",
    "z = 5 - y\n",
    "o = 3*z ** 2\n",
    "oo = o.mean() #we have to make it into one nubmer .... for backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "oo.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-99940., -99840., -99740.],\n",
       "        [-99740., -99840., -99940.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-99940"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-10 * (5- 10*1 + 9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1.,2,3],[3,2,1]],requires_grad=True)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.arange(3.).view(3,1)\n",
    "w.shape #[3,1]\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.],\n",
       "        [4.]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = x @ w\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oo = o.mean()\n",
    "oo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "oo.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6., grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.5000, 1.0000],\n",
       "        [0.0000, 0.5000, 1.0000]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot product\n",
    "X = [\n",
    "      X11 X12 X13\n",
    "      X21 X22 X23\n",
    "                  ]\n",
    "\n",
    "W = [\n",
    "      W1 \n",
    "      W2 \n",
    "      W3 ]\n",
    "\n",
    "O = X @ W = [ \n",
    "            X11W1 + X12W2 + X13W3 \n",
    "            X21W1 + X22W2 + X23W3\n",
    "                                    ]\n",
    "\n",
    "OO = 1/2 (O)\n",
    "\n",
    "dOO/dX = dOO/dO * dO/dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dot product\n",
    "X = [\n",
    "        1   2   3\n",
    "        3   2   1\n",
    "                  ]\n",
    "\n",
    "W = [\n",
    "      0 \n",
    "      1 \n",
    "      2 ]\n",
    "\n",
    "O = X @ W = [ \n",
    "                1*0 + 2*1 + 3*2\n",
    "                3*0 + 2*1 + 1*2\n",
    "                            ]\n",
    "\n",
    "O = [ \n",
    "      8\n",
    "      4 ]\n",
    "\n",
    "OO = 1/2 (O)\n",
    "\n",
    "dOO/dX = dOO/dO * dO/dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1.,2,3],[3,2,1]],requires_grad=True)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.arange(3.).view(3,1)\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.]\n",
      " [4.]]\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "m,_ = x.shape\n",
    "_,n = w.shape\n",
    "o = np.zeros((m,n))\n",
    "\n",
    "# multiply matrix\n",
    "for i in range(len(x)):\n",
    "   for j in range(len(w[0])):\n",
    "      for k in range(len(w)):\n",
    "         o[i][j] += x[i][k] * w[k][j]\n",
    "\n",
    "print(o)\n",
    "print(o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 2.]\n",
      " [0. 3. 6.]] \n",
      "\n",
      "[[0.  0.5 1. ]\n",
      " [0.  1.5 3. ]]\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1.,2,3],[3,2,1]],requires_grad=True)\n",
    "x.shape\n",
    "\n",
    "w = torch.arange(3.).view(1,3)\n",
    "w.shape\n",
    "\n",
    "m,_ = x.shape\n",
    "_,n = w.shape\n",
    "o = np.zeros((m,n))\n",
    "\n",
    "# multiply matrix\n",
    "for i in range(len(x)):\n",
    "   # for j in range(len(w[0])):\n",
    "   for j in range(len(w[0])):\n",
    "      for k in range(len(w)):\n",
    "         o[i][j] = x[i][k] * w[k][j]\n",
    "         \n",
    "print(o,'\\n')\n",
    "print(o/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
