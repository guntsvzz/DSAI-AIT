{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch for Linear Regreesion\n",
    "So we can understand how PyTorch works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.1+cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guntsv\\AppData\\Local\\Temp\\ipykernel_14676\\888719105.py:4: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#as you all know, things can speed up if you have NVIDIA GPU\n",
    "#CUDA is the framwork that NVIDIA develops, which allows us to use the GPU for calculations\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan for today :\n",
    "\n",
    "1. ETL :   \n",
    "    1. Specify set some random input\n",
    "    2. PyTorch Dataset and DataLoader\n",
    "2. EDA - we gonnna just skip because we are lazy\n",
    "3. Feature Engineering / Cleaning - which we don't need to...\n",
    "4. Modeling\n",
    "    1. 'nn.Linear' (luckily, you already understand this! Yay!)\n",
    "    2. Define loss function (mse for regression, ce for classification) \n",
    "    3. Define the potimizer function (gradient descent; adam)\n",
    "    4. Train the model\n",
    "5. Inference / Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 ETL some inmput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider this data:\n",
    "\n",
    "<img src = \"figures/japan.png\" width=\"400\">\n",
    "\n",
    "In a linear regression model, each target variable is estimated to be a weighted sum of the input variables, offset by some constant, known as a bias :\n",
    "\n",
    "$$\\text{yield}_\\text{apple}  = w_{11} * \\text{temp} + w_{12} * \\text{rainfall} + w_{13} * \\text{humidity} + b_{1}$$\n",
    "\n",
    "$$\\text{yield}_\\text{orange} = w_{21} * \\text{temp} + w_{22} * \\text{rainfall} + w_{23} * \\text{humidity} + b_{2}$$\n",
    "\n",
    "Visually, it means that the yield of apples is a linear or planar function of temperature, rainfall and humidity:\n",
    "\n",
    "<img src = \"figures/japan2.png\" width=\"400\">\n",
    "\n",
    "The learning part of linear regression is to figure out a set of weights <code>w11, w12,... w23, b1 \\& b2</code> using gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X (temp, rainfall, humidity)\n",
    "X_train = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n",
    "                   [102, 43, 37], [69, 96, 70], [73, 67, 43], \n",
    "                   [91, 88, 64], [87, 134, 58], [102, 43, 37], \n",
    "                   [69, 96, 70], [73, 67, 43], [91, 88, 64], \n",
    "                   [87, 134, 58], [102, 43, 37], [69, 96, 70]], \n",
    "                  dtype='float64')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "Y_train = np.array([[56, 70], [81, 101], [119, 133], \n",
    "                    [22, 37], [103, 119], [56, 70], \n",
    "                    [81, 101], [119, 133], [22, 37], \n",
    "                    [103, 119], [56, 70], [81, 101], \n",
    "                    [119, 133], [22, 37], [103, 119]], \n",
    "                   dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15, 3]), torch.Size([15, 2]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Please create tensors from these numpy array\n",
    "#torch.from_numpy(copy) or torch.tensor (not a copy)\n",
    "inputs = torch.tensor(X_train)\n",
    "targets = torch.tensor(Y_train)\n",
    "\n",
    "#please print the shape of these tensors\n",
    "#use either .size() or .shape\n",
    "inputs.shape,targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Dataset\n",
    "\n",
    "We gonna create 'TensorDataset' on top of these tensors, so we can access each row as a from inputs and targets as tuples.\n",
    "\n",
    "Note: This must be done, if we want to use 'DataLoader'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put this dataset on top of our inputs and targerts\n",
    "#format : TensorDataset(X,y) where X.shape is (m,n) and y.shape (m, k)\n",
    "ds = TensorDataset(inputs,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([91., 88., 64.]), tensor([ 81., 101.]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[1] #this is a tuple of two tensors, the x and the corresponding y\n",
    "#this IS THIS FORMAT that pyTorch wants!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 DataLoader\n",
    "\n",
    "By default, PyTorch works in batch (remember the mini-batch gradient descent!).\n",
    "\n",
    "In simple words, it will ALWAYS take some mini-batch, and perform gradient descent\n",
    "\n",
    "Why PyToch assume mini-batch; because PyTorch assumes you won't be able to fit in ~1M samples into GPU ram.....(3,4,6,11,12,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this dataloader will automatically create an enumerator, look at each baych \n",
    "#means, you can simple perform a for loop onto this dataloader\n",
    "#if you DON'T WANT TO use this DataLoader, it's fine! But you have\n",
    "#to manually select the mini-batch (just like we do in our LR mini-batch class)\n",
    "from torch.utils.data import DataLoader #this guy is randomized (if you set shuffle=true)\n",
    "\n",
    "batch_size = 3 #this is any number you like\n",
    "#too small then your code runs slow\n",
    "#too big then you mayg get \"out of memory\" error\n",
    "\n",
    "dl = DataLoader(ds, batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now, this dl is bascially an enumerator, in which we can loop on....\n",
    "\n",
    "# for x,y in dl:\n",
    "#     print(f\"X: {x}\") \n",
    "#     print(f\"Y: {y}\")\n",
    "#     break\n",
    "\n",
    "#this dl has an internal counter, that keeps where it is currently\n",
    "#this dl keeps on running: which is interntional; because we have the concept of \"epochs\"\n",
    "#epochs mean that how many times we \"exhaust\" the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.EDA - skip because we are lazy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define our neural network\n",
    "-how many linear layers we want???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn #stand for neural network; modules that contain many possible layers\n",
    "#define our neural network\n",
    "#just use one layer....\n",
    "#we gonna come back here and add one more layer...\n",
    "#format nn.Linear(in_features,out_features)\n",
    "#format: nn.Lineart(temp;rainfall;hum, orange,apple)\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(3,4),\n",
    "#     nn.Linear(4,2)\n",
    "# # )\n",
    "# model = nn.Linear(3,2) #<--- hidden layer\n",
    "\n",
    "#Linear layers are basically simple matrix multiplication...\n",
    "#Many other names: In Keras, we called Dense. In Tensorflow, we called FullyConnected\n",
    "\n",
    "\n",
    "#Keras are very high-level - not good for research / development (mainly for education....)\n",
    "#TensorFlow is developed by Google - it's qutie good\n",
    "\n",
    "#for very huge, complex, high performance model - TensorFlow is much better / optimized\n",
    "    #they are more low-level than PyTorch\n",
    "#for very generally almost any model that we use (even in research) - PyTorch is much better\n",
    "#due to its computational graph....\n",
    "\n",
    "#TensorFlow support something called TensorFlowLite, which is the way\n",
    "#you want to use for moblie phones..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I wonder whether have one extra layer, can reduce the loss\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(3,10),\n",
    "    nn.Linear(10,2)\n",
    ")   #this is fine, but this is not the best practice!!\n",
    "    #because in the future, there are many layers and complex stuffs in neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class is the perfect and the best practice for creating a neural network of any type....\n",
    "#format:\n",
    "'''\n",
    "class anyNameCapitalized(nn.Module): #it must inherit nn.Module\n",
    "    def __init():\n",
    "        super().__init__() #super is basically inheriting nn.Module init\n",
    "        #we define all the layers here....\n",
    "\n",
    "    def foward(self, x): # YOU CANNOT CHANGE THIS NAME, it MUST BE \"forward\"\n",
    "        x = layer1()\n",
    "        x = layer2()\n",
    "        return x\n",
    "'''\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(3,5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.weight\n",
    "# model.fc1.weight\n",
    "# model.fc2.weight\n",
    "# model.fc1.weight #by default, these weights are uniformly close to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.weight.shape #this one is basically in the shape (out_features, in_feature)\n",
    "\n",
    "#you can image X @ W^T\n",
    "#after you tranpose W, W^T becomes [3,2]\n",
    "#which now you can do X @ W^T which is (anything,3) @ (3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.bias #why two bias???\n",
    "# model.fc1.bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model.parameters()) #this will list all the parameters (it's a object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#p.numel() just flatten everything...\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "#why 8 here??? - 6 weights and 2 bias...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (fc1): Linear(in_features=3, out_features=5, bias=True)\n",
       "  (fc2): Linear(in_features=5, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so how do we use our model \n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so you can perform a forward passs, simply using\n",
    "#format: model(inputs)\n",
    "\n",
    "# print(\"Inputs:\", inputs.shape)\n",
    "\n",
    "# output= model(inputs) #(15,3) @ (3,2) = (15,2)\n",
    "# print(\"Outputs:\", output.shape) #why output.shape is 15,2 ???\n",
    "# output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Define the loss function\n",
    "\n",
    "- should we use MSE or Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "#under the nn module, there are many loss function\n",
    "J_fn = nn.MSELoss()\n",
    "\n",
    "#later on, you will know how to use this...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Define the optimizer\n",
    "\n",
    "- Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normally, in sklearn, we simple call fit, and it will do gradient descent\n",
    "#in code from scatch, we need to like specify how we want to update the gradients\n",
    "#optimizaer handles HOW we update the parameters\n",
    "#   if we use w = w - alpha(gradient) ==> gradient descent\n",
    "#optimizer is handles by the 'torch.optim' module\n",
    "#stochastic gradient descent ==> is NOT one sample; is basically mini-batch\n",
    "optim = torch.optim.SGD(model.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Actually train the model\n",
    "\n",
    "- 1. Predict\n",
    "- 2. Loss\n",
    "- 3. Gradient\n",
    "- 4. Update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def fit():\n",
    "# num_epochs = 50 #it depends.... trail and error\n",
    "# #for num_epochs \n",
    "# for epoch in range(num_epochs):\n",
    "#     #for dataloader\n",
    "#     for x,y in dl: #what is the shape of x here and y here\n",
    "#         #x and y are the minibatch of X_train and y_train (batch size = 3)\n",
    "#         #x and y will have 3 samples each, but the number of feature are the same!\n",
    "#         #x: (batch,feature) = (3,3)\n",
    "#         #y: (batch,target) = (3,2)\n",
    "        \n",
    "#         #optional : you can put your x and y inside the CUDA (GPU) for speed up\n",
    "#         x.to(device) #device is either cpu or cuda\n",
    "#         y.to(device)\n",
    "\n",
    "#         #1. predict (forward pass)\n",
    "#         yhat = model(x)\n",
    "\n",
    "#         #2. calculate loss\n",
    "#         #sklearn.metric.mse(y,yhat)\n",
    "#         #format : J_fn(inputs,targets)\n",
    "#         loss = J_fn(yhat,y)\n",
    "\n",
    "#         #3. calculate gradient\n",
    "#         #3.1 clear out the previous gradients\n",
    "#         #format: optimizer.zero_grad()\n",
    "#         optim.zero_grad()\n",
    "\n",
    "#         #3.2 called backward() on loss to retrieve all the gradients (backpropagation/back pass)\n",
    "#         loss.backward() #why called backward on loss\n",
    "#         #backward DOES NOT adjust the weight YET.... just backpropagation\n",
    "#         #we want to calculate the gradients of all parameters (8 - 6 weights and 2 bias)\n",
    "#         # IN RESPECT TO THE LOSS ... dJ/dw11, dJ/dw12, dJ/dw13....dJ/db1 , dJ/db2 \n",
    "\n",
    "#         #4. update the parameters using the optim\n",
    "#         #W = W - alpha * gradient #we don't need to do this here\n",
    "#         optim.step() #optim already has learning rate, it also know all the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - Loss : 12282.9970703125\n",
      "Epoch: 0 - Loss : 3114.697021484375\n",
      "Epoch: 0 - Loss : 17589.255859375\n",
      "Epoch: 0 - Loss : 349513.84375\n",
      "Epoch: 0 - Loss : 3546279680.0\n",
      "Epoch: 1 - Loss : 4.1221627533307245e+21\n",
      "Epoch: 1 - Loss : inf\n",
      "Epoch: 1 - Loss : inf\n",
      "Epoch: 1 - Loss : nan\n",
      "Epoch: 1 - Loss : nan\n",
      "Epoch: 2 - Loss : nan\n",
      "Epoch: 2 - Loss : nan\n",
      "Epoch: 2 - Loss : nan\n",
      "Epoch: 2 - Loss : nan\n",
      "Epoch: 2 - Loss : nan\n",
      "Epoch: 3 - Loss : nan\n",
      "Epoch: 3 - Loss : nan\n",
      "Epoch: 3 - Loss : nan\n",
      "Epoch: 3 - Loss : nan\n",
      "Epoch: 3 - Loss : nan\n",
      "Epoch: 4 - Loss : nan\n",
      "Epoch: 4 - Loss : nan\n",
      "Epoch: 4 - Loss : nan\n",
      "Epoch: 4 - Loss : nan\n",
      "Epoch: 4 - Loss : nan\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for x,y in dl: #minibatch\n",
    "        x.to(device)\n",
    "        y.to(device)\n",
    "\n",
    "        yhat = model(x)\n",
    "        loss = J_fn(yhat,y)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step() \n",
    "\n",
    "        print(f\"Epoch: {epoch} - Loss : {loss}\")\n",
    "\n",
    "        #can you guys help tell what is the best hidden size?\n",
    "        #final exam is no Nov 22 9-12\n",
    "            #singal prcessing\n",
    "            #deep learinging = pytorch\n",
    "        #project maybe on Nov 25 13-16\n",
    "        #8 classes... 14 lectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference / Testing\n",
    "\n",
    "Test some random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[73., 67., 43.],\n",
       "         [91., 88., 64.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.]]))"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[nan, nan],\n",
      "        [nan, nan]], grad_fn=<AddmmBackward0>)\n",
      "tensor(nan, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#please create two numpy array of\n",
    "# [74,68,42],[92,88,65]\n",
    "X_tensor = np.array([[74,68,42],[92,88,65]],dtype='float64')\n",
    "#please make it a tensor\n",
    "X_tensor = torch.tensor(X_tensor)\n",
    "#then use our model to predict the number of oranges and apples\n",
    "yhat = model(X_tensor)\n",
    "print(yhat)\n",
    "#print the loss function comparing with ds[0] and ds[1] - look at the y part ok...\n",
    "ytest = ds[0:2][1]\n",
    "loss = J_fn(yhat,ytest)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
