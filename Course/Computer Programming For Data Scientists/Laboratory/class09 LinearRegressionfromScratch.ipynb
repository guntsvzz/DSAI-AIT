{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where are we now?\n",
    "\n",
    "- Bootcamp: Python \n",
    "- Numpy, Pandas, Sklearn, (Matplotlib)\n",
    "- If you reach here, and you can do the quiz VERY WELL, you can already go outside and work....\n",
    "\n",
    "- Series of machine learning from scratch\n",
    "    - Why? Chaky? We already have sklearn, why scratch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback\n",
    "- Generally, everyone did very well.....\n",
    "\n",
    "- High level comment: Be less a programmer, Be more like a scientist\n",
    "    - Think thrice, code once\n",
    "\n",
    "- Label encoding vs. One-hot encoding\n",
    "    - Use label encoding when there are way too many categories.... like 30 or 50......\n",
    "    - when you use one-hot encoding, before you use, maybe you can think about grouping them, to make less categories.....\n",
    "\n",
    "- None of you has hypothesis......\n",
    "- None of you has research question......\n",
    "- Very few of you has discussion, interpretation........."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression from Scratch\n",
    "\n",
    "| | Egg price  | Gold price    | Oil price   | GDP   |\n",
    "|---:|:-------------|:-----------|:------|:------|\n",
    "| 1 | 3  | 100       | 4   | 21   |\n",
    "| 2 | 4  | 500    | 7   | 43     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample 1\n",
    "x1 = np.array([3,100,4])\n",
    "y1 = np.array([21])\n",
    "\n",
    "#what's the idea of prediction? What is machine learning?\n",
    "#- find the weights that can bring you from x1 to y1\n",
    "\n",
    "#first smaple\n",
    "#3 * w1 + 100 * w2 + 4 * w3 = 21\n",
    "#3 * 1 + 100 * 1 + 4 * 1 = 107\n",
    "#3 * 7 + 100 * 1 + 4 * -25 = 21\n",
    "\n",
    "#machine learning is trying to find the 'best' weights\n",
    "\n",
    "#second smaple\n",
    "#4 * w1 + 500 * w2 + 7 * w3 = 43\n",
    "#4 * 7 + 500 * 1 + 7 * -25 = 353\n",
    "\n",
    "#machine laering is trying to find the 'best' weights ACROSS all samples....\n",
    "\n",
    "#all deep learning is based on these weight systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definiton of terms and notations\n",
    "\n",
    "#2 samples\n",
    "#3 features - egg price, gold price, oil price\n",
    "    #features are the variables used for predictingthe label\n",
    "    #factors, independent variable, predictiors, X\n",
    "\n",
    "#egg price - x_1 --> always a vector, e.g. [3,4]\n",
    "#gold price - x_2 --> always a vector, e.g. [100,500]\n",
    "#oil price - x_3 --> always a vector, e.g. [4,7]\n",
    "#we call egg price + gold price + oil price - whole 'feature matrix' --> \\mathbg{x}\n",
    "\n",
    "#1 label -gdp\n",
    "    #label is the variable that we want to predict\n",
    "    #target, outcome, y\n",
    "    #y_1 = y = a vector of labels, e.g.[21,43]\n",
    "\n",
    "#Tips: small and big\n",
    "#small mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Math notations:\n",
    "- normal  a -> scalar (one number)\n",
    "- bold $\\mathbf{a}$ --> vector (a 1D numpy array)\n",
    "- bold $\\mathbf{A}$ --> matrix (a 2D numpy array) \n",
    "\n",
    "- $\\mathbf{x}_1^2$ --> feature 1, second sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[3,100,4],[4,500,7]])\n",
    "X.shape #(2,3) means 2 samples = m, 3 features = n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#weight = theta = params\n",
    "theta = np.array([7,1,-25])\n",
    "theta.shape #weight must be the sampleshape as X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 21, 353])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X.dot(theta)\n",
    "#To be able to dot, the number should be same in the close pair\n",
    "#(2,3) @ (3, ) = (2,)\n",
    "#(4,6) @ (6, ) = (4,)\n",
    "#(4,6,1) @ (1,2) = (4,6,1,2)\n",
    "\n",
    "X @ theta\n",
    "\n",
    "#the common error: matmul error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][0] * theta[0] + X[0][1] * theta[1] + X[0][2] * theta[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to find the best weight?\n",
    "- There are many ways, e.g, closed from, gradient descrent, expectation\n",
    "- gradient descent / backpropagation\n",
    "    - you adjust the weight slightly, based on the errors....\n",
    "    - How to adjust basaed on the erorrs\n",
    "\n",
    "0. You first use any randomized weight\n",
    "    \n",
    "    [1,2,3]\n",
    "\n",
    "1. We need to find out how to measure errors\n",
    "\n",
    "$\\sum_0^m (\\hat{y}_i - y_i)^2 $\n",
    "\n",
    "2. We need to know how much to adjust the weight? HOW?\n",
    "\n",
    "Just try all number in the world........ NOOOOO\n",
    "\n",
    "Find the derivative first!\n",
    "\n",
    "$(\\hat{y}_i - y_i) *x_j$\n",
    "\n",
    "$\\mathbf{X}^\\top(\\mathbf{\\hat{y} - \\mathbf{y}})$\n",
    "\n",
    "\n",
    "\n",
    "3. We need to cahgne the weight accordingly\n",
    "\n",
    "$ weight_j = weight_j - \\alpha * derivative $\n",
    "\n",
    "only for one sample\n",
    "\n",
    "$ weight_j = weight_j - \\alpha* (\\hat{y}_i - y_i) * x_j $\n",
    "\n",
    "do for all sample\n",
    "\n",
    "$ \\mathbf{w} = \\mathbf{w} - \\alpha * \\mathbf{X}^\\top(\\mathbf{\\hat{y} - \\mathbf{y}}) $\n",
    "\n",
    "Note: $\\alpha$ - 0.1, 0.01, 0.001, 0.0001\n",
    "\n",
    "4. Run predict again....\n",
    "\n",
    "5. We stop when our errors are decreased no more....or reach some max iterations..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'error/cost')"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAwr0lEQVR4nO3deXxU5dn/8c+VnRAgEBKWhBAgYVUEDJugiIrgiltV3NBi0Z9o3ftYu2hrra1P1dblsYobKqJoRVBRi6ACsoYdWUMCWdhCWBISsl+/P+bQRiRkApmcyeR6v17zysw9Z+Z8D0dz5Zz7PvcRVcUYY4w5kSC3AxhjjPF/ViyMMcbUyoqFMcaYWlmxMMYYUysrFsYYY2oV4nYAX2jbtq0mJSW5HcMYYxqVFStW7FPV2OO9F5DFIikpibS0NLdjGGNMoyIiO2p6z05DGWOMqZUVC2OMMbWyYmGMMaZWViyMMcbUyoqFMcaYWvmsWIhIJxH5RkQ2iMgPInKv0/64iOSKyGrncXG1z/xaRNJFZLOIjK7WPsZpSxeRR3yV2RhjzPH5cuhsBfCgqq4UkRbAChGZ47z3nKr+rfrCItIbuB7oA3QEvhaR7s7bLwGjgBxguYjMUtUNPsxujDGmGp8dWajqLlVd6TwvBDYC8Sf4yFjgfVUtVdVMIB0Y5DzSVTVDVcuA951l693B4jKem7OFzbsLffH1xhjjUx+vzOHDtGyffHeD9FmISBLQH1jqNN0tImtF5A0Rae20xQPVtzLHaaup/dh1TBSRNBFJy8vLO+msL3+3jfeW1nhdijHG+KWqKuXZOVv4ZHWuT77f58VCRKKAfwH3qWoB8DLQDegH7AKeqY/1qOqrqpqqqqmxsce9Wr1W0ZFhjOnTnhmrcikpr6yPWMYY0yAWZ+STc+AI16Z28sn3+7RYiEgonkIxVVU/BlDVPapaqapVwGQ8p5kAcoHqW5ngtNXU7hPXD+pEQUkFX6zf5atVGGNMvftgeTYtI0IY3ae9T77fl6OhBHgd2Kiqz1Zr71BtsSuB9c7zWcD1IhIuIl2AFGAZsBxIEZEuIhKGpxN8lq9yD+kSQ+eYSKYt8815P2OMqW8Hisr4cv1uruwfT0RosE/W4cvRUMOAm4F1IrLaaXsUGCci/QAFtgN3AKjqDyIyHdiAZyTVJFWtBBCRu4GvgGDgDVX9wVehg4KE6wZ24ukvN5ORd5iusVG+WpUxxtSLj1flUlZZxbjBiT5bh6iqz77cLampqXoqs87uLSxh6FPzuH14F359ca96TGaMMfVLVbnwufk0Dw/hk0nDTum7RGSFqqYe7z27gvs44lpEcH7POD5akUNZRZXbcYwxpkYrsw6wde9hxg3yTcf2UVYsajBuUCL5RWXM3bjH7SjGGFOj95Zm0zwsmEv7dvTpeqxY1OCc7rF0aBXBtOXW0W2M8U+HjpTz+bqdXN4vnubhvr2XnRWLGgQHCT9L7cSCrXnkHCh2O44xxvzErNW5lJRX+fwUFFixOKFrUxMAmJ6W43ISY4z5MVVl2rJsendoyenxrXy+PisWJ5DQOpKzU2L5MC2byqrAGzVmjGm81uUeYsOuAsYN6oTnsjbfsmJRi3EDO7HrUAnzt5z8fFPGGFPfpi3LJiI0iLH9TzQ/a/2xYlGL83u1I6Z5GO8vz3I7ijHGAFBUWsGs1blccnpHWkaENsg6rVjUIiwkiGvOTGDuxr3sLSxxO44xxvDZ2p0UlVU2SMf2UVYsvHDtwE5UVCkfrbCObmOM+6YtyyY5LoozO7eufeF6YsXCC91ioxjStQ3TlmVRZR3dxhgXbdpdwOrsg1w/sGE6to+yYuGlm4Z0Jnv/Eb7bah3dxhj3vLc0i7CQIK4akNCg67Vi4aULe7enbVQ4U5fYXfSMMe4oKq3g45W5XHp6B9o0D2vQdVux8FJYSBDXDUxg3qa95B484nYcY0wT9MnqXA6XVnDjkM4Nvm4rFnUwblAiCry/zIbRGmMalqry7pIsenVoyYDE6AZfvxWLOkhoHcnIHnG8vzyb8kqbutwY03BWZh1k464CbhqS2KAd20dZsaijm4YkkldYypwNNnW5MabhTF2yg6jwEK7o1zBXbB/LikUdjegeR3x0M961jm5jTAPZX1TGZ+t2cdUA309FXhMrFnUUHCTcMDiRRdvy2ZZ32O04xpgm4MO0bMoqqrjJhY7to6xYnIRrUzsRGixMXWId3cYY36qqUt5blsWgpDZ0b9fCtRxWLE5CbItwRvdpz0crsikpr3Q7jjEmgC1I38eO/GJuHJLoag4rFifppiGdKSip4NM1O92OYowJYO8u2UFM8zDGnNbe1RxWLE7S4C5tSI6L4t2ldirKGOMbOw8eYe7GPVw7sBPhIcGuZrFicZJEhBsHJ7Im+yDrcw+5HccYE4DeX5aFAjcMcvcUFFixOCVXDUggMiyYKYu2ux3FGBNgyiurmLY8m5E94ujUJtLtOFYsTkWrZqFc2T+emWt2sr+ozO04xpgA8tUPu8krLOUmlzu2j7JicYpuPSuJsooqptl8UcaYevTW99vpHBPJud3j3I4CWLE4ZSntWjAsOYapS3ZQYfNFGWPqwfrcQ6TtOMAtQ5MICmr4eaCOx4pFPRg/NImdh0psvihjTL14a9F2IsOC+Vlqw97g6ESsWNSD83u1I6F1M96yjm5jzCnKP1zKrDU7uXpAAi0jQt2O8x9WLOpBcJBw85DOLM3cz8ZdBW7HMcY0Yu8v98wDNf4s9+aBOh6fFQsR6SQi34jIBhH5QUTuddrbiMgcEdnq/GzttIuIPC8i6SKyVkQGVPuu8c7yW0VkvK8yn4rrBnYiIjSItxdvdzuKMaaRKq+s4p3FOzg7pS3Jce7NA3U8vjyyqAAeVNXewBBgkoj0Bh4B5qpqCjDXeQ1wEZDiPCYCL4OnuACPAYOBQcBjRwuMP4mODOOKfvHMWJXLwWIbRmuMqbuvftjN7oISbj0rye0oP+GzYqGqu1R1pfO8ENgIxANjgSnOYlOAK5znY4G31WMJEC0iHYDRwBxV3a+qB4A5wBhf5T4V489KoqS8ig+WZ7sdxRjTCE1Z5BkuO7KHfwyXra5B+ixEJAnoDywF2qnqLuet3UA753k8UP23bI7TVlP7seuYKCJpIpKWl5dXvxvgpV4dWjK4SxveWbKDyip1JYMxpnFan3uI5dsPcPOQzn4zXLY6nxcLEYkC/gXcp6o/6v1VVQXq5beqqr6qqqmqmhobG1sfX3lSbj0riZwDnsm/jDHGW1P+M1y2k9tRjsunxUJEQvEUiqmq+rHTvMc5vYTzc6/TngtU/1dKcNpqavdLo3q3o2OrCKZYR7cxxkv5h0uZ6QyXbdXMf4bLVufL0VACvA5sVNVnq701Czg6omk8MLNa+y3OqKghwCHndNVXwIUi0trp2L7QafNLIcFB3DikM9+n57N5d6HbcYwxjYC/DpetzpdHFsOAm4HzRGS187gY+AswSkS2Ahc4rwFmAxlAOjAZuAtAVfcDTwDLnccfnTa/dcOgRCJCg3hjYabbUYwxfq68sop3l/jncNnqQnz1xaq6EKipl+b84yyvwKQavusN4I36S+dbrZuHcc2ZCUxPy+Gh0T2IbRHudiRjjJ+avW4Xuw6V8KcrTnM7ygnZFdw+8vNhXSir8PzFYIwxx6OqvL4wk66xzf1yuGx1Vix8pGtsFBf0iuPdJTsoKa90O44xxg8ty9zP2pxD3D68q18Ol63OioUPTRjelfyiMj5Z5beDt4wxLnptYSatI0O5asBPLh3zO1YsfGhI1zb06diS1xZm4umSMcYYj8x9RXy9cQ83D+lMRGiw23FqZcXCh0SE28/uQvrew3y3xZ2ryo0x/umNhZmEBgVx89Akt6N4xYqFj11yekfiWoTzug2jNcY4DhaX8eGKbK7o37HRjJa0YuFjYSFBjD8riQVb97Fpt93rwhgDU5dmUVJexYThXd2O4jUrFg3gxsGJNAsNtov0jDGUVlTy1qLtnNM9lh7t/fcivGNZsWgA0ZGei/Q+WbWTvMJSt+MYY1z02Zpd5BWWcvvwLm5HqRMrFg3ktmFJlFdV8Y5dpGdMk6WqTF6QQY92LTg7pa3bcerEikUD6Robxfk92/HO4u0cKbOL9IxpihZty2fT7kImnN0Fz1yrjYcViwZ0x4iuHCguZ3qa3UnPmKbon99to21UOJef0dHtKHVmxaIBDUxqw5mdWzN5QQYVlVVuxzHGNKD1uYdYsHUfPx+e1CguwjuWFYsGdueIbuQcOMLn63bVvrAxJmC8Mj+DqPAQbhzsv/esOBErFg3s/J5xpMRF8fK322wKEGOaiB35RXy+dic3Dkn02zvh1caKRQMLChImntOVTbsL+damADGmSZi8IIOQoCAmDGtcw2Wrs2LhgrH94unQKoJ/frvN7SjGGB/LKyxleloOVw2IJ65lhNtxTpoVCxeEhQQxYXgXlmbuZ1XWAbfjGGN86K1FmZRXVjHxnMYztcfxWLFwyfWDEmkZEcI/v7OjC2MCVWFJOe8s3sGYPu3pGhvldpxTYsXCJVHhIdwyNIl/b9jDtrzDbscxxvjAtGVZFJRUcOeIbm5HOWVWLFx067AkwoKDePW7DLejGGPqWWlFJa8vzOSsbjGc0Sna7TinzIqFi9pGhfOz1AQ+XpXDnoISt+MYY+rRzFU72VNQGhBHFWDFwnUTz+5GlcLk+XZ0YUygqKxS/jl/G306tmx0EwbWxIqFyxJjIhl7RkemLs0i/7BNX25MIPhi/S4y8or4f+d2a3QTBtbEioUfuGtkMiXO+U1jTONWVaW8OC+dbrHNuei0Dm7HqTdWLPxAclwUF5/egbcX7+BQcbnbcYwxp+DrjXvYtLuQSSOTCQ4KjKMKsGLhN+4emczh0greWrTd7SjGmJOkqrwwL53OMZGNchryE7Fi4Sd6dWjJqN7teOP7TApL7OjCmMbo2y15rMs9xF3ndiMkOLB+vQbW1jRy95yXzKEj5by7JMvtKMaYOlJVXpi7lfjoZlzZP8HtOPXOioUf6ZsQzYjusby2IIPisgq34xhj6mDxtnxWZh3kzhFdCQsJvF+tPtsiEXlDRPaKyPpqbY+LSK6IrHYeF1d779ciki4im0VkdLX2MU5buog84qu8/uKe85LJLypj2jK79aoxjcnz87YS1yKcn6V2cjuKT/iy/L0FjDlO+3Oq2s95zAYQkd7A9UAf5zP/JyLBIhIMvARcBPQGxjnLBqzUpDYM7RrDq/O3UVJe6XYcY4wXlm/fz5KM/dwxolujvGWqN7wqFiIy15u26lR1PrDfyxxjgfdVtVRVM4F0YJDzSFfVDFUtA953lg1o95yXzJ6CUj5ckeN2FGOMF56fu5WY5mHcMCjR7Sg+c8JiISIRItIGaCsirUWkjfNIAuJPcp13i8ha5zRVa6ctHqh+3iXHaaup/XhZJ4pImoik5eU17jvQDe0Ww5mdW/PyN+mUVtjRhTH+bHX2QRZs3cftZ3elWVhgHlVA7UcWdwArgJ7Oz6OPmcCLJ7G+l4FuQD9gF/DMSXzHcanqq6qaqqqpsbGx9fW1rhAR7rsghZ2HSpieZkcXxvizv3+9hejIUG4e2tntKD51wmKhqv9Q1S7AQ6raVVW7OI8zVLXOxUJV96hqpapWAZPxnGYCyAWq9wolOG01tQe84cltGZjUmpfmpVvfhTF+asWOA3y7OY+J53QlKjzE7Tg+5W0H924RaQEgIr8VkY9FZEBdVyYi1SdKuRI4OlJqFnC9iISLSBcgBVgGLAdSRKSLiITh6QSfVdf1NkYiwv2jurO7oIRpy+y6C2P80XNzthDTPIzxQ5PcjuJz3haL36lqoYgMBy4AXsdzSqlGIjINWAz0EJEcEZkAPC0i60RkLTASuB9AVX8ApgMbgC+BSc4RSAVwN/AVsBGY7izbJJzVrS1Du8bw0jfbOFJmRxfG+JOlGfksTN/HnSO60TzAjyoARFVrX0hklar2F5GngHWq+t7RNt9HrLvU1FRNS0tzO0a9WJa5n2tfWcxvLu7FLxr5Dd+NCRSqyvWvLiFjXxHzHx4ZMB3bIrJCVVOP9563Rxa5IvIKcB0wW0TC6/BZcwoGdWnD2Sltefm7bRSV2lXdxviDxdvyWZq5n0nndguYQlEbb3/hX4vnVNBoVT0ItAEe9lUo82P3j+rO/qIypize7nYUY5o8VeWZOVvo0CqC6wP4uopjeVUsVLUY2AaMFpG7gThV/bdPk5n/GJDYmpE9Ynl1fobNSGuMy77bkseKHQeYNDI5YK/WPh5vr+C+F5gKxDmPd0XkHl8GMz92/6juHCwu583vt7sdxZgmS1V5bs4W4qObcW2AzgFVE29PQ00ABqvq71X198AQ4Be+i2WO1TchmlG92zF5QYbdTc8Yl8zduJc1OYf45fnJATmz7Il4u7UCVB+7Wem0mQZ0/wXdKSyp4NUF29yOYkyTU1Wl/O3fm+kcE8lVAwLvfhW18bZYvAksdaYYfxxYgudaC9OAendsyWVndOT1hZnsLShxO44xTcqsNTvZtLuQB0Z1JzTA7oLnDW87uJ8FbsMzi+x+4DZV/bsPc5kaPDiqOxWVyj/mbnU7ijFNRllFFc/M2Uyfji25rG9g3VvbW952cA8Btqrq86r6PLBNRAb7Npo5nqS2zRk3KJH3l2eTua/I7TjGNAnvLd1B9v4j/GpMT4KCmuYZeG+PpV4GDld7fZhapvswvnPP+cmEBQfxzL83ux3FmIB3uLSCF+alM7RrDOektHU7jmu87uDWavOCOLPGBv5kKH4qrkUEE4Z34bO1u1iXc8jtOMYEtNcWZJBfVMavxvRApGkeVYD3xSJDRH4pIqHO414gw5fBzIlNHNGV1pGhPP3VJrejGBOw8g+XMnl+BmP6tKd/YuvaPxDAvC0WdwJn4bmXRA4wGJjoq1Cmdi0jQpk0MpkFW/fxffo+t+MYE5Be/CadI+WVPDS6h9tRXFfbbVXHiUiMqu5V1etVNU5V26nqDaq6t6FCmuO7aUhnOraK4OkvN+HN7MHGGO9l7y9m6pIsrk3tRHJclNtxXFfbkUUi8KGILHCusRgsTfmknZ+JCA3mvlHdWZNziNnrdrsdx5iA8tycLYjAvRekuB3FL9R2W9W/qup5wMXAGuDnwEoReU9EbhGRdg0R0tTs6gEJdG8XxdNfbaK0wm6QZEx9WJdziI9X5XLrsCQ6tGrmdhy/4G2fRbSqzlDVO5wbHv0JiAXe9l00443gIOHRi3uxI7+YdxbvcDuOMY2eqvKnzzfQpnkYk0Ymux3Hb3hbLGZXf6GqG1T1GVUd7YNMpo7O7RHHOd1jeX7uVg4Ulbkdx5hGbc6GPSzN3M/9F6TQMiLU7Th+w9tisVJEBvo0iTklv7m4F4dLK2waEGNOQVlFFU99sYnkuCjGNaEbG3nD22IxGFgsIttEZK2IrBORtb4MZuqmR/sWXDcwkXeX7CAj73DtHzDG/MTUpTvI3FfEoxf3JKQJThZ4It7+a4wGugHnAZcBlzo/jR95YFR3wkOCeOoLu1DPmLo6VFzOP+ZuZXhyW0b2iHM7jt/xdtbZHUA0ngJxGZ4Ob+tN9TOxLcK5a2QyczbsYfG2fLfjGNOovDBvK4eOlPPoxb2a9LQeNbHbqgaYCcO70LFVBE/O3kBVlV2oZ4w3tu8rYsri7Vx7Zid6d2zpdhy/ZLdVDTARocH8akxP1ucW8PGqXLfjGNMo/PXLTYQGB/Hghd3djuK37LaqAejyMzpyRkIrnv5yE4dLK9yOY4xfW7RtH1+s382dI7oR1zLC7Th+y26rGoCCgoTHL+/D3sJSXphnQ2mNqUlFZRV/mLWBhNbNmHhOV7fj+LVai4WIBOEpDnZb1Uakf2JrrjkzgTcWZrLNhtIac1zvLtnB5j2F/PaS3kSEBrsdx6/VWiycGx29pKorj95WVVVXNUA2c4r+Z0xPIkKC+eOnG2xWWmOOkX+4lGfnbOHslLaM7mPT3NXG29NQc0XkaptxtnGJbRHOvRek8N2WPL7eaDPKG1Pd/361meKySh67rI8NlfWCt8XiDuBDoFRECkSkUEQKfJjL1JPxZyWREhfFE59toKTcZqU1BmBtzkE+SMvmtmFJdq8KL3nbZzFGVYNUNUxVW6pqC1W1wciNQGhwEI9f3oes/cVMnm93wjWmqkp5bNYPxDQP55fn270qvOVtn8WLdf1iEXlDRPaKyPpqbW1EZI6IbHV+tnbaRUSeF5F0Z+6pAdU+M95ZfquIjK9rDgPDktty0WnteenbdHIPHnE7jjGumrEql1VZB3nkop60sFllvebLPou3gDHHtD0CzFXVFGCu8xrgIiDFeUwEXgZPcQEewzOR4SDgsaMFxtTNby7pBcCTn29wOYkx7jl0pJynvthE/8Roruof73acRqUufRbTqUOfharOxzPMtrqxwBTn+RTgimrtb6vHEiBaRDrgmcBwjqruV9UDwBx+WoCMFxJaRzLp3GRmr9vNt5uts9s0Tf/71Sb2F5XyxNjTCAqyTu268LZYtAJuBf7k9FX0AUadxPraqeou5/lu4Oh4tXggu9pyOU5bTe0/ISITRSRNRNLy8vJOIlrgmziiK11jm/O7mes5Umad3aZpWZV1gKlLs7htWBdOi2/ldpxGx9ti8RKe+aDGOa8LOYl+jOrUM/C/3gb/q+qrqpqqqqmxsbH19bUBJTwkmCevOJ3s/Ufsym7TpFRUVvHojPW0bxnB/aNs/qeT4fXNj1R1ElAC4JwSCjuJ9e1xTi/h/Dx6PiQX6FRtuQSnraZ2c5KGdovh6gEJvDo/gy17Ct2OY0yDeGvRdjbuKuCxy/oQFR7idpxGydtiUS4iwThHAiISC1SdxPpmAUdHNI0HZlZrv8UZFTUEOOScrvoKuFBEWjsd2xc6beYU/OaSXkRFhPCbGetsGnMT8HIPHuHZOVs4v2ecXal9CrwtFs8DM4A4EXkSWAj8+UQfEJFpwGKgh4jkiMgE4C/AKBHZClzgvAaYDWQA6cBk4C4AVd0PPAEsdx5/dNrMKWjTPIxHL+rF8u0H+HBFdu0fMKYR+8OsH6hS5fHL7UrtUyHezhkkIj2B8/FMTT5XVTf6MtipSE1N1bS0NLdj+DVV5bpXlrBlbyFzHxhBTFS425GMqXdzNuzhF2+n8chFPblzRDe34/g9EVmhqqnHe8/rO5Kr6iZVfUlVX/TnQmG8IyI8eeVpFJVW8KfPbXeawFNYUs7vZ66nR7sWTBjexe04jZ7XxcIEnpR2LbhzRDdmrMrlm0127YUJLE99sYk9BSX89Zq+hAbbr7pTZf+CTdzd5yWTEhfFozPWUVhS7nYcY+rF4m35vLc0iwnDu9CvU7TbcQKCFYsmLjwkmKev6cueghKe+mKT23GMOWVHyip55OO1JMVE8sCoHm7HCRhWLAz9E1szYXgX3luaxaJt+9yOY8wpeebfm9mRX8xfru5LszC7+119sWJhAHhgVA+SYiJ55F/rKC6rcDuOMSdlZdYBXv8+kxsHJzKka4zbcQKKFQsDQLOwYP56dV+y9hfzt6+2uB3HmDorrajkVx+tpUPLCB65qKfbcQKOFQvzH4O7xnDzkM68uSiTFTvs2kfTuLwwN530vYd58qrT7T4VPmDFwvzI/1zUk46tmvHQh2vtdJRpNFZmHeD/vk3n6gEJjOwR53acgGTFwvxIVHgIf/vZGWzPL+Kp2TY6yvi/4rIKHpy+hg6tmvHY5b3djhOwrFiYnxjaLYYJw7rwzpIddqMk4/eemr2JzH1F/O/P+tLSTj/5jBULc1wPje5B93ZR/OqjtRwoKnM7jjHH9d2WPN5ZsoMJw7twVre2bscJaFYszHFFhAbz3HX9OFBcxm8/WY+3E04a01AOFpfxq4/WkBIXxcOj7eI7X7NiYWrUp2Mr7h/Vnc/X7WLm6p1uxzHmR3438wfyD5fx3HX9iAi1i+98zYqFOaE7zulGaufW/G7menYePOJ2HGMAmLk6l0/X7OS+C1LsftoNxIqFOaHgIOHZa/tRVaXc98FqKipP5gaJxtSfrPxifjNjPWd2bm33qGhAVixMrRJjIvnTlaexLHM/L8xLdzuOacLKK6u45/1VBAn84/p+hNjU4w3G/qWNV67sn8DVAxJ4Yd5WFm/LdzuOaaL+9u/NrMk+yF+v7ktC60i34zQpViyM1/44tg9JMc2574NV7LfhtKaBzd+SxyvfZXDD4EQuOr2D23GaHCsWxmvNw0N44Yb+HCgq56EP19hwWtNg9haW8MD01XRvF8XvL7WrtN1gxcLUSZ+OrfjNJb2Yt2kvry/MdDuOaQKqqpQHp6+hsKSCF28YYMNkXWLFwtTZLUM7c2Hvdvz1y02syT7odhwT4J6ft5UFW/fx2GV96N6uhdtxmiwrFqbORISnr+lLXIsI7pq60vovjM98u3kv/5i7lav6xzNuUCe34zRpVizMSYmODOPlmwaQd7iUX05bRWWV9V+Y+pW9v5j7PlhNj3YtePLK0xERtyM1aVYszEnrmxDNE2P7sDB9H8/O2ex2HBNASsoruWvqSiorlX/edKbdS9sPWLEwp+S6gYlcP7ATL32zjX//sNvtOCZA/OHTDazLPcQz155BUtvmbscxWLEw9eDxy/twenwrHpy+hsx9RW7HMY3c9LRspi3L4v+d240L+7R3O45xWLEwpywiNJiXbxpAcLBwxztpHC6127Gak7NixwF+O2M9w5JjeHBUd7fjmGqsWJh6kdA6khfHDWBbXhH3vW8d3qbudh48wh3vrKBDdAQv3TDA5n3yM7Y3TL0ZntKWxy7rzdcb9/L0V3b/buO94rIKfvF2GiXllbx2SyrRkWFuRzLHcKVYiMh2EVknIqtFJM1payMic0Rkq/OztdMuIvK8iKSLyFoRGeBGZuOdW4YmcdOQRF75LoOPVuS4Hcc0AqrKwx+uZcOuAl4Y158Uu/DOL7l5ZDFSVfupaqrz+hFgrqqmAHOd1wAXASnOYyLwcoMnNXXy2GV9GJYcw6Mfr2PFjv1uxzF+7vm56Xy+bhePjOnJyJ5xbscxNfCn01BjgSnO8ynAFdXa31aPJUC0iNiUk34sNDiIl24YQMfoCCa+vYKcA8VuRzJ+6tM1O3nu6y1c1T+eied0dTuOOQG3ioUC/xaRFSIy0Wlrp6q7nOe7gXbO83ggu9pnc5y2HxGRiSKSJiJpeXl5vsptvBQdGcbrtw6kvLKKW99czsFimxLE/NiSjHwenL6GgUmt+fNVdoW2v3OrWAxX1QF4TjFNEpFzqr+pnrmv6zScRlVfVdVUVU2NjY2tx6jmZHWLjeLVW1LJyi/+T+elMQBb9xQy8e00OrVpxuRbUm0m2UbAlWKhqrnOz73ADGAQsOfo6SXn515n8Vyg+gxiCU6baQSGdI3h2evOYPn2A9z/wWobUmvYU1DCrW8uJzw0mLduG2QjnxqJBi8WItJcRFocfQ5cCKwHZgHjncXGAzOd57OAW5xRUUOAQ9VOV5lG4NK+Hfndpb35Yv1u/vjpD3bTpCbscGkFP39rOQeKy3jz1oF0amO3Rm0sQlxYZztghnN+MgR4T1W/FJHlwHQRmQDsAK51lp8NXAykA8XAbQ0f2ZyqCcO7sOvgEV5bmEn7Vs34f+d2czuSaWAl5ZVMfDuNTbsLeW18KqfFt3I7kqmDBi8WqpoBnHGc9nzg/OO0KzCpAaIZH3v04l7sLijhr19uIjoylHGDEt2OZBpIeWUVd7+3ikXb8nn22jMY2cOGyDY2bhxZmCYqKEh45tozKCyp4NEZ64gMC2Zsv58MbDMBpqpKefjDNXy9cQ9PjO3DVQMS3I5kToI/XWdhmoDwkGBeuflMBndpwwPT1/CVTWse0FSV381czyerd/Lw6B7cPDTJ7UjmJFmxMA0uIjSY18YPpG9CK+55bxXfbbHrYgKRqvKXLzYxdWkWd47oxqSRyW5HMqfAioVxRVR4CG/dOojkuCjueCeNRdv2uR3J1KOjheKV+RncPKQz/zOmh9uRzCmyYmFc0yoylHcmDCKxTSS3vbmc+XaEERBUlT/P3sgr8zO4ZWhn/ji2j12dHQCsWBhXxUSFM+0XQ+gaG8XtU9KYt2mP25HMKVBVnvhsI5MXZHLrWUn84XIrFIHCioVxnadgDKZH+xbc8c4K6/RupKqqlD98uoE3vs/ktmFJPHZZbysUAcSKhfEL0ZFhvHv7YPp0bMWkqSv5dM1OtyOZOiivrOLhj9by1qLtTBjehd9faoUi0FixMH6jVTNPH8aAxNb88v1VvPV9ptuRjBdKyiu5850V/GtlDvdf0J3fXtLLCkUAsmJh/EqLiFDenjCIC3q14/FPN/C/X22yuaT82KEj5dz8+lLmbd7LE1ecxr0XpFihCFBWLIzfiQgN5uUbBzBuUCIvfbONX320lorKKrdjmWPsKSjhulcWszr7IC+M68/NQzq7Hcn4kE33YfxSSHAQf77yNOJahPOPuVvZd7iU58f1p0VEqNvRDLA+9xC3T0mjoKSc18cP5Jzudg+ZQGdHFsZviQj3j+rOn688nflb93H1y4vIyrdbtLptzoY9/OyfiwkS+OjOs6xQNBFWLIzfu2FwIu/8fBB7CkoZ+9JClmTkux2pSVJVJs/PYOI7aXRvF8Unk4bRu2NLt2OZBmLFwjQKZyW35ZNJw2jdPIybXlvKtGVZbkdqUorLKrj/g9U8OXsjF53WnvcnDiWuZYTbsUwDsmJhGo0ubZsz465hnJXcll9/vI7/+Wit3de7AWzfV8RV/7eImWt28uCo7rw4bgDNwuye2U2NFQvTqLRqFsob41OZNLIbH6Rlc8VL35O5r8jtWAFrzoY9XPbiQnYXlPDWbYO45/wUgoJsaGxTZMXCNDohwUE8PLonb942kN0FJVz2wkI+W2tXfNenkvJK/vjpBn7xdhpJMc359O7hjLCO7CbNioVptEb2iGP2L88mpV0Ud7+3ioc+XENhSbnbsRq9zbsLueKl73nje89kgB/eOZRObSLdjmVcZsXCNGodo5sx/Y6h3HNeMh+vzGHM3xfYaKmTVFWlTFm0ncteXMi+w6W8eetAHr+8DxGh1j9hrFiYABAaHMSDF/bgwzvPIjRYGDd5CU9+voEjZdb57a2MvMOMm7yEx2b9wLBuMXx53zmM7BnndizjRyQQ591JTU3VtLQ0t2MYFxSVVvDn2RuZujSLTm2a8cTY0zi3h/3Sq0l5ZRWTF2Tw96+3Eh4SxG8v6cW1qZ1sfqcmSkRWqGrqcd+zYmEC0eJt+fzmk3Vk5BVxad8O/P7S3nZdwDHStu/n9zN/YMOuAsb0ac8fx/axf6MmzoqFaZJKKyp55bsMXvwmnbDgIO4a2Y2fD+vS5M/B5x48wl++2MSna3bSoVUEj13WmzGndXA7lvEDVixMk5a5r4gnP9/A1xv3Eh/djF+N6cFlfTs2uesFCkvKeW1BJq/M34Yq3DGiG3eO6EpkmM0najysWBgDLNq2jz/P3sj63AJOj2/FveencH6vuIA/P19cVsHbi3fwz++2cbC4nEv6duDXF/UkobUNhzU/ZsXCGEdVlTJjVS5/n7uF7P1H6N2hJfecl8zoPu0D7kijoKSc95dl8er8DPYdLuPcHrE8MKo7fROi3Y5m/JQVC2OOUV5ZxczVO3npm3Qy9xXRLbY5t56VxFUDEmge3rhPy+QePMKbCzN5f3k2h0srGJYcwwOjunNm5zZuRzN+zoqFMTWorFI+W7uTyQsyWJ9bQIvwEK5JTeCGQYmktGvhdjyvVVRW8d2WPN5fns28TXsBuLRvB24f3pXTE1q5nM40FlYsjKmFqrIq+yBTFm1n9rpdlFcqp8W35Ip+8VzeryNxLfxvSKmq8sPOAj5ft4t/rchhb2EpbaPCufrMeG4ZmkR8dDO3I5pGxoqFMXWw73Aps1bv5JPVuazNOUSQwIDE1pzXK44LerUjJS7KtU7xsooqVmUd4OuNe/hi/W5yDhwhOEg4t3ss1w7sxHk94wgNtokZzMkJiGIhImOAfwDBwGuq+pealrViYepL+t5CZq3ZxbxNe1ifWwBAx1YRDOrShtSkNgzq0obk2CifdY4XlVbww84CVmUd4Ptt+SzP3M+R8kpCg4XhyW256LQOXNC7HW2ah/lk/aZpafTFQkSCgS3AKCAHWA6MU9UNx1veioXxhd2HSpi7aQ+L0vNZtn0/eYWlADQLDaZ7uyi6t2tBj/YtiI9uRvtWEXSMbkZM8zBCTvCXvqpSVFbJgaIydheUsCO/mKz8IjLzi9mw8xAZ+4o4+r9oclwUw5Pbcla3GIZ0i6FlRGhDbLZpQgKhWAwFHlfV0c7rXwOo6lPHW96KhfE1VSVrfzHLtx9gw84CNu8pYPPuw+w7XPqTZSNCg4gKDyEyLIQggUpVKiuVskrl0JEyyit//P9gkHhm0+3ZvgWnx0dzekJLTo+PJrZFeENtnmmiTlQsGssYwXggu9rrHGBw9QVEZCIwESAxMbHhkpkmSUToHNOczjHN4cz/th8sLmPnwRJ2HTrCzkMl5B8upbisksOlFRSVVqAKIUFCUJAQGiy0ahZGm+ahREeGEdcinM4xzYmPbkZYiPU7GP/SWIpFrVT1VeBV8BxZuBzHNFHRkWFER4bRu2NLt6MYU68ay58vuUCnaq8TnDZjjDENoLEUi+VAioh0EZEw4HpglsuZjDGmyWgUp6FUtUJE7ga+wjN09g1V/cHlWMYY02Q0imIBoKqzgdlu5zDGmKaosZyGMsYY4yIrFsYYY2plxcIYY0ytrFgYY4ypVaOY7qOuRCQP2HEKX9EW2FdPcRqLprbNTW17wba5qTiVbe6sqrHHeyMgi8WpEpG0muZHCVRNbZub2vaCbXNT4attttNQxhhjamXFwhhjTK2sWBzfq24HcEFT2+amtr1g29xU+GSbrc/CGGNMrezIwhhjTK2sWBhjjKmVFYtqRGSMiGwWkXQRecTtPL4gIp1E5BsR2SAiP4jIvU57GxGZIyJbnZ+t3c5a30QkWERWichnzusuIrLU2d8fONPfBwwRiRaRj0Rkk4hsFJGhgb6fReR+57/r9SIyTUQiAm0/i8gbIrJXRNZXazvufhWP551tXysiA052vVYsHCISDLwEXAT0BsaJSG93U/lEBfCgqvYGhgCTnO18BJirqinAXOd1oLkX2Fjt9V+B51Q1GTgATHAlle/8A/hSVXsCZ+DZ9oDdzyISD/wSSFXV0/DczuB6Am8/vwWMOaatpv16EZDiPCYCL5/sSq1Y/NcgIF1VM1S1DHgfGOtypnqnqrtUdaXzvBDPL5B4PNs6xVlsCnCFKwF9REQSgEuA15zXApwHfOQsElDbLCKtgHOA1wFUtUxVDxLg+xnPbReaiUgIEAnsIsD2s6rOB/Yf01zTfh0LvK0eS4BoEelwMuu1YvFf8UB2tdc5TlvAEpEkoD+wFGinqruct3YD7dzK5SN/B34FVDmvY4CDqlrhvA60/d0FyAPedE69vSYizQng/ayqucDfgCw8ReIQsILA3s9H1bRf6+33mhWLJkpEooB/AfepakH199QznjpgxlSLyKXAXlVd4XaWBhQCDABeVtX+QBHHnHIKwP3cGs9f0l2AjkBzfnq6JuD5ar9asfivXKBTtdcJTlvAEZFQPIViqqp+7DTvOXp46vzc61Y+HxgGXC4i2/GcXjwPz/n8aOd0BQTe/s4BclR1qfP6IzzFI5D38wVApqrmqWo58DGefR/I+/momvZrvf1es2LxX8uBFGfkRBiejrFZLmeqd865+teBjar6bLW3ZgHjnefjgZkNnc1XVPXXqpqgqkl49us8Vb0R+Aa4xlks0LZ5N5AtIj2cpvOBDQTwfsZz+mmIiEQ6/50f3eaA3c/V1LRfZwG3OKOihgCHqp2uqhO7grsaEbkYz7ntYOANVX3S3UT1T0SGAwuAdfz3/P2jePotpgOJeKZ3v1ZVj+1Ea/RE5FzgIVW9VES64jnSaAOsAm5S1VIX49UrEemHp0M/DMgAbsPzB2LA7mcR+QNwHZ5Rf6uA2/Gcow+Y/Swi04Bz8UxFvgd4DPiE4+xXp2i+iOd0XDFwm6qmndR6rVgYY4ypjZ2GMsYYUysrFsYYY2plxcIYY0ytrFgYY4yplRULY4wxtbJiYUwDcKbbOOHElCLylohcc5z2JBG5wXfpjKmdFQtjGoCq3q6qG07y40mAFQvjKisWxtSBiDwsIr90nj8nIvOc5+eJyFQRuVBEFovIShH50JmDCxH5VkRSnecTRGSLiCwTkcki8mK1VZwjIotEJKPaUcZfgLNFZLWI3N+Am2vMf1ixMKZuFgBnO89TgShnrq2zgbXAb4ELVHUAkAY8UP3DItIR+B2ee4kMA3oe8/0dgOHApXiKBHgmAFygqv1U9bl63yJjvBBS+yLGmGpWAGeKSEugFFiJp2icjWcent7A955ZFggDFh/z+UHAd0en2BCRD4Hu1d7/RFWrgA0iEjDTh5vGz4qFMXWgquUikgncCizCczQxEkgGMoE5qjruFFZRfc4iOYXvMaZe2WkoY+puAfAQMN95fieeCeqWAMNEJBlARJqLSPdjPrscGCEirZ1ps6/2Yn2FQIv6Cm/MybBiYUzdLcDTt7BYVfcAJXj6FPLwHHFME5G1eE5B/ahPwrmb25+BZcD3wHY8d3Q7kbVApYissQ5u4xabddaYBiYiUap62DmymIFnOvwZbucy5kTsyMKYhve4iKwG1uPp5/jE1TTGeMGOLIwxxtTKjiyMMcbUyoqFMcaYWlmxMMYYUysrFsYYY2plxcIYY0yt/j/+WuQm/bOVoAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(0,100,1000)\n",
    "y = (x-50)**2\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('weight')\n",
    "plt.ylabel(\"error/cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps for linear regression / gradient descent\n",
    "\n",
    "### Gradient descent is basically backpropagation in deep learning......\n",
    "\n",
    "Step 1: Randomize your weight\n",
    "\n",
    "- weight.shape (n, )\n",
    "\n",
    "Step 2: Use this inital weight to predict\n",
    "\n",
    "- you will get errors\n",
    "\n",
    "Step 3: Find the derivative\n",
    "\n",
    "- $\\mathbf{X}^\\top(\\mathbf{\\hat{y} - \\mathbf{y}})$\n",
    "\n",
    "Step 4: Change the weight\n",
    "\n",
    "- $ \\mathbf{w} = \\mathbf{w} - \\alpha * \\mathbf{X}^\\top(\\mathbf{\\hat{y} - \\mathbf{y}}) $\n",
    "\n",
    "Step 5: Repeat Step 2, 3, 4, until you either (1) reach the max iteration, or (2) your validation loss does not decrease anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load some toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "#print the shape of X and y\n",
    "X.shape, y.shape\n",
    "assert X.ndim == 2\n",
    "assert y.ndim == 1\n",
    "#print one of X, and maybe try to see what it is \n",
    "# x[0]\n",
    "#print one of y, and maybe try to see what it is \n",
    "# y[0]\n",
    "# diabetes.feature_names\n",
    "#label is blood glucose level....\n",
    "\n",
    "#please help me set m and n \n",
    "m,n = X.shape\n",
    "\n",
    "#write an assert function to check that X and y has same amount of samples...\n",
    "assert m == y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We skip EDA and cleaninig, because we are lazy; but actually this dataset is already clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split here\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=999)\n",
    "\n",
    "#assert that X_train and y_train has the same amount of samples\n",
    "#assert that X_test and y_test has the same amount of samples\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((309, 10), (133, 10), (309,), (133,))"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Standardziation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the StandatdScalar\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "#standardize the traing set\n",
    "X_train = sc.fit_transform(X_train)\n",
    "\n",
    "#standardize the test set\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Add intercept to your X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: if your X is        [  [3, 2, 4],    [2, 6, 8]  ]\n",
    "# I want you to make it into   [  [1, 3, 2, 4], [1, 2, 6, 8]  ]\n",
    "# Why 1?  because imagine you have another weight, which let's call w0\n",
    "# this w0 is actually the intercept; so multiply with 1, will do nothing\n",
    "# so we can still use X @ theta....\n",
    "\n",
    "intercept = np.ones((X_train.shape[0],1))\n",
    "intercept.shape\n",
    "\n",
    "#hint: use np.concatenate with X_train on axis = 1, to add these ones to X_train\n",
    "# X_train = np.concatenate((intercept, X_train), axis=1)\n",
    "# or \n",
    "X_train = np.insert(X_train,0,1,axis=1) #insert(array,position,value,axis)\n",
    "X_test = np.insert(X_test,0,1,axis=1) #insert(array,position,value,axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((309, 11), (133, 11))"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Fitting!!! Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put everything fit()\n",
    "alpha = 0.001\n",
    "max_iter = 10000\n",
    "#1. randomize out theta\n",
    "#please help me create a random theta of size (X_train.shape[1])\n",
    "theta = np.ones(X_train.shape[1])\n",
    "\n",
    "def predict(X,theta):\n",
    "    y_pred = X @ theta\n",
    "    return y_pred\n",
    "\n",
    "def mean_squared_error(y_hat,y): #loss or cost function\n",
    "    # return np.sum(np.square(y_pred - y))\n",
    "    return ((y_hat - y)**2).sum() /y.shape[0]\n",
    "\n",
    "def _gradient(X,error): #it's only for internal purpose \n",
    "    #not for external purpose\n",
    "    #what do you mean by external: i mean the main() program of python\n",
    "    return X.T @ error\n",
    "\n",
    "def fit(X, y, theta, alpha, max_iter):\n",
    "    for i in range(max_iter):\n",
    "        #2. predict \n",
    "        y_hat = predict(X,theta)\n",
    "\n",
    "        #2.1 can you guys compute the squared error\n",
    "        #print(the mean sqaure error, we can see whether MSE goes down eventually)\n",
    "        mse = mean_squared_error(y_hat,y)\n",
    "        # if ( i%50 ==0 ):\n",
    "        #     print(f'MSE: {mse}')\n",
    "        # print('MSE',mse)\n",
    "\n",
    "        #3. get derivativaes\n",
    "        deriv = _gradient(X_train, y_hat - y)\n",
    "        #4. update weight\n",
    "        theta = theta - alpha*deriv\n",
    "\n",
    "        #5.repeat 1,2,3,4\n",
    "        #please put a for loop for 2,3,4\n",
    "        #set 1000 call it max_iter\n",
    "    # return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = fit(X_train,y_train,theta,alpha,max_iter) \n",
    "#model is the weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Guntsv\\Downloads\\AIT\\AT82.01 Computer Programming for Data Science and Artificial Intelligence (PDS)\\Coding-Along\\class9 LinearRegressionfromScratch.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Downloads/AIT/AT82.01%20Computer%20Programming%20for%20Data%20Science%20and%20Artificial%20Intelligence%20%28PDS%29/Coding-Along/class9%20LinearRegressionfromScratch.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m y_hat \u001b[39m=\u001b[39m predict(X_test,theta)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Downloads/AIT/AT82.01%20Computer%20Programming%20for%20Data%20Science%20and%20Artificial%20Intelligence%20%28PDS%29/Coding-Along/class9%20LinearRegressionfromScratch.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m mean_squared_error(y_hat,y_test)\n",
      "\u001b[1;32mc:\\Users\\Guntsv\\Downloads\\AIT\\AT82.01 Computer Programming for Data Science and Artificial Intelligence (PDS)\\Coding-Along\\class9 LinearRegressionfromScratch.ipynb Cell 32\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(X, theta)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Downloads/AIT/AT82.01%20Computer%20Programming%20for%20Data%20Science%20and%20Artificial%20Intelligence%20%28PDS%29/Coding-Along/class9%20LinearRegressionfromScratch.ipynb#X42sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(X,theta):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Downloads/AIT/AT82.01%20Computer%20Programming%20for%20Data%20Science%20and%20Artificial%20Intelligence%20%28PDS%29/Coding-Along/class9%20LinearRegressionfromScratch.ipynb#X42sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m X \u001b[39m@\u001b[39;49m theta\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Downloads/AIT/AT82.01%20Computer%20Programming%20for%20Data%20Science%20and%20Artificial%20Intelligence%20%28PDS%29/Coding-Along/class9%20LinearRegressionfromScratch.ipynb#X42sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m y_pred\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "y_hat = predict(X_test,theta)\n",
    "\n",
    "mean_squared_error(y_hat,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Many things I wanna do today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives:\n",
    "\n",
    "1. Understand better about cross validation\n",
    "2. Remind how to write a  class\n",
    "3. Learn about stochastic and mini-batch gradient descent, which are very important concepts later on in deep learing......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "minibatch-gradient descent\n",
    "- use only a subset of data for finding the gradient\n",
    "- why?\n",
    "- because using the whole set of data to find the gradient takes time\n",
    "- we assume that the subset of data, should give a general good slope anyway for the whole population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stochastic-gradient descent\n",
    "- use only one sample to get the gradient\n",
    "- very fast (but NOT good! usually)\n",
    "- Exercise: add a parameter at init called 'method'\n",
    "    - if method = mini_batch, then do mini_batch\n",
    "    - if method = sto, go sto ==> please do without replacement\n",
    "    - otherwise, do the normally gradient_descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ex1 : Please Write this whole thing & class called LinearRegression()\n",
    "    #it should have two functions at least\n",
    "    #fit() to fit the model\n",
    "    #predict() to inference\n",
    "    #trivial: try to do like what sklearn do....\n",
    "    #15:10 - 15:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "class LinearRegression():\n",
    "    kfold = KFold(n_splits=5)\n",
    "    def __init__(self,max_iter=1000,alpha=0.0001,num_epochs=5,batch_size=50,method='batch',\n",
    "        cv=kfold):\n",
    "        self.max_iter = max_iter\n",
    "        self.alpha = alpha\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.method = method\n",
    "        self.cv = cv\n",
    "\n",
    "    def predict(self,X):\n",
    "        y_pred = X @ self.theta\n",
    "        return y_pred\n",
    "\n",
    "    def mean_squared_error(self,y_hat,y):\n",
    "        return ((y_hat - y)**2).sum() /y.shape[0]\n",
    "\n",
    "    def _gradient(self,X,error):\n",
    "        return X.T @ error\n",
    "        \n",
    "    def cross_validation(X,fold=3):\n",
    "        dataset_split = list()\n",
    "        return dataset_split\n",
    "\n",
    "    def fit(self,X_train, y_train):\n",
    "        #using training .......\n",
    "\n",
    "        #create a list of kfold scores\n",
    "        self.kfold = list()\n",
    "\n",
    "        #please change it ot cross-validation\n",
    "        for fold,(train_idx, val_idx) in enumerate(self.cv.split(X_train)):\n",
    "            X_cross_train, X_cross_val = X_train[train_idx], X_train[val_idx]\n",
    "            y_cross_train, y_cross_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "            #create self.theta\n",
    "            self.theta = np.zeros(X_cross_train.shape[1]) #based on features\n",
    "\n",
    "            #define X_train as only a subset of the data\n",
    "            #how big is this subset ? mini-batch size ==> 50\n",
    "            \n",
    "            #one epoch will exhaust the WHOLE training set\n",
    "            for epoch in range(self.num_epochs):\n",
    "                #with replacement or no replacement\n",
    "                #with replacement mean jsut randomize\n",
    "                #with no replace menes 0:50 51:100 101:150.... 300:323\n",
    "                #shuffle you index\n",
    "                #===> please shuffle your index\n",
    "                perm = np.random.permutation(X_cross_train.shape[0]) #return as a list of index\n",
    "                # print(permuted_index)\n",
    "                X_cross_train = X_cross_train[perm]\n",
    "                y_cross_train = y_cross_train[perm]\n",
    "\n",
    "                if self.method == 'mini':\n",
    "                    for batch_idx in range(0,X_cross_train.shape[0], self.batch_size):\n",
    "                        #batch_idx = 0,50,100,150\n",
    "                        X_method_train = X_cross_train[batch_idx:batch_idx+self.batch_size,:]\n",
    "                        y_method_train = y_cross_train[batch_idx:batch_idx+self.batch_size]\n",
    "                        # print(X_method_train.shape)\n",
    "                        self._train(X_method_train, y_method_train)\n",
    "                elif self.method == 'sto':\n",
    "                    for sto_idx in range(0,X_cross_train.shape[0]):\n",
    "                        X_method_train = X_cross_train[sto_idx,:].reshape(1,-1) #(11,) => (1,11)\n",
    "                        y_method_train = y_cross_train[sto_idx]\n",
    "                        # print(X_method_train.shape)\n",
    "                        self._train(X_method_train, y_method_train)\n",
    "                else: #'gradient_descent'\n",
    "                    X_method_train = X_cross_train\n",
    "                    y_method_train = y_cross_train\n",
    "                    self._train(X_method_train, y_method_train)\n",
    "            #print the validation\n",
    "            yhat_val = self.predict(X_cross_val)\n",
    "            self.kfold.append(mean_squared_error(y_cross_val,yhat_val))\n",
    "            # print(f'Fold {fold:} : {mean_squared_error(y_cross_val,yhat_val)}')\n",
    "\n",
    "    def _train(self, X, y):\n",
    "        y_hat = self.predict(X)\n",
    "        #1.1 Optionally, print loss\n",
    "        # mse = self.mean_squared_error(y_hat,y_train)\n",
    "        #2.get gradient\n",
    "        deriv = self._gradient(X, y_hat - y)\n",
    "        #3. adjust theta/coef\n",
    "        self.theta = self.theta - self.alpha*deriv\n",
    "\n",
    "    def _coef(self): #<------ basically theta\n",
    "        return self.theta[1:]   #remind that theta is (w0,w1,w2,w3,w4,....,wn)\n",
    "                                #w0 is the bias or the intercepts\n",
    "                                #w1 .... wn are the weights/ coefficients / theta\n",
    "\n",
    "    def _bias(self):\n",
    "        return self.theta[0]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(method='sto')\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[27880.862074943092,\n",
       " 23238.037436715316,\n",
       " 21116.883388792816,\n",
       " 23715.462654768115,\n",
       " 24569.876942679075]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.69189699,  0.17352329,  5.15926454,  3.44343493,  1.83484542,\n",
       "        1.58441267, -2.40741838,  2.66884436,  3.99019904])"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._coef() # the weight associated with the ten feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
