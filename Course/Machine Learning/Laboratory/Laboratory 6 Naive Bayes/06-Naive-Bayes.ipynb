{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Todsavad Tangtortan\"\n",
    "ID = \"123012\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 06: Generative classifiers: Naive Bayes\n",
    "\n",
    "As discussed in class, a Naive Bayes classifier works as follows:\n",
    "$$\\begin{eqnarray}\n",
    "p(y \\mid \\mathbf{x} ; \\theta) & = & \\frac{p(\\mathbf{x} \\mid y ; \\theta) p(y ; \\theta)}{p(\\mathbf{x} ; \\theta)} \\\\\n",
    "& \\propto & p(\\mathbf{x} \\mid y ; \\theta) p(y ; \\theta) \\\\\n",
    "& \\approx & p(y ; \\theta) \\prod_j p(x_j \\mid y ; \\theta)\n",
    "\\end{eqnarray}$$\n",
    "We will use Naive Bayes to perform diabetes diagnosis and text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Diabetes classification\n",
    "\n",
    "In this example we predict wheter a patient with specific diagnostic measurements has diabetes or not. As the features are\n",
    "continuous, we will model the conditional probabilities\n",
    "$p(x_j \\mid y ; \\theta)$ as univariate Gaussians with mean $\\mu_{j,y}$ and standard deviation $\\sigma_{j,y}$.\n",
    "\n",
    "The data are originally from the U.S. National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK) and are available\n",
    "from [Kaggle](https://www.kaggle.com/uciml/pima-indians-diabetes-database)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manipulation\n",
    "\n",
    "First we have some functions to read the dataset, split it into train and test, and partition it according to target class ($y$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV file\n",
    "def loadCsv(filename):\n",
    "    data_raw = pd.read_csv(filename)\n",
    "    headers = data_raw.columns\n",
    "    dataset = data_raw.values\n",
    "    return dataset, headers\n",
    "\n",
    "# Split dataset into test and train with given ratio\n",
    "def splitDataset(test_size,*arrays,**kwargs):\n",
    "    return train_test_split(*arrays,test_size=test_size,**kwargs)\n",
    "\n",
    "# Separate training data according to target class\n",
    "# Return key value pairs array in which keys are possible target variable values\n",
    "# and values are the data records.\n",
    "\n",
    "def data_split_byClass(dataset):\n",
    "    Xy = {}\n",
    "    for i in range(len(dataset)):\n",
    "        datapair = dataset[i]\n",
    "        # datapair[-1] (the last column) is the target class for this record.\n",
    "        # Check if we already have this value as a key in the return array\n",
    "        if (datapair[-1] not in Xy):\n",
    "            # Add class as key\n",
    "            Xy[datapair[-1]] = []\n",
    "        # Append this record to array of records for this class key\n",
    "        Xy[datapair[-1]].append(datapair)\n",
    "    return Xy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "Next we have some functions used for training the model. Parameters include mean and standard deviation, used\n",
    "to partition numerical variables into categorical variables, as well as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters of a Gaussian are its mean and standard deviation\n",
    "\n",
    "def mean(numbers):\n",
    "    return sum(numbers)/float(len(numbers))\n",
    "\n",
    "def stdev(numbers):\n",
    "    avg = mean(numbers)\n",
    "    variance = sum([pow(x-avg,2) for x in numbers])/float(len(numbers)-1)\n",
    "    return math.sqrt(variance)\n",
    "\n",
    "# Calculate Gaussian parameters mu and sigma for each attribute over a dataset\n",
    "\n",
    "def get_gaussian_parameters(X,y):\n",
    "    parameters = {}\n",
    "    unique_y = np.unique(y)\n",
    "    for uy in unique_y:\n",
    "        mean = np.mean(X[y==uy],axis=0)\n",
    "        std = np.std(X[y==uy],axis=0)\n",
    "        py = y[y==uy].size/y.size\n",
    "        parameters[uy] = {'prior':py,'mean':mean,'std':std}\n",
    "    return parameters, unique_y\n",
    "\n",
    "def calculateProbability(x, mu, sigma):\n",
    "    sigma = np.diag(sigma**2)\n",
    "    x = x.reshape(-1,1)\n",
    "    mu = mu.reshape(-1,1)\n",
    "    exponent = np.exp(-1/2*(x-mu).T@np.linalg.inv(sigma)@(x-mu))\n",
    "    return ((1/(np.sqrt(((2*np.pi)**x.size)*np.linalg.det(sigma))))*exponent)[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model testing\n",
    "\n",
    "Next some functions for testing the model on a test set and computing its accuracy. Note that we assume\n",
    "$$ p(y \\mid \\mathbf{x} ; \\theta) \\propto p(\\mathbf{x} \\mid y ; \\theta), $$\n",
    "which means we assume that the priors $p(y)$ are equal for each possible value of $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class conditional probabilities for given input data vector\n",
    "\n",
    "def predict_one(x,parameters,unique_y,prior = True):\n",
    "    probabilities = []\n",
    "    for key in parameters.keys():\n",
    "        probabilities.append(calculateProbability(x,parameters[key]['mean'],parameters[key]['std'])*(parameters[key]['prior']**(float(prior))))\n",
    "    probabilities = np.array(probabilities)\n",
    "    return unique_y[np.argmax(probabilities)]\n",
    "\n",
    "def getPredictions(X, parameters, unique_y,prior=True):\n",
    "    predictions = []\n",
    "    for i in range(X.shape[0]):\n",
    "        predictions.append(predict_one(X[i],parameters,unique_y,prior))\n",
    "    return np.array(predictions)\n",
    "\n",
    "# Get accuracy for test set\n",
    "\n",
    "def getAccuracy(y, y_pred):\n",
    "    correct = len(y[y==y_pred])\n",
    "    return correct/y.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "\n",
    "Here we load the diabetes dataset, split it into training and test data, train a Gaussian NB model, and test the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total = 768 Train = 460 Test = 308\n",
      "Accuracy with Prior = 0.7272727272727273\n",
      "Accuracy without Prior = 0.7077922077922078\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "\n",
    "filename = 'diabetes.csv'\n",
    "dataset, headers = loadCsv(filename)\n",
    "#print(headers)\n",
    "#print(np.array(dataset)[0:5,:])\n",
    "\n",
    "# Split into training and test\n",
    "X_train,X_test,y_train,y_test = splitDataset(0.4,dataset[:,:-1],dataset[:,-1])\n",
    "print(\"Total =\",len(dataset),\"Train =\", len(X_train),\"Test =\",len(X_test))\n",
    "\n",
    "# Train model\n",
    "parameters, unique_y = get_gaussian_parameters(X_train,y_train)\n",
    "prediction = getPredictions(X_test,parameters,unique_y)\n",
    "print(\"Accuracy with Prior =\",getAccuracy(y_test,prediction))\n",
    "\n",
    "# Test model\n",
    "prediction = getPredictions(X_test,parameters,unique_y,prior = False)\n",
    "print(\"Accuracy without Prior =\",getAccuracy(y_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a0bcbc8407fdfb3f7d96cc07ad4d3124",
     "grade": false,
     "grade_id": "cell-9a740adb2ea13611",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "###  Exercise In lab / take home work (20 points)\n",
    "\n",
    "Find out the proportion of the records in your dataset are positive vs. negative.  Can we conclude that $p(y=1) = p(y=0)$? If not, add\n",
    "the priors $p(y=1)$ and $p(y=0)$ to your NB model. Does it improve the result?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_set(df, training_ratio=0.6):\n",
    "    m, n = df.shape\n",
    "    idx = np.arange(0, m)\n",
    "    random.shuffle(idx)\n",
    "    m_train = int(m * training_ratio)\n",
    "    train_idx = idx[0:m_train]\n",
    "    test_idx = idx[m_train:]\n",
    "    return df.iloc[train_idx], df.iloc[test_idx]\n",
    "\n",
    "class NaiveBay:\n",
    "    def __init__(self, with_prior=True):\n",
    "        self.x_mean = {}\n",
    "        self.x_std = {}\n",
    "        \n",
    "        self.priors = {}\n",
    "        self.y_class = []\n",
    "        self.pred_with_prior = with_prior\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.priors = {} # reset priority \n",
    "        \n",
    "        # reset saving parameters\n",
    "        self.x_mean = {}\n",
    "        self.x_std = {}\n",
    "        \n",
    "        # xy_df should be in format columns = [x0, x1, ..., xn, y]\n",
    "        self.y_class = np.unique(y)\n",
    "        \n",
    "        for y_class in self.y_class : \n",
    "            y_index = np.where(y == y_class) # select X by class \n",
    "            \n",
    "            # select X data of y_class answer\n",
    "            y_X = X.copy()[y_index] # X.copy() => prevent update the original X variable.\n",
    "            \n",
    "            x_mean = y_X.mean(axis=0) # find mean of each column\n",
    "            x_std = y_X.std(axis=0) # find the stdev of each column\n",
    "            # save parameter values \n",
    "            self.x_mean[y_class] = x_mean\n",
    "            self.x_std[y_class] = x_std\n",
    "            \n",
    "            self.priors[y_class] = 1\n",
    "            if self.pred_with_prior : \n",
    "                self.priors[y_class] *=  (y_X.shape[0]/ X.shape[0])                 \n",
    "                \n",
    "    def calculate_probability(self, x, mean, stdev):\n",
    "        top = np.exp(-1 * np.power((x-mean), 2) / (2*np.power(stdev,2)))\n",
    "        bot = np.sqrt(2*np.pi*np.power(stdev,2))\n",
    "        p = top/bot\n",
    "        return p \n",
    "    \n",
    "    def predict(self, X):\n",
    "        pred_df = pd.DataFrame() # save prob for each class \n",
    "        \n",
    "        for y_class in self.y_class : \n",
    "            prior_y = self.priors[y_class]\n",
    "            # a vector of mean \n",
    "            y_params_mean = self.x_mean[y_class]\n",
    "            # a vector of variance \n",
    "            y_params_std = self.x_std[y_class]\n",
    "            # probability of y given X \n",
    "            yp_by_features = self.calculate_probability(X, y_params_mean, y_params_std)\n",
    "            yp_by_features =  yp_by_features * (prior_y ** prior_y)         \n",
    "            yp = np.prod(yp_by_features, axis=1)\n",
    "            pred_df[y_class] = yp\n",
    "    \n",
    "        pred_df['y_pred'] = np.argmax(pred_df.values, axis=1)\n",
    "        pred_df['y_pred'] = [pred_df.columns[v] for v in pred_df['y_pred']]\n",
    "        self.pred_df = pred_df\n",
    "        return pred_df['y_pred'].values\n",
    "\n",
    "def get_accuracy(y_pred, y_true):\n",
    "    return (y_pred == y_true).sum() / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome Proportion\n",
      "{0: 0.6433224755700325, 1: 0.3566775244299674}\n",
      "Can we conclude p(y=1) == p(y=0) ? \n",
      "The answer is False\n",
      "==============================\n",
      "Model without prior\n",
      "Train Accuracy : 0.742671009771987\n",
      "Test Accuracy :  0.7207792207792207\n",
      "==============================\n",
      "Model with prior\n",
      "{0: 0.6433224755700325, 1: 0.3566775244299674}\n",
      "Train Accuracy : 0.760586319218241\n",
      "Test Accuracy :  0.7467532467532467\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "df = pd.read_csv('diabetes.csv')\n",
    "training_set, test_set = split_data_set(df, 0.8)\n",
    "\n",
    "outcome_ratio = (training_set.Outcome.value_counts() / len(training_set)).to_dict()\n",
    "print('Outcome Proportion')\n",
    "print(outcome_ratio)\n",
    "\n",
    "print('Can we conclude p(y=1) == p(y=0) ? ')\n",
    "print('The answer is', outcome_ratio[0] == outcome_ratio[1])\n",
    "\n",
    "X_train, y_train = training_set.drop(columns='Outcome').values, training_set['Outcome'].values\n",
    "X_test, y_test = test_set.drop(columns='Outcome').values, test_set['Outcome'].values\n",
    "\n",
    "nb= NaiveBay(with_prior=False)\n",
    "print('='*30)\n",
    "print('Model without prior')\n",
    "nb.fit(X_train, y_train)\n",
    "train_pred = nb.predict(X_train)\n",
    "test_pred = nb.predict(X_test)\n",
    "print('Train Accuracy :', get_accuracy(train_pred, y_train))\n",
    "print('Test Accuracy : ', get_accuracy(test_pred, y_test))\n",
    "print('='*30)\n",
    "\n",
    "print('Model with prior')\n",
    "nb = NaiveBay(with_prior=True)\n",
    "nb.fit(X_train, y_train)\n",
    "print(nb.priors)\n",
    "my_pred = nb.predict(X_test)\n",
    "train_pred = nb.predict(X_train)\n",
    "test_pred = nb.predict(X_test)\n",
    "print('Train Accuracy :', get_accuracy(train_pred, y_train))\n",
    "print('Test Accuracy : ', get_accuracy(test_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f416d2fceb4d17b1865da03585d6f44f",
     "grade": false,
     "grade_id": "cell-a32adf75650dfd55",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**Explain that you can conclude that $p(y=1) = p(y=0)$? If not, add\n",
    "the priors $p(y=1)$ and $p(y=0)$ to your NB model. Does it improve the result? (double click to explain)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even imbalancing data of outcome but we is still able to change $p(y=1) = p(y=0)$ by balancing data instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome Proportion\n",
      "{0: 0.6433224755700325, 1: 0.3566775244299674}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0    395\n",
       " 1    219\n",
       " Name: Outcome, dtype: int64,\n",
       " 0    105\n",
       " 1     49\n",
       " Name: Outcome, dtype: int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome_ratio = (training_set.Outcome.value_counts() / len(training_set)).to_dict()\n",
    "\n",
    "print('Outcome Proportion')\n",
    "print(outcome_ratio)\n",
    "\n",
    "#Imbalancing Data\n",
    "training_set.Outcome.value_counts(),test_set.Outcome.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balancing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    219\n",
       "0    219\n",
       "Name: Outcome, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For Training Set\n",
    "Outcome_1_train = training_set.loc[training_set.Outcome == 1]\n",
    "Outcome_1_train.shape\n",
    "Outcome_2_train = training_set.loc[training_set.Outcome == 0].sample(n=219, random_state=999)\n",
    "Outcome_2_train.shape\n",
    "training_set = pd.concat([Outcome_1_train, Outcome_2_train])\n",
    "training_set.Outcome.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    49\n",
       "0    49\n",
       "Name: Outcome, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For Testing Set\n",
    "Outcome_1_test = test_set.loc[test_set.Outcome == 1]\n",
    "Outcome_1_test.shape\n",
    "Outcome_2_test = test_set.loc[test_set.Outcome == 0].sample(n=49, random_state=999)\n",
    "Outcome_2_test.shape\n",
    "test_set = pd.concat([Outcome_1_test, Outcome_2_test])\n",
    "test_set.Outcome.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome Proportion\n",
      "{1: 0.5, 0: 0.5}\n",
      "Can we conclude p(y=1) == p(y=0) ? \n",
      "The answer is True\n",
      "==============================\n",
      "Model without prior\n",
      "Train Accuracy : 0.7214611872146118\n",
      "Test Accuracy :  0.7040816326530612\n",
      "==============================\n",
      "Model with prior\n",
      "{0: 0.5, 1: 0.5}\n",
      "Train Accuracy : 0.7214611872146118\n",
      "Test Accuracy :  0.7040816326530612\n"
     ]
    }
   ],
   "source": [
    "outcome_ratio = (training_set.Outcome.value_counts() / len(training_set)).to_dict()\n",
    "print('Outcome Proportion')\n",
    "print(outcome_ratio)\n",
    "\n",
    "print('Can we conclude p(y=1) == p(y=0) ? ')\n",
    "print('The answer is', outcome_ratio[0] == outcome_ratio[1])\n",
    "\n",
    "X_train, y_train = training_set.drop(columns='Outcome').values, training_set['Outcome'].values\n",
    "X_test, y_test = test_set.drop(columns='Outcome').values, test_set['Outcome'].values\n",
    "\n",
    "nb= NaiveBay(with_prior=False)\n",
    "print('='*30)\n",
    "print('Model without prior')\n",
    "nb.fit(X_train, y_train)\n",
    "train_pred = nb.predict(X_train)\n",
    "test_pred = nb.predict(X_test)\n",
    "print('Train Accuracy :', get_accuracy(train_pred, y_train))\n",
    "print('Test Accuracy : ', get_accuracy(test_pred, y_test))\n",
    "print('='*30)\n",
    "\n",
    "print('Model with prior')\n",
    "nb = NaiveBay(with_prior=True)\n",
    "nb.fit(X_train, y_train)\n",
    "print(nb.priors)\n",
    "my_pred = nb.predict(X_test)\n",
    "train_pred = nb.predict(X_train)\n",
    "test_pred = nb.predict(X_test)\n",
    "print('Train Accuracy :', get_accuracy(train_pred, y_train))\n",
    "print('Test Accuracy : ', get_accuracy(test_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even we has balanced data, it stil does not improve the model a much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Text classification\n",
    "\n",
    "This example has been adapted from a post by Jaya Aiyappan, available at\n",
    "[Analytics Vidhya](https://medium.com/analytics-vidhya/naive-bayes-classifier-for-text-classification-556fabaf252b#:~:text=The%20Naive%20Bayes%20classifier%20is,time%20and%20less%20training%20data).\n",
    "\n",
    "We will generate a small dataset of sentences that are classified as either \"statements\" or \"questions.\"\n",
    "\n",
    "We will assume that occurance and placement of words within a sentence is independent of each other\n",
    "(i.e., the features are conditionally independent given $y$). So the sentence \"this is my book\" is the same as \"is this my book.\"\n",
    "We will treat words as case insensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             sentence      class\n",
      "0               This is my novel book  statement\n",
      "1  this book has more than one author  statement\n",
      "2                     is this my book   question\n",
      "3                     They are novels  statement\n",
      "4             have you read this book   question\n",
      "5            who is the novels author   question\n",
      "6             what are the characters   question\n",
      "7       This is how I bought the book  statement\n",
      "8         I like fictional characters  statement\n",
      "9          what is your favorite book   question\n",
      "\n",
      "------------------------------------------\n",
      "\n",
      "                        sentence      class\n",
      "0               this is the book  statement\n",
      "1  who are the novels characters   question\n",
      "2             is this the author   question\n",
      "3                  I like apples       None\n"
     ]
    }
   ],
   "source": [
    "# Generate text data for two classes, \"statement\" and \"question\"\n",
    "\n",
    "text_train = [['This is my novel book', 'statement'],\n",
    "              ['this book has more than one author', 'statement'],\n",
    "              ['is this my book', 'question'],\n",
    "              ['They are novels', 'statement'],\n",
    "              ['have you read this book', 'question'],\n",
    "              ['who is the novels author', 'question'],\n",
    "              ['what are the characters', 'question'],\n",
    "              ['This is how I bought the book', 'statement'],\n",
    "              ['I like fictional characters', 'statement'],\n",
    "              ['what is your favorite book', 'question']]\n",
    "\n",
    "text_test = [['this is the book', 'statement'], \n",
    "             ['who are the novels characters', 'question'], \n",
    "             ['is this the author', 'question'],\n",
    "            ['I like apples']]\n",
    "\n",
    "# Load training and test data into pandas data frames\n",
    "\n",
    "training_data = pd.DataFrame(text_train, columns= ['sentence', 'class'])\n",
    "print(training_data)\n",
    "print('\\n------------------------------------------\\n')\n",
    "testing_data = pd.DataFrame(text_test, columns= ['sentence', 'class'])\n",
    "print(testing_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition training data by class\n",
    "\n",
    "stmt_docs = [train['sentence'] for index,train in training_data.iterrows() if train['class'] == 'statement']\n",
    "question_docs = [train['sentence'] for index,train in training_data.iterrows() if train['class'] == 'question']\n",
    "all_docs = [train['sentence'] for index,train in training_data.iterrows()]\n",
    "\n",
    "# Get word frequencies for each sentence and class\n",
    "\n",
    "def get_words(text):\n",
    "    # Initialize word list\n",
    "    words = [];\n",
    "    # Loop through each sentence in input array\n",
    "    for text_row in text:       \n",
    "        # Check the number of words. Assume each word is separated by a blank space\n",
    "        # so that the number of words is the number of blank spaces + 1\n",
    "        number_of_spaces = text_row.count(' ')\n",
    "        # loop through the sentence and get words between blank spaces.\n",
    "        for i in range(number_of_spaces):\n",
    "            # Check for for last word\n",
    "            words.append([text_row[:text_row.index(' ')].lower()])\n",
    "            text_row = text_row[text_row.index(' ')+1:]  \n",
    "            i = i + 1        \n",
    "        words.append([text_row])\n",
    "    return np.unique(words)\n",
    "\n",
    "# Get frequency of each word in each document\n",
    "\n",
    "def get_doc_word_frequency(words, text):  \n",
    "    word_freq_table = np.zeros((len(text),len(words)), dtype=int)\n",
    "    i = 0\n",
    "    for text_row in text:\n",
    "        # Insert extra space between each pair of words to prevent\n",
    "        # partial match of words\n",
    "        text_row_temp = ''\n",
    "        for idx, val in enumerate(text_row):\n",
    "            if val == ' ':\n",
    "                 text_row_temp = text_row_temp + '  '\n",
    "            else:\n",
    "                  text_row_temp = text_row_temp + val.lower()\n",
    "        text_row = ' ' + text_row_temp + ' '\n",
    "        j = 0\n",
    "        for word in words: \n",
    "            word = ' ' + word + ' '\n",
    "            freq = text_row.count(word)\n",
    "            word_freq_table[i,j] = freq\n",
    "            j = j + 1\n",
    "        i = i + 1\n",
    "    \n",
    "    return word_freq_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   are  author  book  bought  characters  fictional  has  how  i  is  like  \\\n",
      "0    0       0     1       0           0          0    0    0  0   1     0   \n",
      "1    0       1     1       0           0          0    1    0  0   0     0   \n",
      "2    1       0     0       0           0          0    0    0  0   0     0   \n",
      "3    0       0     1       1           0          0    0    1  1   1     0   \n",
      "4    0       0     0       0           1          1    0    0  1   0     1   \n",
      "\n",
      "   more  my  novel  novels  one  than  the  they  this  \n",
      "0     0   1      1       0    0     0    0     0     1  \n",
      "1     1   0      0       0    1     1    0     0     1  \n",
      "2     0   0      0       1    0     0    0     1     0  \n",
      "3     0   0      0       0    0     0    1     0     1  \n",
      "4     0   0      0       0    0     0    0     0     0  \n"
     ]
    }
   ],
   "source": [
    "# Get word frequencies for statement documents\n",
    "\n",
    "word_list_s = get_words(stmt_docs)\n",
    "word_freq_table_s = get_doc_word_frequency(word_list_s, stmt_docs)\n",
    "tdm_s = pd.DataFrame(word_freq_table_s, columns=word_list_s)\n",
    "print(tdm_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'are': 1, 'author': 1, 'book': 3, 'bought': 1, 'characters': 1, 'fictional': 1, 'has': 1, 'how': 1, 'i': 2, 'is': 2, 'like': 1, 'more': 1, 'my': 1, 'novel': 1, 'novels': 1, 'one': 1, 'than': 1, 'the': 1, 'they': 1, 'this': 3}\n"
     ]
    }
   ],
   "source": [
    "# Get word frequencies over all statement documents\n",
    "\n",
    "freq_list_s = word_freq_table_s.sum(axis=0) \n",
    "freq_s = dict(zip(word_list_s,freq_list_s))\n",
    "print(freq_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   are  author  book  characters  favorite  have  is  my  novels  read  the  \\\n",
      "0    0       0     1           0         0     0   1   1       0     0    0   \n",
      "1    0       0     1           0         0     1   0   0       0     1    0   \n",
      "2    0       1     0           0         0     0   1   0       1     0    1   \n",
      "3    1       0     0           1         0     0   0   0       0     0    1   \n",
      "4    0       0     1           0         1     0   1   0       0     0    0   \n",
      "\n",
      "   this  what  who  you  your  \n",
      "0     1     0    0    0     0  \n",
      "1     1     0    0    1     0  \n",
      "2     0     0    1    0     0  \n",
      "3     0     1    0    0     0  \n",
      "4     0     1    0    0     1  \n"
     ]
    }
   ],
   "source": [
    "# Get word frequencies for question documents\n",
    "\n",
    "word_list_q = get_words(question_docs)\n",
    "word_freq_table_q = get_doc_word_frequency(word_list_q, question_docs)\n",
    "tdm_q = pd.DataFrame(word_freq_table_q, columns=word_list_q)\n",
    "print(tdm_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'are': 1, 'author': 1, 'book': 3, 'characters': 1, 'favorite': 1, 'have': 1, 'is': 3, 'my': 1, 'novels': 1, 'read': 1, 'the': 2, 'this': 2, 'what': 2, 'who': 1, 'you': 1, 'your': 1}\n",
      "[1 1 3 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 3]\n",
      "[1 1 3 1 1 1 3 1 1 1 2 2 2 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Get word frequencies over all question documents\n",
    "\n",
    "freq_list_q = word_freq_table_q.sum(axis=0) \n",
    "freq_q = dict(zip(word_list_q,freq_list_q))\n",
    "print(freq_q)\n",
    "print(freq_list_s)\n",
    "print(freq_list_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of words for \"statement\" class \n",
      "\n",
      "{'are': 0.043478260869565216, 'author': 0.043478260869565216, 'book': 0.08695652173913043, 'bought': 0.043478260869565216, 'characters': 0.043478260869565216, 'fictional': 0.043478260869565216, 'has': 0.043478260869565216, 'how': 0.043478260869565216, 'i': 0.06521739130434782, 'is': 0.06521739130434782, 'like': 0.043478260869565216, 'more': 0.043478260869565216, 'my': 0.043478260869565216, 'novel': 0.043478260869565216, 'novels': 0.043478260869565216, 'one': 0.043478260869565216, 'than': 0.043478260869565216, 'the': 0.043478260869565216, 'they': 0.043478260869565216, 'this': 0.08695652173913043}\n",
      "------------------------------------------- \n",
      "\n",
      "Probability of words for \"question\" class \n",
      "\n",
      "{'are': 0.05128205128205128, 'author': 0.05128205128205128, 'book': 0.10256410256410256, 'characters': 0.05128205128205128, 'favorite': 0.05128205128205128, 'have': 0.05128205128205128, 'is': 0.10256410256410256, 'my': 0.05128205128205128, 'novels': 0.05128205128205128, 'read': 0.05128205128205128, 'the': 0.07692307692307693, 'this': 0.07692307692307693, 'what': 0.07692307692307693, 'who': 0.05128205128205128, 'you': 0.05128205128205128, 'your': 0.05128205128205128}\n"
     ]
    }
   ],
   "source": [
    "# Get word probabilities for statement class\n",
    "a = 1\n",
    "prob_s = []\n",
    "for count in freq_list_s:\n",
    "    #print(word, count)\n",
    "    prob_s.append((count+a)/(sum(freq_list_s)+len(freq_list_s)*a))\n",
    "prob_s.append(a/(sum(freq_list_s)+len(freq_list_s)*a))\n",
    "    \n",
    "# Get word probabilities for question class\n",
    "\n",
    "prob_q = []\n",
    "for count in freq_list_q:\n",
    "    prob_q.append((count+a)/(sum(freq_list_q)+len(freq_list_q)*a))\n",
    "prob_q.append(a/(sum(freq_list_q)+len(freq_list_q)*a))   \n",
    "    \n",
    "    \n",
    "print('Probability of words for \"statement\" class \\n')\n",
    "print(dict(zip(word_list_s, prob_s)))\n",
    "print('------------------------------------------- \\n')\n",
    "print('Probability of words for \"question\" class \\n')\n",
    "print(dict(zip(word_list_q, prob_q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prior for one class\n",
    "def prior(className):    \n",
    "    denominator = len(stmt_docs) + len(question_docs)\n",
    "    \n",
    "    if className == 'statement':\n",
    "        numerator =  len(stmt_docs)\n",
    "    else:\n",
    "        numerator =  len(question_docs)\n",
    "        \n",
    "    return np.divide(numerator,denominator)\n",
    "    \n",
    "# Calculate class conditional probability for a sentence\n",
    "def classCondProb(sentence, className):\n",
    "    words = get_words(sentence)\n",
    "    prob = 1\n",
    "    for word in words:\n",
    "        if className == 'statement':\n",
    "            idx = np.where(word_list_s == word)\n",
    "            prob = prob * prob_s[np.array(idx)[0,0]]\n",
    "        else:\n",
    "            idx = np.where(word_list_q == word)\n",
    "            prob = prob * prob_q[np.array(idx)[0,0]]   \n",
    "    \n",
    "    return prob\n",
    "\n",
    "# Predict class of a sentence\n",
    "def predict(sentence):\n",
    "    prob_statement = classCondProb(sentence, 'statement') * prior('statement')\n",
    "    prob_question = classCondProb(sentence, 'question') * prior('question')\n",
    "    if  prob_statement > prob_question:\n",
    "        return 'statement'\n",
    "    else:\n",
    "        return 'question'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06f31d4f4657c9d7a61ca287b3a51c86",
     "grade": false,
     "grade_id": "cell-3b166fac02ec4711",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### In-lab exercise: Laplace smoothing\n",
    "\n",
    "Run the code below and figure out why it fails.\n",
    "\n",
    "When a word does not appear with a specific class in the training data, its class-conditional probability is 0, and we are unable to\n",
    "get a reasonable probability for that class.\n",
    "\n",
    "Research Laplace smoothing, and modify the code above to implement Laplace smoothing (setting the frequency of all words with frequency 0 to a frequency of 1).\n",
    "Run the modified code on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting prediction for \"this is the book\"\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 1 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Guntsv\\Downloads\\AIT\\AT82.03 Machine Learning (ML)\\Laboratory\\ML2022-06-Naive-Bayes\\06-Naive-Bayes.ipynb Cell 36\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Downloads/AIT/AT82.03%20Machine%20Learning%20%28ML%29/Laboratory/ML2022-06-Naive-Bayes/06-Naive-Bayes.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test_docs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m([test[\u001b[39m'\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m index,test \u001b[39min\u001b[39;00m testing_data\u001b[39m.\u001b[39miterrows()])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Downloads/AIT/AT82.03%20Machine%20Learning%20%28ML%29/Laboratory/ML2022-06-Naive-Bayes/06-Naive-Bayes.ipynb#X46sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mGetting prediction for \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m test_docs[\u001b[39m0\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Downloads/AIT/AT82.03%20Machine%20Learning%20%28ML%29/Laboratory/ML2022-06-Naive-Bayes/06-Naive-Bayes.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m predict(test_docs[\u001b[39m0\u001b[39;49m])\n",
      "\u001b[1;32mc:\\Users\\Guntsv\\Downloads\\AIT\\AT82.03 Machine Learning (ML)\\Laboratory\\ML2022-06-Naive-Bayes\\06-Naive-Bayes.ipynb Cell 36\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Downloads/AIT/AT82.03%20Machine%20Learning%20%28ML%29/Laboratory/ML2022-06-Naive-Bayes/06-Naive-Bayes.ipynb#X46sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(sentence):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Downloads/AIT/AT82.03%20Machine%20Learning%20%28ML%29/Laboratory/ML2022-06-Naive-Bayes/06-Naive-Bayes.ipynb#X46sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     prob_statement \u001b[39m=\u001b[39m classCondProb(sentence, \u001b[39m'\u001b[39;49m\u001b[39mstatement\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m*\u001b[39m prior(\u001b[39m'\u001b[39m\u001b[39mstatement\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Downloads/AIT/AT82.03%20Machine%20Learning%20%28ML%29/Laboratory/ML2022-06-Naive-Bayes/06-Naive-Bayes.ipynb#X46sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     prob_question \u001b[39m=\u001b[39m classCondProb(sentence, \u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m*\u001b[39m prior(\u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Downloads/AIT/AT82.03%20Machine%20Learning%20%28ML%29/Laboratory/ML2022-06-Naive-Bayes/06-Naive-Bayes.ipynb#X46sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39mif\u001b[39;00m  prob_statement \u001b[39m>\u001b[39m prob_question:\n",
      "\u001b[1;32mc:\\Users\\Guntsv\\Downloads\\AIT\\AT82.03 Machine Learning (ML)\\Laboratory\\ML2022-06-Naive-Bayes\\06-Naive-Bayes.ipynb Cell 36\u001b[0m in \u001b[0;36mclassCondProb\u001b[1;34m(sentence, className)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Downloads/AIT/AT82.03%20Machine%20Learning%20%28ML%29/Laboratory/ML2022-06-Naive-Bayes/06-Naive-Bayes.ipynb#X46sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mif\u001b[39;00m className \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstatement\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Downloads/AIT/AT82.03%20Machine%20Learning%20%28ML%29/Laboratory/ML2022-06-Naive-Bayes/06-Naive-Bayes.ipynb#X46sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(word_list_s \u001b[39m==\u001b[39m word)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Downloads/AIT/AT82.03%20Machine%20Learning%20%28ML%29/Laboratory/ML2022-06-Naive-Bayes/06-Naive-Bayes.ipynb#X46sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     prob \u001b[39m=\u001b[39m prob \u001b[39m*\u001b[39m prob_s[np\u001b[39m.\u001b[39;49marray(idx)[\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m]]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Downloads/AIT/AT82.03%20Machine%20Learning%20%28ML%29/Laboratory/ML2022-06-Naive-Bayes/06-Naive-Bayes.ipynb#X46sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Downloads/AIT/AT82.03%20Machine%20Learning%20%28ML%29/Laboratory/ML2022-06-Naive-Bayes/06-Naive-Bayes.ipynb#X46sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(word_list_q \u001b[39m==\u001b[39m word)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 1 with size 0"
     ]
    }
   ],
   "source": [
    "test_docs = list([test['sentence'] for index,test in testing_data.iterrows()])\n",
    "print('Getting prediction for \"%s\"' % test_docs[0])\n",
    "predict(test_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c92cc1750ff6519f167950df416dbad7",
     "grade": false,
     "grade_id": "cell-9d86c9d269d1a550",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.1 (10 points)\n",
    "\n",
    "Explain Why it failed and explain how to solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a8b94cb831bb4a934e82d63cdf77be3",
     "grade": false,
     "grade_id": "cell-d424d31d1e17fd89",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "Explanation here! (Double click to explain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It said index 0 is out of bounds for axis 1 with size 0. \n",
    "#We can see this statment \"prob = prob * prob_s[np.array(idx)[0,0]\" \n",
    "#has required row and column so We have to reshape instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a281f0b0b151358f4dbbcbfb6adbd38",
     "grade": false,
     "grade_id": "cell-e17217b48752c1fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Exercise 1.2 (20 points)\n",
    "\n",
    "Modify your code and make it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laplace smooth adds words to each class that appears in other class but not in its own with a count 1\n",
    "# It also increments the counts of other words by 1 so as not to change the overall probabilities much\n",
    "\n",
    "def laplace_smooth(word_list_s, freq_list_s, word_list_q, freq_list_q):\n",
    "    s_extra = np.setdiff1d(word_list_q, word_list_s, assume_unique=True)\n",
    "    q_extra = np.setdiff1d(word_list_s, word_list_q, assume_unique=True)\n",
    "    \n",
    "    word_list_s = np.concatenate((word_list_s, s_extra))\n",
    "    word_list_q = np.concatenate((word_list_q, q_extra))  \n",
    "    \n",
    "    freq_list_s = np.concatenate((freq_list_s, np.zeros(s_extra.shape, dtype=int)))\n",
    "    freq_list_q = np.concatenate((freq_list_q, np.zeros(q_extra.shape, dtype=int)))\n",
    "    \n",
    "    freq_list_s = freq_list_s + 1\n",
    "    freq_list_q = freq_list_q + 1\n",
    "    \n",
    "    return word_list_s, freq_list_s, word_list_q, freq_list_q\n",
    "\n",
    "word_list_s, freq_list_s, word_list_q, freq_list_q = laplace_smooth(word_list_s, freq_list_s, word_list_q, freq_list_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of words for \"statement\" class \n",
      "\n",
      "{'are': 0.1111111111111111, 'author': 0.1111111111111111, 'book': 0.18518518518518517, 'bought': 0.1111111111111111, 'characters': 0.1111111111111111, 'fictional': 0.1111111111111111, 'has': 0.1111111111111111, 'how': 0.1111111111111111, 'i': 0.14814814814814814, 'is': 0.14814814814814814, 'like': 0.1111111111111111, 'more': 0.1111111111111111, 'my': 0.1111111111111111, 'novel': 0.1111111111111111, 'novels': 0.1111111111111111, 'one': 0.1111111111111111, 'than': 0.1111111111111111, 'the': 0.1111111111111111, 'they': 0.1111111111111111, 'this': 0.18518518518518517, 'favorite': 0.07407407407407407, 'have': 0.07407407407407407, 'read': 0.07407407407407407, 'what': 0.07407407407407407, 'who': 0.07407407407407407, 'you': 0.07407407407407407, 'your': 0.07407407407407407}\n",
      "------------------------------------------- \n",
      "\n",
      "Probability of words for \"question\" class \n",
      "\n",
      "{'are': 0.1111111111111111, 'author': 0.1111111111111111, 'book': 0.18518518518518517, 'characters': 0.1111111111111111, 'favorite': 0.1111111111111111, 'have': 0.1111111111111111, 'is': 0.18518518518518517, 'my': 0.1111111111111111, 'novels': 0.1111111111111111, 'read': 0.1111111111111111, 'the': 0.14814814814814814, 'this': 0.14814814814814814, 'what': 0.14814814814814814, 'who': 0.1111111111111111, 'you': 0.1111111111111111, 'your': 0.1111111111111111, 'bought': 0.07407407407407407, 'fictional': 0.07407407407407407, 'has': 0.07407407407407407, 'how': 0.07407407407407407, 'i': 0.07407407407407407, 'like': 0.07407407407407407, 'more': 0.07407407407407407, 'novel': 0.07407407407407407, 'one': 0.07407407407407407, 'than': 0.07407407407407407, 'they': 0.07407407407407407}\n"
     ]
    }
   ],
   "source": [
    "# Get word probabilities for statement class\n",
    "\n",
    "prob_s = []\n",
    "for word, count in zip(word_list_s, freq_list_s):\n",
    "    #print(word, count)\n",
    "    prob_s.append(count/len(word_list_s))\n",
    "    \n",
    "# Get word probabilities for question class\n",
    "\n",
    "prob_q = []\n",
    "for count in freq_list_q:\n",
    "    prob_q.append(count/len(word_list_q))\n",
    "    \n",
    "print('Probability of words for \"statement\" class \\n')\n",
    "print(dict(zip(word_list_s, prob_s)))\n",
    "print('------------------------------------------- \\n')\n",
    "print('Probability of words for \"question\" class \\n')\n",
    "print(dict(zip(word_list_q, prob_q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prior for one class\n",
    "\n",
    "def prior(className):    \n",
    "    denominator = len(stmt_docs) + len(question_docs)\n",
    "    \n",
    "    if className == 'statement':\n",
    "        numerator =  len(stmt_docs)\n",
    "    else:\n",
    "        numerator =  len(question_docs)\n",
    "        \n",
    "    return np.divide(numerator,denominator)\n",
    "    \n",
    "# Calculate class conditional probability for a sentence\n",
    "    \n",
    "def classCondProb(sentence, className):\n",
    "    words = get_words(sentence)\n",
    "#     print(\"words: \",words)\n",
    "    prob = 1\n",
    "    for word in words:\n",
    "        if className == 'statement':\n",
    "#             print(\"word is: \",word)\n",
    "            idx = np.where(word_list_s == word)\n",
    "            # print(\"Shape of idx is: \",idx)\n",
    "            prob = prob * prob_s[np.array(idx)[0,0]]\n",
    "        else:\n",
    "            idx = np.where(word_list_q == word)\n",
    "            # print(\"Shape of idx is: \",idx)\n",
    "            prob = prob * prob_q[np.array(idx)[0,0]]   \n",
    "    \n",
    "    return prob\n",
    "\n",
    "# Predict class of a sentence\n",
    "\n",
    "def predict(sentence):\n",
    "    prob_statement = classCondProb(sentence, 'statement') * prior('statement')\n",
    "    prob_question = classCondProb(sentence, 'question') * prior('question')\n",
    "    if  prob_statement > prob_question:\n",
    "        return 'statement'\n",
    "    else:\n",
    "        return 'question'\n",
    "\n",
    "def accuracy(test_docs, target):\n",
    "    correct = 0\n",
    "    for i in range(len(test_docs)):\n",
    "        predicted_as = predict(test_docs[i])\n",
    "        if predicted_as == target[i]:\n",
    "            correct += 1\n",
    "    return correct/len(test_docs) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classCondProb(sentence, className): #fix prob\n",
    "    words = get_words(sentence)\n",
    "    prob = 1\n",
    "    for word in words:\n",
    "        if className == 'statement':\n",
    "            idx = np.where(word_list_s == word)\n",
    "            if np.array(idx).shape[1] !=0:\n",
    "                prob = prob * prob_s[np.array(idx)[0,0]]\n",
    "            else:\n",
    "                prob = prob * prob_s[-1]    \n",
    "        else:\n",
    "            idx = np.where(word_list_q == word)\n",
    "            if np.array(idx).shape[1] !=0:\n",
    "                prob = prob * prob_q[np.array(idx)[0,0]]\n",
    "            else:\n",
    "                prob = prob * prob_q[-1]    \n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting prediction for \"this is the book\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'question'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_docs = list([[test['sentence']] for index,test in testing_data.iterrows()])\n",
    "print('Getting prediction for \"%s\"' % test_docs[0][0])\n",
    "predict(test_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b12f4c8d6b4163d2f6cfe464b3ba62f3",
     "grade": true,
     "grade_id": "cell-c576e7ed4dd3046a",
     "locked": true,
     "points": 20,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting prediction for this is the book\"\n",
      "statement\n",
      "Getting prediction for who are the novels characters\"\n",
      "question\n",
      "Getting prediction for is this the author\"\n",
      "statement\n",
      "Getting prediction for I like apples\"\n",
      "statement\n",
      "success!\n"
     ]
    }
   ],
   "source": [
    "# Test function: Do not remove\n",
    "test_docs = list([test['sentence'] for index,test in testing_data.iterrows()])\n",
    "\n",
    "for sentence in test_docs:\n",
    "    print('Getting prediction for %s\"' % sentence)\n",
    "    print(predict(sentence))\n",
    "    \n",
    "print(\"success!\")\n",
    "# End Test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "63a751b5f418dd7b04bf156032e8435f",
     "grade": false,
     "grade_id": "cell-12db07859804f68d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Expect result**:\\\n",
    "Getting prediction for this is the book\"\\\n",
    "question\\\n",
    "Getting prediction for who are the novels characters\"\\\n",
    "question\\\n",
    "Getting prediction for is this the author\"\\\n",
    "question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1a3302658ee7212f23942b2d4d93e70c",
     "grade": false,
     "grade_id": "cell-2bc1a154cce1dd7a",
     "locked": true,
     "points": 50,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "### Take home exercise\n",
    "\n",
    "Find a more substantial text classification dataset, clean up the documents, and build your NB classifier. Write a brief report on your in-lab and take home exercises and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"product_review.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_ID</th>\n",
       "      <th>Product_Description</th>\n",
       "      <th>Product_Type</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3057</td>\n",
       "      <td>The Web DesignerÛªs Guide to iOS (and Android...</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6254</td>\n",
       "      <td>RT @mention Line for iPad 2 is longer today th...</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8212</td>\n",
       "      <td>Crazy that Apple is opening a temporary store ...</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4422</td>\n",
       "      <td>The lesson from Google One Pass: In this digit...</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5526</td>\n",
       "      <td>RT @mention At the panel: &amp;quot;Your mom has a...</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Text_ID                                Product_Description  Product_Type  \\\n",
       "0     3057  The Web DesignerÛªs Guide to iOS (and Android...             9   \n",
       "1     6254  RT @mention Line for iPad 2 is longer today th...             9   \n",
       "2     8212  Crazy that Apple is opening a temporary store ...             9   \n",
       "3     4422  The lesson from Google One Pass: In this digit...             9   \n",
       "4     5526  RT @mention At the panel: &quot;Your mom has a...             9   \n",
       "\n",
       "   Sentiment  \n",
       "0          2  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Guntsv\\AppData\\Local\\Temp\\ipykernel_2940\\468263207.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  data['Product_Description'] = data['Product_Description'].str.replace('[^a-zA-Z0-9 \\n\\.]|\\.', '').str.lower()\n"
     ]
    }
   ],
   "source": [
    "## remove some special characters and convert to lower case\n",
    "data['Product_Description'] = data['Product_Description'].str.replace('[^a-zA-Z0-9 \\n\\.]|\\.', '').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_ID</th>\n",
       "      <th>Product_Description</th>\n",
       "      <th>Product_Type</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3057</td>\n",
       "      <td>the web designers guide to ios and android app...</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6254</td>\n",
       "      <td>rt mention line for ipad 2 is longer today tha...</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8212</td>\n",
       "      <td>crazy that apple is opening a temporary store ...</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4422</td>\n",
       "      <td>the lesson from google one pass in this digita...</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5526</td>\n",
       "      <td>rt mention at the panel quotyour mom has an ip...</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Text_ID                                Product_Description  Product_Type  \\\n",
       "0     3057  the web designers guide to ios and android app...             9   \n",
       "1     6254  rt mention line for ipad 2 is longer today tha...             9   \n",
       "2     8212  crazy that apple is opening a temporary store ...             9   \n",
       "3     4422  the lesson from google one pass in this digita...             9   \n",
       "4     5526  rt mention at the panel quotyour mom has an ip...             9   \n",
       "\n",
       "   Sentiment  \n",
       "0          2  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    3765\n",
       "3    2089\n",
       "1     399\n",
       "0     111\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self, with_prior):\n",
    "        self.word_freqs = {} \n",
    "        self.n_words = {}\n",
    "        self.y_class = {}\n",
    "        self.with_prior = with_prior\n",
    "        \n",
    "    def get_word_frequency(self, sentence_array):\n",
    "        # make text_array into 1d array \n",
    "        sentence_series = pd.Series(sentence_array)\n",
    "        sentence_series += ' ' # add space for spliting words\n",
    "        word_series = pd.Series(sentence_series.sum().split(' '))\n",
    "        word_series = word_series[word_series != ' '] # drop unwanted elements\n",
    "        return word_series.value_counts().to_dict(), len(word_series)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # X => array of vector\n",
    "        # y => array of class\n",
    "        self.y_class = pd.Series(y).value_counts().to_dict()\n",
    "        for y_class in self.y_class : \n",
    "            y_index = np.where(y == y_class)\n",
    "            \n",
    "            y_X = X.copy()[y_index]\n",
    "            self.word_freqs[y_class], self.n_words[y_class] = self.get_word_frequency(y_X)\n",
    "            \n",
    "        all_X = X.copy()\n",
    "        self.word_freqs['all'], self.n_words['all'] = self.get_word_frequency(all_X)\n",
    "        self.y_class['all'] = X.shape[0] # count the number of samples\n",
    "        \n",
    "    def predict(self, X, alpha=1):\n",
    "        d = len(self.word_freqs['all']) # the number of possible words in all categories\n",
    "        pred_df = pd.DataFrame()        \n",
    "        n = self.y_class['all'] # number of training size\n",
    "        for y_class in self.y_class :\n",
    "            if y_class != 'all' : \n",
    "                y_hat = []\n",
    "                n_y = self.y_class[y_class] # the number of class y in training set \n",
    "                n_wy = self.n_words[y_class] # the number of words occur in y_class \n",
    "                for sentence in X : \n",
    "                    p_xy = 1\n",
    "                    words = pd.Series(sentence.split(' '))                \n",
    "                    for w in words : \n",
    "                        n_xy = 0\n",
    "                        if w in self.word_freqs[y_class] : \n",
    "                            n_xy = self.word_freqs[y_class][w] # the number of word x happens in class y   \n",
    "                        p_xy *= (n_xy + alpha) / (n_wy + alpha*d) \n",
    "\n",
    "                    if self.with_prior :\n",
    "                        p_xy *= n_y / n\n",
    "                    y_hat.append(p_xy)               \n",
    "                pred_df[y_class] = y_hat\n",
    "    \n",
    "        pred_df['y_pred'] = np.argmax(pred_df.values, axis=1)\n",
    "        pred_df['y_pred'] = [pred_df.columns[v] for v in pred_df['y_pred']]\n",
    "        self.pred_df = pred_df\n",
    "        return pred_df['y_pred'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into test and train set\n",
    "import random\n",
    "def split_dataset(df, training_ratio=0.6):\n",
    "    m, n = df.shape\n",
    "    idx = np.arange(0, m)\n",
    "    random.shuffle(idx)\n",
    "    m_train = int(m * training_ratio)\n",
    "    train_idx = idx[0:m_train]\n",
    "    test_idx = idx[m_train:]\n",
    "    return df.iloc[train_idx], df.iloc[test_idx]\n",
    "data_train , data_test = split_dataset(data, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train['Product_Description'].values\n",
    "y_train = data_train['Sentiment'].values\n",
    "X_test= data_test['Product_Description'].values\n",
    "y_test = data_test['Sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = NaiveBayes(True)\n",
    "m.fit(X_train, y_train)\n",
    "train_pred = m.predict(X_train)\n",
    "test_pred = m.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "Training set accuracy 0.8218424670988018\n",
      "Test set accuracy 0.6433621366849961\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy')\n",
    "print('Training set accuracy', get_accuracy(train_pred, y_train))\n",
    "print('Test set accuracy', get_accuracy(test_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: \n",
    "\n",
    "Part 1: Handle some values\n",
    "\n",
    "- Remove some special characters and convert to lower cases to reduce error effect\n",
    "\n",
    "Part 2: Setup X and Y data\n",
    "\n",
    "- Test Train Split Data as Training set and test set\n",
    "\n",
    "Part 3: Train data using Naive Bayes\n",
    "\n",
    "- create Naviy Bayes Class\n",
    "\n",
    "- fit model \n",
    "\n",
    "- predict model\n",
    "\n",
    "- Accuracy\n",
    "\n",
    "    From training set, Accuracy has shown 82.2%.\n",
    "    \n",
    "    From testing set, Accuracy has shown 64.33%.\n",
    "\n",
    "    The test accuract value is low than training set obviously, it may cause the unbalanced dataset that\n",
    "\n",
    "    we have in terms of number of data for each sentiment value. \n",
    "\n",
    "    As below coding has show that we should add more data set of sentiment 0 and 1\n",
    "\n",
    "    Preprossesing Data is needed to make sure that testing accuracy will be reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    3765\n",
       "3    2089\n",
       "1     399\n",
       "0     111\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd_0 = data.loc[data['Sentiment'] == 0]\n",
    "# pd_0.shape\n",
    "# pd_1 = data.loc[data['Sentiment'] == 1].sample(n=111, random_state=999)\n",
    "# pd_1.shape\n",
    "# pd_2 = data.loc[data['Sentiment'] == 2].sample(n=111, random_state=999)\n",
    "# pd_2.shape\n",
    "# pd_3 = data.loc[data['Sentiment'] == 3].sample(n=111, random_state=999)\n",
    "# pd_3.shape\n",
    "# data = pd.concat([pd_0,pd_1, pd_2,pd_3])\n",
    "# data['Sentiment'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
