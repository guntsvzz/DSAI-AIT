{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Exam, Machine Learning, August 2022 Semester, AIT\n",
    "\n",
    "Happy Thursday! This is the final exam for Machine Learning in the August 2022 semester.\n",
    "\n",
    "This exam is 2.5 hours long. Once the exam starts, you will have exactly 2.5 hours to finish your work\n",
    "and upload your notebook to Google Classroom.\n",
    "\n",
    "Please fill in this notebook with your code and short answers. Be sure to put all of your code\n",
    "in the cells marked with\n",
    "\n",
    "    # YOUR CODE GOES HERE\n",
    "    \n",
    "and please put your answers to the short answer questions exactly where you see the remark\n",
    "\n",
    "*You answer goes here.*\n",
    "\n",
    "Be complete and precise in your answers! Be sure to answer the question that's being asked. Don't dump random information in the hope that it'll give you partial credit. I give generous partial credit, but I will deduct points for answers that are not on point.\n",
    "\n",
    "Also beware that if I discover any cheating, I will give you a 0 for the entire exam, or worse, and you will likely fail the class. Just don't do it!\n",
    "\n",
    "OK, that's all for the advice. Relax, take a deep breath, and good luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (10 points)\n",
    "\n",
    "Consider the following code for the toy cliff walking problem in reinforcement learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action U in state (3, 0) gives new state (2, 0) reward -1.0 and terminal status False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Environment receives a current state and action and returns a new state and reward\n",
    "\n",
    "ROWS = 4\n",
    "COLS = 12\n",
    "START = (3, 0)\n",
    "GOAL = (3, 11)\n",
    "\n",
    "A = ['U', 'D', 'L', 'R']\n",
    "\n",
    "def env(s, a):\n",
    "    row = s[0]\n",
    "    col = s[1]\n",
    "    if a == 'U':\n",
    "        row -= 1\n",
    "    elif a == 'D':\n",
    "        row += 1\n",
    "    elif a == 'L':\n",
    "        col -= 1\n",
    "    elif a == 'R':\n",
    "        col += 1\n",
    "    else:\n",
    "        raise Exception(\"Invalid action '\" + a + \"'\")\n",
    "        \n",
    "    if row < 0:\n",
    "        row = 0\n",
    "    if row >= ROWS:\n",
    "        row = ROWS-1\n",
    "    if col < 0:\n",
    "        col = 0\n",
    "    if col >= COLS:\n",
    "        col = COLS-1\n",
    "\n",
    "    s = (row, col)\n",
    "    \n",
    "    if row < ROWS-1 or s == START:\n",
    "        return s, -1.0, False\n",
    "\n",
    "    if s == GOAL:\n",
    "        return GOAL, 0.0, True\n",
    "    \n",
    "    return START, -100.0, True\n",
    "\n",
    "# Set up Q table\n",
    "\n",
    "def init_q():\n",
    "    Q = {}\n",
    "    for row in range(ROWS):\n",
    "        for col in range(COLS):\n",
    "            Q[(row, col)] = np.array([ 0.0 for i in range(len(A))])\n",
    "    return Q\n",
    "\n",
    "# Epsilon greedy policy\n",
    "\n",
    "epsilon = 0.1\n",
    "\n",
    "def eps_greedy(Q, s):\n",
    "    if np.random.uniform() < epsilon:\n",
    "        return np.random.choice(len(A))\n",
    "    return np.argmax(Q[s])\n",
    "\n",
    "# Example of how to use the functions\n",
    "\n",
    "Q = init_q()\n",
    "s = START\n",
    "a = eps_greedy(Q, s)\n",
    "ns, r, terminal = env(s, A[a])\n",
    "\n",
    "print('Action', A[a], 'in state', s, 'gives new state', ns, 'reward', r, 'and terminal status', terminal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the space below, fill in the implementation of the Q-learning algorithm.\n",
    "After running Q-learning to convergence, output the resulting policy as a mapping from states to optimal actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Not implemented yet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f94ca9ed79fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mlearn_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_learn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_q_max\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f94ca9ed79fe>\u001b[0m in \u001b[0;36mq_learn\u001b[0;34m(episodes, gamma, alpha)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;31m# Q-learning update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;31m# YOUR CODE HERE to implement the Q table update for Q learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Not implemented yet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Not implemented yet"
     ]
    }
   ],
   "source": [
    "# Number of episodes to run\n",
    "\n",
    "def q_learn(episodes, gamma, alpha):\n",
    "    Q = init_q()\n",
    "    for episode in range(episodes):\n",
    "        s = START\n",
    "        terminal = False\n",
    "        while not terminal:\n",
    "            a = eps_greedy(Q, s)\n",
    "            ns, r, terminal = env(s, A[a])\n",
    "            # Q-learning update\n",
    "            # YOUR CODE HERE to implement the Q table update for Q learning\n",
    "            raise Exception('Not implemented yet')\n",
    "            s = ns\n",
    "    return Q\n",
    "\n",
    "episodes = 10000\n",
    "gamma = 0.9\n",
    "learn_rate = 0.1\n",
    "Q = q_learn(episodes, gamma, learn_rate)\n",
    "\n",
    "def print_q_max(Q):\n",
    "    print('-------------------------')\n",
    "    for row in range(ROWS):\n",
    "        for col in range(COLS):\n",
    "            if (row, col) == GOAL:\n",
    "                print('|G', end='')\n",
    "            elif row == ROWS-1 and col > 0:\n",
    "                print('|X', end='')\n",
    "            else:\n",
    "                print('|%s' % A[Q[(row, col)].argmax()], end='')\n",
    "        print('|')\n",
    "        print('-------------------------')\n",
    "\n",
    "\n",
    "# Print out the optimal policy learned by Q-learning\n",
    "# YOUR CODE HERE to print out the optimal policy\n",
    "raise Exception('Not implemented yet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (10 points)\n",
    "\n",
    "Write a function `sarsa_learn` similar to the `q_learn` function in the previous question that implements\n",
    "the SARSA reinforcement learning algorithm for the cliff walking problem.\n",
    "\n",
    "After running SARSA to convergence, output the optimal policy according to the resulting Q table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Not implemented yet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-10a4b505b245>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Not implemented yet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: Not implemented yet"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "raise Exception('Not implemented yet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (10 points)\n",
    "\n",
    "Explain why the optimal policies in Question 1 and Question 2 are different, even though the environment\n",
    "is the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (10 points)\n",
    "\n",
    "The Gaussian mixture model for clustering is a probabilistic model with negative log likelihood as the cost function.\n",
    "\n",
    "The k-means clustering method uses a simpler cost function.\n",
    "\n",
    "In the space below, explain the differences between the two cost functions.\n",
    "\n",
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (10 points)\n",
    "\n",
    "We saw that the cost function for linear regression can be derived as a negative log likelihood cost\n",
    "function when we assume Gaussian errors for the targets (i.e., $y \\sim \\mathcal{N}(\\mathbf{\\theta}^T \\mathbf{x}, \\sigma^2)$).\n",
    "\n",
    "Is it possible for the k-means cost function to be derived in a similar way based on some probabilistic\n",
    "model for the data distribution? What about the cluster membership?\n",
    "\n",
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 (30 points)\n",
    "\n",
    "In PyTorch, implement the neural network given at the Tensorflow Playground with these parameters:\n",
    "\n",
    "https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=15&networkShape=4,2&seed=0.40825&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false\n",
    "\n",
    "(The network has two ReLU hidden layers with 4 and 2 units, respectively.)\n",
    "\n",
    "Demonstrate that it can solve the 2D annulus classification problem.\n",
    "\n",
    "To solve this problem, you should generate a synthetic training set as we have done in lab in the past\n",
    "and show that your PyTorch neural network can learn to distinguish the two classes using a plot. Note that there\n",
    "should be some noise/overlap between the two class distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 (10 points)\n",
    "\n",
    "Explain how we should set up the hyperparameters of a SVM (kernel, c, other parameters)\n",
    "to solve the problem in Question 6. You don't have to write any code! Just explain how you would\n",
    "set up the hyperparamters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your explanation here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8 (10 points)\n",
    "\n",
    "Write a numpy function to transform the $\\mathbf{x}$ data from Question 6 to a\n",
    "new 2D repesentation that would enable a logistic regression model to obtain good accuracy.\n",
    "\n",
    "Demonstrate that your function transforms the training set from Question 6 effectively (plot the transformed\n",
    "data).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
