{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Word2Vec\n",
    "1.  Try a real corpus (instead of banana apple, try something real... on the internet....) - not so big!  \n",
    "\n",
    "Just you have a good taste of real stuff....like 50 documents, each have 50 words....\n",
    "\n",
    "2. Try window size of 2\n",
    "\n",
    "3. Try CBOW (instead of skipgrams)\n",
    "\n",
    "4. Compare normal version of skipgrams vs. negative sampling version of skipgrams in terms of time (using real corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.2'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import pandas as pd\n",
    "spacy.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "A real corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('./en_core_web_sm/')\n",
    "text = open('./dataset/alchemist.txt',mode='r') #change later\n",
    "df = pd.DataFrame(text.readlines())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df_col):\n",
    "    corpus = []\n",
    "    for item in df_col:\n",
    "        item = re.sub('[^A-Za-z0-9]+', ' ', str(item)) # remove special characters\n",
    "        item = item.lower() # lower all characters\n",
    "        item = item.split() # split data\n",
    "        corpus.append(' '.join(str(x) for x in item))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. tokenize\n",
    "#data cleaned\n",
    "corpus = clean_data(df[0])\n",
    "#data tokenized\n",
    "corpus_tokenized = [sent.split(\" \") for sent in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1233"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. numericalize (vocab)\n",
    "#2.1 get all the unique words\n",
    "#we want to flatten unit (basically merge all list)\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabs = list(set(flatten(corpus_tokenized)))\n",
    "\n",
    "#2.2 assign id to all these vocabs\n",
    "word2index = {v: idx for idx, v in enumerate(vocabs)}\n",
    "\n",
    "#adding unknown word\n",
    "vocabs.append('<UNK>')\n",
    "word2index['<UNK>'] = 1233\n",
    "\n",
    "voc_size = len(vocabs)\n",
    "voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgrams = []\n",
    "cbow = []\n",
    "for sent in corpus_tokenized:\n",
    "    #for each sent ('apple', 'banana', 'fruit')\n",
    "    for i in range(2,len(sent)-2): #start from 1 to second last\n",
    "        center_word = sent[i]\n",
    "        outside_word = [sent[i-2],sent[i-1],sent[i+1],sent[i+2]] #window size = 2\n",
    "        cbow.append((outside_word, center_word))\n",
    "        for o in outside_word: \n",
    "            skipgrams.append((center_word,o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('crowning', 'high'),\n",
       " ('crowning', 'up'),\n",
       " ('crowning', 'the'),\n",
       " ('crowning', 'grassy'),\n",
       " ('the', 'up')]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgrams[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['high', 'up', 'the', 'grassy'], 'crowning'),\n",
       " (['up', 'crowning', 'grassy', 'summit'], 'the'),\n",
       " (['crowning', 'the', 'summit', 'of'], 'grassy'),\n",
       " (['the', 'grassy', 'of', 'a'], 'summit')]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Bag-of-Words (CBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare normal version of skipgrams vs. negative sampling version of skipgrams in terms of time (using real corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, corpus):\n",
    "    skipgrams = []\n",
    "    #for each corpus\n",
    "    for sent in corpus_tokenized:\n",
    "        #for each sent ('apple', 'banana', 'fruit')\n",
    "        for i in range(2,len(sent)-2): #start from 1 to second last\n",
    "            # print(sent[i])\n",
    "            center_word = word2index[sent[i]]\n",
    "            outside_word = [word2index[sent[i-2]],word2index[sent[i-1]],word2index[sent[i+1]],word2index[sent[i+2]]] #window_size = 2\n",
    "            #here we want to create (banana, apple), (banana, fruit) append to some list\n",
    "            for o in outside_word:\n",
    "                skipgrams.append([center_word,o])\n",
    "    #only get a batch, mot the entire lsit\n",
    "    random_index = np.random.choice(range(len(skipgrams)),batch_size,replace=False)\n",
    "    \n",
    "    #appending some list of inputs and labels\n",
    "    random_inputs, random_labels = [] , []\n",
    "    for index in random_index:\n",
    "        random_inputs.append([skipgrams[index][0]]) #center words, this will be as shape of (1,) -> (1,1) for modeling\n",
    "        random_labels.append([skipgrams[index][1]])\n",
    "\n",
    "    return np.array(random_inputs),np.array(random_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1233])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preparing all_vocabs\n",
    "batch_size = 2\n",
    "\n",
    "def prepare_seqeunce(seq, word2index):\n",
    "    #map(fucntion, list of something)\n",
    "    #map will look at each of element in this list, and apply this function\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_seqeunce(list(vocabs),word2index).expand(batch_size, voc_size)\n",
    "all_vocabs.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skipgram(nn.Module):\n",
    "    def __init__(self,voc_size, emb_size):\n",
    "        super(Skipgram,self).__init__()\n",
    "        self.embedding_center_word = nn.Embedding(voc_size, emb_size) #is a lookup table mapping all ids in voc_size, into some vector of size emb_size\n",
    "        self.embedding_outside_word = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center_word, outside_word, all_vocabs):\n",
    "        #center_word, outside_word: (batch_size,1)\n",
    "        #all_vocabs : (batch_size, voc_size)\n",
    "        #convert them into embedding\n",
    "        center_word_embed = self.embedding_center_word(center_word)     #v_c (batch_size,1, emb_size)\n",
    "        outside_word_embed = self.embedding_outside_word(outside_word)  #u_o (batch_size,1, emb_size)\n",
    "        all_vocabs_embed = self.embedding_outside_word(all_vocabs)      #u_w (batch_size,voc_size, emb_size)\n",
    "        print(center_word_embed.shape,outside_word_embed.shape,all_vocabs_embed.shape)\n",
    "        #bmm is basically @ or .dot but across batches (ie., ignore the batch dimension)\n",
    "        top_term = outside_word_embed.bmm(center_word_embed.transpose(1,2)).squeeze(2)\n",
    "        #(batch_size,1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) ===> (batch_size, 1)\n",
    "        top_term_exp = torch.exp(top_term) #exp(uo vc)\n",
    "        #(batch_size, 1)\n",
    "        lower_term = all_vocabs_embed.bmm(center_word_embed.transpose(1,2)).squeeze(2)\n",
    "        #(batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) ===> (batch_size, voc_size)\n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term)) #sum exp(uw, vc)\n",
    "        #(batch_size, 1)\n",
    "        loss_fn = -torch.mean(torch.log(top_term_exp/lower_term_sum))\n",
    "        #(batc_size,1) / (batch_size,1) ==mena==> scalar\n",
    "        return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1233])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preparing all_vocabs\n",
    "batch_size = 2\n",
    "\n",
    "def prepare_seqeunce(seq, word2index):\n",
    "    #map(fucntion, list of something)\n",
    "    #map will look at each of element in this list, and apply this function\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_seqeunce(list(vocabs),word2index).expand(batch_size, voc_size)\n",
    "all_vocabs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1]), torch.Size([2, 1]))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, label = random_batch(2, corpus_tokenized)\n",
    "input_tensor = torch.LongTensor(input)\n",
    "label_tensor  = torch.LongTensor(label)\n",
    "input_tensor.shape,label_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 #why? no reason\n",
    "emb_size = 2 #why? no reason; usually 50,100, 300 but 2 so we can plot (50 can also plot, but need PCA)\n",
    "model = Skipgram(voc_size,emb_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #-log\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1]) torch.Size([2, 1]) torch.Size([2, 1233])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\DSAI-AIT-2022\\Course\\Natural Language Understanding\\Assignment\\01 - Word2Vec.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Natural%20Language%20Understanding/Assignment/01%20-%20Word2Vec.ipynb#X31sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(input_batch\u001b[39m.\u001b[39mshape,label_batch\u001b[39m.\u001b[39mshape,all_vocabs\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Natural%20Language%20Understanding/Assignment/01%20-%20Word2Vec.ipynb#X31sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# break\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Natural%20Language%20Understanding/Assignment/01%20-%20Word2Vec.ipynb#X31sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Natural%20Language%20Understanding/Assignment/01%20-%20Word2Vec.ipynb#X31sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#loss = model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Natural%20Language%20Understanding/Assignment/01%20-%20Word2Vec.ipynb#X31sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m model(input_batch,label_batch,all_vocabs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Natural%20Language%20Understanding/Assignment/01%20-%20Word2Vec.ipynb#X31sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#backpropagate\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Natural%20Language%20Understanding/Assignment/01%20-%20Word2Vec.ipynb#X31sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\DSAI-AIT-2022\\Course\\Natural Language Understanding\\Assignment\\01 - Word2Vec.ipynb Cell 25\u001b[0m in \u001b[0;36mSkipgram.forward\u001b[1;34m(self, center_word, outside_word, all_vocabs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Natural%20Language%20Understanding/Assignment/01%20-%20Word2Vec.ipynb#X31sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m center_word_embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_center_word(center_word)     \u001b[39m#v_c (batch_size,1, emb_size)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Natural%20Language%20Understanding/Assignment/01%20-%20Word2Vec.ipynb#X31sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m outside_word_embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_outside_word(outside_word)  \u001b[39m#u_o (batch_size,1, emb_size)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Natural%20Language%20Understanding/Assignment/01%20-%20Word2Vec.ipynb#X31sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m all_vocabs_embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding_outside_word(all_vocabs)      \u001b[39m#u_w (batch_size,voc_size, emb_size)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Natural%20Language%20Understanding/Assignment/01%20-%20Word2Vec.ipynb#X31sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mprint\u001b[39m(center_word_embed\u001b[39m.\u001b[39mshape,outside_word_embed\u001b[39m.\u001b[39mshape,all_vocabs_embed\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Natural%20Language%20Understanding/Assignment/01%20-%20Word2Vec.ipynb#X31sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#bmm is basically @ or .dot but across batches (ie., ignore the batch dimension)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[0;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[0;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_epochs = 5000\n",
    "#for epoch\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    #get random batch\n",
    "    input_batch, label_batch = random_batch(batch_size,corpus)\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    label_batch = torch.LongTensor(label_batch)\n",
    "\n",
    "    print(input_batch.shape,label_batch.shape,all_vocabs.shape)\n",
    "    # break\n",
    "\n",
    "    #loss = model\n",
    "    loss = model(input_batch,label_batch,all_vocabs)\n",
    "    #backpropagate\n",
    "    loss.backward()\n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    #print epoch loss\n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "\n",
    "    # if (epoch + 1) % 1000 == 0:\n",
    "    print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'digressed': 21,\n",
       "         'primeval': 21,\n",
       "         'trapdoor': 21,\n",
       "         'with': 263,\n",
       "         'disliking': 21,\n",
       "         'knowledge': 21,\n",
       "         'beard': 21,\n",
       "         'branch': 21,\n",
       "         'works': 21,\n",
       "         'high': 35,\n",
       "         'pierre': 70,\n",
       "         'rous': 21,\n",
       "         'nearby': 21,\n",
       "         'humanity': 21,\n",
       "         'education': 21,\n",
       "         'spirit': 21,\n",
       "         'sinister': 48,\n",
       "         'snatched': 21,\n",
       "         'forest': 35,\n",
       "         'dwelled': 21,\n",
       "         'race': 21,\n",
       "         'spell': 35,\n",
       "         'gold': 21,\n",
       "         'familiar': 21,\n",
       "         'directly': 21,\n",
       "         'seeking': 21,\n",
       "         'days': 48,\n",
       "         'woods': 35,\n",
       "         'lips': 21,\n",
       "         'my': 542,\n",
       "         'speaking': 21,\n",
       "         'feudalism': 21,\n",
       "         'language': 21,\n",
       "         'proclaiming': 21,\n",
       "         'particularly': 21,\n",
       "         'rational': 21,\n",
       "         'custom': 21,\n",
       "         'discuss': 21,\n",
       "         'line': 90,\n",
       "         'murd': 21,\n",
       "         'steadily': 21,\n",
       "         'studied': 21,\n",
       "         'mouth': 21,\n",
       "         'attribute': 21,\n",
       "         'glass': 21,\n",
       "         'capable': 21,\n",
       "         'each': 48,\n",
       "         'malice': 21,\n",
       "         'wizard': 35,\n",
       "         'late': 35,\n",
       "         'secrets': 21,\n",
       "         'scene': 21,\n",
       "         'things': 21,\n",
       "         'meaning': 21,\n",
       "         'provisions': 21,\n",
       "         'short': 21,\n",
       "         'fateful': 21,\n",
       "         'ago': 21,\n",
       "         'unable': 21,\n",
       "         'afterward': 59,\n",
       "         'pausing': 21,\n",
       "         'defied': 21,\n",
       "         'black': 80,\n",
       "         'fallen': 21,\n",
       "         'meadowland': 21,\n",
       "         'pursuits': 35,\n",
       "         'piteous': 21,\n",
       "         'just': 21,\n",
       "         'apprehensions': 21,\n",
       "         'castle': 90,\n",
       "         'small': 70,\n",
       "         'resolved': 35,\n",
       "         'partake': 21,\n",
       "         'ne': 21,\n",
       "         'appeared': 35,\n",
       "         'least': 21,\n",
       "         'neighboring': 21,\n",
       "         'beheld': 21,\n",
       "         'exceeding': 21,\n",
       "         'a': 470,\n",
       "         'circumstance': 35,\n",
       "         'tell': 35,\n",
       "         'wainscots': 21,\n",
       "         'two': 90,\n",
       "         'allude': 21,\n",
       "         'care': 35,\n",
       "         'sagging': 21,\n",
       "         'be': 80,\n",
       "         'it': 160,\n",
       "         'reputation': 21,\n",
       "         'years': 135,\n",
       "         'strangest': 21,\n",
       "         'myself': 59,\n",
       "         'feared': 21,\n",
       "         'innocent': 21,\n",
       "         'hope': 21,\n",
       "         'placed': 21,\n",
       "         'commenced': 21,\n",
       "         'ending': 21,\n",
       "         'otherwise': 21,\n",
       "         'permitted': 21,\n",
       "         'body': 21,\n",
       "         'stained': 35,\n",
       "         'menaced': 21,\n",
       "         'changed': 21,\n",
       "         'reached': 21,\n",
       "         'saw': 48,\n",
       "         'man': 127,\n",
       "         'cottage': 35,\n",
       "         'approached': 48,\n",
       "         'terminated': 21,\n",
       "         'existence': 59,\n",
       "         'instinct': 21,\n",
       "         'associates': 21,\n",
       "         'now': 127,\n",
       "         'else': 21,\n",
       "         'exhaling': 21,\n",
       "         'connect': 21,\n",
       "         'rank': 21,\n",
       "         'restriction': 21,\n",
       "         'event': 21,\n",
       "         'eye': 21,\n",
       "         'great': 80,\n",
       "         'seek': 35,\n",
       "         'impending': 21,\n",
       "         'antoine': 21,\n",
       "         'manifest': 21,\n",
       "         'would': 59,\n",
       "         'stone': 59,\n",
       "         'imagine': 21,\n",
       "         'clothed': 21,\n",
       "         'wander': 21,\n",
       "         'noble': 35,\n",
       "         'huge': 35,\n",
       "         'thine': 21,\n",
       "         'natural': 35,\n",
       "         'hill': 48,\n",
       "         'tenantry': 21,\n",
       "         'impotent': 21,\n",
       "         'words': 59,\n",
       "         'more': 109,\n",
       "         'die': 21,\n",
       "         'melancholy': 21,\n",
       "         'mount': 21,\n",
       "         'of': 1132,\n",
       "         'acute': 21,\n",
       "         'peasants': 35,\n",
       "         'revenge': 35,\n",
       "         'further': 35,\n",
       "         'heirs': 21,\n",
       "         'hear': 21,\n",
       "         'gorgeously': 21,\n",
       "         'sorcerers': 21,\n",
       "         'claimed': 21,\n",
       "         'encrusted': 21,\n",
       "         'horror': 21,\n",
       "         'paved': 21,\n",
       "         'lives': 35,\n",
       "         'shining': 21,\n",
       "         'debased': 21,\n",
       "         'generations': 35,\n",
       "         'length': 35,\n",
       "         'firm': 21,\n",
       "         'pronounced': 21,\n",
       "         'for': 199,\n",
       "         'immovable': 21,\n",
       "         'whole': 35,\n",
       "         'fiendish': 21,\n",
       "         'resisting': 21,\n",
       "         'roaming': 21,\n",
       "         'animated': 21,\n",
       "         'will': 21,\n",
       "         'amongst': 35,\n",
       "         'top': 21,\n",
       "         'evidence': 21,\n",
       "         'careful': 35,\n",
       "         'terror': 21,\n",
       "         'filial': 21,\n",
       "         'inhuman': 21,\n",
       "         'company': 21,\n",
       "         'sparkled': 21,\n",
       "         'kind': 21,\n",
       "         'escaped': 21,\n",
       "         'toward': 21,\n",
       "         'broke': 21,\n",
       "         'fact': 21,\n",
       "         'assassination': 21,\n",
       "         'curtain': 21,\n",
       "         'scorn': 21,\n",
       "         'nightmare': 21,\n",
       "         'could': 59,\n",
       "         'grant': 21,\n",
       "         'drowned': 21,\n",
       "         'source': 21,\n",
       "         'another': 21,\n",
       "         'lived': 35,\n",
       "         'degree': 21,\n",
       "         'held': 35,\n",
       "         'reconciled': 21,\n",
       "         'sensations': 21,\n",
       "         'le': 118,\n",
       "         'far': 35,\n",
       "         'save': 21,\n",
       "         'only': 48,\n",
       "         'too': 35,\n",
       "         'flight': 21,\n",
       "         'family': 59,\n",
       "         'him': 70,\n",
       "         'retrace': 21,\n",
       "         'fulfilled': 35,\n",
       "         'viewed': 21,\n",
       "         'unfortunate': 21,\n",
       "         'passed': 35,\n",
       "         'kill': 21,\n",
       "         'did': 21,\n",
       "         'pause': 21,\n",
       "         'lately': 21,\n",
       "         'deadly': 21,\n",
       "         'possessor': 21,\n",
       "         'so': 80,\n",
       "         'backwards': 21,\n",
       "         'thy': 21,\n",
       "         'murder': 21,\n",
       "         'delved': 21,\n",
       "         'latent': 21,\n",
       "         'around': 21,\n",
       "         'shrank': 21,\n",
       "         'longest': 21,\n",
       "         'senses': 21,\n",
       "         'confronted': 21,\n",
       "         'near': 48,\n",
       "         'parent': 21,\n",
       "         'whereupon': 21,\n",
       "         'marble': 21,\n",
       "         'learn': 35,\n",
       "         'er': 21,\n",
       "         'arrow': 35,\n",
       "         'next': 35,\n",
       "         'already': 21,\n",
       "         'skeleton': 21,\n",
       "         'magnified': 21,\n",
       "         'spacious': 21,\n",
       "         'horrible': 21,\n",
       "         'were': 199,\n",
       "         'proficient': 21,\n",
       "         'mind': 59,\n",
       "         'hideous': 48,\n",
       "         'thoroughly': 21,\n",
       "         'frame': 21,\n",
       "         'opponent': 21,\n",
       "         'vast': 21,\n",
       "         'thrown': 35,\n",
       "         'gunpowder': 21,\n",
       "         'courtyard': 21,\n",
       "         'footsteps': 21,\n",
       "         'impregnable': 21,\n",
       "         'reluctance': 21,\n",
       "         'released': 21,\n",
       "         'walls': 59,\n",
       "         'surround': 21,\n",
       "         'gave': 35,\n",
       "         'experience': 21,\n",
       "         'stood': 35,\n",
       "         'descent': 21,\n",
       "         'self': 21,\n",
       "         'grandeur': 21,\n",
       "         'falling': 21,\n",
       "         'surroundings': 21,\n",
       "         'passage': 48,\n",
       "         'utmost': 21,\n",
       "         'wife': 21,\n",
       "         'subterranean': 21,\n",
       "         'vengeful': 21,\n",
       "         'meet': 21,\n",
       "         'hour': 35,\n",
       "         'dread': 35,\n",
       "         'you': 80,\n",
       "         'found': 80,\n",
       "         'gnarled': 35,\n",
       "         'might': 35,\n",
       "         'maintain': 21,\n",
       "         'our': 48,\n",
       "         'issued': 21,\n",
       "         'mark': 21,\n",
       "         'these': 70,\n",
       "         'voluminous': 21,\n",
       "         'slayer': 21,\n",
       "         'hissing': 21,\n",
       "         'behind': 35,\n",
       "         'earth': 21,\n",
       "         'telling': 21,\n",
       "         'closed': 21,\n",
       "         'bony': 21,\n",
       "         'nature': 48,\n",
       "         'understanding': 21,\n",
       "         'untenanted': 21,\n",
       "         'spun': 21,\n",
       "         'narrative': 21,\n",
       "         'long': 118,\n",
       "         'wonder': 21,\n",
       "         'fatal': 21,\n",
       "         'premature': 21,\n",
       "         'companionship': 21,\n",
       "         'paving': 21,\n",
       "         'reminder': 21,\n",
       "         'draw': 21,\n",
       "         'mauvais': 70,\n",
       "         'extinct': 21,\n",
       "         'must': 59,\n",
       "         'conversed': 21,\n",
       "         'fate': 35,\n",
       "         'considerable': 21,\n",
       "         'incapable': 21,\n",
       "         'france': 21,\n",
       "         'been': 144,\n",
       "         'go': 21,\n",
       "         'shoulder': 21,\n",
       "         'once': 90,\n",
       "         'loss': 21,\n",
       "         'seigneur': 21,\n",
       "         'unfolded': 21,\n",
       "         'below': 35,\n",
       "         'until': 35,\n",
       "         'portion': 21,\n",
       "         'asked': 21,\n",
       "         'yet': 144,\n",
       "         'poring': 21,\n",
       "         'watched': 21,\n",
       "         'stoutly': 21,\n",
       "         'nights': 21,\n",
       "         'lower': 21,\n",
       "         'lowly': 21,\n",
       "         'whereon': 21,\n",
       "         'avenge': 21,\n",
       "         'point': 21,\n",
       "         'upon': 213,\n",
       "         'doomed': 21,\n",
       "         'world': 21,\n",
       "         'secretly': 21,\n",
       "         'alive': 21,\n",
       "         'lined': 21,\n",
       "         'parks': 21,\n",
       "         'despair': 21,\n",
       "         'paralyzed': 21,\n",
       "         'fell': 48,\n",
       "         'pass': 21,\n",
       "         'caves': 21,\n",
       "         'many': 80,\n",
       "         'door': 59,\n",
       "         'horrid': 21,\n",
       "         'robert': 48,\n",
       "         'lit': 35,\n",
       "         'disconnected': 35,\n",
       "         'even': 90,\n",
       "         'forced': 21,\n",
       "         'begun': 21,\n",
       "         'protest': 21,\n",
       "         'imposed': 21,\n",
       "         'find': 35,\n",
       "         'immense': 21,\n",
       "         'sought': 21,\n",
       "         'seen': 35,\n",
       "         'remains': 21,\n",
       "         'whilst': 48,\n",
       "         'much': 80,\n",
       "         'men': 35,\n",
       "         'burned': 35,\n",
       "         'than': 80,\n",
       "         'centuries': 90,\n",
       "         'hidden': 35,\n",
       "         'towers': 35,\n",
       "         'against': 48,\n",
       "         'invaded': 21,\n",
       "         'middle': 35,\n",
       "         'hillside': 35,\n",
       "         'pavement': 21,\n",
       "         'sorcier': 118,\n",
       "         'dilapidated': 21,\n",
       "         'strangely': 35,\n",
       "         'raising': 21,\n",
       "         'ages': 70,\n",
       "         'orbits': 21,\n",
       "         'well': 35,\n",
       "         'eyes': 100,\n",
       "         'alleviation': 21,\n",
       "         'wildest': 21,\n",
       "         'become': 21,\n",
       "         'purpose': 21,\n",
       "         'maddening': 21,\n",
       "         'usually': 21,\n",
       "         'godfrey': 70,\n",
       "         'finding': 21,\n",
       "         'reputed': 21,\n",
       "         'fool': 21,\n",
       "         'direction': 21,\n",
       "         'presence': 21,\n",
       "         'abandoned': 21,\n",
       "         'knew': 59,\n",
       "         'clock': 21,\n",
       "         'steel': 21,\n",
       "         'spectral': 21,\n",
       "         'apparent': 21,\n",
       "         'throat': 21,\n",
       "         'free': 21,\n",
       "         'dark': 80,\n",
       "         'affection': 21,\n",
       "         'obtained': 21,\n",
       "         'accomplishments': 21,\n",
       "         'augmented': 21,\n",
       "         'traversed': 21,\n",
       "         'vegetation': 21,\n",
       "         'fear': 35,\n",
       "         'never': 70,\n",
       "         'prompted': 21,\n",
       "         'claw': 21,\n",
       "         'narrow': 21,\n",
       "         'set': 21,\n",
       "         'but': 152,\n",
       "         'counts': 48,\n",
       "         'form': 59,\n",
       "         'applied': 21,\n",
       "         'died': 48,\n",
       "         'what': 80,\n",
       "         'memory': 21,\n",
       "         'gloated': 21,\n",
       "         'life': 118,\n",
       "         'voice': 35,\n",
       "         'handed': 21,\n",
       "         'scions': 21,\n",
       "         'succeeded': 21,\n",
       "         'confirmed': 21,\n",
       "         'age': 127,\n",
       "         'isolated': 35,\n",
       "         'partook': 21,\n",
       "         'folk': 21,\n",
       "         'blackness': 21,\n",
       "         'flapped': 21,\n",
       "         'can': 21,\n",
       "         'shaken': 21,\n",
       "         'solely': 21,\n",
       "         'place': 48,\n",
       "         'usual': 21,\n",
       "         'he': 191,\n",
       "         'dire': 21,\n",
       "         'strange': 80,\n",
       "         'seat': 21,\n",
       "         'stronghold': 21,\n",
       "         'reduced': 21,\n",
       "         'battlements': 35,\n",
       "         'hushed': 21,\n",
       "         'determined': 21,\n",
       "         'rise': 21,\n",
       "         'immediate': 21,\n",
       "         'cowardly': 21,\n",
       "         'bats': 21,\n",
       "         'mighty': 35,\n",
       "         'still': 21,\n",
       "         'effect': 21,\n",
       "         'account': 48,\n",
       "         'danger': 21,\n",
       "         'not': 118,\n",
       "         'invader': 21,\n",
       "         'ruined': 35,\n",
       "         'descending': 21,\n",
       "         'twenty': 21,\n",
       "         'folds': 21,\n",
       "         'sound': 48,\n",
       "         'tunic': 48,\n",
       "         'continuing': 21,\n",
       "         'aghast': 21,\n",
       "         'demise': 21,\n",
       "         'practices': 21,\n",
       "         'exercised': 21,\n",
       "         'tale': 21,\n",
       "         'wholly': 21,\n",
       "         'cap': 21,\n",
       "         'dying': 35,\n",
       "         'prevented': 35,\n",
       "         'most': 127,\n",
       "         'apprehension': 21,\n",
       "         'shadowy': 21,\n",
       "         'machicolated': 21,\n",
       "         'unknown': 35,\n",
       "         'colourless': 21,\n",
       "         'surprised': 21,\n",
       "         'distance': 21,\n",
       "         'senility': 21,\n",
       "         'am': 21,\n",
       "         'alchemist': 48,\n",
       "         'abysmal': 21,\n",
       "         'cracked': 21,\n",
       "         'rooting': 21,\n",
       "         'tomes': 21,\n",
       "         'blacker': 21,\n",
       "         'know': 35,\n",
       "         'helpless': 21,\n",
       "         'curse': 152,\n",
       "         'child': 21,\n",
       "         'return': 21,\n",
       "         'killed': 48,\n",
       "         'and': 715,\n",
       "         'named': 21,\n",
       "         'gain': 21,\n",
       "         'perhaps': 35,\n",
       "         'value': 21,\n",
       "         'six': 48,\n",
       "         'intensity': 21,\n",
       "         'dreaded': 35,\n",
       "         'ninety': 21,\n",
       "         'henri': 35,\n",
       "         'moss': 21,\n",
       "         'devolved': 21,\n",
       "         'beyond': 70,\n",
       "         'eleven': 21,\n",
       "         'deep': 48,\n",
       "         'movement': 21,\n",
       "         'drew': 35,\n",
       "         'science': 21,\n",
       "         'narrator': 21,\n",
       "         'ancestry': 21,\n",
       "         'unsteady': 21,\n",
       "         'progressed': 21,\n",
       "         'called': 35,\n",
       "         'dry': 21,\n",
       "         'leaping': 21,\n",
       "         'lofty': 21,\n",
       "         'relentless': 21,\n",
       "         'considered': 21,\n",
       "         'about': 48,\n",
       "         'nitre': 21,\n",
       "         'fury': 21,\n",
       "         'foul': 21,\n",
       "         'confusion': 21,\n",
       "         'yellow': 21,\n",
       "         'parents': 21,\n",
       "         'hollowness': 21,\n",
       "         'opened': 21,\n",
       "         'flint': 21,\n",
       "         'mysterious': 21,\n",
       "         'under': 21,\n",
       "         'fortresses': 21,\n",
       "         'wooded': 21,\n",
       "         'blackened': 21,\n",
       "         'learning': 21,\n",
       "         'bearing': 21,\n",
       "         'ghastly': 21,\n",
       "         'unused': 35,\n",
       "         'thirty': 90,\n",
       "         'fierce': 21,\n",
       "         'last': 90,\n",
       "         'armands': 21,\n",
       "         'twisted': 21,\n",
       "         'solution': 21,\n",
       "         'wanderings': 21,\n",
       "         'dripping': 21,\n",
       "         'radiance': 21,\n",
       "         'hatred': 21,\n",
       "         'offspring': 21,\n",
       "         'paper': 21,\n",
       "         'harmlessly': 21,\n",
       "         'alchemy': 35,\n",
       "         'c': 35,\n",
       "         'whiteness': 21,\n",
       "         'burden': 21,\n",
       "         'stands': 21,\n",
       "         'occult': 35,\n",
       "         'chronicle': 21,\n",
       "         'excluding': 21,\n",
       "         'watch': 21,\n",
       "         'supernatural': 21,\n",
       "         'servitor': 21,\n",
       "         'turned': 59,\n",
       "         'slimy': 21,\n",
       "         'record': 21,\n",
       "         'chambers': 21,\n",
       "         'surname': 21,\n",
       "         'first': 100,\n",
       "         'crumbling': 35,\n",
       "         'strongly': 21,\n",
       "         'cause': 48,\n",
       "         'furniture': 21,\n",
       "         'elixir': 48,\n",
       "         'hinges': 21,\n",
       "         'endeavor': 21,\n",
       "         'purport': 21,\n",
       "         'storms': 21,\n",
       "         'freely': 21,\n",
       "         'holders': 21,\n",
       "         'within': 70,\n",
       "         'learned': 21,\n",
       "         'all': 168,\n",
       "         'preceptor': 21,\n",
       "         'plains': 21,\n",
       "         'total': 21,\n",
       "         'opening': 21,\n",
       "         'bands': 21,\n",
       "         'dreadful': 21,\n",
       "         'prone': 21,\n",
       "         'attributing': 21,\n",
       "         'sputter': 21,\n",
       "         'fright': 21,\n",
       "         'morning': 21,\n",
       "         'gloomy': 35,\n",
       "         'intelligence': 21,\n",
       "         'estates': 35,\n",
       "         'young': 70,\n",
       "         'greater': 21,\n",
       "         'peasant': 48,\n",
       "         'wall': 21,\n",
       "         'strength': 21,\n",
       "         'is': 59,\n",
       "         'ill': 21,\n",
       "         'half': 21,\n",
       "         'frantic': 21,\n",
       "         'known': 21,\n",
       "         'bent': 21,\n",
       "         'inquiry': 21,\n",
       "         'they': 59,\n",
       "         'hair': 21,\n",
       "         'again': 21,\n",
       "         'beholding': 21,\n",
       "         'louis': 21,\n",
       "         'glow': 21,\n",
       "         'shriek': 21,\n",
       "         'tapestries': 21,\n",
       "         'whose': 70,\n",
       "         'an': 118,\n",
       "         'accursed': 21,\n",
       "         'pondered': 21,\n",
       "         'tragedy': 21,\n",
       "         'flagged': 21,\n",
       "         'breath': 35,\n",
       "         'real': 21,\n",
       "         'objects': 21,\n",
       "         'almost': 35,\n",
       "         'laboratory': 21,\n",
       "         'redeeming': 21,\n",
       "         'belief': 21,\n",
       "         'happy': 21,\n",
       "         'every': 21,\n",
       "         'staircases': 21,\n",
       "         'averted': 21,\n",
       "         'birthday': 35,\n",
       "         'virtuous': 21,\n",
       "         'dimensions': 21,\n",
       "         'document': 21,\n",
       "         'fall': 35,\n",
       "         'estate': 35,\n",
       "         'passageway': 21,\n",
       "         'pair': 21,\n",
       "         'formed': 21,\n",
       "         'joyful': 21,\n",
       "         'gothic': 35,\n",
       "         'elsewhere': 21,\n",
       "         'violently': 21,\n",
       "         'ruin': 21,\n",
       "         'intended': 21,\n",
       "         'sat': 21,\n",
       "         'explanation': 21,\n",
       "         'head': 21,\n",
       "         'less': 21,\n",
       "         'himself': 35,\n",
       "         'dull': 35,\n",
       "         'staircase': 21,\n",
       "         'keep': 21,\n",
       "         'became': 48,\n",
       "         'analysis': 21,\n",
       "         'caught': 35,\n",
       "         'piece': 21,\n",
       "         'hold': 21,\n",
       "         'levels': 21,\n",
       "         'dearer': 21,\n",
       "         'servants': 21,\n",
       "         'though': 48,\n",
       "         'ears': 21,\n",
       "         'distorted': 21,\n",
       "         'thoughts': 21,\n",
       "         'repellent': 21,\n",
       "         'doorway': 35,\n",
       "         'shun': 21,\n",
       "         'unwilling': 21,\n",
       "         'other': 21,\n",
       "         'standing': 21,\n",
       "         'faced': 21,\n",
       "         'raised': 35,\n",
       "         'distant': 21,\n",
       "         'deemed': 35,\n",
       "         'wood': 21,\n",
       "         'ponder': 21,\n",
       "         'rugged': 21,\n",
       "         'pride': 21,\n",
       "         'remove': 21,\n",
       "         'occurred': 35,\n",
       "         'wild': 48,\n",
       "         'able': 35,\n",
       "         'pile': 21,\n",
       "         'seated': 21,\n",
       "         'shrieked': 21,\n",
       "         'home': 21,\n",
       "         'since': 70,\n",
       "         'lords': 21,\n",
       "         'when': 118,\n",
       "         'like': 59,\n",
       "         'depress': 21,\n",
       "         'garment': 21,\n",
       "         'slow': 21,\n",
       "         'was': 384,\n",
       "         'oaken': 21,\n",
       "         'utter': 21,\n",
       "         'mastered': 21,\n",
       "         'metal': 21,\n",
       "         'madness': 21,\n",
       "         'remaining': 35,\n",
       "         'through': 100,\n",
       "         'designated': 21,\n",
       "         'reach': 21,\n",
       "         'this': 118,\n",
       "         'ancient': 70,\n",
       "         'falter': 21,\n",
       "         'demonologists': 21,\n",
       "         'latin': 21,\n",
       "         'level': 21,\n",
       "         'said': 48,\n",
       "         'tongue': 21,\n",
       "         'left': 70,\n",
       "         'farther': 21,\n",
       "         'searching': 21,\n",
       "         'society': 21,\n",
       "         'continued': 35,\n",
       "         'cease': 21,\n",
       "         'honest': 21,\n",
       "         'climbing': 21,\n",
       "         'dislodged': 21,\n",
       "         'land': 21,\n",
       "         'wide': 21,\n",
       "         'sides': 35,\n",
       "         'deaths': 35,\n",
       "         'summit': 21,\n",
       "         'thus': 80,\n",
       "         'unhappy': 21,\n",
       "         'noxious': 21,\n",
       "         'single': 21,\n",
       "         'impression': 21,\n",
       "         'boiling': 21,\n",
       "         'nightly': 21,\n",
       "         'vanishment': 21,\n",
       "         'its': 144,\n",
       "         'overgrown': 21,\n",
       "         'enthusiasm': 21,\n",
       "         'four': 35,\n",
       "         'house': 80,\n",
       "         'proud': 21,\n",
       "         'profusion': 35,\n",
       "         'floors': 21,\n",
       "         'impeded': 21,\n",
       "         'toppling': 21,\n",
       "         'suspected': 21,\n",
       "         'afternoon': 21,\n",
       "         'remember': 21,\n",
       "         'felled': 21,\n",
       "         'flung': 21,\n",
       "         'tried': 21,\n",
       "         'lean': 21,\n",
       "         'led': 21,\n",
       "         'alchemists': 21,\n",
       "         'turning': 21,\n",
       "         'heavy': 21,\n",
       "         'housed': 21,\n",
       "         'sight': 35,\n",
       "         'up': 35,\n",
       "         'inky': 21,\n",
       "         'hours': 35,\n",
       "         'birth': 48,\n",
       "         'floor': 35,\n",
       "         'month': 21,\n",
       "         'spake': 21,\n",
       "         'rumbling': 21,\n",
       "         'proceeded': 35,\n",
       "         'chatter': 21,\n",
       "         'fumes': 21,\n",
       "         'trusted': 21,\n",
       "         'frightfully': 21,\n",
       "         'attention': 21,\n",
       "         'sunken': 35,\n",
       "         'filled': 35,\n",
       "         'survive': 21,\n",
       "         'face': 48,\n",
       "         'disappearance': 21,\n",
       "         'coming': 35,\n",
       "         'resounded': 21,\n",
       "         'over': 80,\n",
       "         'may': 48,\n",
       "         'by': 249,\n",
       "         'ran': 35,\n",
       "         'out': 35,\n",
       "         'studies': 48,\n",
       "         'poverty': 21,\n",
       "         'caused': 35,\n",
       "         'new': 35,\n",
       "         'remained': 21,\n",
       "         'concerning': 21,\n",
       "         'from': 206,\n",
       "         'proportions': 21,\n",
       "         'peculiar': 21,\n",
       "         'chilled': 21,\n",
       "         'swelling': 21,\n",
       "         'grassy': 21,\n",
       "         'cauldron': 21,\n",
       "         'charred': 21,\n",
       "         'grown': 21,\n",
       "         'ungoverned': 21,\n",
       "         'guardian': 21,\n",
       "         'back': 48,\n",
       "         'perpetrated': 21,\n",
       "         'skull': 21,\n",
       "         'such': 59,\n",
       "         'journey': 21,\n",
       "         'aged': 48,\n",
       "         'abode': 21,\n",
       "         'wizards': 21,\n",
       "         'lifted': 21,\n",
       "         'wings': 21,\n",
       "         'returned': 48,\n",
       "         'early': 59,\n",
       "         'mystery': 21,\n",
       "         'eternal': 48,\n",
       "         'undergone': 21,\n",
       "         'honored': 21,\n",
       "         'evident': 35,\n",
       "         'count': 100,\n",
       "         'thing': 35,\n",
       "         'ended': 21,\n",
       "         'night': 48,\n",
       "         'grew': 21,\n",
       "         'accents': 35,\n",
       "         'doom': 21,\n",
       "         'twin': 21,\n",
       "         'malevolence': 35,\n",
       "         'idle': 21,\n",
       "         'faint': 35,\n",
       "         'massive': 35,\n",
       "         'realizing': 21,\n",
       "         'worm': 21,\n",
       "         'preserving': 21,\n",
       "         'cheeks': 21,\n",
       "         'startling': 21,\n",
       "         'wed': 21,\n",
       "         'efforts': 21,\n",
       "         'base': 35,\n",
       "         'examination': 21,\n",
       "         'span': 21,\n",
       "         'trembled': 21,\n",
       "         'philosopher': 21,\n",
       "         'wretch': 21,\n",
       "         'acquisition': 21,\n",
       "         'pristine': 21,\n",
       "         'liquid': 21,\n",
       "         'moisture': 21,\n",
       "         'title': 35,\n",
       "         'very': 21,\n",
       "         'apartment': 21,\n",
       "         'hearths': 21,\n",
       "         'commercial': 21,\n",
       "         'dimly': 21,\n",
       "         'fragments': 21,\n",
       "         'trace': 21,\n",
       "         'lay': 21,\n",
       "         'affected': 21,\n",
       "         'interpret': 21,\n",
       "         'part': 21,\n",
       "         'pockets': 21,\n",
       "         'often': 21,\n",
       "         'them': 21,\n",
       "         'understand': 21,\n",
       "         'henris': 21,\n",
       "         'want': 35,\n",
       "         'lost': 21,\n",
       "         'unmoved': 21,\n",
       "         'carried': 35,\n",
       "         'flowing': 21,\n",
       "         'buried': 35,\n",
       "         'underground': 21,\n",
       "         'century': 21,\n",
       "         'speech': 21,\n",
       "         'meanwhile': 21,\n",
       "         'guess': 21,\n",
       "         'shocking': 21,\n",
       "         'produced': 35,\n",
       "         'screamed': 21,\n",
       "         'steps': 59,\n",
       "         'week': 21,\n",
       "         'are': 21,\n",
       "         'shunned': 21,\n",
       "         'spoke': 48,\n",
       "         'charles': 135,\n",
       "         'exploration': 35,\n",
       "         'death': 48,\n",
       "         'incredible': 35,\n",
       "         'warning': 21,\n",
       "         'seized': 35,\n",
       "         'threw': 21,\n",
       "         'forests': 21,\n",
       "         'gloom': 21,\n",
       "         'mediaeval': 35,\n",
       "         'relation': 21,\n",
       "         'the': 1566,\n",
       "         'release': 21,\n",
       "         'water': 21,\n",
       "         'framed': 21,\n",
       "         'gather': 21,\n",
       "         'hue': 21,\n",
       "         'natures': 21,\n",
       "         'sacrifice': 21,\n",
       "         'alchemical': 35,\n",
       "         'nerves': 21,\n",
       "         'fancied': 21,\n",
       "         'wrapt': 21,\n",
       "         'overtake': 21,\n",
       "         'that': 322,\n",
       "         'crowning': 21,\n",
       "         'away': 35,\n",
       "         'read': 35,\n",
       "         'day': 48,\n",
       "         'assassin': 35,\n",
       "         'clad': 21,\n",
       "         'as': 315,\n",
       "         'ominous': 21,\n",
       "         'excursions': 21,\n",
       "         'approaching': 21,\n",
       "         'researches': 35,\n",
       "         'entailed': 21,\n",
       "         'discourse': 35,\n",
       "         'alone': 21,\n",
       "         'laid': 35,\n",
       "         'forth': 21,\n",
       "         'ceasing': 21,\n",
       "         'there': 90,\n",
       "         'proved': 35,\n",
       "         'profound': 35,\n",
       "         'without': 70,\n",
       "         'solitude': 21,\n",
       "         's': 80,\n",
       "         'shrivelled': 21,\n",
       "         'aim': 21,\n",
       "         'slightest': 21,\n",
       "         'spent': 48,\n",
       "         'on': 127,\n",
       "         'little': 80,\n",
       "         'leading': 21,\n",
       "         'old': 168,\n",
       "         'hundred': 35,\n",
       "         'older': 21,\n",
       "         'plebeian': 21,\n",
       "         'moments': 21,\n",
       "         'haunted': 70,\n",
       "         'vain': 35,\n",
       "         'father': 100,\n",
       "         'me': 242,\n",
       "         'ray': 21,\n",
       "         'second': 21,\n",
       "         'has': 35,\n",
       "         'apparition': 21,\n",
       "         'suddenly': 59,\n",
       "         'covered': 21,\n",
       "         'fire': 21,\n",
       "         'heir': 21,\n",
       "         'then': 70,\n",
       "         'manner': 21,\n",
       "         'barons': 21,\n",
       "         'maintaining': 35,\n",
       "         'foot': 59,\n",
       "         'contents': 35,\n",
       "         'simple': 21,\n",
       "         'moment': 35,\n",
       "         'modern': 21,\n",
       "         'greatest': 21,\n",
       "         'whispers': 21,\n",
       "         'revealed': 35,\n",
       "         'their': 100,\n",
       "         'came': 48,\n",
       "         'phial': 48,\n",
       "         'formidable': 21,\n",
       "         'excavated': 21,\n",
       "         'piercing': 21,\n",
       "         'own': 48,\n",
       "         'poor': 21,\n",
       "         'mother': 21,\n",
       "         'library': 35,\n",
       "         'exact': 35,\n",
       "         'hitherto': 35,\n",
       "         'at': 296,\n",
       "         'break': 21,\n",
       "         'parapets': 35,\n",
       "         'why': 21,\n",
       "         'creak': 21,\n",
       "         'perpetual': 21,\n",
       "         'limit': 21,\n",
       "         'seared': 21,\n",
       "         'should': 80,\n",
       "         'same': 21,\n",
       "         ...})"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Unigram Distribution\n",
    "z = 0.0001\n",
    "#count all the occurence of vocabs\n",
    "from collections import Counter\n",
    "\n",
    "word_count = Counter(flatten(corpus_tokenized))\n",
    "word_count\n",
    "\n",
    "num_total_words = sum([c for w, c in word_count.items()])\n",
    "num_total_words\n",
    "\n",
    "unigram_table = []\n",
    "\n",
    "for v in vocabs:\n",
    "    uw = word_count[v]/num_total_words\n",
    "    uw_alpha = uw ** 0.75\n",
    "    uw_alpha_dividebyz = int(uw_alpha/z)\n",
    "    # print('Vocab :',v)\n",
    "    # print('distribution :', uw_alpha_dividebyz)\n",
    "    unigram_table.extend([v] * uw_alpha_dividebyz)\n",
    "\n",
    "Counter(unigram_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    #map(fucntion, list of something)\n",
    "    #map will look at each of element in this list, and apply this function\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "    \n",
    "import random\n",
    "#you don't want to pick samples = targets, basically negative samples\n",
    "#k = number of negative samples - how many? they found 10 is the best\n",
    "#will be run during training\n",
    "#after random_batch, \n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    #targets is already in id.....\n",
    "    #but the unigram_table is in word....\n",
    "    #1. get the batch size of this targets\n",
    "    batch_size = targets.shape[0]\n",
    "    neg_samples = []\n",
    "    #2. for each batch\n",
    "    for i in range(batch_size):\n",
    "        #randomly pick k negative words from unigram_table\n",
    "        target_index = targets[i].item()  #looping each of the batch....\n",
    "        nsample = []\n",
    "        while len(nsample) < k:\n",
    "            neg = random.choice(unigram_table)\n",
    "            #if this word == target, skip this word\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        #append this word to some list\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).reshape(1, -1))  #tensor[], tensor[]\n",
    "    return torch.cat(neg_samples)  #tensor[[], []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_neg = 2 # in the real code, we gonna use 10 (like in the paper)\n",
    "neg_samples = negative_sampling(label_batch, unigram_table, num_neg)\n",
    "# neg_samples[0].shape\n",
    "neg_samples.shape "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model will accept three vectors - u_o, v_c, u_k\n",
    "#u_o - vectos for outside words\n",
    "#v_C - vector for center word\n",
    "#u_k - vectors for negative word\n",
    "\n",
    "class SkipgramNeg(nn.Module):\n",
    "    def __init__(self,voc_size, emb_size):\n",
    "        super(SkipgramNeg,self).__init__()\n",
    "        self.embedding_center_word = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside_word = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "\n",
    "    def forward(self, center_words, outside_words, negative_words):\n",
    "        #center_words, outside_words  (batch_size,1)\n",
    "        #negative_words (batch_size,k) \n",
    "        center_embed    = self.embedding_center_word(center_words)      #(batch_size,1, emb_size)\n",
    "        outside_embed   = self.embedding_outside_word(outside_words)   #(batch_size,1, emb_size)\n",
    "        neg_embed       = self.embedding_outside_word(negative_words)      #(batch_size,k, emb_size)\n",
    "        \n",
    "        uovc            = outside_embed.bmm(center_embed.transpose(1,2)).squeeze(2)\n",
    "        ukvc            = -neg_embed.bmm(center_embed.transpose(1,2)).squeeze(2)\n",
    "        ukvc_sum        =  torch.sum(ukvc, 1).view(-1, 1) #(batch_size, 1)\n",
    "        loss = self.logsigmoid(uovc) + self.logsigmoid(ukvc_sum) #(batch_size,1)+(batch_size,1)\n",
    "        \n",
    "        return -torch.mean(loss) #scalar, loss should be scalar, to call backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size = len(vocabs)\n",
    "voc_size\n",
    "\n",
    "batch_size = 2 #why? no reason\n",
    "emb_size = 2 #why? no reason; usually 50,100, 300 but 2 so we can plot (50 can also plot, but need PCA)\n",
    "model = SkipgramNeg(voc_size,emb_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #-log\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1000 | cost: 1.553973 | time: 0m 0s\n",
      "Epoch: 2000 | cost: 2.089709 | time: 0m 0s\n",
      "Epoch: 3000 | cost: 1.233664 | time: 0m 0s\n",
      "Epoch: 4000 | cost: 1.328872 | time: 0m 0s\n",
      "Epoch: 5000 | cost: 1.754867 | time: 0m 0s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Training\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    \n",
    "    input_batch, target_batch = random_batch(batch_size, corpus)\n",
    "    \n",
    "    #input_batch: [batch_size, 1]\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    #target_batch: [batch_size, 1]\n",
    "    target_batch = torch.LongTensor(target_batch)\n",
    "    \n",
    "    #negs_batch:   [batch_size, num_neg]\n",
    "    negs_batch = negative_sampling(target_batch, unigram_table, num_neg)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "        \n",
    "    loss = model(input_batch, target_batch, negs_batch)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start, end)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch: {epoch + 1} | cost: {loss:.6f} | time: {epoch_mins}m {epoch_secs}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dataBags(corpus_tokenized):\n",
    "#     # data - [(context), target]\n",
    "#     data = []\n",
    "#     for sent in corpus_tokenized:\n",
    "#         for i in range(2, len(sent) - 2):\n",
    "#             context = [sent[i - 2], sent[i - 1], sent[i + 1], sent[i + 2]]\n",
    "#             target = sent[i]\n",
    "#             data.append((context, target))\n",
    "\n",
    "#     return data\n",
    "\n",
    "# cbow = dataBags(corpus_tokenized)\n",
    "# cbow[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>athens</td>\n",
       "      <td>greece</td>\n",
       "      <td>1368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bangkok</td>\n",
       "      <td>thailand</td>\n",
       "      <td>1178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>barcelona</td>\n",
       "      <td>spain</td>\n",
       "      <td>1280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>berlin</td>\n",
       "      <td>east_germany</td>\n",
       "      <td>3481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>birmingham</td>\n",
       "      <td>united_kingdom</td>\n",
       "      <td>1112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>tokyo</td>\n",
       "      <td>japan</td>\n",
       "      <td>8535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>toronto</td>\n",
       "      <td>canada</td>\n",
       "      <td>668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>vienna</td>\n",
       "      <td>austria</td>\n",
       "      <td>1766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>warsaw</td>\n",
       "      <td>poland</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>yokohama</td>\n",
       "      <td>japan</td>\n",
       "      <td>1143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          City         Country  Population\n",
       "0       athens          greece        1368\n",
       "1      bangkok        thailand        1178\n",
       "2    barcelona           spain        1280\n",
       "3       berlin    east_germany        3481\n",
       "4   birmingham  united_kingdom        1112\n",
       "..         ...             ...         ...\n",
       "66       tokyo           japan        8535\n",
       "67     toronto          canada         668\n",
       "68      vienna         austria        1766\n",
       "69      warsaw          poland         965\n",
       "70    yokohama           japan        1143\n",
       "\n",
       "[71 rows x 3 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example to import db file\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "def ReadSQL(filename):\n",
    "    connection = sqlite3.connect(filename)\n",
    "    data = pd.read_sql(\"SELECT * from city_table\",connection)\n",
    "    return data\n",
    "\n",
    "ReadSQL('city.db')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
