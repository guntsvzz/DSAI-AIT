```
Paper : Distilling the Knowledge in a Neural Network
Link : https://arxiv.org/abs/1503.02531
Venue: NIPS
```

| Topic        | Distilling the Knowledge in a Neural Network |
|--------------|----------------------------------------------------------------|
| Question     | average ensemble of model is expensive |
| Related Work | |
| Solution     | distillation transfer the knowledge from model to a smaller model which proper for deployment by training it on a transfer seet and using soft target distribution|
| Method       | |
| Result       | |
| Conclusion   | |
| Limitation   | |