```
Paper : Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks
Link : https://arxiv.org/abs/1908.10084
Venue: EMNLP 
```

| Topic        | Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks |
|--------------|----------------------------------------------------------------|
| Question     | BERT and RoBERTa require massive computational time|
| Related Work | 1.Skip-Thought trains an encoder-decoder architecture to predict hhe surroudning sentences. </br> 2. InferSent uses labeled data to train siamese BiLSTM with max-pooling over the input|
| Solution     | |
| Method       | |
| Result       | |
| Conclusion   | |
| Limitation   | |