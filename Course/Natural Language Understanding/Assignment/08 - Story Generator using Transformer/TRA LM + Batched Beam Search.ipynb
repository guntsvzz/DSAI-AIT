{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer and Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torchtext, datasets, math\n",
    "from tqdm import tqdm\n",
    "\n",
    "from queue import PriorityQueue\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA RTX A6000'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "# torch.manual_seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Load data - Wiki Text\n",
    "\n",
    "We will be using wikitext which contains a large corpus of text, perfect for language modeling task.  This time, we will use the `datasets` library from HuggingFace to load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/home/chaklams/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cbca3ed4ff94c179dbca4a6e7cc3a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "#there are raw and preprocessed version; we used the raw one and preprocessed ourselves for fun\n",
    "dataset = datasets.load_dataset('wikitext', 'wikitext-2-raw-v1')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " During the same time frame as the Hitchcock rumors , goaltender Curtis Sanford returned from his groin injury on November 13 . He made his first start of the season against the Boston Bruins , losing 2 – 1 in a shootout . Sanford continued his strong play , posting a 3 – 1 – 2 record , 1 @.@ 38 goals against average and .947 save percentage over his next six games . Sanford started 12 consecutive games before Steve Mason made his next start . The number of starts might not have been as numerous , but prior to the November 23 game , Mason was hit in the head by a shot from Rick Nash during pre @-@ game warm @-@ ups and suffered a concussion . Mason returned from his concussion after two games , making a start against the Vancouver Canucks . Mason allowed only one goal in the game despite suffering from cramping in the third period , temporarily being replaced by Sanford for just over three minutes . Columbus won the game 2 – 1 in a shootout , breaking a nine @-@ game losing streak to the Canucks . After the game , Arniel stated that Sanford was still seen as the team 's number one goaltender . However , Mason started four of the next six games with the Blue Jackets going 0 – 5 – 1 during that stretch . \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nIf you try to change the index you might notice that sometimes there is no paragraph \\nand rather an empty string so we will have to care of that later.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset['train'][333]['text'])\n",
    "\n",
    "'''\n",
    "If you try to change the index you might notice that sometimes there is no paragraph \n",
    "and rather an empty string so we will have to care of that later.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "Simply tokenize the given text to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-2f3ef63bf035b562.arrow\n",
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-8af47333e652a54e.arrow\n",
      "Loading cached processed dataset at /home/chaklams/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126/cache-6a28ab3703035b28.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['during', 'the', 'same', 'time', 'frame', 'as', 'the', 'hitchcock', 'rumors', ',', 'goaltender', 'curtis', 'sanford', 'returned', 'from', 'his', 'groin', 'injury', 'on', 'november', '13', '.', 'he', 'made', 'his', 'first', 'start', 'of', 'the', 'season', 'against', 'the', 'boston', 'bruins', ',', 'losing', '2', '–', '1', 'in', 'a', 'shootout', '.', 'sanford', 'continued', 'his', 'strong', 'play', ',', 'posting', 'a', '3', '–', '1', '–', '2', 'record', ',', '1', '@', '.', '@', '38', 'goals', 'against', 'average', 'and', '.', '947', 'save', 'percentage', 'over', 'his', 'next', 'six', 'games', '.', 'sanford', 'started', '12', 'consecutive', 'games', 'before', 'steve', 'mason', 'made', 'his', 'next', 'start', '.', 'the', 'number', 'of', 'starts', 'might', 'not', 'have', 'been', 'as', 'numerous', ',', 'but', 'prior', 'to', 'the', 'november', '23', 'game', ',', 'mason', 'was', 'hit', 'in', 'the', 'head', 'by', 'a', 'shot', 'from', 'rick', 'nash', 'during', 'pre', '@-@', 'game', 'warm', '@-@', 'ups', 'and', 'suffered', 'a', 'concussion', '.', 'mason', 'returned', 'from', 'his', 'concussion', 'after', 'two', 'games', ',', 'making', 'a', 'start', 'against', 'the', 'vancouver', 'canucks', '.', 'mason', 'allowed', 'only', 'one', 'goal', 'in', 'the', 'game', 'despite', 'suffering', 'from', 'cramping', 'in', 'the', 'third', 'period', ',', 'temporarily', 'being', 'replaced', 'by', 'sanford', 'for', 'just', 'over', 'three', 'minutes', '.', 'columbus', 'won', 'the', 'game', '2', '–', '1', 'in', 'a', 'shootout', ',', 'breaking', 'a', 'nine', '@-@', 'game', 'losing', 'streak', 'to', 'the', 'canucks', '.', 'after', 'the', 'game', ',', 'arniel', 'stated', 'that', 'sanford', 'was', 'still', 'seen', 'as', 'the', 'team', \"'\", 's', 'number', 'one', 'goaltender', '.', 'however', ',', 'mason', 'started', 'four', 'of', 'the', 'next', 'six', 'games', 'with', 'the', 'blue', 'jackets', 'going', '0', '–', '5', '–', '1', 'during', 'that', 'stretch', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "#function to tokenize\n",
    "tokenize_data = lambda example, tokenizer: {'tokens': tokenizer(example['text'])}  \n",
    "\n",
    "#map the function to each example\n",
    "tokenized_dataset = dataset.map(tokenize_data, remove_columns=['text'], fn_kwargs={'tokenizer': tokenizer})\n",
    "print(tokenized_dataset['train'][333]['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalizing\n",
    "\n",
    "We will tell torchtext to add any word that has occurred at least three times in the dataset to the vocabulary because otherwise it would be too big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29475\n",
      "['<unk>', '<pad>', '<sos>', '<eos>', 'the', ',', '.', 'of', 'and', 'in']\n"
     ]
    }
   ],
   "source": [
    "## numericalizing\n",
    "\n",
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "\n",
    "vocab = torchtext.vocab.build_vocab_from_iterator(tokenized_dataset['train']['tokens'], min_freq=3, specials=special_symbols)   \n",
    "\n",
    "vocab.set_default_index(vocab['<unk>'])   \n",
    "print(len(vocab))                         \n",
    "print(vocab.get_itos()[:10])       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Prepare the batch loader\n",
    "\n",
    "### Prepare data\n",
    "\n",
    "Given \"Chaky loves eating at AIT\", and \"I really love deep learning\", and given batch size = 3, we will get three batches of data \"Chaky loves eating at\", \"AIT `<eos>` I really\", \"love deep learning `<eos>`\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(dataset, vocab, batch_size):\n",
    "    data = []                                                   \n",
    "    for example in dataset:\n",
    "        if example['tokens']:         \n",
    "            #appends eos so we know it ends....so model learn how to end...                             \n",
    "            tokens = example['tokens'].append('<eos>')   \n",
    "            #numericalize          \n",
    "            tokens = [vocab[token] for token in example['tokens']] \n",
    "            data.extend(tokens)                                    \n",
    "    data = torch.LongTensor(data)                                 \n",
    "    num_batches = data.shape[0] // batch_size #get the int number of batches...\n",
    "    data = data[:num_batches * batch_size] #make the batch evenly, and cut out any remaining                      \n",
    "    data = data.view(batch_size, num_batches)          \n",
    "    return data #[batch size, bunch of tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_data = get_data(tokenized_dataset['train'], vocab, batch_size)\n",
    "valid_data = get_data(tokenized_dataset['validation'], vocab, batch_size)\n",
    "test_data  = get_data(tokenized_dataset['test'], vocab, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        #Q = [batch size, query len, hid dim]\n",
    "        #K = [batch size, key len, hid dim]\n",
    "        #V = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        #Q = [batch size, n heads, query len, head dim]\n",
    "        #K = [batch size, n heads, key len, head dim]\n",
    "        #V = [batch size, n heads, value len, head dim]\n",
    "                \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        #energy = [batch size, n heads, query len, key len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        #attention = [batch size, n heads, query len, key len]\n",
    "                \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #x = [batch size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        #x = [batch size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        #x = [batch size, seq len, pf dim]\n",
    "        \n",
    "        x = self.fc_2(x)\n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am using Batched Beam Search, where instead of feeding each hypothesis one by one, which takes a lot of time;  I simply concat everything into one list and feed them all at once, which is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, device, pad_idx, max_length = 100):\n",
    "                \n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim, \n",
    "                                                  dropout, \n",
    "                                                  device)\n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        self.pad_idx = pad_idx\n",
    "    \n",
    "    def make_mask(self, x):\n",
    "        \n",
    "        #x = [batch size, len]\n",
    "        \n",
    "        pad_mask = (x != self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #pad_mask = [batch size, 1, 1, len]\n",
    "        \n",
    "        x_len = x.shape[1]\n",
    "        \n",
    "        sub_mask = torch.tril(torch.ones((x_len, x_len), device = self.device)).bool()\n",
    "        #sub_mask = [len, len]\n",
    "            \n",
    "        mask = pad_mask & sub_mask\n",
    "        #mask = [batch size, 1, len, len]\n",
    "        \n",
    "        return mask \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, len]\n",
    "                \n",
    "        batch_size = x.shape[0]\n",
    "        x_len      = x.shape[1]\n",
    "        \n",
    "        #get mask here since we remove seq2seq class\n",
    "        mask   = self.make_mask(x)\n",
    "        #mask = [batch size, 1, len, len]\n",
    "\n",
    "        pos = torch.arange(0, x_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)          \n",
    "            \n",
    "        x = self.dropout((self.tok_embedding(x) * self.scale) + self.pos_embedding(pos))\n",
    "        #x = [batch size, len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x, attention = layer(x, mask)\n",
    "        \n",
    "        #x = [batch size, len, hid dim]\n",
    "        #attention = [batch size, n heads, len, len]\n",
    "        \n",
    "        output = self.fc_out(x)\n",
    "        #output = [batch size, len, output dim]\n",
    "            \n",
    "        return output, attention\n",
    "\n",
    "    def beam_decode(self, penalty_alpha = 0.9, max_length = 5, beam_size = 5):\n",
    "        \n",
    "        # Start with SOS Harry Potter is\n",
    "        prompt = 'Harry Potter is '\n",
    "        \n",
    "        tokens = tokenizer(prompt)\n",
    "        indices = [SOS_IDX] + [vocab[t] for t in tokens]\n",
    "\n",
    "        decoder_input = torch.Tensor([indices]).long().to(device)\n",
    "        #decoder_input: [batch size, len] = [1, 1]\n",
    "        scores = torch.Tensor([0.]).to(device)\n",
    "        #scores: [1]\n",
    "        \n",
    "        for i in range(max_length):\n",
    "            \n",
    "            # print(f\"========Length: {i}\")\n",
    "            \n",
    "            # Decoder prediction\n",
    "            logits, _ = self.forward(decoder_input)\n",
    "            #[beam_size, current dec len=i, vocab_size]\n",
    "                        \n",
    "            logits = logits[:, -1] \n",
    "            # Last sequence step: [beam_size, current dec len=i, vocab_size] => [beam_size, vocab_size]\n",
    "            \n",
    "            # print(f\"{logits.shape=}\")\n",
    "\n",
    "            # Softmax\n",
    "            # Log softmax is better, since beam search accumulates probability\n",
    "            # if simply softmax, the probability can get too small and then become unstable\n",
    "            log_probs = torch.log_softmax(logits, dim=1)\n",
    "    \n",
    "            # Add length penalty, otherwise, always very short sentence will win...\n",
    "            penalty   = ((5 + (i+1)) / (5 + 1)) ** penalty_alpha #see https://arxiv.org/abs/1609.08144\n",
    "            log_probs = log_probs / penalty\n",
    "            \n",
    "            # print(f\"{decoder_input[:, -1]=}\")\n",
    "            \n",
    "            # Update score where EOS has not been reached\n",
    "            log_probs[decoder_input[:, -1]==EOS_IDX, :] = -2 #discouraged it to end\n",
    "            log_probs[decoder_input[:, -1]==UNK_IDX, :] = -10 #very discouraged to spit out unk\n",
    "            scores = scores.unsqueeze(1) + log_probs \n",
    "            # scores: [beam_size, vocab_size]\n",
    "            # log_probs: [beam_size, vocab_size]\n",
    "\n",
    "            # print(f\"{log_probs.shape=}\")\n",
    "            # print(f\"{scores.shape=}\")\n",
    "            #log_probs: torch.Size([1, 29475])\n",
    "            #scores.shape=torch.Size([1, 29475])\n",
    "            \n",
    "            # Flatten scores from [beams, vocab_size] to [beams * vocab_size] to get top k, and reconstruct beam indices and token indices\n",
    "            # Since we flatten it, we have to retrieve the actual beam indices and token_indices using floor division and remainder\n",
    "            # You can try on paper; it will make sense\n",
    "            scores, indices = torch.topk(scores.reshape(-1), beam_size) #scores: [beam_size]; #indices: [beam_size]\n",
    "            beam_indices  = torch.divide   (indices, self.output_dim, rounding_mode='floor') # indices // vocab_size\n",
    "            token_indices = torch.remainder(indices, self.output_dim)                        # indices %  vocab_size\n",
    "            \n",
    "            # print(f\"{scores=}\")\n",
    "            # print(f\"{indices.shape=}\")\n",
    "            \n",
    "            # print(f\"{indices=}\")\n",
    "            # print(f\"{beam_indices=}\")\n",
    "            # print(f\"{token_indices=}\")\n",
    "            \n",
    "            # Build the next decoder input\n",
    "            # For efficiency, the trick is to concatenate all hypotheses into one string and sent to decoder at once\n",
    "            # We can later chop it ...\n",
    "            next_decoder_input = []\n",
    "            for beam_index, token_index in zip(beam_indices, token_indices):\n",
    "                # print(f\"{beam_index=}\")\n",
    "                prev_decoder_input = decoder_input[beam_index]\n",
    "                # print(f\"{prev_decoder_input=}\")\n",
    "                if prev_decoder_input[-1]==EOS_IDX:\n",
    "                    token_index = EOS_IDX # once EOS, always EOS\n",
    "                token_index = torch.LongTensor([token_index]).long().to(device)\n",
    "                next_decoder_input.append(torch.cat([prev_decoder_input, token_index]))\n",
    "                # print(\"here: \" + \" \".join([vocab.lookup_token(i) for i in next_decoder_input[-1]]) + \"; score: \" + str(scores[beam_index].item()))\n",
    "            decoder_input = torch.vstack(next_decoder_input)\n",
    "            \n",
    "            # print(f\"{decoder_input=}\")\n",
    "            \n",
    "             # If all beams are finished, and the length is at least 5, exit\n",
    "            if i > 5:\n",
    "                if (decoder_input[:, -1]==EOS_IDX).sum() == beam_size:\n",
    "                    break\n",
    "                \n",
    "        # convert the top scored sequence to a list of text tokens\n",
    "        decoder_output, _ = max(zip(decoder_input, scores), key=lambda x: x[1])\n",
    "        decoder_output = decoder_output[1:].cpu().numpy() # remove SOS\n",
    "        \n",
    "        return [vocab.lookup_token(i) for i in decoder_output if i != EOS_IDX] # remove EOS if exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        \n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)        \n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        \n",
    "        #x = [batch size, len, hid dim]\n",
    "        #mask = [batch size, 1, len, len]\n",
    "        \n",
    "        #multi attention, skip and then norm\n",
    "        _x, attention = self.self_attention(x, x, x, mask)\n",
    "        x = self.self_attn_layer_norm(x + self.dropout(_x))\n",
    "        #x = [batch size, len, hid dim]\n",
    "        #attention = [batch size, n heads, len, len]\n",
    "    \n",
    "        #positionwise feedforward\n",
    "        _x = self.positionwise_feedforward(x)\n",
    "        x = self.ff_layer_norm(x + self.dropout(_x))\n",
    "        #x = [batch size, len, hid dim]\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "hid_dim    = 256                \n",
    "dec_layers = 3               \n",
    "dec_heads  = 8\n",
    "dec_pf_dim = 512\n",
    "dec_dropout = 0.1     \n",
    "lr = 1e-3                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 16,727,587 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = Decoder(vocab_size, hid_dim, dec_layers, dec_heads, dec_pf_dim, dec_dropout, device, PAD_IDX).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'The model has {num_params:,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len, idx):\n",
    "    #data #[batch size, bunch of tokens]\n",
    "    src    = data[:, idx:idx+seq_len]                   \n",
    "    target = data[:, idx+1:idx+seq_len+1]  #target simply is ahead of src by 1            \n",
    "    return src, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, data, optimizer, criterion, batch_size, seq_len, clip, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    # drop all batches that are not a multiple of seq_len\n",
    "    # data #[batch size, bunch of tokens]\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]  #we need to -1 because we start at 0\n",
    "    num_batches = data.shape[-1]\n",
    "        \n",
    "    for idx in tqdm(range(0, num_batches - 1, seq_len), desc='Training: ',leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        src, target = get_batch(data, seq_len, idx) #src, target: [batch size, seq len]\n",
    "        src, target = src.to(device), target.to(device)\n",
    "        batch_size = src.shape[0]\n",
    "        prediction, _ = model(src)               \n",
    "\n",
    "        #need to reshape because criterion expects pred to be 2d and target to be 1d\n",
    "        prediction = prediction.reshape(batch_size * seq_len, -1)  #prediction: [batch size * seq len, vocab size]  \n",
    "        target = target.reshape(-1)\n",
    "        loss = criterion(prediction, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * seq_len\n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion, batch_size, seq_len, device):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    model.eval()\n",
    "    num_batches = data.shape[-1]\n",
    "    data = data[:, :num_batches - (num_batches -1) % seq_len]\n",
    "    num_batches = data.shape[-1]\n",
    "    \n",
    "    decoded_batch_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, num_batches - 1, seq_len):\n",
    "            src, target = get_batch(data, seq_len, idx)\n",
    "            src, target = src.to(device), target.to(device)\n",
    "            #target = [batch size, dec len]\n",
    "\n",
    "            batch_size= src.shape[0]\n",
    "            prediction, _ = model(src)\n",
    "            #prediction = [batch size, dec len, output_dim]\n",
    "            \n",
    "            prediction = prediction.reshape(batch_size * seq_len, -1)\n",
    "            target = target.reshape(-1)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            epoch_loss += loss.item() * seq_len\n",
    "            \n",
    "    #decoding using beam_search as example (you don't need to put here, because beam_search is for intference)\n",
    "    decoded_batch = model.beam_decode()\n",
    "    print(\"Sample beam sentence: \" + \" \".join(decoded_batch))\n",
    "            \n",
    "    return epoch_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will be using a `ReduceLROnPlateau` learning scheduler which decreases the learning rate by a factor, if the loss don't improve by a certain epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry potter is a few years .\n",
      "Epoch: 1:\n",
      "\tTrain Perplexity: 530.221\n",
      "\tValid Perplexity: 295.877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry potter is used in the most recent\n",
      "Epoch: 2:\n",
      "\tTrain Perplexity: 265.409\n",
      "\tValid Perplexity: 241.811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry potter is one of the most common\n",
      "Epoch: 3:\n",
      "\tTrain Perplexity: 193.361\n",
      "\tValid Perplexity: 220.590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry potter is one of the most common\n",
      "Epoch: 4:\n",
      "\tTrain Perplexity: 154.279\n",
      "\tValid Perplexity: 213.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry potter is the most common starling of\n",
      "Epoch: 5:\n",
      "\tTrain Perplexity: 130.375\n",
      "\tValid Perplexity: 211.744\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "seq_len  = 50 #<----decoding length\n",
    "clip    = 0.25\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=0)\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_data, optimizer, criterion, \n",
    "                batch_size, seq_len, clip, device)\n",
    "    valid_loss = evaluate(model, valid_data, criterion, batch_size, \n",
    "                seq_len, device)\n",
    "\n",
    "    lr_scheduler.step(valid_loss)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-val-tr_lm.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch+1}:')\n",
    "    print(f'\\tTrain Perplexity: {math.exp(train_loss):.3f}')\n",
    "    print(f'\\tValid Perplexity: {math.exp(valid_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample beam sentence: harry potter is the most common starling of\n",
      "Test Perplexity: 194.983\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best-val-tr_lm.pt',  map_location=device))\n",
    "test_loss = evaluate(model, test_data, criterion, batch_size, seq_len, device)\n",
    "print(f'Test Perplexity: {math.exp(test_loss):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. Real-world inference\n",
    "\n",
    "Here I only use pure sampling.  You may want to put the beam search here and compare.  I will leave them as your practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(prompt, max_seq_len, temperature, model, tokenizer, vocab, device, seed=None):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    model.eval()\n",
    "    tokens = tokenizer(prompt)\n",
    "    indices = [vocab[t] for t in tokens]\n",
    "    batch_size = 1\n",
    "    with torch.no_grad():\n",
    "        for i in range(max_seq_len):\n",
    "            src = torch.LongTensor([indices]).to(device)\n",
    "            prediction, _ = model(src)\n",
    "            \n",
    "            #prediction: [batch size, seq len, vocab size]\n",
    "            #prediction[:, -1]: [batch size, vocab size] #probability of last vocab\n",
    "            \n",
    "            probs = torch.softmax(prediction[:, -1] / temperature, dim=-1)  \n",
    "            prediction = torch.multinomial(probs, num_samples=1).item()    \n",
    "            \n",
    "            while prediction == vocab['<unk>']: #if it is unk, we sample again\n",
    "                prediction = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "            if prediction == vocab['<eos>']:    #if it is eos, we stop\n",
    "                break\n",
    "\n",
    "            indices.append(prediction) #autoregressive, thus output becomes input\n",
    "            \n",
    "            #####################################################################\n",
    "            #I only do pure sampling....\n",
    "            #you may want to compare here with top-k, top-p, and beam search here\n",
    "            #####################################################################\n",
    "\n",
    "    itos = vocab.get_itos()\n",
    "    tokens = [itos[i] for i in indices]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "harry potter is the most popular culture .\n",
      "\n",
      "0.7\n",
      "harry potter is the man of the kings and the year of a few @-@ old vic @-@ old vic scene , and the third @-@ year @-@ old testament , and was\n",
      "\n",
      "0.75\n",
      "harry potter is the man of the kings and the year of a few @-@ old vic @-@ old vic scene , and the third @-@ year @-@ old testament , as follows\n",
      "\n",
      "0.8\n",
      "harry potter is the man of the kings and the year in a 2 – 0 – 11 season record title , and the third season . in 2012 , the crimson tide\n",
      "\n",
      "1.0\n",
      "harry potter is the spatial man of the kings and the year in a cruciform province .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Harry Potter is '\n",
    "max_seq_len = 30\n",
    "seed = 0\n",
    "\n",
    "#smaller the temperature, more diverse tokens but comes \n",
    "#with a tradeoff of less-make-sense sentence\n",
    "temperatures = [0.5, 0.7, 0.75, 0.8, 1.0]\n",
    "for temperature in temperatures:\n",
    "    generation = generate(prompt, max_seq_len, temperature, model, tokenizer, \n",
    "                          vocab, device, seed)\n",
    "    print(str(temperature)+'\\n'+' '.join(generation)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
