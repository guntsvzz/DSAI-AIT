{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Attetion :\n",
    "\n",
    "$e_i = s^T h_i \\in \\mathbb{R} ;d_1 = d_2$\n",
    "\n",
    "Multiplicative Attention : \n",
    "\n",
    "$e_i = s^T W h_i \\in \\mathbb{R} ; \\textbf{W} \\in \\mathbb{R}^{d_2 × d_1}$\n",
    "\n",
    "Addtive Attention : ( $d_3$ is a hyperparameter )\n",
    "\n",
    "$e_i = v^t tanh ( \\textbf{W}_1 h_i + \\textbf{W}_2 s) \\in \\mathbb{R} ; $\n",
    "\n",
    "$ \\textbf{W}_1 \\in \\mathbb{R}^{d_3 × d_1}, \\textbf{W}_2 \\in \\mathbb{R}^{d_3 × d_2},v \\in \\mathbb{R}^{d_3}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim, variants):\n",
    "        super().__init__()\n",
    "        self.variants = variants\n",
    "        self.v = nn.Linear(hid_dim, 1, bias = False)\n",
    "        self.W = nn.Linear(hid_dim,     hid_dim) #for decoder\n",
    "        self.U = nn.Linear(hid_dim * 2, hid_dim) #for encoder outputs\n",
    "                \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        \n",
    "        #hidden = [batch size, hid dim]\n",
    "        #encoder_outputs = [src len, batch size, hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #encoder_outputs = [batch size, src len, hid dim * 2]\n",
    "\n",
    "        if self.variants == 'additive': #work\n",
    "            #repeat decoder hidden state src_len times\n",
    "            hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "            #hidden = [batch size, src len, hid dim]\n",
    "            \n",
    "            energy = torch.tanh(self.W(hidden) + self.U(encoder_outputs))\n",
    "            #energy = [batch size, src len, hid dim]\n",
    "            \n",
    "            attention = self.v(energy).squeeze(2)\n",
    "            #attention = [batch size, src len]\n",
    "\n",
    "            #use masked_fill_ if you want in-place\n",
    "            attention = attention.masked_fill(mask, -1e10)\n",
    "            \n",
    "        elif self.variants == 'general': #work\n",
    "            hidden = hidden.unsqueeze(1).repeat(1, 1, 2)\n",
    "            #hidden = [batch size, 1, hid dimb * 2]\n",
    "            #encoder_outputs = [batch size, hid dim * 2, src len]\n",
    "\n",
    "            energy = torch.bmm(hidden, encoder_outputs.transpose(1, 2))\n",
    "            #energy = [batch size, 1, src len]\n",
    "            attention = energy.squeeze(1)\n",
    "            #attention = [batch size, src len]\n",
    "\n",
    "        elif self.variants == 'multiplicative':\n",
    "            wh = self.W(hidden).unsqueeze(1).repeat(1, 1, 2)\n",
    "            #wh = [batch size, 1, hid dim*2]\n",
    "            #encoder_outputs = [batch size, hid dim * 2, src len]\n",
    "\n",
    "            energy = torch.bmm(wh, encoder_outputs.transpose(1, 2))\n",
    "            attention = energy.squeeze(1)\n",
    "\n",
    "        #attention = [batch size, src len]\n",
    "        return F.softmax(attention, dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
