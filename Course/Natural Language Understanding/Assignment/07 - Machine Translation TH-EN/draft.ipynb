{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import random, math, time\n",
    "\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "#uncomment this if you are not using our department puffer\n",
    "import os\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ETL: Loading the dataset\n",
    "\n",
    "**Note**: Here I chose to translate English to German, simply it is easier for myself, since I don't understand German so it is difficult for me to imagine a sentence during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en_text</th>\n",
       "      <th>th_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Doesn't snap together well.</td>\n",
       "      <td>เข้ากันไม่ค่อยดี</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Charged it after every use as directed for abo...</td>\n",
       "      <td>เรียกเก็บเงินหลังจากใช้งานทุกครั้งตามที่กำกับไ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My son wanted this movie so badly, that he sai...</td>\n",
       "      <td>ลูกชายของฉันต้องการภาพยนตร์เรื่องนี้เพื่อไม่ดี...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>But his writing has degenerated in later books.</td>\n",
       "      <td>แต่หนังสือเล่มที่ผ่านมาของเขามันดูด้อยคุณภาพลง</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It was supposed to be a good bag, well you get...</td>\n",
       "      <td>มันควรที่จะเป็นกระเป๋าที่ดี เอ้อ เธอได้ในสิ่งท...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             en_text  \\\n",
       "0                        Doesn't snap together well.   \n",
       "1  Charged it after every use as directed for abo...   \n",
       "2  My son wanted this movie so badly, that he sai...   \n",
       "3    But his writing has degenerated in later books.   \n",
       "4  It was supposed to be a good bag, well you get...   \n",
       "\n",
       "                                             th_text  \n",
       "0                                   เข้ากันไม่ค่อยดี  \n",
       "1  เรียกเก็บเงินหลังจากใช้งานทุกครั้งตามที่กำกับไ...  \n",
       "2  ลูกชายของฉันต้องการภาพยนตร์เรื่องนี้เพื่อไม่ดี...  \n",
       "3     แต่หนังสือเล่มที่ผ่านมาของเขามันดูด้อยคุณภาพลง  \n",
       "4  มันควรที่จะเป็นกระเป๋าที่ดี เอ้อ เธอได้ในสิ่งท...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./scb-mt-en-th-2020/generated_reviews_crowd.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.values.tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA - simple investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24587"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size = len(dataset)\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(dataset)\n",
    "train_data_set_size = int(dataset_size *0.8)\n",
    "\n",
    "train_dataset = dataset[:train_data_set_size]\n",
    "test = dataset[train_data_set_size:]\n",
    "\n",
    "train_size = int(train_data_set_size *0.8)\n",
    "train = train_dataset[:train_size]\n",
    "val = train_dataset[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum((len(train),len(val),len(test))) == dataset_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ตัด', 'คำ', 'ได้', 'ดี', 'มาก']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ตัด', 'คำ', 'ได้', 'ดี', 'มาก']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from attacut import tokenize, Tokenizer\n",
    "\n",
    "# tokenize `txt` using our best model `attacut-sc`\n",
    "txt = 'ตัดคำได้ดีมาก'\n",
    "words = tokenize(txt)\n",
    "print(words)\n",
    "# alternatively, an AttaCut tokenizer might be instantiated directly, allowing\n",
    "# one to specify whether to use `attacut-sc` or `attacut-c`.\n",
    "atta = Tokenizer(model=\"attacut-sc\")\n",
    "words = atta.tokenize(txt)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = 'th'\n",
    "TRG_LANGUAGE = 'en'\n",
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 17:18:09.632141: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-28 17:18:10.864477: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-28 17:18:10.864638: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-28 17:18:10.864652: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-28 17:18:11.895385: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-02-28 17:18:11.895537: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-02-28 17:18:11.895606: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-28 17:18:11.895670: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-02-28 17:18:11.895726: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "from attacut import tokenize, Tokenizer\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "token_transform[SRC_LANGUAGE] = Tokenizer(model=\"attacut-sc\")\n",
    "token_transform[TRG_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  ขยะชัดๆ !! ไม่คุ้มค่าเลย !!!!!\n",
      "Tokenization:  ['ขยะ', 'ชัด', 'ๆ', ' ', '!', '!', ' ', 'ไม่', 'คุ้ม', 'ค่า', 'เลย', ' ', '!', '!!!', '!']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the thai part\n",
    "print(\"Sentence: \", train[0][1])\n",
    "print(\"Tokenization: \", token_transform[SRC_LANGUAGE].tokenize(train[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  A piece of junk!! Not worth it!!!!!.\n",
      "Tokenization:  ['A', 'piece', 'of', 'junk', '!', '!', 'Not', 'worth', 'it', '!', '!', '!', '!', '!', '.']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the english part\n",
    "print(\"Sentence: \", train[0][0])\n",
    "print(\"Tokenization: \", token_transform[TRG_LANGUAGE](train[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "# here data can be `train` or `val` or `test`\n",
    "def yield_tokens(data, language):\n",
    "    language_index = {SRC_LANGUAGE: 0, TRG_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data:\n",
    "        if language_index[language] == 0:\n",
    "            yield token_transform[language].tokenize(data_sample[language_index[language]])\n",
    "        elif language_index[language] == 1:\n",
    "            yield token_transform[language](data_sample[language_index[language]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    # Create torchtext's Vocab object \n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train, ln), \n",
    "                                                    min_freq=2,   #if not, everything will be treated as UNK\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True) #indicates whether to insert symbols at the beginning or at the end                                            \n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 581, 0, 581]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[TRG_LANGUAGE](['here', 'is', 'a', 'unknownword', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ยิ่งไปกว่านั้น'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()\n",
    "# #print 1816, for example\n",
    "mapping[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "mapping[0]\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "955"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<attacut.tokenizer.Tokenizer at 0x7fce0d2b7580>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_transform['th']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'th': Vocab(), 'en': Vocab()}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            if transform == token_transform[SRC_LANGUAGE]:\n",
    "                txt_input = transform.tokenize(txt_input)\n",
    "            else:\n",
    "                txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for trg_sample, src_sample in batch:\n",
    "        processed_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(val, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test, batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for th, _, en in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thai shape:  torch.Size([52, 64])\n",
      "English shape:  torch.Size([48, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"Thai shape: \", th.shape)  # (seq len, batch_size)\n",
    "print(\"English shape: \", en.shape)   # (seq len, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader_length = len(list(iter(train_loader)))\n",
    "# val_loader_length   = len(list(iter(valid_loader)))\n",
    "# test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = 246\n",
    "val_loader_length = 62\n",
    "test_loader_length = 77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(246, 62, 77)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader_length, val_loader_length, test_loader_length  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model\n",
    "\n",
    "### Encoder\n",
    "\n",
    "The changes here all within the `forward` method. It now accepts the lengths of the source sentences as well as the sentences themselves. \n",
    "\n",
    "After the source sentence (padded automatically within the iterator) has been embedded, we can then use `pack_padded_sequence` on it with the lengths of the sentences. Note that the tensor containing the lengths of the sequences must be a CPU tensor as of the latest version of PyTorch, which we explicitly do so with `to('cpu')`. `packed_embedded` will then be our packed padded sequence. This can be then fed to our RNN as normal which will return `packed_outputs`, a packed tensor containing all of the hidden states from the sequence, and `hidden` which is simply the final hidden state from our sequence. `hidden` is a standard tensor and not packed in any way, the only difference is that as the input was a packed sequence, this tensor is from the final **non-padded element** in the sequence.\n",
    "\n",
    "We then unpack our `packed_outputs` using `pad_packed_sequence` which returns the `outputs` and the lengths of each, which we don't need. \n",
    "\n",
    "The first dimension of `outputs` is the padded sequence lengths however due to using a packed padded sequence the values of tensors when a padding token was the input will be all zeros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn       = nn.GRU(emb_dim, hid_dim, bidirectional=True)\n",
    "        self.fc        = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.dropout   = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        \n",
    "        #src: [src len, batch size]\n",
    "        #src len: [batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, \n",
    "                            src_len.to('cpu'), enforce_sorted=False)\n",
    "        \n",
    "        #packed_outputs contain all hidden states including padding guy\n",
    "        #hidden contains the last hidden states of the non-padded guys\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "        #hidden: [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #convert packed_outputs to the guy that does not contain hidden states for padding\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
    "        #outputs: [src len, batch size, hid dim * num directions]\n",
    "        \n",
    "        #take the last hidden states from backward and forward\n",
    "        #hidden: (f, b, f, b)\n",
    "        forward  = hidden[-2, :, :]  #[batch size, hid dim]\n",
    "        backward = hidden[-1, :, :]  #[batch size, hid dim]\n",
    "        \n",
    "        hidden = torch.tanh(self.fc(torch.cat((forward, backward), dim = 1))) \n",
    "        #hidden: [batch size, hid dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "The attention used here is additive attention which is defined by:\n",
    "\n",
    "$$e = v\\text{tanh}(W_hh + W_ss + b)$$\n",
    "\n",
    "Previously, we allowed this module to \"pay attention\" to padding tokens within the source sentence. However, using *masking*, we can force the attention to only be over non-padding elements.\n",
    "\n",
    "The `forward` method now takes a `mask` input. This is a `[batch size, source sentence length]` tensor that is 1 when the source sentence token is not a padding token, and 0 when it is a padding token. For example, if the source sentence is: `[\"hello\", \"how\", \"are\", \"you\", \"?\", `<pad>`, `<pad>`]`, then the mask would be `[1, 1, 1, 1, 1, 0, 0]`.\n",
    "\n",
    "We apply the mask after the attention has been calculated, but before it has been normalized by the `softmax` function. It is applied using `masked_fill`. This fills the tensor at each element where the first argument (`mask == 0`) is true, with the value given by the second argument (`-1e10`). In other words, it will take the un-normalized attention values, and change the attention values over padded elements to be `-1e10`. As these numbers will be miniscule compared to the other values they will become zero when passed through the `softmax` layer, ensuring no attention is payed to padding tokens in the source sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim, variants ='additive'):\n",
    "        super().__init__()\n",
    "        self.variants = variants\n",
    "        self.v = nn.Linear(hid_dim, 1, bias = False)\n",
    "        self.W = nn.Linear(hid_dim,     hid_dim) #for decoder\n",
    "        self.U = nn.Linear(hid_dim * 2, hid_dim) #for encoder outputs \n",
    "                \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        \n",
    "        #hidden = [batch size, hid dim]\n",
    "        #encoder_outputs = [src len, batch size, hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        #hidden = [batch size, src len, hid dim]\n",
    "\n",
    "        if self.variants == 'additive':\n",
    "            encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "            #encoder_outputs = [batch size, src len, hid dim * 2]\n",
    "            energy = torch.tanh(self.W(hidden) + self.U(encoder_outputs))\n",
    "            #energy = [batch size, src len, hid dim]\n",
    "            attention = self.v(energy).squeeze(2)\n",
    "            #attention = [batch size, src len]\n",
    "\n",
    "        elif self.variants == 'general':\n",
    "            energy = torch.bmm(encoder_outputs, hidden)\n",
    "            print('energy',energy)\n",
    "            attention = energy.squeeze(2)\n",
    "\n",
    "        elif self.variants == 'multiplicative':\n",
    "            wh = self.W(hidden)\n",
    "            energy = torch.bmm(encoder_outputs,wh)\n",
    "            attention = energy.squeeze(2)\n",
    "        \n",
    "        #use masked_fill_ if you want in-place\n",
    "        attention = attention.masked_fill(mask, -1e10)\n",
    "        \n",
    "        return F.softmax(attention, dim = 1)\n",
    "    \n",
    "    #inspiration : http://www.adeveloperdiary.com/data-science/deep-learning/nlp/machine-translation-using-attention-with-pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the masked_fill work, just so you have faith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[           9, -10000000000,            7,            2, -10000000000,\n",
      "         -10000000000],\n",
      "        [          99, -10000000000, -10000000000,            0,            8,\n",
      "                    9]])\n"
     ]
    }
   ],
   "source": [
    "#example of masked_fill\n",
    "#reall that 1 is pad_idx\n",
    "x = torch.tensor([ [9, 1, 7, 2, 1, 1], [99, 1, 1, 0, 8, 9] ])\n",
    "\n",
    "mask = (x == PAD_IDX)\n",
    "\n",
    "x.masked_fill_(mask, -1e10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "The decoder only needs a few small changes. It needs to accept a mask over the source sentence and pass this to the attention module. As we want to view the values of attention during inference, we also return the attention tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.gru = nn.GRU((hid_dim * 2) + emb_dim, hid_dim)\n",
    "        self.fc = nn.Linear((hid_dim * 2) + hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, hid dim]\n",
    "        #encoder_outputs = [src len, batch size, hid dim * 2]\n",
    "        #mask = [batch size, src len]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "        #a = [batch size, src len]\n",
    "\n",
    "        a = a.unsqueeze(1)\n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #encoder_outputs = [batch size, src len, hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs) #Ct\n",
    "        #weighted = [batch size, 1, hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        #weighted = [1, batch size, hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        #rnn_input = [1, batch size, (hid dim * 2) + emb dim]\n",
    "        \n",
    "        output, hidden = self.gru(rnn_input, hidden.unsqueeze(0))\n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        assert (output == hidden).all()\n",
    "\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output   = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together (become Seq2Seq!)\n",
    "\n",
    "The overarching seq2seq model also needs a few changes for packed padded sequences, masking and inference. \n",
    "\n",
    "We need to tell it what the indexes are for the pad token and also pass the source sentence lengths as input to the `forward` method.\n",
    "\n",
    "We use the pad token index to create the masks, by creating a mask tensor that is 1 wherever the source sentence is not equal to the pad token. This is all done within the `create_mask` function.\n",
    "\n",
    "The sequence lengths as needed to pass to the encoder to use packed padded sequences.\n",
    "\n",
    "The attention at each time-step is stored in the `attentions` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqPackedAttention(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        #src: [src len, batch size]\n",
    "        \n",
    "        mask = (src == self.src_pad_idx).permute(1, 0)\n",
    "        #mask: [batch size, src len]\n",
    "        #we need to permute to make the mask same shape as attention...\n",
    "        return mask\n",
    "\n",
    "        \n",
    "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src: [src len, batch size]\n",
    "        #src len: [batch size]\n",
    "        #trg: [trg len, batch size]\n",
    "        \n",
    "        batch_size = src.shape[1]\n",
    "        trg_len    = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim #define in decoder\n",
    "        \n",
    "        #because decoder decodes each step....let's create a list that gonna append the result to this guy\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #because decoder decodes each step....let's memorize the attention done in each step....\n",
    "        attentions = torch.zeros(trg_len, batch_size, src.shape[0]).to(self.device)\n",
    "        \n",
    "        #let's start!!!\n",
    "        #1. encoder\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "        #encoder_outputs: [src len, batch size, hid dim * num directions]\n",
    "        #hidden: [batch size, hid dim]\n",
    "        \n",
    "        #set the first input to the decoder\n",
    "        input_ = trg[0,:]  #basically <sos>\n",
    "        \n",
    "        #create the mask for use in this step\n",
    "        mask = self.create_mask(src)\n",
    "        \n",
    "        #2. for each of trg word\n",
    "        for t in range(1, trg_len):\n",
    "\n",
    "            #3. decode (hidden is always carry forward)\n",
    "            output, hidden, attention = self.decoder(input_, hidden, encoder_outputs, mask)\n",
    "            #output:   [batch size, output_dim]\n",
    "            #hidden:   [batch size, hid_dim]\n",
    "            #attention::[batch size, src len]  ==> how each of src token is important to input_ \n",
    "            \n",
    "            #4. append the results to outputs and attentions\n",
    "            outputs[t] = output\n",
    "            attentions[t] = attention\n",
    "            \n",
    "            #5. get the result, using argmax\n",
    "            top1 = output.argmax(1)  #find the maximum index of dimension 1, which is output_dim\n",
    "            \n",
    "            #6. based on the teacher forcing ratio, \n",
    "            teacher_force_or_not = random.random() < teacher_forcing_ratio\n",
    "                    #if teacher forcing, next input is the next trg\n",
    "                    #if no teacher forcing, the next input is the argmax guy...\n",
    "            input_ = trg[t] if teacher_force_or_not else top1  #autoregressive\n",
    "            \n",
    "        return outputs, attentions #outputs for predicting the word, attentions to see which word is important\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training\n",
    "\n",
    "We use a simplified version of the weight initialization scheme used in the paper. Here, we will initialize all biases to zero and all weights from $\\mathcal{N}(0, 0.01)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 General Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqPackedAttention(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(6277, 256)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "      (W): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (U): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(955, 256)\n",
       "    (gru): GRU(1280, 512)\n",
       "    (fc): Linear(in_features=1792, out_features=955, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "emb_dim     = 256  \n",
    "hid_dim     = 512  \n",
    "dropout     = 0.5\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "\n",
    "attn = Attention(hid_dim, variants='general')\n",
    "enc  = Encoder(input_dim,  emb_dim,  hid_dim, dropout)\n",
    "dec  = Decoder(output_dim, emb_dim,  hid_dim, dropout, attn)\n",
    "\n",
    "model_general = Seq2SeqPackedAttention(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "model_general.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1606912\n",
      "393216\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "393216\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "524288\n",
      "   512\n",
      "   512\n",
      "262144\n",
      "   512\n",
      "524288\n",
      "   512\n",
      "244480\n",
      "1966080\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "1711360\n",
      "   955\n",
      "______\n",
      "9997499\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model_general)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss function calculates the average loss per token, however by passing the index of the `<pad>` token as the `ignore_index` argument we ignore the loss whenever the target token is a padding token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model_general.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is very similar to part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_length, trg in loader:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, attentions = model(src, src_length, trg)\n",
    "        \n",
    "        #trg    = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        #the loss function only works on 2d inputs with 1d targets thus we need to flatten each of them\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg    = trg[1:].view(-1)\n",
    "        #trg    = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #clip the gradients to prevent them from exploding (a common issue in RNNs)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an optimizer or a clip value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "        \n",
    "    #turn off dropout (and batch norm if used)\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_length, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, attentions = model(src, src_length, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg    = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg    = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [87, 1024] but got: [64, 87].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[172], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     12\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 14\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model_general, train_loader, optimizer, criterion, clip, train_loader_length)\n\u001b[1;32m     15\u001b[0m     valid_loss \u001b[39m=\u001b[39m evaluate(model_general, valid_loader, criterion, val_loader_length)\n\u001b[1;32m     17\u001b[0m     \u001b[39m#for plotting\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[169], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion, clip, loader_length)\u001b[0m\n\u001b[1;32m      9\u001b[0m trg \u001b[39m=\u001b[39m trg\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m output, attentions \u001b[39m=\u001b[39m model(src, src_length, trg)\n\u001b[1;32m     15\u001b[0m \u001b[39m#trg    = [trg len, batch size]\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m#output = [trg len, batch size, output dim]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m output_dim \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[156], line 51\u001b[0m, in \u001b[0;36mSeq2SeqPackedAttention.forward\u001b[0;34m(self, src, src_len, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39m#2. for each of trg word\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, trg_len):\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m     \u001b[39m#3. decode (hidden is always carry forward)\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     output, hidden, attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(input_, hidden, encoder_outputs, mask)\n\u001b[1;32m     52\u001b[0m     \u001b[39m#output:   [batch size, output_dim]\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[39m#hidden:   [batch size, hid_dim]\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[39m#attention::[batch size, src len]  ==> how each of src token is important to input_ \u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \n\u001b[1;32m     56\u001b[0m     \u001b[39m#4. append the results to outputs and attentions\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     outputs[t] \u001b[39m=\u001b[39m output\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[155], line 25\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, input, hidden, encoder_outputs, mask)\u001b[0m\n\u001b[1;32m     22\u001b[0m embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(\u001b[39minput\u001b[39m))\n\u001b[1;32m     23\u001b[0m \u001b[39m#embedded = [1, batch size, emb dim]\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(hidden, encoder_outputs, mask)\n\u001b[1;32m     26\u001b[0m \u001b[39m#a = [batch size, src len]\u001b[39;00m\n\u001b[1;32m     28\u001b[0m a \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[135], line 30\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, hidden, encoder_outputs, mask)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[39m#attention = [batch size, src len]\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariants \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgeneral\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 30\u001b[0m     energy \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbmm(encoder_outputs, hidden)\n\u001b[1;32m     31\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39menergy\u001b[39m\u001b[39m'\u001b[39m,energy)\n\u001b[1;32m     32\u001b[0m     attention \u001b[39m=\u001b[39m energy\u001b[39m.\u001b[39msqueeze(\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [87, 1024] but got: [64, 87]."
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 1\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model_general.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model_general, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(model_general, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_general.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Seq2SeqPackedAttention:\n\tsize mismatch for encoder.embedding.weight: copying a param with shape torch.Size([5174, 256]) from checkpoint, the shape in current model is torch.Size([6277, 256]).\n\tsize mismatch for decoder.embedding.weight: copying a param with shape torch.Size([6433, 256]) from checkpoint, the shape in current model is torch.Size([955, 256]).\n\tsize mismatch for decoder.fc.weight: copying a param with shape torch.Size([6433, 1792]) from checkpoint, the shape in current model is torch.Size([955, 1792]).\n\tsize mismatch for decoder.fc.bias: copying a param with shape torch.Size([6433]) from checkpoint, the shape in current model is torch.Size([955]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[148], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_general\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(save_path))\n\u001b[1;32m      2\u001b[0m test_loss \u001b[39m=\u001b[39m evaluate(model_general, test_loader, criterion, test_loader_length)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m| Test Loss: \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | Test PPL: \u001b[39m\u001b[39m{\u001b[39;00mmath\u001b[39m.\u001b[39mexp(test_loss)\u001b[39m:\u001b[39;00m\u001b[39m7.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m |\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1671\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1666\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1667\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1668\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1670\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1671\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1672\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1673\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Seq2SeqPackedAttention:\n\tsize mismatch for encoder.embedding.weight: copying a param with shape torch.Size([5174, 256]) from checkpoint, the shape in current model is torch.Size([6277, 256]).\n\tsize mismatch for decoder.embedding.weight: copying a param with shape torch.Size([6433, 256]) from checkpoint, the shape in current model is torch.Size([955, 256]).\n\tsize mismatch for decoder.fc.weight: copying a param with shape torch.Size([6433, 1792]) from checkpoint, the shape in current model is torch.Size([955, 1792]).\n\tsize mismatch for decoder.fc.bias: copying a param with shape torch.Size([6433]) from checkpoint, the shape in current model is torch.Size([955])."
     ]
    }
   ],
   "source": [
    "model_general.load_state_dict(torch.load(save_path))\n",
    "test_loss = evaluate(model_general, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Multiplicative attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqPackedAttention(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(6277, 256)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "      (W): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (U): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(955, 256)\n",
       "    (gru): GRU(1280, 512)\n",
       "    (fc): Linear(in_features=1792, out_features=955, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "emb_dim     = 256  \n",
    "hid_dim     = 512  \n",
    "dropout     = 0.5\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "\n",
    "attn = Attention(hid_dim, variants='multiplicative')\n",
    "enc  = Encoder(input_dim,  emb_dim,  hid_dim, dropout)\n",
    "dec  = Decoder(output_dim, emb_dim,  hid_dim, dropout, attn)\n",
    "\n",
    "model_multiplicative = Seq2SeqPackedAttention(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "model_multiplicative.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1606912\n",
      "393216\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "393216\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "524288\n",
      "   512\n",
      "   512\n",
      "262144\n",
      "   512\n",
      "524288\n",
      "   512\n",
      "244480\n",
      "1966080\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "1711360\n",
      "   955\n",
      "______\n",
      "9997499\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model_multiplicative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss function calculates the average loss per token, however by passing the index of the `<pad>` token as the `ignore_index` argument we ignore the loss whenever the target token is a padding token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model_multiplicative.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encode torch.Size([1024, 54, 64])\n",
      "hidden torch.Size([64, 54, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1024) must match the size of tensor b (64) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     12\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 14\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model_general, train_loader, optimizer, criterion, clip, train_loader_length)\n\u001b[1;32m     15\u001b[0m     valid_loss \u001b[39m=\u001b[39m evaluate(model_general, valid_loader, criterion, val_loader_length)\n\u001b[1;32m     17\u001b[0m     \u001b[39m#for plotting\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[66], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion, clip, loader_length)\u001b[0m\n\u001b[1;32m      9\u001b[0m trg \u001b[39m=\u001b[39m trg\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m output, attentions \u001b[39m=\u001b[39m model(src, src_length, trg)\n\u001b[1;32m     15\u001b[0m \u001b[39m#trg    = [trg len, batch size]\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m#output = [trg len, batch size, output dim]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m output_dim \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[73], line 51\u001b[0m, in \u001b[0;36mSeq2SeqPackedAttention.forward\u001b[0;34m(self, src, src_len, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39m#2. for each of trg word\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, trg_len):\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m     \u001b[39m#3. decode (hidden is always carry forward)\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     output, hidden, attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(input_, hidden, encoder_outputs, mask)\n\u001b[1;32m     52\u001b[0m     \u001b[39m#output:   [batch size, output_dim]\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[39m#hidden:   [batch size, hid_dim]\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[39m#attention::[batch size, src len]  ==> how each of src token is important to input_ \u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \n\u001b[1;32m     56\u001b[0m     \u001b[39m#4. append the results to outputs and attentions\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     outputs[t] \u001b[39m=\u001b[39m output\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[72], line 25\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, input, hidden, encoder_outputs, mask)\u001b[0m\n\u001b[1;32m     22\u001b[0m embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(\u001b[39minput\u001b[39m))\n\u001b[1;32m     23\u001b[0m \u001b[39m#embedded = [1, batch size, emb dim]\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(hidden, encoder_outputs, mask)\n\u001b[1;32m     26\u001b[0m \u001b[39m#a = [batch size, src len]\u001b[39;00m\n\u001b[1;32m     28\u001b[0m a \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[58], line 27\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, hidden, encoder_outputs, mask)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mhidden\u001b[39m\u001b[39m'\u001b[39m,hidden\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     26\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariants \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgeneral\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 27\u001b[0m     energy \u001b[39m=\u001b[39m encoder_outputs\u001b[39m.\u001b[39;49mT \u001b[39m@\u001b[39;49m hidden\n\u001b[1;32m     28\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39menergy\u001b[39m\u001b[39m'\u001b[39m,energy)\n\u001b[1;32m     29\u001b[0m     attention \u001b[39m=\u001b[39m energy\u001b[39m.\u001b[39msqueeze(\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1024) must match the size of tensor b (64) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 10\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model_multiplicative.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model_general, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(model_general, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_general.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multiplicative.load_state_dict(torch.load(save_path))\n",
    "test_loss = evaluate(model_multiplicative, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Additive attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqPackedAttention(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(6277, 256)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "      (W): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (U): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(955, 256)\n",
       "    (gru): GRU(1280, 512)\n",
       "    (fc): Linear(in_features=1792, out_features=955, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "emb_dim     = 256  \n",
    "hid_dim     = 512  \n",
    "dropout     = 0.5\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "\n",
    "attn = Attention(hid_dim, variants='additive')\n",
    "enc  = Encoder(input_dim,  emb_dim,  hid_dim, dropout)\n",
    "dec  = Decoder(output_dim, emb_dim,  hid_dim, dropout, attn)\n",
    "\n",
    "model_additive = Seq2SeqPackedAttention(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "model_additive.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1606912\n",
      "393216\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "393216\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "524288\n",
      "   512\n",
      "   512\n",
      "262144\n",
      "   512\n",
      "524288\n",
      "   512\n",
      "244480\n",
      "1966080\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "1711360\n",
      "   955\n",
      "______\n",
      "9997499\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model_additive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss function calculates the average loss per token, however by passing the index of the `<pad>` token as the `ignore_index` argument we ignore the loss whenever the target token is a padding token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model_additive.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [65, 1024] but got: [64, 65].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[152], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     12\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 14\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model_general, train_loader, optimizer, criterion, clip, train_loader_length)\n\u001b[1;32m     15\u001b[0m     valid_loss \u001b[39m=\u001b[39m evaluate(model_general, valid_loader, criterion, val_loader_length)\n\u001b[1;32m     17\u001b[0m     \u001b[39m#for plotting\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[143], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion, clip, loader_length)\u001b[0m\n\u001b[1;32m      9\u001b[0m trg \u001b[39m=\u001b[39m trg\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m output, attentions \u001b[39m=\u001b[39m model(src, src_length, trg)\n\u001b[1;32m     15\u001b[0m \u001b[39m#trg    = [trg len, batch size]\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m#output = [trg len, batch size, output dim]\u001b[39;00m\n\u001b[1;32m     17\u001b[0m output_dim \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[138], line 51\u001b[0m, in \u001b[0;36mSeq2SeqPackedAttention.forward\u001b[0;34m(self, src, src_len, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39m#2. for each of trg word\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, trg_len):\n\u001b[1;32m     49\u001b[0m \n\u001b[1;32m     50\u001b[0m     \u001b[39m#3. decode (hidden is always carry forward)\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     output, hidden, attention \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(input_, hidden, encoder_outputs, mask)\n\u001b[1;32m     52\u001b[0m     \u001b[39m#output:   [batch size, output_dim]\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     \u001b[39m#hidden:   [batch size, hid_dim]\u001b[39;00m\n\u001b[1;32m     54\u001b[0m     \u001b[39m#attention::[batch size, src len]  ==> how each of src token is important to input_ \u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \n\u001b[1;32m     56\u001b[0m     \u001b[39m#4. append the results to outputs and attentions\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     outputs[t] \u001b[39m=\u001b[39m output\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[137], line 25\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, input, hidden, encoder_outputs, mask)\u001b[0m\n\u001b[1;32m     22\u001b[0m embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(\u001b[39minput\u001b[39m))\n\u001b[1;32m     23\u001b[0m \u001b[39m#embedded = [1, batch size, emb dim]\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m a \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(hidden, encoder_outputs, mask)\n\u001b[1;32m     26\u001b[0m \u001b[39m#a = [batch size, src len]\u001b[39;00m\n\u001b[1;32m     28\u001b[0m a \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[135], line 30\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, hidden, encoder_outputs, mask)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[39m#attention = [batch size, src len]\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvariants \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgeneral\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> 30\u001b[0m     energy \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbmm(encoder_outputs, hidden)\n\u001b[1;32m     31\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39menergy\u001b[39m\u001b[39m'\u001b[39m,energy)\n\u001b[1;32m     32\u001b[0m     attention \u001b[39m=\u001b[39m energy\u001b[39m.\u001b[39msqueeze(\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [65, 1024] but got: [64, 65]."
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 10\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model_additive.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model_general, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(model_general, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_general.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_additive.load_state_dict(torch.load(save_path))\n",
    "test_loss = evaluate(model_additive, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on some random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = text_transform[SRC_LANGUAGE](sample[0]).to(device)\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = text_transform[TRG_LANGUAGE](sample[1]).to(device)\n",
    "trg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_text.reshape(-1, 1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = trg_text.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, attentions = model(src_text, text_length, trg_text, 0) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape #trg_len, batch_size, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since batch size is 1, we just take off that dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall remove the first token since it's zeroes anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output[1:]\n",
    "output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just take the top token with highest probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = output.argmax(1) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mapping of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention\n",
    "\n",
    "Let's display the attentions to understand how the source text links with the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](sample[0]) + ['<eos>']\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_attention(src_tokens, trg_tokens, attentions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from attacut import tokenize, Tokenizer\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer_target = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "atta = Tokenizer(model=\"attacut-sc\")\n",
    "\n",
    "def converting_tokenizer(data):\n",
    "    word_pair_tokenzier = {}\n",
    "    with tqdm(range(data.shape[0])) as pbar:\n",
    "        for i,text in data.iterrows():\n",
    "            source = atta.tokenize(text[1]) #th\n",
    "            target = tokenizer_target(text[0]) #en\n",
    "            word_pair_tokenzier.append((source,target))\n",
    "            pbar.update(1)\n",
    "    return word_pair_tokenzier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_pair_tokenzier = converting_tokenizer(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# # Store data \n",
    "# word_pair_tokenzier = converting_tokenizer(df_train)\n",
    "# with open('tokenizer-attacut.pickle', 'wb') as handle:\n",
    "    # pickle.dump(word_pair_tokenzier, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
