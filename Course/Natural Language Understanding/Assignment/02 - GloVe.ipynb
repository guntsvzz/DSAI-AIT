{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - GloVe : Global Vectors for Word Representation\n",
    "Constraint: Only use our code (not other code....)\n",
    "\n",
    "1. I guess you already try a bigger corpus\n",
    "2. I guess you already try window size 2\n",
    "3. I guess you already have skipgram, skipgram(neg), cbow, glove\n",
    "\n",
    "Do this:\n",
    "1. Compare them based on syntactic accuracy and semantic accuracy, similar to how is done in https://nlp.stanford.edu/pubs/glove.pdf (see Table 2) - NO NEED to try 1000 or 300 embed size.....I just want you to learn how to do experiment.....\n",
    "2. Try to find a correlation with just ONE similarity dataset (which humans judge how similar is two words.....)\n",
    "\n",
    "Point criteria:\n",
    "0:  Not done\n",
    "1: ok\n",
    "2: with comments / explanation / figures just like how Chaky explain thing....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Load Data\n",
    "A real corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19558, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('./en_core_web_sm/')\n",
    "text = open('./dataset/questions-words.txt',mode='r')\n",
    "df = pd.DataFrame(text.readlines())\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 507, 5032, 5899, 8367, 8874, 9867, 10680, 12013, 13136, 14193, 15793, 17354, 18687]\n"
     ]
    }
   ],
   "source": [
    "#Check Header \n",
    "header = df[0].str.startswith(':')\n",
    "index_list = np.where(header)[0].tolist()\n",
    "print(index_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19544, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove Header\n",
    "#format drop(index,inplace=True)\n",
    "df.drop(index_list, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to using only 5000 corpus\n",
    "df = df[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df_col):\n",
    "    corpus = []\n",
    "    for item in df_col:\n",
    "        item = re.sub('[^A-Za-z0-9]+', ' ', str(item)) # remove special characters\n",
    "        item = item.lower() # lower all characters\n",
    "        item = item.split() # split data\n",
    "        corpus.append(' '.join(str(x) for x in item))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['athens', 'greece', 'baghdad', 'iraq'],\n",
       " ['athens', 'greece', 'bangkok', 'thailand'],\n",
       " ['athens', 'greece', 'beijing', 'china'],\n",
       " ['athens', 'greece', 'berlin', 'germany'],\n",
       " ['athens', 'greece', 'bern', 'switzerland']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. tokenize\n",
    "#data cleaned\n",
    "corpus = clean_data(df[0])\n",
    "#data tokenized\n",
    "corpus_tokenized = [sent.split(\" \") for sent in corpus]\n",
    "corpus_tokenized[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. numericalize (vocab)\n",
    "#2.1 get all the unique words\n",
    "#we want to flatten unit (basically merge all list)\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabs = list(set(flatten(corpus_tokenized)))\n",
    "\n",
    "#2.2 assign id to all these vocabs\n",
    "word2index = {v: idx for idx, v in enumerate(vocabs)}\n",
    "\n",
    "#adding unknown word\n",
    "vocabs.append('<UNK>')\n",
    "word2index['<UNK>'] = len(vocabs) - 1\n",
    "\n",
    "voc_size = len(vocabs)\n",
    "voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 233])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preparing all_vocabs\n",
    "batch_size = 2\n",
    "\n",
    "def prepare_seqeunce(seq, word2index):\n",
    "    #map(fucntion, list of something)\n",
    "    #map will look at each of element in this list, and apply this function\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_seqeunce(list(vocabs),word2index).expand(batch_size, voc_size)\n",
    "all_vocabs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = {v:k for k,v in word2index.items()}\n",
    "# index2word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Co-occurence Matrix X\n",
    "\n",
    "Count the occurrences of pair of words using window size of 1 (you can use 2, 3, 4, up to you)\n",
    "\n",
    "E.g., Dog loves to eat meal.\n",
    "\n",
    "['Dog','loves',1],['loves','to',1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use Counter to first count stuffs\n",
    "from collections import Counter\n",
    "# print(corpus_tokenized)\n",
    "\n",
    "#count the frequency of each word\n",
    "#we somehow need this to claiclate the probability Pi\n",
    "X_i = Counter(flatten(corpus_tokenized)) #merge all list ... (flattten is a function I defines)\n",
    "# X_i['apple'] #get the probability of apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_grams = []\n",
    "cbows = []\n",
    "window_size = 1\n",
    "#for each corpus\n",
    "for sent in corpus_tokenized:\n",
    "    for i in range(window_size,len(sent)-window_size): #start from 2 to second last\n",
    "        context_word = []\n",
    "        # print(sent[i])\n",
    "        center_word = sent[i]\n",
    "        for j in range(window_size):\n",
    "            outside_word = [sent[i-j-1],sent[i+j+1]] #window_size adjustable\n",
    "            #here we want to create (banana, apple), (banana, fruit) append to some list\n",
    "            for o in outside_word:\n",
    "                context_word.append(o)\n",
    "                skip_grams.append((center_word,o))\n",
    "            cbows.append((context_word,center_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since we have these occcurences, we can count, to make our co-occurence matrix!!!\n",
    "X_ik_skipgram = Counter(skip_grams)\n",
    "# X_ik_skipgram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Weighting function f\n",
    "\n",
    "GloVe includes a weighting function to scale down too frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting(w_i,w_j,X_ik): #we need w_i and w_j, because we can try its-co-occurrences, if it's too big, we scale it down\n",
    "    #check whether the co-occurrences between these two word exits??\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i,w_j)]\n",
    "    except:\n",
    "        x_ij = 1 #why one, so that the probability thingly won't break....\n",
    "    \n",
    "    #maximum co-occurrences; we follow the paper\n",
    "    x_max = 100\n",
    "    alpha = 0.75\n",
    "\n",
    "    #if the co-occurrences does not exceed x_max, cale it down based on some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij/x_max)**alpha\n",
    "    else:\n",
    "        result = 1 #this is the maximum probability you can havve\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now apply this weighting to all possible pairs\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {} #for keeping the co-occurrences\n",
    "weighting_dic = {} #for keeping all the probability after passing through the weighting function\n",
    "\n",
    "for bigram in combinations_with_replacement(vocabs,2): #we need to also think its reverse\n",
    "    # print(bigram)\n",
    "    #if this bigram exists in X_ik_skipgrams\n",
    "    #we gonna add this to our c-occurence matrix\n",
    "    if X_ik_skipgram.get(bigram) is not None:\n",
    "        cooc = X_ik_skipgram[bigram] #get the co-occurrences\n",
    "        X_ik[bigram] = cooc + 1 #this is agian basically label smoothing.... (stability issue (especailly when divide something))\n",
    "        X_ik[(bigram[1],bigram[0])] = cooc + 1 #trick to get all pairs\n",
    "    else: #otherwise, do nothing\n",
    "        pass\n",
    "    #apply the weighting function using this co-occurrence matrix thingly\n",
    "    weighting_dic[bigram] = weighting(bigram[0],bigram[1],X_ik)\n",
    "    weighting_dic[(bigram[1],bigram[0])] = weighting(bigram[1],bigram[0],X_ik)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare train data\n",
    "You move the window along, and create those tuples as we said in class\n",
    "\n",
    "\n",
    "<img src = \"figures/glove_weighting_func.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def random_batch(batch_size,word_sequence,skip_grams,X_ik,weighting_dic):\n",
    "    #loop through skipgram, and change it id because when sending model, it must number \n",
    "    skipg_grams_id = [(word2index[skip_gram[0]],word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    #randomly pick 'batch_size' indexes\n",
    "    number_of_choices = len(skipg_grams_id)\n",
    "    random_index = np.random.choice(number_of_choices, batch_size, replace=False) #no repeating indexes among  these random indexes\n",
    "    \n",
    "    # print(random_index)\n",
    "    random_inputs = [] #xi, wi (in batches)\n",
    "    random_labels = [] #xj, wj (in batches)\n",
    "    random_coocs  = [] #xij (in batches)\n",
    "    random_weighting = [] #f(xij) (in batches)\n",
    "    #for each of the sample in these indexes\n",
    "    for i in random_index:\n",
    "        random_inputs.append([skipg_grams_id[i][0]]) #same reason why I put bracket here....\n",
    "        random_labels.append([skipg_grams_id[i][1]])\n",
    "\n",
    "        #get coocs\n",
    "        #first check whether it exists.....\n",
    "        pair = skip_grams[i] #e.g., ['banana','fruit']\n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1 #label smoothing\n",
    "\n",
    "        random_coocs.append([math.log(cooc)]) #1. why log, #2 why bracket -> size ==> (,1) #my neural network excepts (,1)\n",
    "        \n",
    "        #for weighting\n",
    "        weighting = weighting_dic[pair] #why not user try... maybe it does not exist\n",
    "        random_weighting.append(weighting)\n",
    "\n",
    "    return np.array(random_inputs),np.array(random_labels),np.array(random_coocs),np.array(random_weighting)\n",
    "        #return xi,xj\n",
    "        #return cooc Xij\n",
    "        #return weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "input, target, coocs, weightin = random_batch(batch_size,corpus_tokenized,skip_grams,X_ik,weighting_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 64],\n",
       "        [168]]),\n",
       " array([[119],\n",
       "        [110]]),\n",
       " array([[4.12713439],\n",
       "        [0.69314718]]),\n",
       " array([0.69870486, 0.05318296]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, target, coocs, weightin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model\n",
    "\n",
    "<img src =\"figures/glove.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model will accept three vectors - u_o, v_c, u_w\n",
    "#u_o - vectos for outside words\n",
    "#v_C - vector for center word\n",
    "#u_w - vectors of all vocabs\n",
    "\n",
    "class GloVe(nn.Module):\n",
    "    def __init__(self,voc_size, emb_size):\n",
    "        super(GloVe,self).__init__()\n",
    "        self.embedding_center_word = nn.Embedding(voc_size, emb_size) #is a lookup table mapping all ids in voc_size, into some vector of size emb_size\n",
    "        self.embedding_outside_word = nn.Embedding(voc_size, emb_size)\n",
    "        \n",
    "        self.bias_i = nn.Embedding(voc_size, 1)\n",
    "        self.bias_j = nn.Embedding(voc_size, 1)\n",
    "    def forward(self, center_words, outside_words, coocs, weighting):\n",
    "        #get the embedding of center_words and outside_words\n",
    "        center_embeds = self.embedding_center_word(center_words)\n",
    "        outside_embeds = self.embedding_outside_word(outside_words)\n",
    "\n",
    "        #create biases #create unique embedding (voc_size,1)\n",
    "        inner_product = center_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        bias_i = self.bias_i(center_words).squeeze(1) #center\n",
    "        bias_j = self.bias_j(outside_words).squeeze(1) #target\n",
    "        #do the product between wi and wj\n",
    "        loss = weighting * torch.pow(inner_product + bias_i + bias_j - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size = len(vocabs)\n",
    "batch_size = 2 #why? no reason\n",
    "emb_size = 50 #why? no reason; usually 50,100, 300 but 2 so we can plot (50 can also plot, but need PCA)\n",
    "model = GloVe(voc_size,emb_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #-log\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 | Loss 44.166718 | Time : 0m 5s\n",
      "Epoch 2000 | Loss 981.925476 | Time : 0m 11s\n",
      "Epoch 3000 | Loss 157.017914 | Time : 0m 16s\n",
      "Epoch 4000 | Loss 13.051638 | Time : 0m 22s\n",
      "Epoch 5000 | Loss 394.138855 | Time : 0m 27s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_epochs = 5000\n",
    "#for epoch\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    #get random batch\n",
    "    input, label, cooc, weightin = random_batch(batch_size,corpus_tokenized,skip_grams,X_ik,weighting_dic)\n",
    "    input_batch = torch.LongTensor(input)\n",
    "    label_batch = torch.LongTensor(label)\n",
    "    cooc_batch = torch.FloatTensor(cooc)\n",
    "    weightin_batch = torch.FloatTensor(weightin)\n",
    "\n",
    "    # print(input_batch.shape,label_batch.shape,cooc_batch.shape,weightin_batch.shape)\n",
    "\n",
    "    #loss = model\n",
    "    loss = model(input_batch,label_batch,cooc_batch,weightin_batch)\n",
    "    #backpropagate\n",
    "    loss.backward()\n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    #print epoch loss\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f'Epoch {epoch+1} | Loss {loss:.6f} | Time : {epoch_mins}m {epoch_secs}s')\n",
    "\n",
    "    # break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plotting the embeddings\n",
    "\n",
    "Is really the related studd are close to each other, and vice versa.\n",
    "\n",
    "The most fun part: Will 'banana' closer to 'fruit' than 'cat'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find embedding of fruit, cat\n",
    "def get_embed(word):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except :\n",
    "        index = word2index['<UNK>'] #unknown\n",
    "    word = torch.LongTensor([index])\n",
    "    \n",
    "    embed =  (model.embedding_center_word(word)+model.embedding_outside_word(word))/2\n",
    "    return np.array(embed[0].detach().numpy())\n",
    "    # return embed[0][0].item(),embed[0][1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #help me plot fruit cat banana on maplotlib\n",
    "# plt.figure(figsize=(10,10))\n",
    "# for i, word in enumerate(vocabs[:100]):\n",
    "#     x,y = get_embed(word)\n",
    "#     plt.scatter(x,y)\n",
    "#     plt.annotate(word,xy=(x,y),xytext=(5,2),textcoords='offset points')\n",
    "# plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Cosine Similarity\n",
    "How do (from Scratch) calcualte cosine similarity?\n",
    "\n",
    "Formally the [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) $s$ between two vectors $p$ and $q$ is defined as:\n",
    "\n",
    "$$s = \\frac{p \\cdot q}{||p|| ||q||}, \\textrm{ where } s \\in [-1, 1] $$ \n",
    "\n",
    "If $p$ and $q$ is super similar, the result is 1 otherwise 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy version\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Athens vs. Greece:  0.21279128\n",
      "Greece vs. Iraq:  0.19210015\n"
     ]
    }
   ],
   "source": [
    "#let's try similarity between first and second, and second and third\n",
    "Athens = get_embed('athens')\n",
    "Greece = get_embed('greece')\n",
    "Iraq = get_embed('iraq')\n",
    "print(f\"Athens vs. Greece: \",cos_sim(Athens, Greece))\n",
    "print(f\"Greece vs. Iraq: \",cos_sim(Greece, Iraq))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Prediction \n",
    "- does the Work analogy task : a is b as c is to ______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_analogy(a,b,c):\n",
    "    emb_a, emb_b, emb_c = get_embed(a),get_embed(b),get_embed(c)\n",
    "    # for c in corpus_tokenized\n",
    "    vector = emb_a - emb_b + emb_c\n",
    "    similarity = -1 \n",
    "    \n",
    "    for vocab in vocabs:\n",
    "        if vocab not in [a,b,c]: #ignore input words itself\n",
    "            index = word2index[vocab]\n",
    "            word = torch.LongTensor([index])\n",
    "            current_sim = cos_sim(vector,get_embed(word))\n",
    "            if current_sim > similarity:\n",
    "                similarity = current_sim\n",
    "                d = vocab,similarity\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('venezuela', 0.65262586)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_analogy('athens','greece','cario')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "- Accuracy = number correct of predictions/total number of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>athens</td>\n",
       "      <td>greece</td>\n",
       "      <td>baghdad</td>\n",
       "      <td>iraq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>athens</td>\n",
       "      <td>greece</td>\n",
       "      <td>bangkok</td>\n",
       "      <td>thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>athens</td>\n",
       "      <td>greece</td>\n",
       "      <td>beijing</td>\n",
       "      <td>china</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>athens</td>\n",
       "      <td>greece</td>\n",
       "      <td>berlin</td>\n",
       "      <td>germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>athens</td>\n",
       "      <td>greece</td>\n",
       "      <td>bern</td>\n",
       "      <td>switzerland</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        A       B        C            D\n",
       "0  athens  greece  baghdad         iraq\n",
       "1  athens  greece  bangkok     thailand\n",
       "2  athens  greece  beijing        china\n",
       "3  athens  greece   berlin      germany\n",
       "4  athens  greece     bern  switzerland"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(corpus_tokenized, columns=[\"A\", \"B\", \"C\", \"D\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_accuracy(data):\n",
    "    corrects = 0\n",
    "    total = len(data)\n",
    "    for idx, row in data.iterrows():\n",
    "        a, b, c, d = row['A'],row['B'],row['C'],row['D']\n",
    "        predict = find_analogy(a,b,c)[0] #predictor\n",
    "        if predict == d: #predict is same as real\n",
    "            corrects +=1 \n",
    "    acc = corrects/total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0076"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy_accuracy(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
