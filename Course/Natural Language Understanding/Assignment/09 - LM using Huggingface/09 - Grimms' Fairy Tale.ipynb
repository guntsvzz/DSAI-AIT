{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "import random, math, time\n",
    "from torch.autograd import Variable\n",
    "import operator\n",
    "\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ETL: Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment this if you are not using our department puffer\n",
    "# import os\n",
    "# os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A certain king had a beautiful garden, and in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Some men are born to good luck: all they do or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There was once an old castle, that stood in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An honest farmer had once an ass that had been...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A shepherd had a faithful dog, called Sultan, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  A certain king had a beautiful garden, and in ...\n",
       "1  Some men are born to good luck: all they do or...\n",
       "2  There was once an old castle, that stood in th...\n",
       "3  An honest farmer had once an ass that had been...\n",
       "4  A shepherd had a faithful dog, called Sultan, ..."
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_grimm = pd.read_csv('./grimms_fairytales.csv')\n",
    "df_grimm.drop(['Unnamed: 0','Title'],axis=1,inplace=True)\n",
    "df_grimm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A certain king had a beautiful garden, and in the garden stood a tree\\nwhich bore golden apples. These apples were always counted, and about\\nthe time when they began to grow ripe it was found that every night one\\nof them was gone. The king became very angry at this, and ordered the\\ngardener to keep watch all night under the tree. The gardener set his\\neldest son to watch; but about twelve o’clock he fell asleep, and in\\nthe morning another of the apples was missing. Then the second son was\\nordered to watch; and at midnight he too fell asleep, and in the morning\\nanother apple was gone. Then the third son offered to keep watch; but\\nthe gardener at first would not let him, for fear some harm should come\\nto him: however, at last he consented, and the young man laid himself\\nunder the tree to watch. As the clock struck twelve he heard a rustling\\nnoise in the air, and a bird came flying that was of pure gold; and as\\nit was snapping at one of the apples with its beak, the gardener’s son\\njumped up and shot an arrow at it. But the arrow did the bird no harm;\\nonly it dropped a golden feather from its tail, and then flew away.\\nThe golden feather was brought to the king in the morning, and all the\\ncouncil was called together. Everyone agreed that it was worth more than\\nall the wealth of the kingdom: but the king said, ‘One feather is of no\\nuse to me, I must have the whole bird.’\\n\\nThen the gardener’s eldest son set out and thought to find the golden\\nbird very easily; and when he had gone but a little way, he came to a\\nwood, and by the side of the wood he saw a fox sitting; so he took his\\nbow and made ready to shoot at it. Then the fox said, ‘Do not shoot me,\\nfor I will give you good counsel; I know what your business is, and\\nthat you want to find the golden bird. You will reach a village in the\\nevening; and when you get there, you will see two inns opposite to each\\nother, one of which is very pleasant and beautiful to look at: go not in\\nthere, but rest for the night in the other, though it may appear to you\\nto be very poor and mean.’ But the son thought to himself, ‘What can\\nsuch a beast as this know about the matter?’ So he shot his arrow at\\nthe fox; but he missed it, and it set up its tail above its back and\\nran into the wood. Then he went his way, and in the evening came to\\nthe village where the two inns were; and in one of these were people\\nsinging, and dancing, and feasting; but the other looked very dirty,\\nand poor. ‘I should be very silly,’ said he, ‘if I went to that shabby\\nhouse, and left this charming place’; so he went into the smart house,\\nand ate and drank at his ease, and forgot the bird, and his country too.\\n\\nTime passed on; and as the eldest son did not come back, and no tidings\\nwere heard of him, the second son set out, and the same thing happened\\nto him. He met the fox, who gave him the good advice: but when he came\\nto the two inns, his eldest brother was standing at the window where\\nthe merrymaking was, and called to him to come in; and he could not\\nwithstand the temptation, but went in, and forgot the golden bird and\\nhis country in the same manner.\\n\\nTime passed on again, and the youngest son too wished to set out into\\nthe wide world to seek for the golden bird; but his father would not\\nlisten to it for a long while, for he was very fond of his son, and\\nwas afraid that some ill luck might happen to him also, and prevent his\\ncoming back. However, at last it was agreed he should go, for he would\\nnot rest at home; and as he came to the wood, he met the fox, and heard\\nthe same good counsel. But he was thankful to the fox, and did not\\nattempt his life as his brothers had done; so the fox said, ‘Sit upon my\\ntail, and you will travel faster.’ So he sat down, and the fox began to\\nrun, and away they went over stock and stone so quick that their hair\\nwhistled in the wind.\\n\\nWhen they came to the village, the son followed the fox’s counsel, and\\nwithout looking about him went to the shabby inn and rested there all\\nnight at his ease. In the morning came the fox again and met him as he\\nwas beginning his journey, and said, ‘Go straight forward, till you come\\nto a castle, before which lie a whole troop of soldiers fast asleep and\\nsnoring: take no notice of them, but go into the castle and pass on and\\non till you come to a room, where the golden bird sits in a wooden cage;\\nclose by it stands a beautiful golden cage; but do not try to take the\\nbird out of the shabby cage and put it into the handsome one, otherwise\\nyou will repent it.’ Then the fox stretched out his tail again, and the\\nyoung man sat himself down, and away they went over stock and stone till\\ntheir hair whistled in the wind.\\n\\nBefore the castle gate all was as the fox had said: so the son went in\\nand found the chamber where the golden bird hung in a wooden cage, and\\nbelow stood the golden cage, and the three golden apples that had been\\nlost were lying close by it. Then thought he to himself, ‘It will be a\\nvery droll thing to bring away such a fine bird in this shabby cage’; so\\nhe opened the door and took hold of it and put it into the golden cage.\\nBut the bird set up such a loud scream that all the soldiers awoke, and\\nthey took him prisoner and carried him before the king. The next morning\\nthe court sat to judge him; and when all was heard, it sentenced him to\\ndie, unless he should bring the king the golden horse which could run as\\nswiftly as the wind; and if he did this, he was to have the golden bird\\ngiven him for his own.\\n\\nSo he set out once more on his journey, sighing, and in great despair,\\nwhen on a sudden his friend the fox met him, and said, ‘You see now\\nwhat has happened on account of your not listening to my counsel. I will\\nstill, however, tell you how to find the golden horse, if you will do as\\nI bid you. You must go straight on till you come to the castle where the\\nhorse stands in his stall: by his side will lie the groom fast asleep\\nand snoring: take away the horse quietly, but be sure to put the old\\nleathern saddle upon him, and not the golden one that is close by it.’\\nThen the son sat down on the fox’s tail, and away they went over stock\\nand stone till their hair whistled in the wind.\\n\\nAll went right, and the groom lay snoring with his hand upon the golden\\nsaddle. But when the son looked at the horse, he thought it a great pity\\nto put the leathern saddle upon it. ‘I will give him the good one,’\\nsaid he; ‘I am sure he deserves it.’ As he took up the golden saddle the\\ngroom awoke and cried out so loud, that all the guards ran in and took\\nhim prisoner, and in the morning he was again brought before the court\\nto be judged, and was sentenced to die. But it was agreed, that, if he\\ncould bring thither the beautiful princess, he should live, and have the\\nbird and the horse given him for his own.\\n\\nThen he went his way very sorrowful; but the old fox came and said, ‘Why\\ndid not you listen to me? If you had, you would have carried away\\nboth the bird and the horse; yet will I once more give you counsel. Go\\nstraight on, and in the evening you will arrive at a castle. At twelve\\no’clock at night the princess goes to the bathing-house: go up to her\\nand give her a kiss, and she will let you lead her away; but take care\\nyou do not suffer her to go and take leave of her father and mother.’\\nThen the fox stretched out his tail, and so away they went over stock\\nand stone till their hair whistled again.\\n\\nAs they came to the castle, all was as the fox had said, and at twelve\\no’clock the young man met the princess going to the bath and gave her the\\nkiss, and she agreed to run away with him, but begged with many tears\\nthat he would let her take leave of her father. At first he refused,\\nbut she wept still more and more, and fell at his feet, till at last\\nhe consented; but the moment she came to her father’s house the guards\\nawoke and he was taken prisoner again.\\n\\nThen he was brought before the king, and the king said, ‘You shall never\\nhave my daughter unless in eight days you dig away the hill that stops\\nthe view from my window.’ Now this hill was so big that the whole world\\ncould not take it away: and when he had worked for seven days, and had\\ndone very little, the fox came and said. ‘Lie down and go to sleep; I\\nwill work for you.’ And in the morning he awoke and the hill was gone;\\nso he went merrily to the king, and told him that now that it was\\nremoved he must give him the princess.\\n\\nThen the king was obliged to keep his word, and away went the young man\\nand the princess; and the fox came and said to him, ‘We will have all\\nthree, the princess, the horse, and the bird.’ ‘Ah!’ said the young man,\\n‘that would be a great thing, but how can you contrive it?’\\n\\n‘If you will only listen,’ said the fox, ‘it can be done. When you come\\nto the king, and he asks for the beautiful princess, you must say, “Here\\nshe is!” Then he will be very joyful; and you will mount the golden\\nhorse that they are to give you, and put out your hand to take leave of\\nthem; but shake hands with the princess last. Then lift her quickly on\\nto the horse behind you; clap your spurs to his side, and gallop away as\\nfast as you can.’\\n\\nAll went right: then the fox said, ‘When you come to the castle where\\nthe bird is, I will stay with the princess at the door, and you will\\nride in and speak to the king; and when he sees that it is the right\\nhorse, he will bring out the bird; but you must sit still, and say that\\nyou want to look at it, to see whether it is the true golden bird; and\\nwhen you get it into your hand, ride away.’\\n\\nThis, too, happened as the fox said; they carried off the bird, the\\nprincess mounted again, and they rode on to a great wood. Then the fox\\ncame, and said, ‘Pray kill me, and cut off my head and my feet.’ But the\\nyoung man refused to do it: so the fox said, ‘I will at any rate give\\nyou good counsel: beware of two things; ransom no one from the gallows,\\nand sit down by the side of no river.’ Then away he went. ‘Well,’\\nthought the young man, ‘it is no hard matter to keep that advice.’\\n\\nHe rode on with the princess, till at last he came to the village where\\nhe had left his two brothers. And there he heard a great noise and\\nuproar; and when he asked what was the matter, the people said, ‘Two men\\nare going to be hanged.’ As he came nearer, he saw that the two men were\\nhis brothers, who had turned robbers; so he said, ‘Cannot they in any\\nway be saved?’ But the people said ‘No,’ unless he would bestow all his\\nmoney upon the rascals and buy their liberty. Then he did not stay to\\nthink about the matter, but paid what was asked, and his brothers were\\ngiven up, and went on with him towards their home.\\n\\nAnd as they came to the wood where the fox first met them, it was so\\ncool and pleasant that the two brothers said, ‘Let us sit down by the\\nside of the river, and rest a while, to eat and drink.’ So he said,\\n‘Yes,’ and forgot the fox’s counsel, and sat down on the side of the\\nriver; and while he suspected nothing, they came behind, and threw him\\ndown the bank, and took the princess, the horse, and the bird, and went\\nhome to the king their master, and said. ‘All this have we won by our\\nlabour.’ Then there was great rejoicing made; but the horse would not\\neat, the bird would not sing, and the princess wept.\\n\\nThe youngest son fell to the bottom of the river’s bed: luckily it was\\nnearly dry, but his bones were almost broken, and the bank was so steep\\nthat he could find no way to get out. Then the old fox came once more,\\nand scolded him for not following his advice; otherwise no evil would\\nhave befallen him: ‘Yet,’ said he, ‘I cannot leave you here, so lay hold\\nof my tail and hold fast.’ Then he pulled him out of the river, and said\\nto him, as he got upon the bank, ‘Your brothers have set watch to kill\\nyou, if they find you in the kingdom.’ So he dressed himself as a poor\\nman, and came secretly to the king’s court, and was scarcely within the\\ndoors when the horse began to eat, and the bird to sing, and the princess\\nleft off weeping. Then he went to the king, and told him all his\\nbrothers’ roguery; and they were seized and punished, and he had the\\nprincess given to him again; and after the king’s death he was heir to\\nhis kingdom.\\n\\nA long while after, he went to walk one day in the wood, and the old fox\\nmet him, and besought him with tears in his eyes to kill him, and cut\\noff his head and feet. And at last he did so, and in a moment the\\nfox was changed into a man, and turned out to be the brother of the\\nprincess, who had been lost a great many many years.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grimm.iloc[0]['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grimm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = df_grimm.Text.values.tolist()\n",
    "len(texts)  # 63 storys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3595"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grim_line = []\n",
    "for text in texts:\n",
    "    for line in text.split('.'):\n",
    "        line = line.strip().lower().replace('\\n', ' ')\n",
    "        grim_line.append(line)\n",
    "len(grim_line) #lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a certain king had a beautiful garden, and in the garden stood a tree which bore golden apples',\n",
       " 'these apples were always counted, and about the time when they began to grow ripe it was found that every night one of them was gone',\n",
       " 'the king became very angry at this, and ordered the gardener to keep watch all night under the tree',\n",
       " 'the gardener set his eldest son to watch; but about twelve o’clock he fell asleep, and in the morning another of the apples was missing',\n",
       " 'then the second son was ordered to watch; and at midnight he too fell asleep, and in the morning another apple was gone',\n",
       " 'then the third son offered to keep watch; but the gardener at first would not let him, for fear some harm should come to him: however, at last he consented, and the young man laid himself under the tree to watch',\n",
       " 'as the clock struck twelve he heard a rustling noise in the air, and a bird came flying that was of pure gold; and as it was snapping at one of the apples with its beak, the gardener’s son jumped up and shot an arrow at it',\n",
       " 'but the arrow did the bird no harm; only it dropped a golden feather from its tail, and then flew away',\n",
       " 'the golden feather was brought to the king in the morning, and all the council was called together',\n",
       " 'everyone agreed that it was worth more than all the wealth of the kingdom: but the king said, ‘one feather is of no use to me, i must have the whole bird']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of line\n",
    "grim_line[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# assume `df` is your Pandas dataframe\n",
    "# split into train and test/validation\n",
    "ds_train, ds_valid = train_test_split(grim_line, test_size=0.2, random_state=555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2876, 719)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_train),len(ds_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['content'],\n",
       "        num_rows: 2876\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['content'],\n",
       "        num_rows: 719\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "raw_datasets_train = Dataset.from_pandas(pd.DataFrame(data = {'content': ds_train}))\n",
    "raw_datasets_valid = Dataset.from_pandas(pd.DataFrame(data = {'content': ds_valid}))\n",
    "\n",
    "#remove .shuffle if you want to train the whole dataset....\n",
    "\n",
    "raw_datasets = DatasetDict(\n",
    "    {\n",
    "        'train':raw_datasets_train,\n",
    "        'valid':raw_datasets_valid\n",
    "    }\n",
    ")\n",
    "\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTENT: ’ the maiden fetched the magic wand, and she took the dead girl’s head and dropped three drops of blood on the ground, one in front of the bed, one in the kitchen, and one on the stairs\n"
     ]
    }
   ],
   "source": [
    "for key in raw_datasets[\"train\"][0]:\n",
    "    print(f\"{key.upper()}: {raw_datasets['train'][0][key][:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 2\n",
      "tensor([[  447,   247,   262, 34827, 11351,  1740,   262,  5536, 11569,    11,\n",
      "           290,   673,  1718,   262,  2636,  2576,   447,   247,    82,  1182,\n",
      "           290,  5710,  1115, 10532,   286,  2910,   319,   262,  2323,    11,\n",
      "           530,   287,  2166,   286,   262,  3996,    11,   530,   287,   262,\n",
      "          9592,    11,   290,   530,   319,   262, 16046],\n",
      "        [  447,   246, 11274,  1110,    11,   308,  1186,   417, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.padding_side = \"right\" # \"left\" or \"right\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "outputs = tokenizer(\n",
    "    raw_datasets[\"train\"][:2][\"content\"], \n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    )\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(outputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 2\n",
      "Input chunk lengths: [47, 8]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# tokenizer.padding_side = \"right\" # \"left\" or \"right\"\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "context_length = 180\n",
    "outputs = tokenizer(\n",
    "    raw_datasets[\"train\"][:2][\"content\"], \n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50623510cf343a5834f4fab5431d29d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81d15c0b79804057a5783640e4a6496c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 2876\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 719\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(element):\n",
    "\n",
    "    outputs = tokenizer(\n",
    "        element[\"content\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "        \n",
    "    # outputs = tokenizer(\n",
    "    #     element[\"content\"], \n",
    "    #     return_tensors=\"pt\",\n",
    "    #     padding=True,\n",
    "    # )\n",
    "    \n",
    "    input_batch = []\n",
    "    for input_ids in outputs[\"input_ids\"]:\n",
    "        input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "# raw_datasets_ = Dataset.from_pandas(pd.DataFrame(data=raw_datasets_train))\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "\n",
    "Our first step is to freshly initialize a GPT-2 model. We’ll use the same configuration for our model as for the small GPT-2 model, so we load the pretrained configuration, make sure that the tokenizer size matches the model vocabulary size and pass the bos and eos (beginning and end of sequence) token IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "context_length  = 180\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that configuration, we can load a new model. Note that this is the first time we don’t use the `from_pretrained()` function, since we’re actually initializing a model ourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword has not single token: Snow White\n",
      "Keyword has not single token: Catherine \n",
      "Keyword has not single token: Cinderella\n",
      "Keyword has not single token: Elsie\n",
      "Keyword has not single token: Hans\n",
      "Keyword has not single token: Hansel\n",
      "Keyword has not single token: Rapunzel\n"
     ]
    }
   ],
   "source": [
    "keytoken_ids = []\n",
    "for keyword in [\n",
    "    \"Snow White\",\n",
    "    \"Catherine \",\n",
    "    \"Cinderella\",\n",
    "    \"Elsie\",\n",
    "    \"Hans\",\n",
    "    \"Hansel\",\n",
    "    \"Rapunzel\"\n",
    "]:\n",
    "    ids = tokenizer([keyword]).input_ids[0]\n",
    "    if len(ids) == 1:\n",
    "        keytoken_ids.append(ids[0])\n",
    "    else:\n",
    "        print(f\"Keyword has not single token: {keyword}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "Great, that seems to work nicely! We can now write a custom loss function that takes the input sequence, the logits, and the key tokens we just selected as inputs. First we need to align the logits and inputs: the input sequence shifted by one to the right forms the labels, since the next token is the label for the current token. We can achieve this by starting the labels from the second token of the input sequence, since the model does not make a prediction for the first token anyway. Then we cut off the last logit, as we don’t have a label for the token that follows the full input sequence. With that we can compute the loss per sample and count the occurrences of all keywords in each sample. Finally, we calculate the weighted average over all samples using the occurrences as weights. Since we don’t want to throw away all the samples that have no keywords, we add 1 to the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "\n",
    "def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_labels = inputs[..., 1:].contiguous()\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    # Calculate per-token loss\n",
    "    loss_fct = CrossEntropyLoss(reduce=False) #change to reduction=None\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    # Resize and average loss per sample\n",
    "    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n",
    "    # Calculate and scale weighting\n",
    "    # weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(\n",
    "    #     axis=[0, 2]\n",
    "    # )\n",
    "    # weights = alpha * (1.0 + weights)\n",
    "    # Calculate weighted average\n",
    "    # weighted_loss = (loss_per_sample * weights).mean()\n",
    "    weighted_loss = loss_per_sample.mean()\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=32, shuffle=True)\n",
    "eval_dataloader  = DataLoader(tokenized_datasets[\"valid\"], batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Next, we group the parameters so that the optimizer knows which ones will get an additional weight decay. Usually, all bias and LayerNorm weights terms are exempt from this; here’s how we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.1\n",
    "\n",
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to evaluate the model regularly on the validation set during training, let’s write a function for that as well. It just runs through the evaluation dataloader and gathers all the losses across processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in tqdm(enumerate(eval_dataloader)):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs = model(batch[\"input_ids\"].to(device), labels=batch[\"input_ids\"].to(device))\n",
    "            outputs.loss = outputs.loss.reshape(1)\n",
    "        losses.append(accelerator.gather(outputs.loss))        \n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `evaluate()` function we can report loss and perplexity at regular intervals. Next, we redefine our model to make sure we train from scratch again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define our optimizer, using the function from before to split the parameters for weight decay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(get_grouped_params(model), lr=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accelerator\n",
    "\n",
    "Now let’s prepare the model, optimizer, and dataloaders so we can start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator(mixed_precision='fp16')\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have sent our `train_dataloader` to `accelerator.prepare()`, we can use its length to compute the number of training steps. Remember that we should always do this after preparing the dataloader, as that method will change its length. We use a classic linear schedule from the learning rate to 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 1\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=1_000,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repository\n",
    "\n",
    "Lastly, to push our model to the Hub, we will need to create a `Repository` object in a working folder. First log in to the Hugging Face Hub, if you aren’t logged in already. We’ll determine the repository name from the model ID we want to give our model (feel free to replace the repo_name with your own choice; it just needs to contain your username, which is what the function `get_full_repo_name()` does):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to C:\\Users\\Guntsv\\.huggingface\\token\n",
      "\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n",
      "\n",
      "git config --global credential.helper store\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global credential.helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'guntsv/grim-gpt2-accelerate'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "model_name = \"grim-gpt2-accelerate\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can clone that repository in a local folder. If it already exists, this local folder should be an existing clone of the repository we are working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Guntsv\\Documents\\GitHub\\DSAI-AIT-2022\\Course\\Natural Language Understanding\\Assignment\\09 - LM using Huggingface\\grim-gpt2-accelerate is already a clone of https://huggingface.co/guntsv/grim-gpt2-accelerate. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "output_dir = \"grim-gpt2-accelerate\"\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now upload anything we save in `output_dir` by calling the `repo.push_to_hub()` method. This will help us upload the intermediate models at the end of each epoch.\n",
    "\n",
    "## 4. Training\n",
    "\n",
    "Before we train, let’s run a quick test to see if the evaluation function works properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are very high values for loss and perplexity, but that’s not surprising as we haven’t trained the model yet. With that, we have everything prepared to write the core part of the training script: the training loop. In the training loop we iterate over the dataloader and pass the batches to the model. With the logits, we can then evaluate our custom loss function. We scale the loss by the number of gradient accumulation steps so as not to create larger losses when aggregating more steps. Before we optimize, we also clip the gradients for better convergence. Finally, every few steps we evaluate the model on the evaluation set with our new `evaluate()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc5bfd17af7443e9bd65e418c140dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [185] at entry 0 and [215] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\DSAI-AIT-2022\\Course\\Natural Language Understanding\\Assignment\\09 - LM using Huggingface\\09 - Grimms' Fairy Tale.ipynb Cell 48\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Natural%20Language%20Understanding/Assignment/09%20-%20LM%20using%20Huggingface/09%20-%20Grimms%27%20Fairy%20Tale.ipynb#Y111sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m completed_steps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Natural%20Language%20Understanding/Assignment/09%20-%20LM%20using%20Huggingface/09%20-%20Grimms%27%20Fairy%20Tale.ipynb#Y111sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_train_epochs):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Natural%20Language%20Understanding/Assignment/09%20-%20LM%20using%20Huggingface/09%20-%20Grimms%27%20Fairy%20Tale.ipynb#Y111sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m step, batch \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(train_dataloader, start\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), total\u001b[39m=\u001b[39mnum_training_steps):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Natural%20Language%20Understanding/Assignment/09%20-%20LM%20using%20Huggingface/09%20-%20Grimms%27%20Fairy%20Tale.ipynb#Y111sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         logits \u001b[39m=\u001b[39m model(batch[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mlogits\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Natural%20Language%20Understanding/Assignment/09%20-%20LM%20using%20Huggingface/09%20-%20Grimms%27%20Fairy%20Tale.ipynb#Y111sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         loss \u001b[39m=\u001b[39m keytoken_weighted_loss(batch[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m], logits, keytoken_ids)\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\tqdm\\notebook.py:259\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[1;32m--> 259\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[0;32m    260\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    261\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m    262\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\accelerate\\data_loader.py:378\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     current_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(dataloader_iter)\n\u001b[0;32m    379\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    380\u001b[0m     \u001b[39myield\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 61\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:128\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mMapping):\n\u001b[0;32m    127\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 128\u001b[0m         \u001b[39mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem})\n\u001b[0;32m    129\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m         \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[0;32m    131\u001b[0m         \u001b[39mreturn\u001b[39;00m {key: collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem}\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:128\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mMapping):\n\u001b[0;32m    127\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 128\u001b[0m         \u001b[39mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m batch], collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem})\n\u001b[0;32m    129\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m         \u001b[39m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[0;32m    131\u001b[0m         \u001b[39mreturn\u001b[39;00m {key: collate([d[key] \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m batch], collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m elem}\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:120\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 120\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[0;32m    122\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    123\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:163\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    161\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    162\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[1;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [185] at entry 0 and [215] at entry 1"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "gradient_accumulation_steps = 8\n",
    "eval_steps = 2\n",
    "\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "for epoch in range(num_train_epochs):\n",
    "    for step, batch in tqdm(enumerate(train_dataloader, start=1), total=num_training_steps):\n",
    "        logits = model(batch[\"input_ids\"]).logits\n",
    "        loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids)\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            accelerator.print(\n",
    "                {\n",
    "                    \"steps\": completed_steps,\n",
    "                    \"loss/train\": loss.item() * gradient_accumulation_steps,\n",
    "                }\n",
    "            )\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        # print(loss)\n",
    "        accelerator.backward(loss) #instance of optimize.backward()\n",
    "\n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            completed_steps += 1\n",
    "            \n",
    "        if (step % (eval_steps * gradient_accumulation_steps)) == 0:\n",
    "            eval_loss, perplexity = evaluate()\n",
    "            accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
    "            model.train()\n",
    "            #save your model\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "            if accelerator.is_main_process:\n",
    "                tokenizer.save_pretrained(output_dir)\n",
    "                repo.push_to_hub(\n",
    "                    commit_message=f\"Training in progress step {step}\", blocking=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference\n",
    "\n",
    "Now is the moment of truth: let’s see how well the trained model actually works! We can see in the logs that the loss went down steadily, but to put the model to the test let’s take a look at how well it works on some prompts. To do that we’ll wrap the model in a text generation pipeline, and we’ll put it on the GPU for fast generations if there is one available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "checkpoints = \"guntsv/grim-gpt2-accelerate\"\n",
    "pipe = pipeline(\"text-generation\", max_length=30, pad_token_id=0, model=checkpoints) #eos_token_id=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am Alice who manifold'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "txt = \"cinderella is going to\"\n",
    "# print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])\n",
    "pipe(txt)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "# # add the EOS token as PAD token to avoid warnings\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "model = GPT2LMHeadModel.from_pretrained(checkpoints, pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode context the generation is conditioned on\n",
    "# input_ids = tokenizer.encode('Alice and Red Queen live at AIT University in Thai', return_tensors='pt')\n",
    "input_ids = tokenizer.encode('cinderella is going to', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference\n",
    "\n",
    "Now is the moment of truth: let’s see how well the trained model actually works! We can see in the logs that the loss went down steadily, but to put the model to the test let’s take a look at how well it works on some prompts. To do that we’ll wrap the model in a text generation pipeline, and we’ll put it on the GPU for fast generations if there is one available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "checkpoints = \"guntsv/alice-in-ait-accelerate\"\n",
    "pipe = pipeline(\"text-generation\", max_length=30, pad_token_id=0, model=checkpoints) #eos_token_id=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am Alice who manifold'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "txt = \"I am Alice who\"\n",
    "# print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])\n",
    "pipe(txt)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am a girl whoactual'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "txt = \"I am a girl who\"\n",
    "# print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])\n",
    "pipe(txt)[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# # add the EOS token as PAD token to avoid warnings\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(checkpoints, pad_token_id=tokenizer.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode context the generation is conditioned on\n",
    "# input_ids = tokenizer.encode('Alice and Red Queen live at AIT University in Thai', return_tensors='pt')\n",
    "input_ids = tokenizer.encode('Alice study at', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
