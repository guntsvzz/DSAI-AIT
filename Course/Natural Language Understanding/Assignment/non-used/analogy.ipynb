{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# for loading the glove word embedding matrix values\n",
    "def load_glove_vectors(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf-8\") as file:\n",
    "        # unique words\n",
    "        words = set()\n",
    "        word_to_vec = {}\n",
    "        # each line starts with a word then the values for the different features\n",
    "        for line in file:\n",
    "            line = line.strip().split()\n",
    "            # take the word \n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            # rest of the features for the word\n",
    "            word_to_vec[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            \n",
    "    return words, word_to_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, word_to_vec = load_glove_vectors('../dataset/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n"
     ]
    }
   ],
   "source": [
    "word2vec = word_to_vec.keys()\n",
    "for w in word2vec:\n",
    "    print(w)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the cosine similarity between u and v\n",
    "'''\n",
    "    Arguments:\n",
    "        u(n,) - vector of words            \n",
    "        v(n,) - vector of words \n",
    "    Returns:\n",
    "        cosine_sim - the cosine similarity between u and v\n",
    "'''\n",
    "def find_cosine_similarity(u, v):\n",
    "    distance = 0.0\n",
    "    \n",
    "    # find the dot product between u and v \n",
    "    dot = np.dot(u,v)\n",
    "    # find the L2 norm of u \n",
    "    norm_u = np.sqrt(np.sum(u**2))\n",
    "    # Compute the L2 norm of v\n",
    "    norm_v = np.sqrt(np.sum(v**2))\n",
    "    # Compute the cosine similarity\n",
    "    cosine_sim = dot/(norm_u)/norm_v\n",
    "    \n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity(king, queen) =  0.7839043010964117\n",
      "cosine_similarity(father, mother) =  0.8909038442893615\n",
      "cosine_similarity(king - queen, father - mother) =  0.661889473579435\n",
      "cosine_similarity(bat, crow) =  0.41574518317394416\n",
      "cosine_similarity(india - delhi, rome - italy) =  -0.6363974204130605\n"
     ]
    }
   ],
   "source": [
    "# sample observations\n",
    "father = word_to_vec[\"father\"]\n",
    "mother = word_to_vec[\"mother\"]\n",
    "king = word_to_vec[\"king\"]\n",
    "queen = word_to_vec[\"queen\"]\n",
    "bat = word_to_vec[\"bat\"]\n",
    "crow = word_to_vec[\"crow\"]\n",
    "india = word_to_vec[\"india\"]\n",
    "italy = word_to_vec[\"italy\"]\n",
    "delhi = word_to_vec[\"delhi\"]\n",
    "rome = word_to_vec[\"rome\"]\n",
    "\n",
    "print(\"cosine_similarity(king, queen) = \", find_cosine_similarity(king, queen))\n",
    "print(\"cosine_similarity(father, mother) = \", find_cosine_similarity(father, mother))\n",
    "print(\"cosine_similarity(king - queen, father - mother) = \",find_cosine_similarity(king - queen, father - mother))\n",
    "print(\"cosine_similarity(bat, crow) = \",find_cosine_similarity(bat, crow))\n",
    "print(\"cosine_similarity(india - delhi, rome - italy) = \",find_cosine_similarity(india - delhi, rome - italy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.3857  , -0.58719 ,  0.06369 ,  0.50941 ,  0.62923 ,  0.69401 ,\n",
       "       -0.19642 ,  0.06021 ,  0.658906,  0.3222  ,  0.44495 ,  0.42136 ,\n",
       "        0.13937 , -0.23748 ,  0.15754 , -0.64295 ,  0.516154, -0.32774 ,\n",
       "       -0.1071  , -0.45887 , -0.6037  , -0.62978 ,  0.03839 , -0.24121 ,\n",
       "        0.57674 , -0.2974  , -0.20195 , -0.7001  ,  0.0617  ,  0.30511 ,\n",
       "        1.4831  , -0.11825 ,  0.27977 ,  0.64785 , -0.34324 , -0.176079,\n",
       "       -0.74841 ,  0.323314, -0.61414 , -0.80316 ,  0.11602 , -0.1559  ,\n",
       "        0.394685, -0.49157 ,  0.037114,  0.59194 , -0.24197 , -0.2056  ,\n",
       "       -0.18271 , -0.2548  ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_vec['india']-word_to_vec['delhi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does the Word analogy task: a is to b as c is to ____\n",
    "def find_analogy(word_a, word_b, word_c, word_to_vec):\n",
    "    # convert words to lower case\n",
    "    word_a = word_a.lower()\n",
    "    word_b = word_b.lower()\n",
    "    word_c = word_c.lower()\n",
    "    \n",
    "    \n",
    "    # find the word embeddings for word_a, word_b, word_c\n",
    "    e_a, e_b, e_c = word_to_vec[word_a], word_to_vec[word_b], word_to_vec[word_c]\n",
    "    \n",
    "    words = word_to_vec.keys()\n",
    "    max_cosine_sim = -999              \n",
    "    best_word = None                  \n",
    "\n",
    "    # search for word_d in the whole word vector set\n",
    "    for w in words:        \n",
    "        # ignore input words\n",
    "        if w in [word_a, word_b, word_c] :\n",
    "            continue\n",
    "\n",
    "        # Compute cosine similarity between the vectors u and v\n",
    "        #u:(e_b - e_a) \n",
    "        #v:((w's vector representation) - e_c)\n",
    "        cosine_sim = find_cosine_similarity(e_b - e_a, word_to_vec[w] - e_c)\n",
    "        \n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            # update word_d\n",
    "            best_word = w\n",
    "        \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "india -> delhi :: japan -> tokyo\n",
      "tall -> taller :: large -> vastly\n"
     ]
    }
   ],
   "source": [
    "examples = [('india', 'delhi', 'japan'), ('tall', 'taller', 'large')]\n",
    "for example in examples:\n",
    "    print ('{} -> {} :: {} -> {}'.format( *example, find_analogy(*example, word_to_vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for taking input from the user and doing word analogy task on that\n",
    "def take_input():\n",
    "    print('a --> b :: c --> d')\n",
    "    print('Enter a, b, c words separated by space')\n",
    "    words = input().split(' ')\n",
    "    \n",
    "    best_pick = find_analogy(*words, word_to_vec)\n",
    "    print ('{} -> {} :: {} -> {}'.format( *words, best_pick))\n",
    "    print('Best pick: ' + best_pick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
