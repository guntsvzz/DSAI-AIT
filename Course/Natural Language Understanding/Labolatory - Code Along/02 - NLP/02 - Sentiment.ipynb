{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using SpaCy\n",
    "\n",
    "## 0. Text processing using SpaCy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Lemmatization\n",
    "\n",
    "It turns your word to its original form. Very common thing you wanna to do, because YouTubeVideo\n",
    "do not want to confuse ypur model that run and running are different.\n",
    "\n",
    "Note: But if you use very powerful neural network like transformer, NO need Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run run\n",
      "ran run\n",
      "running run\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"run ran running\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)\n",
    "\n",
    "#to NOT confuse the model, you want to convert words to thier lemma\n",
    "#for very powetful neural network like Transformer (huggingface) ,NO NEED TO LEMMATIZATION, because they understand"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Stop words\n",
    "\n",
    "Common preprocessing is to remove stopwords, e.g., at, in, on, etc. Removing the helps model memorize only the keywords.\n",
    "\n",
    "Note : In powerful network, we DON'T remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['would', 'before', \"n't\", 'go', 'anyway']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "stopwords = list(STOP_WORDS)\n",
    "print(stopwords[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's demonstrate how to remove stopword\n",
    "doc = nlp(\"Chaky is going to eat at Thammasat with his best friend Peter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chaky', 'going', 'eat', 'Thammasat', 'best', 'friend', 'Peter']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tokens = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.text not in stopwords:\n",
    "            clean_tokens.append(token.text)\n",
    "            \n",
    "clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There', 'movie', 'good', '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"There movie should have been good.\")\n",
    "clean_tokens = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.text not in stopwords:\n",
    "            clean_tokens.append(token.text)\n",
    "            \n",
    "clean_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Removing punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "doc = nlp(\"Chaky, the teacher $ / @ # at AIT,!!!????? like to eat naan.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leverage pos tag\n",
    "# for token in doc:\n",
    "#     print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chaky', 'the', 'teacher', '@', '#', 'at', 'AIT', 'like', 'to', 'eat', 'naan']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_no_punct = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ != 'PUNCT' and token.pos_ != 'SPACE' and token.pos_ != 'SYM':\n",
    "        token_no_punct.append(token.text)\n",
    "\n",
    "token_no_punct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Lowercasing and unnesscary spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chaky',\n",
       " ',',\n",
       " 'the',\n",
       " 'teacher',\n",
       " '$',\n",
       " '/',\n",
       " '@',\n",
       " '#',\n",
       " 'at',\n",
       " 'ait',\n",
       " ',',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '?',\n",
       " '?',\n",
       " '?',\n",
       " '?',\n",
       " '?',\n",
       " 'like',\n",
       " 'to',\n",
       " 'eat',\n",
       " 'naan',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stripped_lowercase_tokens = []\n",
    "\n",
    "for token in doc:\n",
    "    stripped_lowercase_tokens.append(token.text.lower())\n",
    "stripped_lowercase_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5 Combine everything "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nowadays, we don't preprocess anympre, especially for big model, because you lose a lot of information \n",
    "#if fthere is something you can clean, is extra spaces or like duplicate symbols......\n",
    "\n",
    "#if you use ML, e.g., SVM, KNN, EF, you need to preprocess\n",
    "def preprocessing(sentence):\n",
    "\n",
    "    stopwords = list(STOP_WORDS)\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text not in stopwords and token.pos_ != 'PUNCT' and token.pos_ != 'SPACE' and \\\n",
    "            token.pos_ != 'SYM':\n",
    "                cleaned_tokens.append(token.text)\n",
    "    \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Let's do sentiment analysis with the help sklearn and spacy!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stuff\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfTransformer,TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_yelp   = pd.read_csv('../data/yelp_labelled.txt', sep='\\t', header = None, names = ['Review', 'Sentiment'])\n",
    "data_amazon = pd.read_csv('../data/amazon_labelled.txt', sep='\\t', header = None, names = ['Review', 'Sentiment'])\n",
    "data_imdb   = pd.read_csv('../data/imdb_labelled.txt', sep='\\t', header = None, names = ['Review', 'Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Sentiment\n",
       "0                           Wow... Loved this place.          1\n",
       "1                                 Crust is not good.          0\n",
       "2          Not tasty and the texture was just nasty.          0\n",
       "3  Stopped by during the late May bank holiday of...          1\n",
       "4  The selection on the menu was great and so wer...          1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 2), (1000, 2), (748, 2))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_yelp.shape, data_amazon.shape, data_imdb.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 EDA\n",
    "\n",
    "Check the mean and std; check any null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2748, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([data_yelp,data_amazon,data_imdb],ignore_index=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1386\n",
       "0    1362\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review       0\n",
       "Sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chaky' 'coding' 'deep' 'hashtag' 'ilovepython' 'learning' 'python'\n",
      " 'sure']\n",
      "[[1 1 0 0 0 0 1 0]\n",
      " [0 0 2 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1]\n",
      " [0 0 0 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#count the frequency of words in postive and negative samples\n",
    "#CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#preprocessing refers to the function we wrote earlier\n",
    "    #the input should be a bunch of text\n",
    "    #the output should return tokens\n",
    "countvec = CountVectorizer(tokenizer = preprocessing)\n",
    "\n",
    "#let's try\n",
    "corpus = [\n",
    "    'Chaky is coding python     ',\n",
    "    'Deep learning is very deep',\n",
    "    'Are you sure about this?????',\n",
    "    'please hashtag #ilovepython'\n",
    "]\n",
    "result   = countvec.fit_transform(corpus)\n",
    "\n",
    "#list of tokens\n",
    "print(countvec.get_feature_names_out())\n",
    "\n",
    "#count\n",
    "#rows are sentences\n",
    "#columns are\n",
    "print(result.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's look at top words catgprozed by postive and negative\n",
    "import numpy as np\n",
    "\n",
    "neg_cond = data.Sentiment == 0\n",
    "pos_cond = data.Sentiment == 1\n",
    "\n",
    "neg_df = data[neg_cond]\n",
    "pos_df = data[pos_cond]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count \n",
    "neg_result = countvec.fit_transform(neg_df.Review)\n",
    "neg_vocabs = countvec.get_feature_names_out()\n",
    "\n",
    "pos_result = countvec.fit_transform(pos_df.Review)\n",
    "pos_vocabs = countvec.get_feature_names_out()\n",
    "\n",
    "#sum the counts\n",
    "neg_counts = np.sum(neg_result, axis = 0)\n",
    "pos_counts = np.sum(pos_result, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie</th>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phone</th>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0\n",
       "1      103\n",
       "bad     96\n",
       "movie   95\n",
       "0       92\n",
       "phone   78"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data frame\n",
    "df = pd.DataFrame(neg_counts, columns = neg_vocabs).T.sort_values(by=0 ,ascending =False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>film</th>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie</th>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phone</th>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0\n",
       "great  192\n",
       "good   171\n",
       "film    91\n",
       "movie   87\n",
       "phone   87"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pos_counts, columns = pos_vocabs).T.sort_values(by=0 ,ascending =False)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfTransformer\n",
    "- usually, in NLP, we don't use counvectorizer\n",
    "- becausde it makes very frequent words a prominent feature, which we don't want to \n",
    "- we want something like normalized(countervectorizer) ==> tfidvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3155) (1, 3116)\n",
      "(3155,) (3116,)\n"
     ]
    }
   ],
   "source": [
    "tfidvec = TfidfVectorizer(tokenizer=preprocessing)\n",
    "\n",
    "#count\n",
    "neg_result   = tfidvec.fit_transform(neg_df.Review)\n",
    "neg_vocabs   = tfidvec.get_feature_names_out()\n",
    "pos_result   = tfidvec.fit_transform(pos_df.Review)\n",
    "pos_vocabs   = tfidvec.get_feature_names_out()\n",
    "\n",
    "#sum words across all documents\n",
    "neg_counts = np.sum(neg_result, axis=0)\n",
    "pos_counts = np.sum(pos_result, axis=0)\n",
    "\n",
    "print(neg_counts.shape, pos_counts.shape)\n",
    "print(neg_vocabs.shape, pos_vocabs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling and training\n",
    "\n",
    "Use sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1923,), (825,), (1923,), (825,))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC #here i am using machine learning, NOT deep learning\n",
    "#define model\n",
    "classifier  = LinearSVC()\n",
    "tfidvec     = TfidfVectorizer()\n",
    "\n",
    "\n",
    "#define X and y\n",
    "\n",
    "X = data.Review\n",
    "y = data.Sentiment\n",
    "\n",
    "#split data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mkae pipeline\n",
    "clf = Pipeline([('tfidvec',tfidvec),('clf',classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pipiline is the same as:\n",
    "\n",
    "# X_trian_transformed = tfidvec.fit_transform(X_train)\n",
    "# X_trian_transformed.shape #(words, features)\n",
    "\n",
    "# classifier.fit(X_trian_transformed,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.84      0.83       426\n",
      "           1       0.83      0.81      0.82       399\n",
      "\n",
      "    accuracy                           0.82       825\n",
      "   macro avg       0.82      0.82      0.82       825\n",
      "weighted avg       0.82      0.82      0.82       825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "clf.fit(X_train,y_train)\n",
    "#predict\n",
    "yhat = clf.predict(X_test)\n",
    "#metric\n",
    "print(classification_report(yhat,y_test))\n",
    "#confusin matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[358,  68],\n",
       "       [ 77, 322]], dtype=int64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusin matrix\n",
    "confusion_matrix(yhat,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Real-World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['Chaky loves to eat sushi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['This movie is good'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['This movie should have been good'])\n",
    "#double negative is a very good test !!!!\n",
    "#remember the sentiment tree bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['This movie is crazily amazing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['This bad movie is good'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
