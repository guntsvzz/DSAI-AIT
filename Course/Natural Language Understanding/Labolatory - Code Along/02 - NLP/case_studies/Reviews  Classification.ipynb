{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Yelp reviews + Preprocessing\n",
    "\n",
    "In this case study, we shall try to classify yelp reviews into two polarity, positive or negative.  Although this seems a bit boring for our class, I start this to warm you up, and also visit some basics on text preprocessing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Basic text preprocessing\n",
    "\n",
    "Before we actually do classification, let's review on stopwords removal and lemmatization, which is a common procedure in preprocessing the text before we input into our classifier.\n",
    "\n",
    "**Note:  If you are using BERT or other pre-trained huggingface stuffs, please read their model carefully.  Usuallly preprocessing IS NOT required, since they do all these stuffs internally.  Preprocessing may actually harm the model!!**\n",
    "\n",
    "### Lemmatization\n",
    "\n",
    "Lemmatization is an essential step in text preprocessing for NLP. It deals with the structural or morphological analysis of words and break-down of words into their base forms or \"lemmas\".  For Example - The words walk, walking, walks, walked are indicative towards a common activity i.e. walk. And since they have different spelling structure, it makes it a confusing task for our algorithms to treat them differently. So, these will be treated under a single lemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run run\n",
      "runs run\n",
      "running run\n",
      "ran run\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp('run runs running ran')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going go\n",
      "to to\n",
      "eat eat\n",
      "at at\n",
      "Thammasat Thammasat\n",
      "with with\n",
      "my my\n",
      "/ /\n",
      "best good\n",
      "           \n",
      "good good\n",
      "friends friend\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Going to eat at Thammasat with my / best      good friends.')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Stopwords are common words that we wanna remove, such as \"of\", \"until\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['although', 'for', 'could', 'thereupon', 'yet', 'much', \"'s\", 'is', 'nothing', 'should']\n",
      "326\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "stopwords = list(STOP_WORDS)\n",
    "print(stopwords[:10])\n",
    "\n",
    "print(len(stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove stopwords is very easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Going to eat at Thammasat with my / best      good friends."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Going', 'eat', 'Thammasat', '/', 'best', '     ', 'good', 'friends', '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tokens = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.text not in stopwords:  #can also use token.is_stop == False\n",
    "        clean_tokens.append(token.text)\n",
    "        \n",
    "clean_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing punctuation\n",
    "\n",
    "You notice if we were to insert the token into some classifier, it may not be necessary to input the `.`.  Thus we can also consider removing punctuation, using POS taggers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going VERB\n",
      "to PART\n",
      "eat VERB\n",
      "at ADP\n",
      "Thammasat PROPN\n",
      "with ADP\n",
      "my PRON\n",
      "/ SYM\n",
      "best ADJ\n",
      "      SPACE\n",
      "good ADJ\n",
      "friends NOUN\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_)  #notice PUNCT and SPACE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the full list of possible POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ROOT', 'acl', 'acomp', 'advcl', 'advmod')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.get_pipe(\"parser\").labels[:5]  #i limit to 5 to good looking notebook :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove punctuation is quite easy as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Going',\n",
       " 'to',\n",
       " 'eat',\n",
       " 'at',\n",
       " 'Thammasat',\n",
       " 'with',\n",
       " 'my',\n",
       " 'best',\n",
       " 'good',\n",
       " 'friends']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_no_punct = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ != 'PUNCT' and token.pos_ != 'SPACE' and token.pos_ != 'SYM':\n",
    "        token_no_punct.append(token.text)\n",
    "        \n",
    "token_no_punct"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing spaces and lowercasing\n",
    "\n",
    "Last thing we wanna worry is removing unwanted spaces using `strip()` and lowercasing using `.lower()`\n",
    "\n",
    "We shall also combine other techniques we learned so far...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['going',\n",
       " 'to',\n",
       " 'eat',\n",
       " 'at',\n",
       " 'thammasat',\n",
       " 'with',\n",
       " 'my',\n",
       " '/',\n",
       " 'best',\n",
       " '',\n",
       " 'good',\n",
       " 'friends',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stripped_lowercase_token = []\n",
    "\n",
    "for token in doc:\n",
    "    stripped_lowercase_token.append(token.text.lower().strip())\n",
    "    \n",
    "stripped_lowercase_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine everything\n",
    "\n",
    "Let's combine everything into one nice function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentence):\n",
    "    \n",
    "    stopwords = list(STOP_WORDS)\n",
    "\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    cleaned_tokens = []\n",
    "    for token in doc:\n",
    "        # print(token.text, token.pos_, token.is_stop)\n",
    "        if token.text not in stopwords and token.pos_ != 'PUNCT' and token.pos_ != 'SPACE' and \\\n",
    "            token.pos_ != 'SYM':\n",
    "            cleaned_tokens.append(token.lemma_.lower().strip())\n",
    "            \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'eat', 'thammasat', 'good', 'good', 'friend']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try\n",
    "preprocessing(\"Going to eat at Thammasat with my best      good friends.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', 'have', 'lot', 'fun']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try\n",
    "preprocessing(\"We are having a lot of fun.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Classification \n",
    "\n",
    "I will not be going into full classification details.  I think you are already quite versed in this thing in my CP class.  I will just be focusing on discussing `TfidVectorizer` which I did not discuss much."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_yelp = pd.read_csv('../data/yelp_labelled.txt', sep='\\t', header = None, names = ['Review', 'Sentiment'])\n",
    "data_amazon = pd.read_csv('../data/amazon_labelled.txt', sep='\\t', header = None, names = ['Review', 'Sentiment'])\n",
    "data_imdb = pd.read_csv('../data/imdb_labelled.txt', sep='\\t', header = None, names = ['Review', 'Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Sentiment\n",
       "0                           Wow... Loved this place.          1\n",
       "1                                 Crust is not good.          0\n",
       "2          Not tasty and the texture was just nasty.          0\n",
       "3  Stopped by during the late May bank holiday of...          1\n",
       "4  The selection on the menu was great and so wer...          1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Sentiment\n",
       "0  So there is no way for me to plug it in here i...          0\n",
       "1                        Good case, Excellent value.          1\n",
       "2                             Great for the jawbone.          1\n",
       "3  Tied to charger for conversations lasting more...          0\n",
       "4                                  The mic is great.          1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_amazon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Sentiment\n",
       "0  A very, very, very slow-moving, aimless movie ...          0\n",
       "1  Not sure who was more lost - the flat characte...          0\n",
       "2  Attempting artiness with black & white and cle...          0\n",
       "3       Very little music or anything to speak of.            0\n",
       "4  The best scene in the movie was when Gerardo i...          1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_imdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 2), (1000, 2), (748, 2))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_yelp.shape, data_amazon.shape, data_imdb.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's combine all of them into one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2748, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([data_yelp, data_amazon, data_imdb], ignore_index=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the class imbalance.  (if there's class imbalance, use SMOTE INSIDE the cross validation loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1386\n",
       "0    1362\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review       0\n",
       "Sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chaky' 'code' 'deep' 'hashtag' 'ilovepython' 'learning' 'python' 'sure']\n",
      "[[1 1 0 0 0 0 1 0]\n",
      " [0 0 2 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1]\n",
      " [0 0 0 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#preprocessing refers to the function we wrote earlier\n",
    "    #the input should be a bunch of text\n",
    "    #the output should return tokens\n",
    "countvec = CountVectorizer(tokenizer = preprocessing)\n",
    "\n",
    "#let's try\n",
    "corpus = [\n",
    "    'Chaky is coding python     ',\n",
    "    'Deep learning is very deep',\n",
    "    'Are you sure about this?????',\n",
    "    'please hashtag #ilovepython',\n",
    "]\n",
    "result   = countvec.fit_transform(corpus)\n",
    "\n",
    "#list of tokens\n",
    "print(countvec.get_feature_names_out())\n",
    "\n",
    "#count\n",
    "#rows are sentences\n",
    "#columns are\n",
    "print(result.toarray())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to see top words, categorized by positive and negative sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative (1362, 2)\n",
      "Postivie (1386, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "neg_cond = data.Sentiment == 0\n",
    "pos_cond = data.Sentiment == 1\n",
    "\n",
    "#condition the df\n",
    "neg_df = data[neg_cond]\n",
    "pos_df = data[pos_cond]\n",
    "\n",
    "print(\"Negative\", neg_df.shape)\n",
    "print(\"Postivie\", pos_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count\n",
    "neg_result   = countvec.fit_transform(neg_df.Review)\n",
    "neg_vocabs   = countvec.get_feature_names_out()\n",
    "pos_result   = countvec.fit_transform(pos_df.Review)\n",
    "pos_vocabs   = countvec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2682) (1, 2708)\n",
      "(2682,) (2708,)\n"
     ]
    }
   ],
   "source": [
    "#top 10 tokens\n",
    "#sum words across all documents\n",
    "neg_counts = np.sum(neg_result, axis=0)\n",
    "pos_counts = np.sum(pos_result, axis=0)\n",
    "\n",
    "print(neg_counts.shape, pos_counts.shape)\n",
    "print(neg_vocabs.shape, pos_vocabs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie</th>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>film</th>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phone</th>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>service</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waste</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>look</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thing</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minute</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buy</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>well</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terrible</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "bad       146\n",
       "movie     109\n",
       "1         103\n",
       "0          92\n",
       "film       86\n",
       "phone      82\n",
       "time       77\n",
       "like       71\n",
       "good       71\n",
       "food       67\n",
       "place      63\n",
       "work       59\n",
       "service    55\n",
       "waste      46\n",
       "go         45\n",
       "think      37\n",
       "come       37\n",
       "look       34\n",
       "thing      34\n",
       "get        34\n",
       "minute     32\n",
       "buy        31\n",
       "well       31\n",
       "terrible   30\n",
       "money      30"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top ten negative terms\n",
    "df = pd.DataFrame(neg_counts, columns = neg_vocabs).T.sort_values(by=0, ascending=False)\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>film</th>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie</th>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phone</th>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>service</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nice</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excellent</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recommend</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sound</th>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>headset</th>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>character</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "good       230\n",
       "great      194\n",
       "film       103\n",
       "movie      103\n",
       "phone       92\n",
       "work        84\n",
       "love        74\n",
       "like        73\n",
       "place       63\n",
       "food        60\n",
       "time        57\n",
       "service     55\n",
       "nice        51\n",
       "0           46\n",
       "excellent   46\n",
       "price       45\n",
       "recommend   41\n",
       "quality     38\n",
       "sound       37\n",
       "headset     35\n",
       "character   34\n",
       "product     34\n",
       "come        33\n",
       "think       31\n",
       "get         29"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top ten negative terms\n",
    "df = pd.DataFrame(pos_counts, columns = pos_vocabs).T.sort_values(by=0, ascending=False)\n",
    "df.head(25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By doing this, you can see that derive many useful stuffs.  Please do this EDA whenever possible.**  You can also use the `wordcloud` library."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer\n",
    "\n",
    "TfidVectorizer go beyond one more step, i.e., after counting the number of words, we shall perform a normalization process called TF-IDF which focuses on **cutting very frequent words which tend to be less meaningful information like \"the\", \"a\", \"is\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91892665, 0.        , 0.39442846],\n",
       "       [0.84080197, 0.54134281, 0.        ],\n",
       "       [0.39706158, 0.34085938, 0.85214845]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "#imagine that we already have a frequency features.  We can perform normalization\n",
    "#as a follow up\n",
    "#here we got n=3, and m=2\n",
    "counts = [[3, 0, 1],\n",
    "          [2, 1, 0],\n",
    "          [3, 2, 5]]\n",
    "transformer = TfidfTransformer()\n",
    "transformer.fit_transform(counts).toarray()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how it works underhood:\n",
    "\n",
    "The formula is\n",
    "\n",
    "$$ \\text{TF-IDF} =  \\text{TF} * \\text{IDF} $$\n",
    "\n",
    "where TF is \n",
    "\n",
    "$$ \\text{TF}_t = \\frac{\\text{Count of words t in that document}}{\\text{Total count of words in that document}}$$\n",
    "\n",
    "Thus TF = \n",
    "\n",
    "| | 1st word  | 2nd word   | 3rd word |\n",
    "|---:|:-------------|:-----------|:-----------|\n",
    "| doc1 | 3/4 = 0.75  | 0     |  1/4 = 0.25 |\n",
    "| doc2 | 2/3 = 0.66  | 1/3 = 0.33    |  0 |\n",
    "| doc3 | 3/10 = 0.33  | 2/10 = 0.20    |  5/10 = 0.5 |\n",
    "\n",
    "and \n",
    "\n",
    "$$ \\text{IDF} = \\log\\left(\\frac{\\text{Number of documents}}{\\text{Number of documents containing that word}}\\right) + 1$$\n",
    "\n",
    "*Note:  We plus one so that super frequent words will not be ignored entirely*\n",
    "\n",
    "Thus IDF = \n",
    "\n",
    "| | IDF  |    \n",
    "|---:|:-----------|\n",
    "| 1st word | $\\log$ 3/3 + 1 = 1 |\n",
    "| 2nd word | $\\log$ 3/2 + 1 = 1.4055  |\n",
    "| 3rd word | $\\log$ 3/2 + 1 = 1.4055  | \n",
    "\n",
    "*Notice that terms (i.e., 1st word) that appear frequently across documents will get low score.  By multiplying this IDF term with the frequency, it will scale the importance down.*\n",
    "\n",
    "Thus TF * IDF = \n",
    "\n",
    "| | 1st word  | 2nd word | 3rd word|    \n",
    "|---:|:-----------|:-----------|:-----------|\n",
    "| doc1 | 0.75 * 1 = 0.75  | 0 * 1.4055 = 0 | 0.25 * 1.4055 = 0.3514 |\n",
    "| doc2 | 0.66 * 1 = 0.66  | 0.33 * 1.4055 = 0.4685 | 0 * 1.4055 = 0   |\n",
    "| doc3 | 0.33 * 1 = 0.33  | 0.20 * 1.4055 = 0.2811 | 0.5 * 1.4055 =0.7027   |\n",
    "\n",
    "\n",
    "We need to further normalize each word using this formula (since each document has unequaled number of words):\n",
    "\n",
    "$$ norm(t_i) = \\frac{t_i}{\\sqrt{t_1^2 + t_2^2 + ....+t_n^2}} $$ \n",
    "\n",
    "Thus, normalized factor for each document is\n",
    "\n",
    "doc1 = $\\sqrt{0.75^2 + 0^2 + 0.3514^2} = 0.8282$\n",
    "\n",
    "doc2 = $\\sqrt{0.66^2 + 0.4685^2 + 0^2} = 0.8094$\n",
    "\n",
    "doc3 = $\\sqrt{0.33^2 + 0.281^2 + 0.7027^2} = 0.8256$\n",
    "\n",
    "\n",
    "Thus, normalized(TF * IDF) = \n",
    "\n",
    "| | 1st word  | 2nd word | 3rd word|    \n",
    "|---:|:-----------|:-----------|:-----------|\n",
    "| doc1 | 0.75 / 0.8282 = 0.9056 | 0 | 0.3514 / 0.8282 = 0.4243 |\n",
    "| doc2 | 0.66 / 0.8094 = 0.8154  | 0.4685 / 0.8094 = 0.5788 | 0   |\n",
    "| doc3 | 0.33 / 0.8256 = 0.3997  | 0.2811 / 0.8256 = 0.3405 | 0.7027 / 0.8256 = 0.8511 |\n",
    "\n",
    "**Note**\n",
    "- My numbers are not exactly the same due to float precisions\n",
    "- Note I am using TfidfTransformer.  You may want to use TfidfVectorizer which automatically accepts raw data (i.e., text data)\n",
    "\n",
    "So let's try to see whether we got different words....!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note that it already count for you internally in tfidvectorizer\n",
    "tfidvec = TfidfVectorizer(tokenizer = preprocessing)\n",
    "\n",
    "#count\n",
    "neg_result   = tfidvec.fit_transform(neg_df.Review)\n",
    "neg_vocabs   = tfidvec.get_feature_names_out()\n",
    "pos_result   = tfidvec.fit_transform(pos_df.Review)\n",
    "pos_vocabs   = tfidvec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2682) (1, 2708)\n",
      "(2682,) (2708,)\n"
     ]
    }
   ],
   "source": [
    "#top 10 tokens\n",
    "#sum words across all documents\n",
    "neg_counts = np.sum(neg_result, axis=0)\n",
    "pos_counts = np.sum(pos_result, axis=0)\n",
    "\n",
    "print(neg_counts.shape, pos_counts.shape)\n",
    "print(neg_vocabs.shape, pos_vocabs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>39.847912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phone</th>\n",
       "      <td>23.267698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>service</th>\n",
       "      <td>22.505866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>22.169014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>21.564529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie</th>\n",
       "      <td>21.019005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place</th>\n",
       "      <td>19.833446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>19.725452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>19.674071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waste</th>\n",
       "      <td>19.027319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>17.869434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>film</th>\n",
       "      <td>15.883446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>money</th>\n",
       "      <td>14.501533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>go</th>\n",
       "      <td>14.149656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buy</th>\n",
       "      <td>13.633176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terrible</th>\n",
       "      <td>13.169713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>come</th>\n",
       "      <td>12.784991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think</th>\n",
       "      <td>12.738415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recommend</th>\n",
       "      <td>11.875905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointed</th>\n",
       "      <td>11.479982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>11.299373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poor</th>\n",
       "      <td>11.185807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>look</th>\n",
       "      <td>10.878283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get</th>\n",
       "      <td>10.811286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product</th>\n",
       "      <td>10.478940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0\n",
       "bad           39.847912\n",
       "phone         23.267698\n",
       "service       22.505866\n",
       "time          22.169014\n",
       "food          21.564529\n",
       "movie         21.019005\n",
       "place         19.833446\n",
       "good          19.725452\n",
       "work          19.674071\n",
       "waste         19.027319\n",
       "like          17.869434\n",
       "film          15.883446\n",
       "money         14.501533\n",
       "go            14.149656\n",
       "buy           13.633176\n",
       "terrible      13.169713\n",
       "come          12.784991\n",
       "think         12.738415\n",
       "recommend     11.875905\n",
       "disappointed  11.479982\n",
       "quality       11.299373\n",
       "poor          11.185807\n",
       "look          10.878283\n",
       "get           10.811286\n",
       "product       10.478940"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top ten negative terms\n",
    "df = pd.DataFrame(neg_counts, columns = neg_vocabs).T.sort_values(by=0, ascending=False)\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>58.746367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>57.986900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phone</th>\n",
       "      <td>32.194893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>30.731898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>25.858021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>place</th>\n",
       "      <td>23.602051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>film</th>\n",
       "      <td>22.920675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie</th>\n",
       "      <td>22.859297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food</th>\n",
       "      <td>22.801345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>service</th>\n",
       "      <td>22.183989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>20.289906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>19.890513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excellent</th>\n",
       "      <td>19.290905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nice</th>\n",
       "      <td>17.429621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recommend</th>\n",
       "      <td>17.389775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product</th>\n",
       "      <td>17.046047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <td>16.870393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>headset</th>\n",
       "      <td>13.610716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quality</th>\n",
       "      <td>13.417409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>happy</th>\n",
       "      <td>12.331299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sound</th>\n",
       "      <td>11.620129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delicious</th>\n",
       "      <td>10.788580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>10.587168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>definitely</th>\n",
       "      <td>10.235624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awesome</th>\n",
       "      <td>10.224141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0\n",
       "great       58.746367\n",
       "good        57.986900\n",
       "phone       32.194893\n",
       "work        30.731898\n",
       "love        25.858021\n",
       "place       23.602051\n",
       "film        22.920675\n",
       "movie       22.859297\n",
       "food        22.801345\n",
       "service     22.183989\n",
       "price       20.289906\n",
       "like        19.890513\n",
       "excellent   19.290905\n",
       "nice        17.429621\n",
       "recommend   17.389775\n",
       "product     17.046047\n",
       "time        16.870393\n",
       "headset     13.610716\n",
       "quality     13.417409\n",
       "happy       12.331299\n",
       "sound       11.620129\n",
       "delicious   10.788580\n",
       "amazing     10.587168\n",
       "definitely  10.235624\n",
       "awesome     10.224141"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#top ten negative terms\n",
    "df = pd.DataFrame(pos_counts, columns = pos_vocabs).T.sort_values(by=0, ascending=False)\n",
    "df.head(25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, tf-idf works much better than countvectorizer!   Also, a lesson learned is that if we can give very good features to the classifier, the model can work well right away...."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "I gonna keep this short.  Please remind the best practices I teach in CP class.  :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2198,) (550,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.87      0.85       285\n",
      "           1       0.85      0.80      0.82       265\n",
      "\n",
      "    accuracy                           0.84       550\n",
      "   macro avg       0.84      0.84      0.84       550\n",
      "weighted avg       0.84      0.84      0.84       550\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[248,  37],\n",
       "       [ 53, 212]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#define model\n",
    "classifier = LinearSVC()\n",
    "tfidvec    = TfidfVectorizer()\n",
    "\n",
    "#define data\n",
    "X = data['Review']\n",
    "y = data['Sentiment']\n",
    "\n",
    "#split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "#make pipeline\n",
    "clf = Pipeline([('tfidf', tfidvec), ('clf', classifier)])\n",
    "\n",
    "#train\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "#predict\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "#metrics\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "#confusion matrix\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Real world inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['Wow, this is amzing lesson'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['Wow, this sucks'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['Worth of watching it. Please like it'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(['This is not bad'])  #still fail in double negative!!! a very useful test... :-)\n",
    "#possible way to fix this is to get \"spanning\" features....not single token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6 (default, Nov 17 2020, 08:01:36) \n[Clang 12.0.0 (clang-1200.0.32.21)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "714d3f4db9a58ba7d2f2a9a4fffe577af3df8551aebd380095064812e2e0a6a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
