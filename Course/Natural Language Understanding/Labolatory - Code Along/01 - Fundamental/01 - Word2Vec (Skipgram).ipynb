{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec (Skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.23.4', '1.13.1+cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__, torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the sentences / corpus \n",
    "#corpus is define as set of document\n",
    "#document is basically a nuch of sentence(s)\n",
    "corpus = [\"apple banana fruit\", \"banana apple fruit\", \"banana fruit apple\", \n",
    "          \"dog cat animal\", \"cat dog animal\", \"cat animal dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['apple', 'banana', 'fruit'],\n",
       " ['banana', 'apple', 'fruit'],\n",
       " ['banana', 'fruit', 'apple'],\n",
       " ['dog', 'cat', 'animal'],\n",
       " ['cat', 'dog', 'animal'],\n",
       " ['cat', 'animal', 'dog']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. tokenize\n",
    "#usually you use spaCy/ NLTK to tokenize (but we gonna do this later on, we gonna have spaCy)\n",
    "corpus_tokenized = [sent.split(\" \") for sent in corpus]\n",
    "corpus_tokenized #we called each of this as 'tokens' NOT words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. numericalize (vocab)\n",
    "\n",
    "#2.1 get all the unique words\n",
    "#we want to flatten unit (basically merge all list)\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabs = list(set(flatten(corpus_tokenized))) #vocabs is a term degining all unique words your system know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dog': 0, 'banana': 1, 'fruit': 2, 'cat': 3, 'animal': 4, 'apple': 5}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.2 assign id to all these vocabs\n",
    "word2index = {v: idx for idx, v in enumerate(vocabs)}\n",
    "word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add <UNK>, which is a very normal token exits in the world\n",
    "vocabs.append('<UNK>') #chaky, can it be ##UNK, or UNKKKKK, or anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have a way to know what is the id of <UNK>\n",
    "word2index['<UNK>'] = 6 #usually <UNK> is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'dog',\n",
       " 1: 'banana',\n",
       " 2: 'fruit',\n",
       " 3: 'cat',\n",
       " 4: 'animal',\n",
       " 5: 'apple',\n",
       " 6: '<UNK>'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create index2word dictionary\n",
    "# index2word = {idx: v for idx, v in enumerate(vocabs)}\n",
    "# index2word\n",
    "\n",
    "# for key,value in word2index.items():\n",
    "#     print(key,value)\n",
    "\n",
    "index2word = {v:k for k,v in word2index.items()}\n",
    "index2word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare train data\n",
    "You move the window along, and create those tuples as we said in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['banana', 'apple'],\n",
       " ['banana', 'fruit'],\n",
       " ['apple', 'banana'],\n",
       " ['apple', 'fruit'],\n",
       " ['fruit', 'banana'],\n",
       " ['fruit', 'apple'],\n",
       " ['cat', 'dog'],\n",
       " ['cat', 'animal'],\n",
       " ['dog', 'cat'],\n",
       " ['dog', 'animal'],\n",
       " ['animal', 'cat'],\n",
       " ['animal', 'dog']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#move along the corpus\n",
    "#to fit with our corus, we gonna use window_size = 1\n",
    "skipgrams =[]\n",
    "#for each corpus\n",
    "for sent in corpus_tokenized:\n",
    "    #for each sent ('apple', 'banana', 'fruit')\n",
    "    for i in range(1,len(sent)-1): #start from 1 to second last\n",
    "        # print(sent[i])\n",
    "        center_word = sent[i]\n",
    "        outside_word = [sent[i-1],sent[i+1]] #window_size =1\n",
    "        #here we want to create (banana, apple), (banana, fruit) append to some list\n",
    "        for o in outside_word:\n",
    "            skipgrams.append([center_word,o])\n",
    "skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = [  'apple banana fruit','banana apple fruit','banana fruit apple',\n",
    "#             'dog cat animal', 'cat dog animal', 'cat animal dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's make what we have made into a fucntion (batch function)\n",
    "#return a batches of data, e.g., = 2 --> ['banana', 'apple'],['banana','fruit']\n",
    "#also i want these batches ti be id, NOT token --> [5,4]\n",
    "\n",
    "def random_batch(batch_size, corpus):\n",
    "    skipgrams =[]\n",
    "    #for each corpus\n",
    "    for sent in corpus_tokenized:\n",
    "        #for each sent ('apple', 'banana', 'fruit')\n",
    "        for i in range(1,len(sent)-1): #start from 1 to second last\n",
    "            # print(sent[i])\n",
    "            center_word = word2index[sent[i]]\n",
    "            outside_word = [word2index[sent[i-1]],word2index[sent[i+1]]] #window_size =1\n",
    "            #here we want to create (banana, apple), (banana, fruit) append to some list\n",
    "            for o in outside_word:\n",
    "                skipgrams.append([center_word,o])\n",
    "    #only get a batch, mot the entire lsit\n",
    "    random_index = np.random.choice(range(len(skipgrams)),batch_size,replace=False)\n",
    "    \n",
    "    #appending some list of inputs and labels\n",
    "    random_inputs, random_labels = [] , []\n",
    "    for index in random_index:\n",
    "        random_inputs.append([skipgrams[index][0]]) #center words, this will be as shape of (1,) -> (1,1) for modeling\n",
    "        random_labels.append([skipgrams[index][1]])\n",
    "\n",
    "    return np.array(random_inputs),np.array(random_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n",
      "label.shape=(10, 1)\n"
     ]
    }
   ],
   "source": [
    "input,label = random_batch(10, corpus_tokenized)\n",
    "\n",
    "print(f'{input.shape}')\n",
    "print(f'{label.shape=}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}}\\log P(w_{t+j} | w_t; \\theta)$$\n",
    "\n",
    "where $P(w_{t+j} | w_t; \\theta) = $\n",
    "\n",
    "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$\n",
    "\n",
    "where $o$ is the outside words and $c$ is the center word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc_size = len(vocabs)\n",
    "voc_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'banana', 'fruit', 'cat', 'animal', 'apple', '<UNK>']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model will accept three vectors - u_o, v_c, u_w\n",
    "#u_o - vectos for outside words\n",
    "#v_C - vector for center word\n",
    "#u_w - vectors of all vocabs\n",
    "\n",
    "class Skipgram(nn.Module):\n",
    "    def __init__(self,voc_size, emb_size):\n",
    "        super(Skipgram,self).__init__()\n",
    "        self.embedding_center_word = nn.Embedding(voc_size, emb_size) #is a lookup table mapping all ids in voc_size, into some vector of size emb_size\n",
    "        self.embedding_outside_word = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center_word, outside_word, all_vocabs):\n",
    "        #center_word, outside_word: (batch_size,1)\n",
    "        #all_vocabs : (batch_size, voc_size)\n",
    "\n",
    "        #convert them into embedding\n",
    "        center_word_embed = self.embedding_center_word(center_word)     #v_c (batch_size,1, emb_size)\n",
    "        outside_word_embed = self.embedding_outside_word(outside_word)  #u_o (batch_size,1, emb_size)\n",
    "        all_vocabs_embed = self.embedding_outside_word(all_vocabs)      #u_w (batch_size,voc_size, emb_size)\n",
    "        \n",
    "        #bmm is basically @ or .dot but across batches (ie., ignore the batch dimension)\n",
    "        top_term = outside_word_embed.bmm(center_word_embed.transpose(1,2)).squeeze(2)\n",
    "        #(batch_size,1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) ===> (batch_size, 1)\n",
    "        \n",
    "        top_term_exp = torch.exp(top_term) #exp(uo vc)\n",
    "        #(batch_size, 1)\n",
    "\n",
    "        lower_term = all_vocabs_embed.bmm(center_word_embed.transpose(1,2)).squeeze(2)\n",
    "        #(batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) ===> (batch_size, voc_size)\n",
    "        \n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term)) #sum exp(uw, vc)\n",
    "        #(batch_size, 1)\n",
    "        \n",
    "        loss_fn = -torch.mean(torch.log(top_term_exp/lower_term_sum))\n",
    "        #(batc_size,1) / (batch_size,1) ==mena==> scalar\n",
    "\n",
    "        return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preparing all_vocabs\n",
    "batch_size = 2\n",
    "\n",
    "def prepare_seqeunce(seq, word2index):\n",
    "    #map(fucntion, list of something)\n",
    "    #map will look at each of element in this list, and apply this function\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_seqeunce(list(vocabs),word2index).expand(batch_size, voc_size)\n",
    "all_vocabs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, label = random_batch(1, corpus_tokenized)\n",
    "input #center word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label #context word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 2 #usually, this can be 50,100 or 300\n",
    "model = Skipgram(voc_size,emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.LongTensor(input)\n",
    "label_tensor  = torch.LongTensor(label) #longTensor menas integer......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is diffrence\n",
    "torch.LongTensor(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.LongTensor([2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1]), torch.Size([1, 1]), torch.Size([2, 7]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape, label_tensor.shape, all_vocabs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this should give one number\n",
    "# loss = model(input_tensor,label_tensor, all_vocabs)\n",
    "# loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 #why? no reason\n",
    "emb_size = 2 #why? no reason; usually 50,100, 300 but 2 so we can plot (50 can also plot, but need PCA)\n",
    "model = Skipgram(voc_size,emb_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #-log\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 | Loss 2.590891 | Time : ?\n",
      "Epoch 2000 | Loss 2.198295 | Time : ?\n",
      "Epoch 3000 | Loss 1.598184 | Time : ?\n",
      "Epoch 4000 | Loss 1.641230 | Time : ?\n",
      "Epoch 5000 | Loss 1.391963 | Time : ?\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5000\n",
    "#for epoch\n",
    "for epoch in range(num_epochs):\n",
    "    #get random batch\n",
    "    input_batch, label_batch = random_batch(batch_size,corpus)\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    label_batch = torch.LongTensor(label_batch)\n",
    "\n",
    "    # print(input_batch.shape,label_batch.shape,all_vocabs.shape)\n",
    "    \n",
    "    #loss = model\n",
    "    loss = model(input_batch,label_batch,all_vocabs)\n",
    "    #backpropagate\n",
    "    loss.backward()\n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    #print epoch loss\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f'Epoch {epoch+1} | Loss {loss:.6f} | Time : ?')\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1]) torch.Size([2, 1]) torch.Size([2, 7])\n"
     ]
    }
   ],
   "source": [
    "print(input_batch.shape,label_batch.shape,all_vocabs.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plotting the embeddings\n",
    "\n",
    "Is really the related studd are close to each other, and vice versa.\n",
    "\n",
    "The most fun part: Will 'banana' closer to 'fruit' than 'cat'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'banana', 'fruit', 'cat', 'animal', 'apple', '<UNK>']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana = torch.LongTensor([word2index['banana']])\n",
    "banana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5153, -4.5593]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana_center_embed = model.embedding_center_word(banana)\n",
    "banana_outside_embed = model.embedding_outside_word(banana)\n",
    "banana_embed = (banana_center_embed+banana_outside_embed)/2\n",
    "banana_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.159918785095215, -2.2305541038513184)\n",
      "(0.12311295419931412, 3.5641565322875977)\n",
      "(-1.3339006900787354, 1.1150579452514648)\n"
     ]
    }
   ],
   "source": [
    "#find embedding of fruit, cat\n",
    "def get_embed(word):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except :\n",
    "        index = word2index['<UNK>'] #unknown\n",
    "    word = torch.LongTensor([index])\n",
    "    \n",
    "    embed =  (model.embedding_center_word(word)+model.embedding_outside_word(word))/2\n",
    "    return embed[0][0].item(),embed[0][1].item()\n",
    "    \n",
    "print(get_embed('fruit'))\n",
    "print(get_embed('cat'))\n",
    "print(get_embed('chaky'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#help me plot fruit cat banana on maplotlib\n",
    "plt.figure(figsize=(6,3))\n",
    "for i, word in enumerate(vocabs[:20]):\n",
    "    x,y = get_embed(word)\n",
    "    plt.scatter(x,y)\n",
    "    plt.annotate(word,xy=(x,y),xytext=(5,2),textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cosine Similarity\n",
    "How do (from Scratch) calcualte cosine similarity?\n",
    "\n",
    "Formally the [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) $s$ between two vectors $p$ and $q$ is defined as:\n",
    "\n",
    "$$s = \\frac{p \\cdot q}{||p|| ||q||}, \\textrm{ where } s \\in [-1, 1] $$ \n",
    "\n",
    "If $p$ and $q$ is super similar, the result is 1 otherwise 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog', 'banana', 'fruit', 'cat', 'animal', 'apple', '<UNK>']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's try similarity between first and second, and second and third\n",
    "cat          = get_embed('cat')\n",
    "fruit        = get_embed('fruit')\n",
    "animal       = get_embed('animal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat vs. fruit:  -0.6939473158969951\n",
      "cat vs. animal:  0.9999627972748365\n",
      "cat vs. cat:  1.0\n"
     ]
    }
   ],
   "source": [
    "#numpy version\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim\n",
    "    \n",
    "print(f\"cat vs. fruit: \",cos_sim(cat, fruit))\n",
    "print(f\"cat vs. animal: \",cos_sim(cat, animal))\n",
    "print(f\"cat vs. cat: \",cos_sim(cat, cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat vs. fruit:  -0.6939473158969951\n",
      "cat vs. animal:  0.9999627972748366\n",
      "cat vs. cat:  1\n"
     ]
    }
   ],
   "source": [
    "#scipy version\n",
    "from scipy import spatial\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    cos_sim = 1 - spatial.distance.cosine(a, b)  #distance = 1 - similarlity, because scipy only gives distance\n",
    "    return cos_sim\n",
    "\n",
    "print(f\"cat vs. fruit: \",cos_sim(cat, fruit))\n",
    "print(f\"cat vs. animal: \",cos_sim(cat, animal))\n",
    "print(f\"cat vs. cat: \",cos_sim(cat, cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
