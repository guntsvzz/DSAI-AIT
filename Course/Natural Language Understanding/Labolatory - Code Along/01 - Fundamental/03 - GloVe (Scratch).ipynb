{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe : Global Vectors for Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the sentences / corpus \n",
    "#corpus is define as set of document\n",
    "#document is basically a nuch of sentence(s)\n",
    "corpus = [\"apple banana fruit\", \"banana apple fruit\", \"banana fruit apple\", \n",
    "          \"dog cat animal\", \"cat dog animal\", \"cat animal dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['apple', 'banana', 'fruit'],\n",
       " ['banana', 'apple', 'fruit'],\n",
       " ['banana', 'fruit', 'apple'],\n",
       " ['dog', 'cat', 'animal'],\n",
       " ['cat', 'dog', 'animal'],\n",
       " ['cat', 'animal', 'dog']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. tokenize\n",
    "#usually you use spaCy/ NLTK to tokenize (but we gonna do this later on, we gonna have spaCy)\n",
    "corpus_tokenized = [sent.split(\" \") for sent in corpus]\n",
    "corpus_tokenized #we called each of this as 'tokens' NOT words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fruit', 'apple', 'dog', 'banana', 'cat', 'animal']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. numericalize (vocab)\n",
    "\n",
    "#2.1 get all the unique words\n",
    "#we want to flatten unit (basically merge all list)\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabs = list(set(flatten(corpus_tokenized))) #vocabs is a term degining all unique words your system know\n",
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fruit': 0, 'apple': 1, 'dog': 2, 'banana': 3, 'cat': 4, 'animal': 5}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2.2 assign id to all these vocabs\n",
    "word2index = {v: idx for idx, v in enumerate(vocabs)}\n",
    "word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add <UNK>, which is a very normal token exits in the world\n",
    "vocabs.append('<UNK>') #chaky, can it be ##UNK, or UNKKKKK, or anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have a way to know what is the id of <UNK>\n",
    "word2index['<UNK>'] = 6 #usually <UNK> is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'fruit',\n",
       " 1: 'apple',\n",
       " 2: 'dog',\n",
       " 3: 'banana',\n",
       " 4: 'cat',\n",
       " 5: 'animal',\n",
       " 6: '<UNK>'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create index2word dictionary\n",
    "# index2word = {idx: v for idx, v in enumerate(vocabs)}\n",
    "# index2word\n",
    "\n",
    "# for key,value in word2index.items():\n",
    "#     print(key,value)\n",
    "\n",
    "index2word = {v:k for k,v in word2index.items()}\n",
    "index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fruit', 'apple', 'dog', 'banana', 'cat', 'animal', '<UNK>']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Co-occurence Matrix X\n",
    "\n",
    "Count the occurrences of pair of words using window size of 1 (you can use 2, 3, 4, up to you)\n",
    "\n",
    "E.g., Dog loves to eat meal.\n",
    "\n",
    "['Dog','loves',1],['loves','to',1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['apple', 'banana', 'fruit'], ['banana', 'apple', 'fruit'], ['banana', 'fruit', 'apple'], ['dog', 'cat', 'animal'], ['cat', 'dog', 'animal'], ['cat', 'animal', 'dog']]\n"
     ]
    }
   ],
   "source": [
    "#use Counter to first count stuffs\n",
    "from collections import Counter\n",
    "print(corpus_tokenized)\n",
    "\n",
    "#count the frequency of each word\n",
    "#we somehow need this to claiclate the probability Pi\n",
    "X_i = Counter(flatten(corpus_tokenized)) #merge all list ... (flattten is a function I defines)\n",
    "# X_i['apple'] #get the probability of apple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defome a skipgram of window size 1\n",
    "skip_grams = []\n",
    "#loop thriugh each corpus\n",
    "for sent in corpus_tokenized:\n",
    "    #loop through each word from i to n-1 (because 0 and n has no context window)\n",
    "    for idx,i in enumerate(range(1, len(sent)-1)):\n",
    "        # print(sent[i])\n",
    "        target = sent[i] #center word\n",
    "        context = [sent[i-1],sent[i+1]]\n",
    "        #append(i,i+1) and append(i,i-1)\n",
    "        for c in context:\n",
    "            skip_grams.append((target,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['apple', 'banana', 'fruit'],\n",
       " ['banana', 'apple', 'fruit'],\n",
       " ['banana', 'fruit', 'apple'],\n",
       " ['dog', 'cat', 'animal'],\n",
       " ['cat', 'dog', 'animal'],\n",
       " ['cat', 'animal', 'dog']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('banana', 'apple'),\n",
       " ('banana', 'fruit'),\n",
       " ('apple', 'banana'),\n",
       " ('apple', 'fruit'),\n",
       " ('fruit', 'banana'),\n",
       " ('fruit', 'apple'),\n",
       " ('cat', 'dog'),\n",
       " ('cat', 'animal'),\n",
       " ('dog', 'cat'),\n",
       " ('dog', 'animal'),\n",
       " ('animal', 'cat'),\n",
       " ('animal', 'dog')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('banana', 'apple'): 1,\n",
       "         ('banana', 'fruit'): 1,\n",
       "         ('apple', 'banana'): 1,\n",
       "         ('apple', 'fruit'): 1,\n",
       "         ('fruit', 'banana'): 1,\n",
       "         ('fruit', 'apple'): 1,\n",
       "         ('cat', 'dog'): 1,\n",
       "         ('cat', 'animal'): 1,\n",
       "         ('dog', 'cat'): 1,\n",
       "         ('dog', 'animal'): 1,\n",
       "         ('animal', 'cat'): 1,\n",
       "         ('animal', 'dog'): 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#since we have these occcurences, we can count, to make our co-occurence matrix!!!\n",
    "X_ik_skipgram = Counter(skip_grams)\n",
    "X_ik_skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ik_skipgram[('banana','animal')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Weighting function f\n",
    "\n",
    "GloVe includes a weighting function to scale down too frequent words.\n",
    "\n",
    "<img src = \"figures/glove_weighting_func.png\" width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting(w_i,w_j,X_ik): #we need w_i and w_j, because we can try its-co-occurrences, if it's too big, we scale it down\n",
    "    #check whether the co-occurrences between these two word exits??\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i,w_j)]\n",
    "    except:\n",
    "        x_ij = 1 #why one, so that the probability thingly won't break....\n",
    "    \n",
    "    #maximum co-occurrences; we follow the paper\n",
    "    x_max = 100\n",
    "    alpha = 0.75\n",
    "\n",
    "    #if the co-occurrences does not exceed x_max, cale it down based on some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij/x_max)**alpha\n",
    "    else:\n",
    "        result = 1 #this is the maximum probability you can havve\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03162277660168379\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "w_i = 'banana'\n",
    "w_j = 'fruit'\n",
    "w_j2 = 'chaky'\n",
    "\n",
    "print(weighting(w_i,w_j, X_ik_skipgram)) #scale from 1 to 0.0316\n",
    "print(weighting(w_i,w_j2, X_ik_skipgram))  #the paper says that f(0) = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ik_skipgram.get(('banana','fruit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now apply this weighting to all possible pairs\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {} #for keeping the co-occurrences\n",
    "weighting_dic = {} #for keeping all the probability after passing through the weighting function\n",
    "\n",
    "for bigram in combinations_with_replacement(vocabs,2): #we need to also think its reverse\n",
    "    # print(bigram)\n",
    "    #if this bigram exists in X_ik_skipgrams\n",
    "    #we gonna add this to our c-occurence matrix\n",
    "    if X_ik_skipgram.get(bigram) is not None:\n",
    "        cooc = X_ik_skipgram[bigram] #get the co-occurrences\n",
    "        X_ik[bigram] = cooc + 1 #this is agian basically label smoothing.... (stability issue (especailly when divide something))\n",
    "        X_ik[(bigram[1],bigram[0])] = cooc + 1 #trick to get all pairs\n",
    "    else: #otherwise, do nothing\n",
    "        pass\n",
    "    #apply the weighting function using this co-occurrence matrix thingly\n",
    "    weighting_dic[bigram] = weighting(bigram[0],bigram[1],X_ik)\n",
    "    weighting_dic[(bigram[1],bigram[0])] = weighting(bigram[1],bigram[0],X_ik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_ik_skipgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('fruit', 'apple'): 2,\n",
       " ('apple', 'fruit'): 2,\n",
       " ('fruit', 'banana'): 2,\n",
       " ('banana', 'fruit'): 2,\n",
       " ('apple', 'banana'): 2,\n",
       " ('banana', 'apple'): 2,\n",
       " ('dog', 'cat'): 2,\n",
       " ('cat', 'dog'): 2,\n",
       " ('dog', 'animal'): 2,\n",
       " ('animal', 'dog'): 2,\n",
       " ('cat', 'animal'): 2,\n",
       " ('animal', 'cat'): 2}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighting_dic #give small probability to never-occured is called 'label smoothing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare train data\n",
    "You move the window along, and create those tuples as we said in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'fruit']\n",
      "['banana', 'apple', 'fruit']\n",
      "['banana', 'fruit', 'apple']\n",
      "['dog', 'cat', 'animal']\n",
      "['cat', 'dog', 'animal']\n",
      "['cat', 'animal', 'dog']\n"
     ]
    }
   ],
   "source": [
    "for c in corpus_tokenized:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def random_batch(batch_size,word_sequence,skip_grams,X_ik,weighting_dic):\n",
    "    #loop through skipgram, and change it id because when sending model, it must number \n",
    "    skipg_grams_id = [(word2index[skip_gram[0]],word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    #randomly pick 'batch_size' indexes\n",
    "    number_of_choices = len(skipg_grams_id)\n",
    "    random_index = np.random.choice(number_of_choices, batch_size, replace=False) #no repeating indexes among  these random indexes\n",
    "    \n",
    "    # print(random_index)\n",
    "    random_inputs = [] #xi, wi (in batches)\n",
    "    random_labels = [] #xj, wj (in batches)\n",
    "    random_coocs  = [] #xij (in batches)\n",
    "    random_weighting = [] #f(xij) (in batches)\n",
    "    #for each of the sample in these indexes\n",
    "    for i in random_index:\n",
    "        random_inputs.append([skipg_grams_id[i][0]]) #same reason why I put bracket here....\n",
    "        random_labels.append([skipg_grams_id[i][1]])\n",
    "\n",
    "        #get coocs\n",
    "        #first check whether it exists.....\n",
    "        pair = skip_grams[i] #e.g., ['banana','fruit']\n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1 #label smoothing\n",
    "\n",
    "        random_coocs.append([math.log(cooc)]) #1. why log, #2 why bracket -> size ==> (,1) #my neural network excepts (,1)\n",
    "        \n",
    "        #for weighting\n",
    "        weighting = weighting_dic[pair] #why not user try... maybe it does not exist\n",
    "        random_weighting.append(weighting)\n",
    "\n",
    "    return np.array(random_inputs),np.array(random_labels),np.array(random_coocs),np.array(random_weighting)\n",
    "        #return xi,xj\n",
    "        #return cooc Xij\n",
    "        #return weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "input,target,coocs,weightin = random_batch(batch_size,corpus_tokenized,skip_grams,X_ik,weighting_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[3],\n",
       "        [2]]),\n",
       " array([[1],\n",
       "        [4]]),\n",
       " array([[0.69314718],\n",
       "        [0.69314718]]),\n",
       " array([0.05318296, 0.05318296]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input,target,coocs,weightin"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model\n",
    "\n",
    "<img src =\"figures/glove.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model will accept three vectors - u_o, v_c, u_w\n",
    "#u_o - vectos for outside words\n",
    "#v_C - vector for center word\n",
    "#u_w - vectors of all vocabs\n",
    "\n",
    "class GloVe(nn.Module):\n",
    "    def __init__(self,voc_size, emb_size):\n",
    "        super(GloVe,self).__init__()\n",
    "        self.embedding_center_word = nn.Embedding(voc_size, emb_size) #is a lookup table mapping all ids in voc_size, into some vector of size emb_size\n",
    "        self.embedding_outside_word = nn.Embedding(voc_size, emb_size)\n",
    "        \n",
    "        self.bias_i = nn.Embedding(voc_size, 1)\n",
    "        self.bias_j = nn.Embedding(voc_size, 1)\n",
    "    def forward(self, center_words, outside_words, coocs, weighting):\n",
    "        #get the embedding of center_words and outside_words\n",
    "        center_embeds = self.embedding_center_word(center_words)\n",
    "        outside_embeds = self.embedding_outside_word(outside_words)\n",
    "\n",
    "        #create biases #create unique embedding (voc_size,1)\n",
    "        inner_product = center_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        bias_i = self.bias_i(center_words).squeeze(1) #center\n",
    "        bias_j = self.bias_j(outside_words).squeeze(1) #target\n",
    "        #do the product between wi and wj\n",
    "        loss = weighting * torch.pow(inner_product + bias_i + bias_j - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size = len(vocabs)\n",
    "batch_size = 2 #why? no reason\n",
    "emb_size = 2 #why? no reason; usually 50,100, 300 but 2 so we can plot (50 can also plot, but need PCA)\n",
    "model = GloVe(voc_size,emb_size)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #-log\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000 | Loss 0.070185 | Time : 0m 0s\n",
      "Epoch 2000 | Loss 0.181298 | Time : 0m 1s\n",
      "Epoch 3000 | Loss 0.106815 | Time : 0m 2s\n",
      "Epoch 4000 | Loss 0.003855 | Time : 0m 3s\n",
      "Epoch 5000 | Loss 0.043862 | Time : 0m 3s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "num_epochs = 5000\n",
    "#for epoch\n",
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    #get random batch\n",
    "    input, label, cooc, weightin = random_batch(batch_size,corpus_tokenized,skip_grams,X_ik,weighting_dic)\n",
    "    input_batch = torch.LongTensor(input)\n",
    "    label_batch = torch.LongTensor(label)\n",
    "    cooc_batch = torch.FloatTensor(cooc)\n",
    "    weightin_batch = torch.FloatTensor(weightin)\n",
    "\n",
    "    # print(input_batch.shape,label_batch.shape,cooc_batch.shape,weightin_batch.shape)\n",
    "\n",
    "    #loss = model\n",
    "    loss = model(input_batch,label_batch,cooc_batch,weightin_batch)\n",
    "    #backpropagate\n",
    "    loss.backward()\n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    #print epoch loss\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f'Epoch {epoch+1} | Loss {loss:.6f} | Time : {epoch_mins}m {epoch_secs}s')\n",
    "\n",
    "    # break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Plotting the embeddings\n",
    "\n",
    "Is really the related studd are close to each other, and vice versa.\n",
    "\n",
    "The most fun part: Will 'banana' closer to 'fruit' than 'cat'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fruit', 'apple', 'dog', 'banana', 'cat', 'animal', '<UNK>']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana = torch.LongTensor([word2index['banana']])\n",
    "banana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4795,  0.4567]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "banana_center_embed = model.embedding_center_word(banana)\n",
    "banana_outside_embed = model.embedding_outside_word(banana)\n",
    "banana_embed = (banana_center_embed+banana_outside_embed)/2\n",
    "banana_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.21679764986038208, -0.6021071076393127)\n",
      "(0.5676218271255493, -0.6116302609443665)\n",
      "(-0.9124876260757446, 0.08380907028913498)\n"
     ]
    }
   ],
   "source": [
    "#find embedding of fruit, cat\n",
    "def get_embed(word):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except :\n",
    "        index = word2index['<UNK>'] #unknown\n",
    "    word = torch.LongTensor([index])\n",
    "    \n",
    "    embed =  (model.embedding_center_word(word)+model.embedding_outside_word(word))/2\n",
    "    return embed[0][0].item(),embed[0][1].item()\n",
    "    \n",
    "print(get_embed('fruit'))\n",
    "print(get_embed('cat'))\n",
    "print(get_embed('chaky'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#help me plot fruit cat banana on maplotlib\n",
    "plt.figure(figsize=(6,3))\n",
    "for i, word in enumerate(vocabs[:20]):\n",
    "    x,y = get_embed(word)\n",
    "    plt.scatter(x,y)\n",
    "    plt.annotate(word,xy=(x,y),xytext=(5,2),textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cosine Similarity\n",
    "How do (from Scratch) calcualte cosine similarity?\n",
    "\n",
    "Formally the [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) $s$ between two vectors $p$ and $q$ is defined as:\n",
    "\n",
    "$$s = \\frac{p \\cdot q}{||p|| ||q||}, \\textrm{ where } s \\in [-1, 1] $$ \n",
    "\n",
    "If $p$ and $q$ is super similar, the result is 1 otherwise 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fruit', 'apple', 'dog', 'banana', 'cat', 'animal', '<UNK>']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's try similarity between first and second, and second and third\n",
    "cat          = get_embed('cat')\n",
    "fruit        = get_embed('fruit')\n",
    "animal       = get_embed('animal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat vs. fruit:  0.45919316850865327\n",
      "cat vs. animal:  -0.3662895741009273\n",
      "cat vs. cat:  1.0\n"
     ]
    }
   ],
   "source": [
    "#numpy version\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim\n",
    "    \n",
    "print(f\"cat vs. fruit: \",cos_sim(cat, fruit))\n",
    "print(f\"cat vs. animal: \",cos_sim(cat, animal))\n",
    "print(f\"cat vs. cat: \",cos_sim(cat, cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat vs. fruit:  0.4591931685086532\n",
      "cat vs. animal:  -0.36628957410092733\n",
      "cat vs. cat:  1\n"
     ]
    }
   ],
   "source": [
    "#scipy version\n",
    "from scipy import spatial\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    cos_sim = 1 - spatial.distance.cosine(a, b)  #distance = 1 - similarlity, because scipy only gives distance\n",
    "    return cos_sim\n",
    "\n",
    "print(f\"cat vs. fruit: \",cos_sim(cat, fruit))\n",
    "print(f\"cat vs. animal: \",cos_sim(cat, animal))\n",
    "print(f\"cat vs. cat: \",cos_sim(cat, cat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
