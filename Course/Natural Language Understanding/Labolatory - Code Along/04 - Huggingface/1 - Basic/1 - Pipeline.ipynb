{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NYSbLG1msgRL"
      },
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "Huggingface is like sklearn, it provides many built-in functionalities.  However, it is also very modular such that it allows us to explore new models.\n",
        "\n",
        "## Pipeline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2aJnT9-PsgRP"
      },
      "source": [
        "Before anything, make sure you got transformers installed:\n",
        "\n",
        "    pip install datasets evaluate transformers sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-Asz45PNsgRQ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'4.25.1'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import transformers\n",
        "transformers.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'1.13.0'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.__version__"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Pipeline\n",
        "\n",
        "The most basic object in the 🤗 Transformers library is the pipeline() function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GggfGn1ysgRR",
        "outputId": "70c9d534-2237-4862-9129-a9b00b025d85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598049521446228}]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can even pass several sentences!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hmyyQw_ksgRS",
        "outputId": "2b26e984-ce6b-480f-ba87-986938ad83ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
              " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier(\n",
        "    [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default, this pipeline selects a particular pretrained model that has been fine-tuned for sentiment analysis in English. The model is downloaded and cached when you create the classifier object. If you rerun the command, the cached model will be used instead and there is no need to download the model again.\n",
        "\n",
        "There are three main steps involved when you pass some text to a pipeline:\n",
        "\n",
        "1. The text is preprocessed into a format the model can understand.\n",
        "2. The preprocessed inputs are passed to the model.\n",
        "3. The predictions of the model are post-processed, so you can make sense of them.\n",
        "\n",
        "Some of the currently available pipelines are:\n",
        "\n",
        "- feature-extraction (get the vector representation of a text)\n",
        "- fill-mask\n",
        "- ner (named entity recognition)\n",
        "- question-answering\n",
        "- sentiment-analysis\n",
        "- summarization\n",
        "- text-generation\n",
        "- translation\n",
        "- zero-shot-classification\n",
        "\n",
        "Let’s have a look at a few of these!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Zero shot classification\n",
        "\n",
        "This pipeline is called `zero-shot` because you don’t need to fine-tune the model on your data to use it. It can directly return probability scores for any list of labels you want!`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yWuUkLD2sgRT",
        "outputId": "90a6ff69-fb10-479f-ae7c-d0a07952f737"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'sequence': 'This is a course about the Transformers library',\n",
              " 'labels': ['education', 'business', 'politics'],\n",
              " 'scores': [0.8445981740951538, 0.11197477579116821, 0.04342705383896828]}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "classifier(\n",
        "    \"This is a course about the Transformers library\",\n",
        "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text generation\n",
        "\n",
        " The main idea here is that you provide a prompt and the model will auto-complete it by generating the remaining text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fqPF7IA-sgRU",
        "outputId": "a09ba4fa-584f-4195-e9a5-0688fd29a0cc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'In this course, we will teach you how to create custom apps using JavaScript, create new services using Angular, and build apps that look good with an'}]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\", max_length=30, pad_token_id=0)\n",
        "generator(\"In this course, we will teach you how to\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pipeline will use default model if `model` is not specified. Go to the Model Hub (https://huggingface.co/models) for full list.\n",
        "\n",
        "Let’s try the `distilgpt2` model! Here’s how to load it in the same pipeline as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BqZ9s-_RsgRU",
        "outputId": "7aa2769b-2c29-433c-8546-6dd4c6ed08fd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'In this course, we will teach you how to practice mindfulness and become productive from the beginning!!!!!!!!!!!'},\n",
              " {'generated_text': 'In this course, we will teach you how to develop new functions and frameworks to achieve the benefits of the Python programming language with the Python 3 environment.'}]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "generator(\n",
        "    \"In this course, we will teach you how to\",\n",
        "    max_length=30,\n",
        "    num_return_sequences=2,\n",
        "    pad_token_id=0\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mask filling\n",
        "\n",
        "This is basically language modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "uyoP3_mxsgRV",
        "outputId": "b4b40b1f-3795-4c41-e39f-0b7953c1cb92"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'score': 0.1961980164051056,\n",
              "  'token': 30412,\n",
              "  'token_str': ' mathematical',\n",
              "  'sequence': 'This course will teach you all about mathematical models.'},\n",
              " {'score': 0.04052722826600075,\n",
              "  'token': 38163,\n",
              "  'token_str': ' computational',\n",
              "  'sequence': 'This course will teach you all about computational models.'}]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\", model='distilroberta-base')\n",
        "\n",
        "#The top_k argument controls how many possibilities you want to be displayed. \n",
        "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Name entity recognition\n",
        "\n",
        "This is very similar to NER in spaCy but has much more variety of models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Q0qx1KgFsgRW",
        "outputId": "8809dbb5-1379-4d36-fc5f-b77ba07e0f49"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER',\n",
              "  'score': 0.9981694,\n",
              "  'word': 'Sylvain',\n",
              "  'start': 11,\n",
              "  'end': 18},\n",
              " {'entity_group': 'ORG',\n",
              "  'score': 0.9796019,\n",
              "  'word': 'Hugging Face',\n",
              "  'start': 33,\n",
              "  'end': 45},\n",
              " {'entity_group': 'LOC',\n",
              "  'score': 0.9932106,\n",
              "  'word': 'Brooklyn',\n",
              "  'start': 49,\n",
              "  'end': 57}]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# aggregation_strategy=\"simple\" tell the pipeline to regroup together the parts of the sentence that correspond to the same entity: \n",
        "# here the model correctly grouped “Hugging” and “Face” as a single organization, even though the name consists of multiple words. \n",
        "ner = pipeline(\"ner\", aggregation_strategy=\"simple\", model='dbmdz/bert-large-cased-finetuned-conll03-english')\n",
        "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dCeroU72sgRW",
        "outputId": "43a19222-f6d7-4b4c-d004-53f9712bbb26"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'score': 0.6949770450592041, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "#Note that this pipeline is extractive\n",
        "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')\n",
        "question_answerer(\n",
        "    question=\"Where do I work?\",\n",
        "    context = \"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Dd13BzdesgRX",
        "outputId": "caf23e93-54e8-4d01-cf67-f861c9b2a73e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model='sshleifer/distilbart-cnn-12-6',  max_length=100)\n",
        "summarizer(\n",
        "    \"\"\"\n",
        "    America has changed dramatically during recent years. Not only has the number of \n",
        "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
        "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
        "    the premier American universities engineering curricula now concentrate on \n",
        "    and encourage largely the study of engineering science. As a result, there \n",
        "    are declining offerings in engineering subjects dealing with infrastructure, \n",
        "    the environment, and related issues, and greater concentration on high \n",
        "    technology subjects, largely supporting increasingly complex scientific \n",
        "    developments. While the latter is important, it should not be at the expense \n",
        "    of more traditional engineering.\n",
        "\n",
        "    Rapidly developing economies such as China and India, as well as other \n",
        "    industrial countries in Europe and Asia, continue to encourage and advance \n",
        "    the teaching of engineering. Both China and India, respectively, graduate \n",
        "    six and eight times as many traditional engineers as does the United States. \n",
        "    Other industrial countries at minimum maintain their output, while America \n",
        "    suffers an increasingly serious decline in the number of engineering graduates \n",
        "    and a lack of well-educated engineers.\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IxhNSdNtsgRX",
        "outputId": "fc96b383-581c-4659-8428-8f757aa703e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'translation_text': 'This course is produced by Hugging Face.'}]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
        "translator(\"Ce cours est produit par Hugging Face.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. A bit about transformers\n",
        "\n",
        "### History\n",
        "\n",
        "This is the short history of transformers:\n",
        "\n",
        "<img src = \"../../../figures/historybert.png\" width=600>\n",
        "\n",
        "- June 2018: `GPT`, the first pretrained Transformer model, used for fine-tuning on various NLP tasks and obtained state-of-the-art results\n",
        "- October 2018: `BERT`, another large pretrained model, this one designed to produce better summaries of sentences (more on this in the next chapter!)\n",
        "- February 2019: `GPT-2`, an improved (and bigger) version of GPT that was not immediately publicly released due to ethical concerns\n",
        "- October 2019: `DistilBERT`, a distilled version of BERT that is 60% faster, 40% lighter in memory, and still retains 97% of BERT’s performance\n",
        "- October 2019: `BART and T5`, two large pretrained models using the same architecture as the original Transformer model (the first to do so)\n",
        "- May 2020, `GPT-3`, an even bigger version of GPT-2 that is able to perform well on a variety of tasks without the need for fine-tuning (called zero-shot learning)\n",
        "\n",
        "This list is far from comprehensive, and is just meant to highlight a few of the different kinds of Transformer models. Broadly, they can be grouped into three categories:\n",
        "\n",
        "- BERT-like (also called **auto-encoding** or **encoder** Transformer models)\n",
        "- GPT-like (also called **auto-regressive** or **decoder** Transformer models)\n",
        "- BART/T5-like (also called **sequence-to-sequence** or **encoder-decoder** Transformer models)\n",
        "\n",
        "### Encoder\n",
        "\n",
        "Encoder models use only the encoder of a Transformer model. The pretraining of these models usually revolves around masked language modeling.\n",
        "\n",
        "Encoder models are best suited for tasks requiring an understanding of the full sentence, such as **sentence classification**, **named entity recognition (and more generally word classification)**, and **extractive question answering**.\n",
        "\n",
        "Representatives of this family of models include: ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa\n",
        "\n",
        "### Decoder\n",
        "\n",
        "Decoder models use only the decoder of a Transformer model. The pretraining of decoder models usually revolves around predicting the next word in the sentence.\n",
        "\n",
        "These models are best suited for tasks involving **text generation**.\n",
        "\n",
        "Representatives of this family of models include: CTRL, GPT, GPT-2, Transformer XL\n",
        "\n",
        "### Encoder-decoder\n",
        "\n",
        "Encoder-decoder models (also called sequence-to-sequence models) use both parts of the Transformer architecture.\n",
        "\n",
        "The pretraining of these models can be done using the objectives of encoder or decoder models, but usually involves something a bit more complex. For instance, T5 is pretrained by replacing random spans of text (that can contain several words) with a single mask special word, and the objective is then to predict the text that this mask word replaces.\n",
        "\n",
        "Sequence-to-sequence models are best suited for tasks revolving around **generating new sentences depending on a given input**, such as **summarization**, **translation**, or **generative question answering**.\n",
        "\n",
        "Representatives of this family of models include: BART, mBART, Marian, T5\n",
        "  \n",
        "### Transformers are language models\n",
        "\n",
        "All the Transformer models mentioned above (GPT, BERT, BART, T5, etc.) have been trained as language models. This means they have been trained on large amounts of raw text in a **self-supervised fashion**.\n",
        "\n",
        "This type of model develops a statistical understanding of the language it has been trained on (also called **pretraining**), but it’s not very useful for specific practical tasks. Because of this, the general pretrained model then goes through a process called **transfer learning** or **finetuning**. During this process, the model is fine-tuned in a supervised way — that is, using human-annotated labels — on a given task.\n",
        "\n",
        "Note that finetuning is almost always better than training from scratch.  Since the pretrained model already contains many basic statistical understanding of langauge, thus finetuning requires less resource and time to train."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Bias and Limitations\n",
        "\n",
        "Since pretrained model is trained on real-world datasets, it can unintentionally grab human biases.  For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[' translator', ' consultant', ' bartender', ' waiter', ' courier']\n",
            "[' waitress', ' translator', ' nurse', ' bartender', ' consultant']\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\", model='distilroberta-base')\n",
        "result = unmasker(\"This man works as a <mask>.\")\n",
        "print([r[\"token_str\"] for r in result])\n",
        "\n",
        "result = unmasker(\"This woman works as a <mask>.\")\n",
        "print([r[\"token_str\"] for r in result])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Behind the pipeline\n",
        "\n",
        "Let’s take a look at what happened behind the pipeline, so we can make our own what is more useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5R0zD_4-tNfm",
        "outputId": "27f21f36-8a4d-4336-b6ff-3d4278f6a76a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
              " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "classifier(\n",
        "    [\n",
        "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "        \"I hate this so much!\",\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This pipeline groups together three steps: preprocessing, passing the inputs through the model, and postprocessing:\n",
        "\n",
        "<img src = \"../../../figures/pipeline.png\" width = 700>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenizer\n",
        "\n",
        "- Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens\n",
        "- Mapping each token to an integer\n",
        "- Adding additional inputs that may be useful to the model\n",
        "\n",
        "To do this, we use the `AutoTokenizer` class and its `from_pretrained()` method. Using the checkpoint name of our model, it will automatically fetch the data associated with the model’s tokenizer and cache it (so it’s only downloaded the first time you run the code below).\n",
        "\n",
        "Since the default checkpoint of the sentiment-analysis pipeline is `distilbert-base-uncased-finetuned-sst-2-english`, we run the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "--cjwaFWtNfo"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we have the tokenizer, we can directly pass our sentences to it and we’ll get back a dictionary that’s ready to feed to our model! The only thing left to do is to convert the list of input IDs to tensors.\n",
        "\n",
        "To specify the type of tensors we want to get back (PyTorch, TensorFlow, or plain NumPy), we use the `return_tensors` argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fPfOncFJtNfo",
        "outputId": "54f2f5ed-9043-44c2-e99e-90cfb163082d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
            "          2607,  2026,  2878,  2166,  1012,   102],\n",
            "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
          ]
        }
      ],
      "source": [
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\") #pt for pytorch\n",
        "print(inputs)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model\n",
        "\n",
        "We can download our pretrained model the same way we did with our tokenizer. Transformers provides an `AutoModel` class which also has a `from_pretrained()` method.\n",
        "\n",
        "This architecture contains only the base Transformer module: given some inputs, it outputs what we’ll call hidden states, also known as features. For each model input, we’ll retrieve a high-dimensional vector representing the contextual understanding of that input by the Transformer model.\n",
        "\n",
        "While these hidden states can be useful on their own, they’re usually inputs to another part of the model, known as the head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8KjFaBNvtNfp"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c0de204dcb84ef9bcc8915b3d3c00dd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d13cca4533a495f9b92ec8c738ec854",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/255M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModel.from_pretrained(checkpoint)\n",
        "\n",
        "#it is complaining because it expects you to finetune..."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The vector output by the Transformer module is usually large. It generally has three dimensions:\n",
        "\n",
        "- `Batch size`: The number of sequences processed at a time (2 in our example).\n",
        "- `Sequence length`: The length of the numerical representation of the sequence (16 in our example).\n",
        "- `Hidden size`: The vector dimension of each model input.\n",
        "\n",
        "We can see this if we feed the inputs we preprocessed to our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AatezUMgtNfp",
        "outputId": "9c7295a4-70e6-4de4-b46b-5d4aaf88b008"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 16, 768])\n"
          ]
        }
      ],
      "source": [
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state.shape)\n",
        "\n",
        "#Note that the outputs of Transformers models behave like namedtuples or dictionaries. \n",
        "# You can access the elements by attributes (like we did) or by key (outputs[\"last_hidden_state\"]), \n",
        "# or even by index if you know exactly where the thing you are looking for is (outputs[0])."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model heads: Making sense out of numbers\n",
        "\n",
        "The model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension. They are usually composed of one or a few linear layers:\n",
        "\n",
        "<img src = \"../../../figures/heads.png\" width = 700>\n",
        "\n",
        "The output of the Transformer model is sent directly to the model head to be processed.\n",
        "\n",
        "In this diagram, the model is represented by its embeddings layer and the subsequent layers. The embeddings layer converts each input ID in the tokenized input into a vector that represents the associated token. The subsequent layers manipulate those vectors using the attention mechanism to produce the final representation of the sentences.\n",
        "\n",
        "There are many different architectures available, with each one designed around tackling a specific task. Here is a non-exhaustive list:\n",
        "\n",
        "- `Model` (retrieve the hidden states)\n",
        "- `ForCausalLM`\n",
        "- `ForMaskedLM`\n",
        "- `ForMultipleChoice`\n",
        "- `ForQuestionAnswering`\n",
        "- `ForSequenceClassification`\n",
        "- `ForTokenClassification`\n",
        "\n",
        "For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So, we won’t actually use the `AutoModel` class, but `AutoModelForSequenceClassification`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5vA9VxbwtNfq"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now if we look at the shape of our outputs, the dimensionality will be much lower: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeHzYIp6tNfq",
        "outputId": "2a42ccd0-1f47-4ae6-878a-07ea1d7a0eab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 2])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(outputs.logits.shape)  #batch, one-hot encoded targets or two sentences, two labels"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Postprocessing the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hl3XVqHmtNfq",
        "outputId": "f79739be-784b-43b9-f9d6-9a6a385e6a80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.5607,  1.6123],\n",
              "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(outputs.logits)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our model predicted [-1.5607, 1.6123] for the first sentence and [ 4.1692, -3.3464] for the second one. Those are not probabilities but logits, the raw, unnormalized scores outputted by the last layer of the model. To be converted to probabilities, they need to go through a `SoftMax` layer (all Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wLNxzCUqtNfr",
        "outputId": "f87cb897-b789-45dc-f73c-402fec08ca88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[4.0195e-02, 9.5980e-01],\n",
            "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can see that the model predicted [0.0402, 0.9598] for the first sentence and [0.9995, 0.0005] for the second one. These are recognizable probability scores.\n",
        "\n",
        "To get the labels corresponding to each position, we can inspect the `id2label` attribute of the model config."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "di8iIhxWtNfr",
        "outputId": "afefdad4-4ec7-40e8-a760-60d852e37e5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'NEGATIVE', 1: 'POSITIVE'}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.config.id2label"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can conclude that the model predicted the following:\n",
        "\n",
        "First sentence: NEGATIVE: 0.0402, POSITIVE: 0.9598\n",
        "Second sentence: NEGATIVE: 0.9995, POSITIVE: 0.0005"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Transformers, what can they do?",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dsai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9 (default, Oct 26 2021, 07:25:54) \n[Clang 13.0.0 (clang-1300.0.29.30)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "0f2c79af21be9d001248940c049b6176cf8bfb45cabf7aa85848f5cea0f590f6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
