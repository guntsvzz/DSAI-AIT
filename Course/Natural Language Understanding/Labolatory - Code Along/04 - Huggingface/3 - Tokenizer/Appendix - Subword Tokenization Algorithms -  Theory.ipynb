{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyt5g3zwusxe",
    "tags": []
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## Subword Tokenization Algorithms\n",
    "\n",
    "Before we dive more deeply into the three most common subword tokenization algorithms used with Transformer models (Byte-Pair Encoding [BPE], WordPiece, and Unigram), we‚Äôll first take a look at the preprocessing that each tokenizer applies to text. Here‚Äôs a high-level overview of the steps in the tokenization pipeline:\n",
    "\n",
    "<img src = \"../../../figures/pretoken.png\" >\n",
    "\n",
    "Before splitting a text into subtokens (according to its model), the tokenizer performs two steps: normalization and pre-tokenization.\n",
    "\n",
    "The normalization step involves some general cleanup, such as removing needless whitespace, lowercasing, and/or removing accents. If you‚Äôre familiar with Unicode normalization (such as NFC or NFKC), this is also something the tokenizer may apply.\n",
    "\n",
    "The ü§ó Transformers tokenizer has an attribute called `backend_tokenize`r that provides access to the underlying tokenizer from the ü§ó Tokenizers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "q7OyFsPousxk",
    "outputId": "4abdf57c-5979-4f9e-f71e-5a0f9b94c44b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tokenizers.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(tokenizer.backend_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normalizer attribute of the tokenizer object has a `normalize_str()` method that we can use to see how the normalization is performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "uVkkq-lTusxm",
    "outputId": "8a9497fd-177c-49c9-cd1b-1e065dbf0c2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello how are u?\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.backend_tokenizer.normalizer.normalize_str(\"H√©ll√≤ h√¥w are √º?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, since we picked the `bert-base-uncased` checkpoint, the normalization applied lowercasing and removed the accents.\n",
    "\n",
    "As we will see in the next sections, a tokenizer cannot be trained on raw text alone. Instead, we first need to split the texts into small entities, like words. That‚Äôs where the pre-tokenization step comes in. A word-based tokenizer can simply split a raw text into words on whitespace and punctuation. Those words will be the boundaries of the subtokens the tokenizer can learn during its training.\n",
    "\n",
    "To see how a fast tokenizer performs pre-tokenization, we can use the `pre_tokenize_str()` method of the pre_tokenizer attribute of the tokenizer object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uDFhEPppusxn",
    "outputId": "c5522d0b-4478-4178-84f6-9f6ddea751a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('how', (7, 10)),\n",
       " ('are', (11, 14)),\n",
       " ('you', (16, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the tokenizer is already keeping track of the offsets, which is how it can give us the offset mapping we used in the previous section. Here the tokenizer ignores the two spaces and replaces them with just one, but the offset jumps between are and you to account for that.\n",
    "\n",
    "Since we‚Äôre using a BERT tokenizer, the pre-tokenization involves splitting on whitespace and punctuation. Other tokenizers can have different rules for this step. For example, if we use the GPT-2 tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "aUgvHxQlusxn",
    "outputId": "aa177ff9-af38-4835-c0b3-2bed232c3985"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', (0, 5)),\n",
       " (',', (5, 6)),\n",
       " ('ƒ†how', (6, 10)),\n",
       " ('ƒ†are', (10, 14)),\n",
       " ('ƒ†', (14, 15)),\n",
       " ('ƒ†you', (15, 19)),\n",
       " ('?', (19, 20))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#it will split on whitespace and punctuation as well, \n",
    "#but it will keep the spaces and replace them with a ƒ† symbol, \n",
    "#enabling it to recover the original spaces if we decode the \n",
    "#tokens:\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also note that unlike the BERT tokenizer, this tokenizer does not ignore the double space.\n",
    "\n",
    "For a last example, let‚Äôs have a look at the T5 tokenizer, which is based on the **SentencePiece** algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yhc2m7qMusxo",
    "outputId": "0c8db1ad-bd0f-4ef2-d778-89de289f1bfc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaklams/.local/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('‚ñÅHello,', (0, 6)),\n",
       " ('‚ñÅhow', (7, 10)),\n",
       " ('‚ñÅare', (11, 14)),\n",
       " ('‚ñÅyou?', (16, 20))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, how are  you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the GPT-2 tokenizer, this one keeps spaces and replaces them with a specific token (_), but the T5 tokenizer only splits on whitespace, not punctuation. Also note that it added a space by default at the beginning of the sentence (before Hello) and ignored the double space between are and you.\n",
    "\n",
    "Now that we‚Äôve seen a little of how some different tokenizers process text, we can start to explore the underlying algorithms themselves. We‚Äôll begin with a quick look at the broadly widely applicable **SentencePiece**; then, over the next three sections, we‚Äôll examine how the three main algorithms used for subword tokenization work.\n",
    "\n",
    "## SentencePiece\n",
    "\n",
    "SentencePiece is a tokenization algorithm for the preprocessing of text that you can use with any of the models we will see in the next three sections. It considers the text as a sequence of Unicode characters, and replaces spaces with a special character, ‚ñÅ. Used in conjunction with the Unigram algorithm, it doesn‚Äôt even require a pre-tokenization step, which is very useful for languages where the space character is not used (like Chinese or Japanese).\n",
    "\n",
    "The other main feature of SentencePiece is reversible tokenization: since there is no special treatment of spaces, decoding the tokens is done simply by concatenating them and replacing the _s with spaces ‚Äî this results in the normalized text. As we saw earlier, the BERT tokenizer removes repeating spaces, so its tokenization is not reversible.\n",
    "\n",
    "## Algorithm overview\n",
    "\n",
    "In the following sections, we‚Äôll dive into the three main subword tokenization algorithms: BPE (used by GPT-2 and others), WordPiece (used for example by BERT), and Unigram (used by T5 and others). Before we get started, here‚Äôs a quick overview of how they each work.\n",
    "\n",
    "|Model |BPE |WordPiece |Unigram|\n",
    "|---|---|---|---|\n",
    "|Training|\tStarts from a small vocabulary and learns rules to merge tokens| Starts from a small vocabulary and learns rules to merge tokens | Starts from a large vocabulary and learns rules to remove tokens|\n",
    "|Training step | Merges the tokens corresponding to the most common pair | Merges the tokens corresponding to the pair with the best score based on the frequency of the pair, privileging pairs where each individual token is less frequent | Removes all the tokens in the vocabulary that will minimize the loss computed on the whole corpus |\n",
    "| Learns | Merge rules and a vocabulary | Just a vocabulary | A vocabulary with a score for each token |\n",
    "| Encoding | Splits a word into characters and applies the merges learned during training | Finds the longest subword starting from the beginning that is in the vocabulary, then does the same for the rest of the word | Finds the most likely split into tokens, using the scores learned during training\n",
    "\n",
    "Now let‚Äôs dive into BPE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "od2FFM7ZuvW7"
   },
   "source": [
    "## 1. Byte-Pair Encoding tokenization\n",
    "\n",
    "Byte-Pair Encoding (BPE) was initially developed as an algorithm to compress texts, and then used by OpenAI for tokenization when pretraining the GPT model. It‚Äôs used by a lot of Transformer models, including GPT, GPT-2, RoBERTa, BART, and DeBERTa.\n",
    "\n",
    "### 1.1 Training algorithm\n",
    "\n",
    "BPE training starts by computing the unique set of words used in the corpus (after the normalization and pre-tokenization steps are completed), then building the vocabulary by taking all the symbols used to write those words. As a very simple example, let‚Äôs say our corpus uses these five words:\n",
    "\n",
    "    \"hug\", \"pug\", \"pun\", \"bun\", \"hugs\"\n",
    "\n",
    "The base vocabulary will then be [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]. For real-world cases, that base vocabulary will contain all the ASCII characters, at the very least, and probably some Unicode characters as well. If an example you are tokenizing uses a character that is not in the training corpus, that character will be converted to the unknown token. That‚Äôs one reason why lots of NLP models are very bad at analyzing content with emojis, for instance.\n",
    "\n",
    "The GPT-2 and RoBERTa tokenizers (which are pretty similar) have a clever way to deal with this: they don‚Äôt look at words as being written with Unicode characters, but with bytes. This way the base vocabulary has a small size (256), but every character you can think of will still be included and not end up being converted to the unknown token. This trick is called byte-level BPE.\n",
    "\n",
    "After getting this base vocabulary, we add new tokens until the desired vocabulary size is reached by learning merges, which are rules to merge two elements of the existing vocabulary together into a new one. So, at the beginning these merges will create tokens with two characters, and then, as training progresses, longer subwords.\n",
    "\n",
    "At any step during the tokenizer training, the BPE algorithm will search for the most frequent pair of existing tokens (by ‚Äúpair,‚Äù here we mean two consecutive tokens in a word). That most frequent pair is the one that will be merged, and we rinse and repeat for the next step.\n",
    "\n",
    "Going back to our previous example, let‚Äôs assume the words had the following frequencies:\n",
    "\n",
    "    (\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "\n",
    "meaning \"hug\" was present 10 times in the corpus, \"pug\" 5 times, \"pun\" 12 times, \"bun\" 4 times, and \"hugs\" 5 times. We start the training by splitting each word into characters (the ones that form our initial vocabulary) so we can see each word as a list of tokens:\n",
    "\n",
    "    (\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5)\n",
    "\n",
    "Then we look at pairs. The pair (\"h\", \"u\") is present in the words \"hug\" and \"hugs\", so 15 times total in the corpus. It‚Äôs not the most frequent pair, though: that honor belongs to (\"u\", \"g\"), which is present in \"hug\", \"pug\", and \"hugs\", for a grand total of 20 times in the vocabulary.\n",
    "\n",
    "Thus, the first merge rule learned by the tokenizer is (\"u\", \"g\") -> \"ug\", which means that \"ug\" will be added to the vocabulary, and the pair should be merged in all the words of the corpus. At the end of this stage, the vocabulary and corpus look like this:\n",
    "\n",
    "    Vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\"]\n",
    "    Corpus: (\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5)\n",
    "    \n",
    "Now we have some pairs that result in a token longer than two characters: the pair (\"h\", \"ug\"), for instance (present 15 times in the corpus). The most frequent pair at this stage is (\"u\", \"n\"), however, present 16 times in the corpus, so the second merge rule learned is (\"u\", \"n\") -> \"un\". Adding that to the vocabulary and merging all existing occurrences leads us to:\n",
    "\n",
    "    Vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\"]\n",
    "    Corpus: (\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"h\" \"ug\" \"s\", 5)\n",
    "    \n",
    "Now the most frequent pair is (\"h\", \"ug\"), so we learn the merge rule (\"h\", \"ug\") -> \"hug\", which gives us our first three-letter token. After the merge, the corpus looks like this:\n",
    "\n",
    "    Vocabulary: [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]\n",
    "    Corpus: (\"hug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"hug\" \"s\", 5)\n",
    "    \n",
    "And we continue like this until we reach the desired vocabulary size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tokenization algorithm\n",
    "\n",
    "Tokenization follows the training process closely, in the sense that new inputs are tokenized by applying the following steps:\n",
    "\n",
    "1. Normalization\n",
    "2. Pre-tokenization\n",
    "3. Splitting the words into individual characters\n",
    "4. Applying the merge rules learned in order on those splits\n",
    "\n",
    "Let‚Äôs take the example we used during training, with the three merge rules learned:\n",
    "\n",
    "    (\"u\", \"g\") -> \"ug\"\n",
    "    (\"u\", \"n\") -> \"un\"\n",
    "    (\"h\", \"ug\") -> \"hug\"\n",
    "\n",
    "The word \"bug\" will be tokenized as [\"b\", \"ug\"]. \"mug\", however, will be tokenized as [\"[UNK]\", \"ug\"] since the letter \"m\" was not in the base vocabulary. Likewise, the word \"thug\" will be tokenized as [\"[UNK]\", \"hug\"]: the letter \"t\" is not in the base vocabulary, and applying the merge rules results first in \"u\" and \"g\" being merged and then \"hu\" and \"g\" being merged.\n",
    "\n",
    "### 1.3 Implementing BPE\n",
    "\n",
    "Now let‚Äôs take a look at an implementation of the BPE algorithm. This won‚Äôt be an optimized version you can actually use on a big corpus; we just want to show you the code so you can understand the algorithm a little bit better.\n",
    "\n",
    "First we need a corpus, so let‚Äôs create a simple one with a few sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Z4wzz96CuvXB"
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to pre-tokenize that corpus into words. Since we are replicating a BPE tokenizer (like GPT-2), we will use the gpt2 tokenizer for the pre-tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wu46oVrBuvXC"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compute the frequencies of each word in the corpus as we do the pre-tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xVMl7QI5uvXD",
    "outputId": "0562f6a3-b949-4bc3-ccf7-f29d3d8c4ef5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'This': 3, 'ƒ†is': 2, 'ƒ†the': 1, 'ƒ†Hugging': 1, 'ƒ†Face': 1, 'ƒ†Course': 1, '.': 4, 'ƒ†chapter': 1, 'ƒ†about': 1, 'ƒ†tokenization': 1, 'ƒ†section': 1, 'ƒ†shows': 1, 'ƒ†several': 1, 'ƒ†tokenizer': 1, 'ƒ†algorithms': 1, 'Hopefully': 1, ',': 1, 'ƒ†you': 1, 'ƒ†will': 1, 'ƒ†be': 1, 'ƒ†able': 1, 'ƒ†to': 1, 'ƒ†understand': 1, 'ƒ†how': 1, 'ƒ†they': 1, 'ƒ†are': 1, 'ƒ†trained': 1, 'ƒ†and': 1, 'ƒ†generate': 1, 'ƒ†tokens': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "print(word_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to compute the base vocabulary, formed by all the characters used in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "aR6HQ2zVuvXE",
    "outputId": "93b032e5-afc9-4f55-e75e-cf93f4be865d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'ƒ†']\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "\n",
    "for word in word_freqs.keys():\n",
    "    for letter in word:\n",
    "        if letter not in alphabet:\n",
    "            alphabet.append(letter)\n",
    "alphabet.sort()\n",
    "\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add the special tokens used by the model at the beginning of that vocabulary. In the case of GPT-2, the only special token is \"<|endoftext|>\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bhfCscrzuvXF"
   },
   "outputs": [],
   "source": [
    "vocab = [\"<|endoftext|>\"] + alphabet.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to split each word into individual characters, to be able to start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SW4n7ZvyuvXG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': ['T', 'h', 'i', 's'],\n",
       " 'ƒ†is': ['ƒ†', 'i', 's'],\n",
       " 'ƒ†the': ['ƒ†', 't', 'h', 'e'],\n",
       " 'ƒ†Hugging': ['ƒ†', 'H', 'u', 'g', 'g', 'i', 'n', 'g'],\n",
       " 'ƒ†Face': ['ƒ†', 'F', 'a', 'c', 'e'],\n",
       " 'ƒ†Course': ['ƒ†', 'C', 'o', 'u', 'r', 's', 'e'],\n",
       " '.': ['.'],\n",
       " 'ƒ†chapter': ['ƒ†', 'c', 'h', 'a', 'p', 't', 'e', 'r'],\n",
       " 'ƒ†about': ['ƒ†', 'a', 'b', 'o', 'u', 't'],\n",
       " 'ƒ†tokenization': ['ƒ†',\n",
       "  't',\n",
       "  'o',\n",
       "  'k',\n",
       "  'e',\n",
       "  'n',\n",
       "  'i',\n",
       "  'z',\n",
       "  'a',\n",
       "  't',\n",
       "  'i',\n",
       "  'o',\n",
       "  'n'],\n",
       " 'ƒ†section': ['ƒ†', 's', 'e', 'c', 't', 'i', 'o', 'n'],\n",
       " 'ƒ†shows': ['ƒ†', 's', 'h', 'o', 'w', 's'],\n",
       " 'ƒ†several': ['ƒ†', 's', 'e', 'v', 'e', 'r', 'a', 'l'],\n",
       " 'ƒ†tokenizer': ['ƒ†', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r'],\n",
       " 'ƒ†algorithms': ['ƒ†', 'a', 'l', 'g', 'o', 'r', 'i', 't', 'h', 'm', 's'],\n",
       " 'Hopefully': ['H', 'o', 'p', 'e', 'f', 'u', 'l', 'l', 'y'],\n",
       " ',': [','],\n",
       " 'ƒ†you': ['ƒ†', 'y', 'o', 'u'],\n",
       " 'ƒ†will': ['ƒ†', 'w', 'i', 'l', 'l'],\n",
       " 'ƒ†be': ['ƒ†', 'b', 'e'],\n",
       " 'ƒ†able': ['ƒ†', 'a', 'b', 'l', 'e'],\n",
       " 'ƒ†to': ['ƒ†', 't', 'o'],\n",
       " 'ƒ†understand': ['ƒ†', 'u', 'n', 'd', 'e', 'r', 's', 't', 'a', 'n', 'd'],\n",
       " 'ƒ†how': ['ƒ†', 'h', 'o', 'w'],\n",
       " 'ƒ†they': ['ƒ†', 't', 'h', 'e', 'y'],\n",
       " 'ƒ†are': ['ƒ†', 'a', 'r', 'e'],\n",
       " 'ƒ†trained': ['ƒ†', 't', 'r', 'a', 'i', 'n', 'e', 'd'],\n",
       " 'ƒ†and': ['ƒ†', 'a', 'n', 'd'],\n",
       " 'ƒ†generate': ['ƒ†', 'g', 'e', 'n', 'e', 'r', 'a', 't', 'e'],\n",
       " 'ƒ†tokens': ['ƒ†', 't', 'o', 'k', 'e', 'n', 's']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = {word: [c for c in word] for word in word_freqs.keys()}\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are ready for training, let‚Äôs write a function that computes the frequency of each pair. We‚Äôll need to use this at each step of the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XbRFxnLfuvXG"
   },
   "outputs": [],
   "source": [
    "def compute_pair_freqs(splits):\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            pair_freqs[pair] += freq\n",
    "    return pair_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs have a look at a part of this dictionary after the initial splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Q-kS5KRGuvXH",
    "outputId": "5a950c73-8094-41fe-aae1-e28912bdae9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T', 'h'): 3\n",
      "('h', 'i'): 3\n",
      "('i', 's'): 5\n",
      "('ƒ†', 'i'): 2\n",
      "('ƒ†', 't'): 7\n",
      "('t', 'h'): 3\n"
     ]
    }
   ],
   "source": [
    "pair_freqs = compute_pair_freqs(splits)\n",
    "\n",
    "for i, key in enumerate(pair_freqs.keys()):\n",
    "    print(f\"{key}: {pair_freqs[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finding the most frequent pair only takes a quick loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cUnzPfc_uvXI",
    "outputId": "de1f70fe-c3a6-4bb7-ab10-b2e64294542f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ƒ†', 't') 7\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_freq = None\n",
    "\n",
    "for pair, freq in pair_freqs.items():\n",
    "    if max_freq is None or max_freq < freq:\n",
    "        best_pair = pair\n",
    "        max_freq = freq\n",
    "\n",
    "print(best_pair, max_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first merge to learn is ('ƒ†', 't') -> 'ƒ†t', and we add 'ƒ†t' to the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "wn3xnnfpuvXI"
   },
   "outputs": [],
   "source": [
    "merges = {(\"ƒ†\", \"t\"): \"ƒ†t\"}\n",
    "vocab.append(\"ƒ†t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue, we need to apply that merge in our splits dictionary. Let‚Äôs write another function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ybFm0-mduvXJ"
   },
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                split = split[:i] + [a + b] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can have a look at the result of the first merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "mH3o2LL8uvXJ",
    "outputId": "f7a80fd0-e8b3-46a6-8811-4ff29aed6e06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ƒ†t', 'r', 'a', 'i', 'n', 'e', 'd']\n"
     ]
    }
   ],
   "source": [
    "splits = merge_pair(\"ƒ†\", \"t\", splits)\n",
    "print(splits[\"ƒ†trained\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need to loop until we have learned all the merges we want. Let‚Äôs aim for a vocab size of 50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "RKjXZTtvuvXK"
   },
   "outputs": [],
   "source": [
    "vocab_size = 50\n",
    "\n",
    "while len(vocab) < vocab_size:\n",
    "    pair_freqs = compute_pair_freqs(splits)\n",
    "    best_pair = \"\"\n",
    "    max_freq = None\n",
    "    for pair, freq in pair_freqs.items():\n",
    "        if max_freq is None or max_freq < freq:\n",
    "            best_pair = pair\n",
    "            max_freq = freq\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "    vocab.append(best_pair[0] + best_pair[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, we‚Äôve learned 19 merge rules (the initial vocabulary had a size of 31 ‚Äî 30 characters in the alphabet, plus the special token):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "WorsMadjuvXL",
    "outputId": "801a8bb7-33ea-49eb-aefe-79e936ac6128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('ƒ†', 't'): 'ƒ†t', ('i', 's'): 'is', ('e', 'r'): 'er', ('ƒ†', 'a'): 'ƒ†a', ('ƒ†t', 'o'): 'ƒ†to', ('e', 'n'): 'en', ('T', 'h'): 'Th', ('Th', 'is'): 'This', ('o', 'u'): 'ou', ('s', 'e'): 'se', ('ƒ†to', 'k'): 'ƒ†tok', ('ƒ†tok', 'en'): 'ƒ†token', ('n', 'd'): 'nd', ('ƒ†', 'is'): 'ƒ†is', ('ƒ†t', 'h'): 'ƒ†th', ('ƒ†th', 'e'): 'ƒ†the', ('i', 'n'): 'in', ('ƒ†a', 'b'): 'ƒ†ab', ('ƒ†token', 'i'): 'ƒ†tokeni'}\n"
     ]
    }
   ],
   "source": [
    "print(merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the vocabulary is composed of the special token, the initial alphabet, and all the results of the merges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "8aUMVOwRuvXM",
    "outputId": "8c6ea85a-391a-46ff-9bbb-e68ab0f46ff7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y', 'z', 'ƒ†', 'ƒ†t', 'is', 'er', 'ƒ†a', 'ƒ†to', 'en', 'Th', 'This', 'ou', 'se', 'ƒ†tok', 'ƒ†token', 'nd', 'ƒ†is', 'ƒ†th', 'ƒ†the', 'in', 'ƒ†ab', 'ƒ†tokeni']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tokenize a new text, we pre-tokenize it, split it, then apply all the merge rules learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "bO76anQEuvXM"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    splits = [[l for l in word] for word in pre_tokenized_text]\n",
    "    for pair, merge in merges.items():\n",
    "        for idx, split in enumerate(splits):\n",
    "            i = 0\n",
    "            while i < len(split) - 1:\n",
    "                if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
    "                    split = split[:i] + [merge] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            splits[idx] = split\n",
    "\n",
    "    return sum(splits, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try this on any text composed of characters in the alphabet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "51NT6ck-uvXN",
    "outputId": "a0b93624-e488-47b7-d83c-1d631e24aa5d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'ƒ†is', 'ƒ†', 'n', 'o', 't', 'ƒ†a', 'ƒ†token', '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"This is not a token.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c_vgltBux9T"
   },
   "source": [
    "## 2. WordPiece tokenization\n",
    "\n",
    "WordPiece is the tokenization algorithm Google developed to pretrain BERT. It has since been reused in quite a few Transformer models based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and MPNET. It‚Äôs very similar to BPE in terms of the training, but the actual tokenization is done differently.\n",
    "\n",
    "### 2.1 Training algorithm\n",
    "\n",
    "Note: Google never open-sourced its implementation of the training algorithm of WordPiece, so what follows is our best guess based on the published literature. It may not be 100% accurate.\n",
    "\n",
    "Like BPE, WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like ## for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, \"word\" gets split like this:\n",
    "\n",
    "    w ##o ##r ##d\n",
    "\n",
    "Thus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix.\n",
    "\n",
    "Then, again like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula:\n",
    "\n",
    "    score=(freq_of_pair)/(freq_of_first_element√ófreq_of_second_element)\n",
    "\n",
    "By dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary. For instance, it won‚Äôt necessarily merge (\"un\", \"##able\") even if that pair occurs very frequently in the vocabulary, because the two pairs \"un\" and \"##able\" will likely each appear in a lot of other words and have a high frequency. In contrast, a pair like (\"hu\", \"##gging\") will probably be merged faster (assuming the word ‚Äúhugging‚Äù appears often in the vocabulary) since \"hu\" and \"##gging\" are likely to be less frequent individually.\n",
    "\n",
    "Let‚Äôs look at the same vocabulary we used in the BPE training example:\n",
    "\n",
    "    (\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "\n",
    "The splits here will be:\n",
    "\n",
    "    (\"h\" \"##u\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"h\" \"##u\" \"##g\" \"##s\", 5)\n",
    "    \n",
    "so the initial vocabulary will be [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\"] (if we forget about special tokens for now). The most frequent pair is (\"##u\", \"##g\") (present 20 times), but the individual frequency of \"##u\" is very high, so its score is not the highest (it‚Äôs 1 / 36). All pairs with a \"##u\" actually have that same score (1 / 36), so the best score goes to the pair (\"##g\", \"##s\") ‚Äî the only one without a \"##u\" ‚Äî at 1 / 20, and the first merge learned is (\"##g\", \"##s\") -> (\"##gs\").\n",
    "\n",
    "Note that when we merge, we remove the ## between the two tokens, so we add \"##gs\" to the vocabulary and apply the merge in the words of the corpus:\n",
    "\n",
    "    Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\"]\n",
    "    Corpus: (\"h\" \"##u\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"h\" \"##u\" \"##gs\", 5)\n",
    "\n",
    "At this point, \"##u\" is in all the possible pairs, so they all end up with the same score. Let‚Äôs say that in this case, the first pair is merged, so (\"h\", \"##u\") -> \"hu\". This takes us to:\n",
    "\n",
    "    Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\", \"hu\"]\n",
    "    Corpus: (\"hu\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"hu\" \"##gs\", 5)\n",
    "\n",
    "Then the next best score is shared by (\"hu\", \"##g\") and (\"hu\", \"##gs\") (with 1/15, compared to 1/21 for all the other pairs), so the first pair with the biggest score is merged:\n",
    "\n",
    "    Vocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\", \"hu\", \"hug\"]\n",
    "    Corpus: (\"hug\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"hu\" \"##gs\", 5)\n",
    "\n",
    "and we continue like this until we reach the desired vocabulary size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenization algorithm\n",
    "\n",
    "Tokenization differs in WordPiece and BPE in that WordPiece only saves the final vocabulary, not the merge rules learned. Starting from the word to tokenize, WordPiece finds the longest subword that is in the vocabulary, then splits on it. For instance, if we use the vocabulary learned in the example above, for the word \"hugs\" the longest subword starting from the beginning that is inside the vocabulary is \"hug\", so we split there and get [\"hug\", \"##s\"]. We then continue with \"##s\", which is in the vocabulary, so the tokenization of \"hugs\" is [\"hug\", \"##s\"].\n",
    "\n",
    "With BPE, we would have applied the merges learned in order and tokenized this as [\"hu\", \"##gs\"], so the encoding is different.\n",
    "\n",
    "As another example, let‚Äôs see how the word \"bugs\" would be tokenized. \"b\" is the longest subword starting at the beginning of the word that is in the vocabulary, so we split there and get [\"b\", \"##ugs\"]. Then \"##u\" is the longest subword starting at the beginning of \"##ugs\" that is in the vocabulary, so we split there and get [\"b\", \"##u, \"##gs\"]. Finally, \"##gs\" is in the vocabulary, so this last list is the tokenization of \"bugs\".\n",
    "\n",
    "When the tokenization gets to a stage where it‚Äôs not possible to find a subword in the vocabulary, the whole word is tokenized as unknown ‚Äî so, for instance, \"mug\" would be tokenized as [\"[UNK]\"], as would \"bum\" (even if we can begin with \"b\" and \"##u\", \"##m\" is not the vocabulary, and the resulting tokenization will just be [\"[UNK]\"], not [\"b\", \"##u\", \"[UNK]\"]). This is another difference from BPE, which would only classify the individual characters not in the vocabulary as unknown.\n",
    "\n",
    "### 2.3 Implementing WordPiece\n",
    "\n",
    "Now let‚Äôs take a look at an implementation of the WordPiece algorithm. Like with BPE, this is just pedagogical, and you won‚Äôt able to use this on a big corpus.\n",
    "\n",
    "We will use the same corpus as in the BPE example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "W2LEzoQPux9Y"
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to pre-tokenize the corpus into words. Since we are replicating a WordPiece tokenizer (like BERT), we will use the bert-base-cased tokenizer for the pre-tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "bufU8hBdux9Z"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compute the frequencies of each word in the corpus as we do the pre-tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "xL9GKSH7ux9Z",
    "outputId": "1ba338eb-0b3b-42a6-efa5-14fe70b14bef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'This': 3,\n",
       "             'is': 2,\n",
       "             'the': 1,\n",
       "             'Hugging': 1,\n",
       "             'Face': 1,\n",
       "             'Course': 1,\n",
       "             '.': 4,\n",
       "             'chapter': 1,\n",
       "             'about': 1,\n",
       "             'tokenization': 1,\n",
       "             'section': 1,\n",
       "             'shows': 1,\n",
       "             'several': 1,\n",
       "             'tokenizer': 1,\n",
       "             'algorithms': 1,\n",
       "             'Hopefully': 1,\n",
       "             ',': 1,\n",
       "             'you': 1,\n",
       "             'will': 1,\n",
       "             'be': 1,\n",
       "             'able': 1,\n",
       "             'to': 1,\n",
       "             'understand': 1,\n",
       "             'how': 1,\n",
       "             'they': 1,\n",
       "             'are': 1,\n",
       "             'trained': 1,\n",
       "             'and': 1,\n",
       "             'generate': 1,\n",
       "             'tokens': 1})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "word_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw before, the alphabet is the unique set composed of all the first letters of words, and all the other letters that appear in words prefixed by ##:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "bnKMLlUNux9b",
    "outputId": "b1967843-7c05-44ef-f533-b89faaf0b1f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y']\n"
     ]
    }
   ],
   "source": [
    "alphabet = []\n",
    "for word in word_freqs.keys():\n",
    "    if word[0] not in alphabet:\n",
    "        alphabet.append(word[0])\n",
    "    for letter in word[1:]:\n",
    "        if f\"##{letter}\" not in alphabet:\n",
    "            alphabet.append(f\"##{letter}\")\n",
    "\n",
    "alphabet.sort()\n",
    "alphabet\n",
    "\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add the special tokens used by the model at the beginning of that vocabulary. In the case of BERT, it‚Äôs the list [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "3CSCxLP8ux9c"
   },
   "outputs": [],
   "source": [
    "vocab = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + alphabet.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to split each word, with all the letters that are not the first prefixed by ##:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "bWhfRtwnux9d"
   },
   "outputs": [],
   "source": [
    "splits = {\n",
    "    word: [c if i == 0 else f\"##{c}\" for i, c in enumerate(word)]\n",
    "    for word in word_freqs.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are ready for training, let‚Äôs write a function that computes the score of each pair. We‚Äôll need to use this at each step of the training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "kPjCXqudux9d"
   },
   "outputs": [],
   "source": [
    "def compute_pair_scores(splits):\n",
    "    letter_freqs = defaultdict(int)\n",
    "    pair_freqs = defaultdict(int)\n",
    "    for word, freq in word_freqs.items():\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            letter_freqs[split[0]] += freq\n",
    "            continue\n",
    "        for i in range(len(split) - 1):\n",
    "            pair = (split[i], split[i + 1])\n",
    "            letter_freqs[split[i]] += freq\n",
    "            pair_freqs[pair] += freq\n",
    "        letter_freqs[split[-1]] += freq\n",
    "\n",
    "    scores = {\n",
    "        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])\n",
    "        for pair, freq in pair_freqs.items()\n",
    "    }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs have a look at a part of this dictionary after the initial splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "fSD-Mg6jux9e",
    "outputId": "b4b7734b-3b20-4e52-cff2-0d35797d73a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('T', '##h'): 0.125\n",
      "('##h', '##i'): 0.03409090909090909\n",
      "('##i', '##s'): 0.02727272727272727\n",
      "('i', '##s'): 0.1\n",
      "('t', '##h'): 0.03571428571428571\n",
      "('##h', '##e'): 0.011904761904761904\n"
     ]
    }
   ],
   "source": [
    "pair_scores = compute_pair_scores(splits)\n",
    "for i, key in enumerate(pair_scores.keys()):\n",
    "    print(f\"{key}: {pair_scores[key]}\")\n",
    "    if i >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finding the pair with the best score only takes a quick loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "-viNObgTux9e",
    "outputId": "fae35784-f6cb-4983-bf49-7c95faf7687d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', '##b') 0.2\n"
     ]
    }
   ],
   "source": [
    "best_pair = \"\"\n",
    "max_score = None\n",
    "for pair, score in pair_scores.items():\n",
    "    if max_score is None or max_score < score:\n",
    "        best_pair = pair\n",
    "        max_score = score\n",
    "\n",
    "print(best_pair, max_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first merge to learn is ('a', '##b') -> 'ab', and we add 'ab' to the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "HreuSK6Lux9f"
   },
   "outputs": [],
   "source": [
    "vocab.append(\"ab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue, we need to apply that merge in our splits dictionary. Let‚Äôs write another function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "m6KWP3l8ux9f"
   },
   "outputs": [],
   "source": [
    "def merge_pair(a, b, splits):\n",
    "    for word in word_freqs:\n",
    "        split = splits[word]\n",
    "        if len(split) == 1:\n",
    "            continue\n",
    "        i = 0\n",
    "        while i < len(split) - 1:\n",
    "            if split[i] == a and split[i + 1] == b:\n",
    "                merge = a + b[2:] if b.startswith(\"##\") else a + b\n",
    "                split = split[:i] + [merge] + split[i + 2 :]\n",
    "            else:\n",
    "                i += 1\n",
    "        splits[word] = split\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can have a look at the result of the first merge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "AMTQIE3Qux9g",
    "outputId": "b2173342-e62d-4867-ec92-d8ea572bb05a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ab', '##o', '##u', '##t']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = merge_pair(\"a\", \"##b\", splits)\n",
    "splits[\"about\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything we need to loop until we have learned all the merges we want. Let‚Äôs aim for a vocab size of 70:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "nk0bHWiGux9g"
   },
   "outputs": [],
   "source": [
    "vocab_size = 70\n",
    "while len(vocab) < vocab_size:\n",
    "    scores = compute_pair_scores(splits)\n",
    "    best_pair, max_score = \"\", None\n",
    "    for pair, score in scores.items():\n",
    "        if max_score is None or max_score < score:\n",
    "            best_pair = pair\n",
    "            max_score = score\n",
    "    splits = merge_pair(*best_pair, splits)\n",
    "    new_token = (\n",
    "        best_pair[0] + best_pair[1][2:]\n",
    "        if best_pair[1].startswith(\"##\")\n",
    "        else best_pair[0] + best_pair[1]\n",
    "    )\n",
    "    vocab.append(new_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then look at the generated vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "lrVImVaWux9h",
    "outputId": "2dcde952-d726-444a-f00f-472f89ce395c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##t', '##u', '##v', '##w', '##y', '##z', ',', '.', 'C', 'F', 'H', 'T', 'a', 'b', 'c', 'g', 'h', 'i', 's', 't', 'u', 'w', 'y', 'ab', '##fu', 'Fa', 'Fac', '##ct', '##ful', '##full', '##fully', 'Th', 'ch', '##hm', 'cha', 'chap', 'chapt', '##thm', 'Hu', 'Hug', 'Hugg', 'sh', 'th', 'is', '##thms', '##za', '##zat', '##ut']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, compared to BPE, this tokenizer learns parts of words as tokens a bit faster.\n",
    "\n",
    "Note: Using train_new_from_iterator() on the same corpus won‚Äôt result in the exact same vocabulary. This is because the ü§ó Tokenizers library does not implement WordPiece for the training (since we are not completely sure of its internals), but uses BPE instead.\n",
    "\n",
    "To tokenize a new text, we pre-tokenize it, split it, then apply the tokenization algorithm on each word. That is, we look for the biggest subword starting at the beginning of the first word and split it, then we repeat the process on the second part, and so on for the rest of that word and the following words in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "W3BxOcHHux9h"
   },
   "outputs": [],
   "source": [
    "def encode_word(word):\n",
    "    tokens = []\n",
    "    while len(word) > 0:\n",
    "        i = len(word)\n",
    "        while i > 0 and word[:i] not in vocab:\n",
    "            i -= 1\n",
    "        if i == 0:\n",
    "            return [\"[UNK]\"]\n",
    "        tokens.append(word[:i])\n",
    "        word = word[i:]\n",
    "        if len(word) > 0:\n",
    "            word = f\"##{word}\"\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs test it on one word that‚Äôs in the vocabulary, and another that isn‚Äôt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Q91uMDniux9i",
    "outputId": "1042a7da-2bad-43cf-fec4-48ddaf6580aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hugg', '##i', '##n', '##g']\n",
      "['[UNK]']\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"Hugging\"))\n",
    "print(encode_word(\"HOgging\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let‚Äôs write a function that tokenizes a text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "MICur761ux9i"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in pre_tokenize_result]\n",
    "    encoded_words = [encode_word(word) for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try it on any text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "VV4k1Lqgux9i",
    "outputId": "02ca05f6-d0f2-4fe6-9b48-08984b699ef9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Th',\n",
       " '##i',\n",
       " '##s',\n",
       " 'is',\n",
       " 'th',\n",
       " '##e',\n",
       " 'Hugg',\n",
       " '##i',\n",
       " '##n',\n",
       " '##g',\n",
       " 'Fac',\n",
       " '##e',\n",
       " 'c',\n",
       " '##o',\n",
       " '##u',\n",
       " '##r',\n",
       " '##s',\n",
       " '##e',\n",
       " '[UNK]']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"This is the Hugging Face course!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1btSte-bu0ej"
   },
   "source": [
    "## 3. Unigram tokenization\n",
    "\n",
    "The Unigram algorithm is often used in SentencePiece, which is the tokenization algorithm used by models like AlBERT, T5, mBART, Big Bird, and XLNet.\n",
    "\n",
    "### 3.1 Training algorithm\n",
    "\n",
    "Compared to BPE and WordPiece, Unigram works in the other direction: it starts from a big vocabulary and removes tokens from it until it reaches the desired vocabulary size. There are several options to use to build that base vocabulary: we can take the most common substrings in pre-tokenized words, for instance, or apply BPE on the initial corpus with a large vocabulary size.\n",
    "\n",
    "At each step of the training, the Unigram algorithm computes a loss over the corpus given the current vocabulary. Then, for each symbol in the vocabulary, the algorithm computes how much the overall loss would increase if the symbol was removed, and looks for the symbols that would increase it the least. Those symbols have a lower effect on the overall loss over the corpus, so in a sense they are ‚Äúless needed‚Äù and are the best candidates for removal.\n",
    "\n",
    "This is all a very costly operation, so we don‚Äôt just remove the single symbol associated with the lowest loss increase, but the \n",
    "p (p being a hyperparameter you can control, usually 10 or 20) percent of the symbols associated with the lowest loss increase. This process is then repeated until the vocabulary has reached the desired size.\n",
    "\n",
    "Note that we never remove the base characters, to make sure any word can be tokenized.\n",
    "\n",
    "Now, this is still a bit vague: the main part of the algorithm is to compute a loss over the corpus and see how it changes when we remove some tokens from the vocabulary, but we haven‚Äôt explained how to do this yet. This step relies on the tokenization algorithm of a Unigram model, so we‚Äôll dive into this next.\n",
    "\n",
    "We‚Äôll reuse the corpus from the previous examples:\n",
    "\n",
    "    (\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "\n",
    "and for this example, we will take all strict substrings for the initial vocabulary :\n",
    "\n",
    "    [\"h\", \"u\", \"g\", \"hu\", \"ug\", \"p\", \"pu\", \"n\", \"un\", \"b\", \"bu\", \"s\", \"hug\", \"gs\", \"ugs\"]\n",
    "    \n",
    "### 3.2 Tokenization algorithm\n",
    "\n",
    "A Unigram model is a type of language model that considers each token to be independent of the tokens before it. It‚Äôs the simplest language model, in the sense that the probability of token X given the previous context is just the probability of token X. So, if we used a Unigram language model to generate text, we would always predict the most common token.\n",
    "\n",
    "The probability of a given token is its frequency (the number of times we find it) in the original corpus, divided by the sum of all frequencies of all tokens in the vocabulary (to make sure the probabilities sum up to 1). For instance, \"ug\" is present in \"hug\", \"pug\", and \"hugs\", so it has a frequency of 20 in our corpus.\n",
    "\n",
    "Here are the frequencies of all the possible subwords in the vocabulary:\n",
    "\n",
    "    (\"h\", 15) (\"u\", 36) (\"g\", 20) (\"hu\", 15) (\"ug\", 20) (\"p\", 17) (\"pu\", 17) (\"n\", 16)\n",
    "    (\"un\", 16) (\"b\", 4) (\"bu\", 4) (\"s\", 5) (\"hug\", 15) (\"gs\", 5) (\"ugs\", 5)\n",
    "\n",
    "So, the sum of all frequencies is 210, and the probability of the subword \"ug\" is thus 20/210.\n",
    "\n",
    "Now, to tokenize a given word, we look at all the possible segmentations into tokens and compute the probability of each according to the Unigram model. Since all tokens are considered independent, this probability is just the product of the probability of each token. For instance, the tokenization [\"p\", \"u\", \"g\"] of \"pug\" has the probability:\n",
    "\n",
    "$$P[‚Äúp‚Äù, ‚Äúu‚Äù, ‚Äúg‚Äù] = 17/210 * 36/210 * 20/210 = 0.00132$$\n",
    "\n",
    "Comparatively, the tokenization [\"pu\", \"g\"] has the probability:\n",
    "\n",
    "$$P[‚Äúpu‚Äù, ‚Äúg‚Äù] = 17/210 x 20/210 = 0.0077$$\n",
    "\n",
    "so $P[‚Äúpu‚Äù, ‚Äúg‚Äù]$ is more likely.  In the example of \"pug\", here are the probabilities we would get for each possible segmentation:\n",
    "\n",
    "    [\"p\", \"u\", \"g\"] : 0.00132\n",
    "    [\"p\", \"ug\"] : 0.0077\n",
    "    [\"pu\", \"g\"] : 0.0077\n",
    "\n",
    "So, \"pug\" would be tokenized as [\"p\", \"ug\"] or [\"pu\", \"g\"], depending on which of those segmentations is encountered first (note that in a larger corpus, equality cases like this will be rare).\n",
    "\n",
    "In this case, it was easy to find all the possible segmentations and compute their probabilities, but in general it‚Äôs going to be a bit harder. There is a classic algorithm used for this, called the Viterbi algorithm. Essentially, we can build a graph to detect the possible segmentations of a given word by saying there is a branch from character a to character b if the subword from a to b is in the vocabulary, and attribute to that branch the probability of the subword.\n",
    "\n",
    "To find the path in that graph that is going to have the best score the Viterbi algorithm determines, for each position in the word, the segmentation with the best score that ends at that position. Since we go from the beginning to the end, that best score can be found by looping through all subwords ending at the current position and then using the best tokenization score from the position this subword begins at. Then, we just have to unroll the path taken to arrive at the end.\n",
    "\n",
    "Let‚Äôs take a look at an example using our vocabulary and the word \"unhug\". For each position, the subwords with the best scores ending there are the following:\n",
    "\n",
    "    Character 0 (u): \"u\" (score 0.171429)\n",
    "    Character 1 (n): \"un\" (score 0.076191)\n",
    "    Character 2 (h): \"un\" \"h\" (score 0.005442)\n",
    "    Character 3 (u): \"un\" \"hu\" (score 0.005442)\n",
    "    Character 4 (g): \"un\" \"hug\" (score 0.005442)\n",
    "    \n",
    "Thus \"unhug\" would be tokenized as [\"un\", \"hug\"].\n",
    "\n",
    "### 3.3 Back to training\n",
    "\n",
    "Now that we have seen how the tokenization works, we can dive a little more deeply into the loss used during training. At any given stage, this loss is computed by tokenizing every word in the corpus, using the current vocabulary and the Unigram model determined by the frequencies of each token in the corpus (as seen before).\n",
    "\n",
    "Each word in the corpus has a score, and the loss is the negative log likelihood of those scores ‚Äî that is, the sum for all the words in the corpus of all the -log(P(word)).\n",
    "\n",
    "Let‚Äôs go back to our example with the following corpus:\n",
    "\n",
    "    (\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n",
    "\n",
    "The tokenization of each word with their respective scores is:\n",
    "\n",
    "    \"hug\": [\"hug\"] (score 0.071428)\n",
    "    \"pug\": [\"pu\", \"g\"] (score 0.007710)\n",
    "    \"pun\": [\"pu\", \"n\"] (score 0.006168)\n",
    "    \"bun\": [\"bu\", \"n\"] (score 0.001451)\n",
    "    \"hugs\": [\"hug\", \"s\"] (score 0.001701)\n",
    "\n",
    "So the loss is:\n",
    "\n",
    "    10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8\n",
    "    \n",
    "Now we need to compute how removing each token affects the loss. This is rather tedious, so we‚Äôll just do it for two tokens here and save the whole process for when we have code to help us. In this (very) particular case, we had two equivalent tokenizations of all the words: as we saw earlier, for example, \"pug\" could be tokenized [\"p\", \"ug\"] with the same score. Thus, removing the \"pu\" token from the vocabulary will give the exact same loss.\n",
    "\n",
    "On the other hand, removing \"hug\" will make the loss worse, because the tokenization of \"hug\" and \"hugs\" will become:\n",
    "\n",
    "    \"hug\": [\"hu\", \"g\"] (score 0.006802)\n",
    "    \"hugs\": [\"hu\", \"gs\"] (score 0.001701)\n",
    "\n",
    "These changes will cause the loss to rise by:\n",
    "\n",
    "    - 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5\n",
    "    \n",
    "Therefore, the token \"pu\" will probably be removed from the vocabulary, but not \"hug\".\n",
    "\n",
    "### 3.4 Implementing Unigram\n",
    "\n",
    "Now let‚Äôs implement everything we‚Äôve seen so far in code. Like with BPE and WordPiece, this is not an efficient implementation of the Unigram algorithm (quite the opposite), but it should help you understand it a bit better.\n",
    "\n",
    "We will use the same corpus as before as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "vW1jWZzMu0eo"
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the Hugging Face Course.\",\n",
    "    \"This chapter is about tokenization.\",\n",
    "    \"This section shows several tokenizer algorithms.\",\n",
    "    \"Hopefully, you will be able to understand how they are trained and generate tokens.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we will use xlnet-base-cased as our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "atTSrrLIu0ep"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like for BPE and WordPiece, we begin by counting the number of occurrences of each word in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "WNWnXwDQu0ep"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'‚ñÅThis': 3,\n",
       "             '‚ñÅis': 2,\n",
       "             '‚ñÅthe': 1,\n",
       "             '‚ñÅHugging': 1,\n",
       "             '‚ñÅFace': 1,\n",
       "             '‚ñÅCourse.': 1,\n",
       "             '‚ñÅchapter': 1,\n",
       "             '‚ñÅabout': 1,\n",
       "             '‚ñÅtokenization.': 1,\n",
       "             '‚ñÅsection': 1,\n",
       "             '‚ñÅshows': 1,\n",
       "             '‚ñÅseveral': 1,\n",
       "             '‚ñÅtokenizer': 1,\n",
       "             '‚ñÅalgorithms.': 1,\n",
       "             '‚ñÅHopefully,': 1,\n",
       "             '‚ñÅyou': 1,\n",
       "             '‚ñÅwill': 1,\n",
       "             '‚ñÅbe': 1,\n",
       "             '‚ñÅable': 1,\n",
       "             '‚ñÅto': 1,\n",
       "             '‚ñÅunderstand': 1,\n",
       "             '‚ñÅhow': 1,\n",
       "             '‚ñÅthey': 1,\n",
       "             '‚ñÅare': 1,\n",
       "             '‚ñÅtrained': 1,\n",
       "             '‚ñÅand': 1,\n",
       "             '‚ñÅgenerate': 1,\n",
       "             '‚ñÅtokens.': 1})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_freqs = defaultdict(int)\n",
    "for text in corpus:\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    new_words = [word for word, offset in words_with_offsets]\n",
    "    for word in new_words:\n",
    "        word_freqs[word] += 1\n",
    "\n",
    "word_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to initialize our vocabulary to something larger than the vocab size we will want at the end. We have to include all the basic characters (otherwise we won‚Äôt be able to tokenize every word), but for the bigger substrings we‚Äôll only keep the most common ones, so we sort them by frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "5u5CoOm7u0eq",
    "outputId": "977c771e-9c03-409e-ab2c-dafbf828cd82",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('‚ñÅt', 7),\n",
       " ('is', 5),\n",
       " ('er', 5),\n",
       " ('‚ñÅa', 5),\n",
       " ('‚ñÅto', 4),\n",
       " ('to', 4),\n",
       " ('en', 4),\n",
       " ('‚ñÅT', 3),\n",
       " ('‚ñÅTh', 3),\n",
       " ('‚ñÅThi', 3)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_freqs = defaultdict(int)\n",
    "subwords_freqs = defaultdict(int)\n",
    "for word, freq in word_freqs.items():\n",
    "    for i in range(len(word)):\n",
    "        char_freqs[word[i]] += freq\n",
    "        # Loop through the subwords of length at least 2\n",
    "        for j in range(i + 2, len(word) + 1):\n",
    "            subwords_freqs[word[i:j]] += freq\n",
    "\n",
    "# Sort subwords by frequency\n",
    "sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_subwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We group the characters with the best subwords to arrive at an initial vocabulary of size 300:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "5C2Q8vNXu0er"
   },
   "outputs": [],
   "source": [
    "token_freqs = list(char_freqs.items()) + sorted_subwords[: 300 - len(char_freqs)]\n",
    "token_freqs = {token: freq for token, freq in token_freqs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the sum of all frequencies, to convert the frequencies into probabilities. For our model we will store the logarithms of the probabilities, because it‚Äôs more numerically stable to add logarithms than to multiply small numbers, and this will simplify the computation of the loss of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "62ZYV8xPu0es"
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the main function is the one that tokenizes words using the Viterbi algorithm. As we saw before, that algorithm computes the best segmentation of each substring of the word, which we will store in a variable named best_segmentations. We will store one dictionary per position in the word (from 0 to its total length), with two keys: the index of the start of the last token in the best segmentation, and the score of the best segmentation. With the index of the start of the last token, we will be able to retrieve the full segmentation once the list is completely populated.\n",
    "\n",
    "Populating the list is done with just two loops: the main loop goes over each start position, and the second loop tries all substrings beginning at that start position. If the substring is in the vocabulary, we have a new segmentation of the word up until that end position, which we compare to what is in best_segmentations.\n",
    "\n",
    "Once the main loop is finished, we just start from the end and hop from one start position to the next, recording the tokens as we go, until we reach the start of the word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "qYtPP6MFu0es"
   },
   "outputs": [],
   "source": [
    "def encode_word(word, model):\n",
    "    best_segmentations = [{\"start\": 0, \"score\": 1}] + [\n",
    "        {\"start\": None, \"score\": None} for _ in range(len(word))\n",
    "    ]\n",
    "    for start_idx in range(len(word)):\n",
    "        # This should be properly filled by the previous steps of the loop\n",
    "        best_score_at_start = best_segmentations[start_idx][\"score\"]\n",
    "        for end_idx in range(start_idx + 1, len(word) + 1):\n",
    "            token = word[start_idx:end_idx]\n",
    "            if token in model and best_score_at_start is not None:\n",
    "                score = model[token] + best_score_at_start\n",
    "                # If we have found a better segmentation ending at end_idx, we update\n",
    "                if (\n",
    "                    best_segmentations[end_idx][\"score\"] is None\n",
    "                    or best_segmentations[end_idx][\"score\"] > score\n",
    "                ):\n",
    "                    best_segmentations[end_idx] = {\"start\": start_idx, \"score\": score}\n",
    "\n",
    "    segmentation = best_segmentations[-1]\n",
    "    if segmentation[\"score\"] is None:\n",
    "        # We did not find a tokenization of the word -> unknown\n",
    "        return [\"<unk>\"], None\n",
    "\n",
    "    score = segmentation[\"score\"]\n",
    "    start = segmentation[\"start\"]\n",
    "    end = len(word)\n",
    "    tokens = []\n",
    "    while start != 0:\n",
    "        tokens.insert(0, word[start:end])\n",
    "        next_start = best_segmentations[start][\"start\"]\n",
    "        end = start\n",
    "        start = next_start\n",
    "    tokens.insert(0, word[start:end])\n",
    "    return tokens, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already try our initial model on some words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "qUEAJw6gu0et",
    "outputId": "cd8168e4-58f6-4121-d085-879f7995eec7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.5157494601402)\n",
      "(['This'], 6.288267030694535)\n"
     ]
    }
   ],
   "source": [
    "print(encode_word(\"Hopefully\", model))\n",
    "print(encode_word(\"This\", model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it‚Äôs easy to compute the loss of the model on the corpus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "S9UNScmJu0eu"
   },
   "outputs": [],
   "source": [
    "def compute_loss(model):\n",
    "    loss = 0\n",
    "    for word, freq in word_freqs.items():\n",
    "        _, word_loss = encode_word(word, model)\n",
    "        loss += freq * word_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check it works on the model we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "D7SITtRmu0ev",
    "outputId": "f0bf0773-1422-4b5b-b881-f5b81048919a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413.10377642940875"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the scores for each token is not very hard either; we just have to compute the loss for the models obtained by deleting each token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "p9d8hHASu0ew"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def compute_scores(model):\n",
    "    scores = {}\n",
    "    model_loss = compute_loss(model)\n",
    "    for token, score in model.items():\n",
    "        # We always keep tokens of length 1\n",
    "        if len(token) == 1:\n",
    "            continue\n",
    "        model_without_token = copy.deepcopy(model)\n",
    "        _ = model_without_token.pop(token)\n",
    "        scores[token] = compute_loss(model_without_token) - model_loss\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try it on a given token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "WalbO9cSu0ew",
    "outputId": "0cc35821-b67e-4015-d199-52d1319b520d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.376412403623874\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "scores = compute_scores(model)\n",
    "print(scores[\"ll\"])\n",
    "print(scores[\"his\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since \"ll\" is used in the tokenization of \"Hopefully\", and removing it will probably make us use the token \"l\" twice instead, we expect it will have a positive loss. \"his\" is only used inside the word \"This\", which is tokenized as itself, so we expect it to have a zero loss. Here are the results:\n",
    "\n",
    "With all of this in place, the last thing we need to do is add the special tokens used by the model to the vocabulary, then loop until we have pruned enough tokens from the vocabulary to reach our desired size:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "zHpOqVfnu0ew"
   },
   "outputs": [],
   "source": [
    "percent_to_remove = 0.1\n",
    "while len(model) > 100:\n",
    "    scores = compute_scores(model)\n",
    "    sorted_scores = sorted(scores.items(), key=lambda x: x[1])\n",
    "    # Remove percent_to_remove tokens with the lowest scores.\n",
    "    for i in range(int(len(model) * percent_to_remove)):\n",
    "        _ = token_freqs.pop(sorted_scores[i][0])\n",
    "\n",
    "    total_sum = sum([freq for token, freq in token_freqs.items()])\n",
    "    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to tokenize some text, we just need to apply the pre-tokenization and then use our encode_word() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "yt6qCLOYu0ex",
    "outputId": "1f1a656d-f45d-488b-9fad-fcc9a6ba4eef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['‚ñÅThis',\n",
       " '‚ñÅis',\n",
       " '‚ñÅthe',\n",
       " '‚ñÅHugging',\n",
       " '‚ñÅFace',\n",
       " '‚ñÅ',\n",
       " 'c',\n",
       " 'ou',\n",
       " 'r',\n",
       " 's',\n",
       " 'e',\n",
       " '.']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text, model):\n",
    "    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)\n",
    "    pre_tokenized_text = [word for word, offset in words_with_offsets]\n",
    "    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]\n",
    "    return sum(encoded_words, [])\n",
    "\n",
    "\n",
    "tokenize(\"This is the Hugging Face course.\", model)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Normalization and pre-tokenization",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6 (default, Nov 17 2020, 08:01:36) \n[Clang 12.0.0 (clang-1200.0.32.21)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "714d3f4db9a58ba7d2f2a9a4fffe577af3df8551aebd380095064812e2e0a6a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
