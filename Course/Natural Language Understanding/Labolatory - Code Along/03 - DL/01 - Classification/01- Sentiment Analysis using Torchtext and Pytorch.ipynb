{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using Torchtext and Pytorch\n",
    "\n",
    "- bindirectional LSTM on some news\n",
    "- some labeled data on news...\n",
    "\n",
    "- Torchtext to help us numericalize adn load some data\n",
    "- Torchtext is backed by Pytorch.... so naturally troch text is quite good\n",
    "- Torchtext is not meant to replace spacy... spacy is still litke better in general\n",
    "    \n",
    "- Pytorch to help us make some neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchdata\n",
    "import torchtext\n",
    "from torch import nn\n",
    "import time\n",
    "\n",
    "#1. puffer - it's outdated\n",
    "#2. spend some money - 300 baht get collab pro\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#reproducibility\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cpu'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.5.1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchdata.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.14.1'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the dataset\n",
    "\n",
    "Make our life easy by using some ready-to-be-used dataset by torchtext\n",
    "\n",
    "- In your assignment, i will ask you to use penn treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you are using puffer\n",
    "# import os\n",
    "# os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "from torchtext.datasets import AG_NEWS\n",
    "train, test = AG_NEWS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShardingFilterIterDataPipe"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train #a new object by torchdata ......... streaming data (yield)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.EDA - exploratory data analysis\n",
    "\n",
    "- check common words\n",
    "- look at some random sample.... how it looks, so that we can design power, neural network\n",
    "- visualize statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object ShardingFilterIterDataPipe.__iter__ at 0x000001CA73399150>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(iter(train))[1] #generator\n",
    "\n",
    "(\"World\", \"Sports\", \"Business\", \"Sci/Tech\")\n",
    "1,         2,        3,          4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([y for y,x in list(iter(train))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i gonna cheat a little bit, not gonna use all.... my fans will work too hard....\n",
    "too_much, train, valid = train.random_split(total_length = train_size, \n",
    "                                            weights = {\"too much\":0.7, \"smaller_train\":0.2,\"valid\":0.1},\n",
    "                                            seed = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "valid_size = len(list(iter(valid)))\n",
    "test_size = len(list(iter(test)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing\n",
    "\n",
    "- tokenizing, numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.1 Tokenizing\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer('spacy',language='en_core_web_sm')\n",
    "#check whether the tokenizer works....\n",
    "# tokens = tokenizer('Chaky likes deep learning very much and wants his student to be nubmer 1 in Asia')\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.2 Numericalization\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def yield_tokens(data_iter): #data_iter, e.g., train\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "    \n",
    "vocab = build_vocab_from_iterator(yield_tokens(train), specials=['<unk>','<pad>','<bos>','<eos>'], special_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.set_default_index(vocab[\"<unk>\"]) #if you don't the id of this word, set it unk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 5609, 2827, 7685, 1173, 301, 12, 944, 38, 3956, 8, 43, 0, 109, 11, 787]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['Chaky','likes','deep','learning','very','much','and','wants','his','student','to','be','nubmer','1','in','Asia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['<pad>','<bos>','<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52686"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab) #23k unique words......."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. FastText embeddding\n",
    "\n",
    "we gonna insert this embedding to the NN on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache\\wiki.simple.vec: 293MB [02:09, 2.27MB/s]                               \n",
      "  0%|          | 0/111051 [00:00<?, ?it/s]Skipping token b'111051' with 1-dimensional vector [b'300']; likely a header\n",
      "100%|██████████| 111051/111051 [00:15<00:00, 7235.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import FastText\n",
    "fast_vectors = FastText(language='simple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_embedding = fast_vectors.get_vecs_by_tokens(vocab.get_itos()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52686, 300])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([300])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#please lookup the fasttext embedding of id 100\n",
    "fast_embedding[100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0935,  0.0915,  0.2640,  0.0387,  0.0843,  0.3809, -0.1776,  0.1745,\n",
       "        -0.0362, -0.0278])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_embedding[100][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preparing dataloader\n",
    "\n",
    "Optional - you can either make your owen bacth loader....\n",
    "you can use pytorch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad_sequence([1,2,3],[1,2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pipeline = lambda x: int(x) - 1 #1,2,3,,4 --> 0,1,2,3\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwhy padding ????\\nin the same batch, e.g., batch size = 2\\n\\n\"Chaky eat sushi\" ==> \"Chaky\", \"eat\", \"sushi\" ==> 0,22,11\\n\"Chaky sleep\" ==> \"Chaky\", \"sleep\" ==> 0,99, <pad>\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "why padding ????\n",
    "in the same batch, e.g., batch size = 2\n",
    "\n",
    "\"Chaky eat sushi\" ==> \"Chaky\", \"eat\", \"sushi\" ==> 0,22,11\n",
    "\"Chaky sleep\" ==> \"Chaky\", \"sleep\" ==> 0,99, <pad>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence #mkaing each batch batch same length\n",
    "\n",
    "pad_ix = vocab['<pad>']\n",
    "\n",
    "#this function gonna be called by DataLoader\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, length_list = [],[],[]\n",
    "    for (_label,_text) in batch:\n",
    "        label_list.append(label_pipeline(_label)) # (3, 1, 0, 2, 1)\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64) #(0, 44, 25, 1)\n",
    "        text_list.append(processed_text)\n",
    "        length_list.append(processed_text.size(0)) #for padding\n",
    "    return torch.tensor(label_list,dtype=torch.int64), \\\n",
    "        pad_sequence(text_list, padding_value=pad_ix,batch_first=True), \\\n",
    "        torch.tensor(length_list,dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "batcgh_size = 64\n",
    "\n",
    "train_loader = DataLoader(train,batch_size=batcgh_size,\n",
    "                            shuffle=True,collate_fn=collate_batch)\n",
    "val_loader = DataLoader(valid,batch_size=batcgh_size,\n",
    "                            shuffle=True,collate_fn=collate_batch)\n",
    "test_loader = DataLoader(test,batch_size=batcgh_size,\n",
    "                            shuffle=True,collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64]), torch.Size([64, 135]), torch.Size([64]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for label,text,length in train_loader:\n",
    "#     break\n",
    "#label : [batch_size,]\n",
    "#text  : [batch_size, longest length of this batch] ==> [batch size, seq len] ==> [b,l]\n",
    "#length: [batch_size,]\n",
    "# label, text, length #why we need length --> we can later ignore padding......\n",
    "# label.shape, text.shape, length.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Designing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self,input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        #input dim = how many vocab you have\n",
    "        #emb dim = 300 --> we use fasttext\n",
    "        #padding_idx tells this lookup to ignore and just randomize....\n",
    "        #<unk>, <bos>, <eos>\n",
    "        self.embedding_layer = nn.Embedding(input_dim, emb_dim, padding_idx=pad_ix)\n",
    "        self.lstm = nn.LSTM(emb_dim, \n",
    "                            hid_dim, \n",
    "                            num_layers = num_layers, \n",
    "                            bidirectional = bidirectional, \n",
    "                            dropout = dropout, #dropout is applied between layers\n",
    "                            batch_first = True) \n",
    "        self.fc = nn.Linear(hid_dim*2, output_dim)\n",
    "    def forward(self, x, embedded, lengths):\n",
    "\n",
    "        #x : [batch size, seq len]\n",
    "        embedded_x = self.embedding_layer(x)\n",
    "        #x : [batch size, seq len, emb dim]\n",
    "\n",
    "        #pack this embedded_x in such a way that RNN knows to ignore padding....\n",
    "        #without batch first = True; thing will become [seq len, batch size, emb size]\n",
    "        pack_embedded = nn.utils.rnn.pack_padded_sequence(embedded_x, \n",
    "                                                            lengths.to('cpu'),\n",
    "                                                            enforce_sorted=False,\n",
    "                                                            batch_first=True)\n",
    "        #packed_output is basically all hidden states ==> POS tagging, NER\n",
    "\n",
    "        #h is the last hidden state => sentiment analysis\n",
    "        #c is the last cell state\n",
    "        packed_output, (h,c) = self.lstm(pack_embedded)\n",
    "\n",
    "        #h : [num_layers * num_directions, batch_size, hidden dim] \n",
    "\n",
    "        #it happen that because packed_ouputs is all hidden states... some hidden state near the end is\n",
    "        #hidden state for padding, pytorch guys help you\n",
    "        #by using this pack_padded_sequence, them all the hiddden states will only be not padding\n",
    "        output, output_length = nn.utils.rnn.pad_packed_sequence(packed_output,batch_first=True)\n",
    "\n",
    "        #last hidden state - concat forward and backward states\n",
    "        last_hidden_state = torch.cat((h[-1,:,:], h[-2,:,:]), dim = 1)\n",
    "        #last_hidden_state: [batch_size, hidden_dim * 2]\n",
    "\n",
    "        #for sentiment analysis... what whould i sent to my linear layer.....\n",
    "        return self.fc(last_hidden_state) #[batch_size, output_dim] ==> [batch_size,4]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explicitly initialize weights for better learning\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.orthogonal_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim  = len(vocab)\n",
    "hid_dim    = 256\n",
    "emb_dim    = 300         #**<----change to 300\n",
    "output_dim = 4 #four classes\n",
    "\n",
    "#for biLSTM\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.5\n",
    "\n",
    "model = LSTM(input_dim, emb_dim, hid_dim, output_dim, num_layers, bidirectional, dropout).to(device)\n",
    "model.apply(initialize_weights)\n",
    "model.embedding.weight.data = fast_embedding #**<------applied the fast text embedding as the initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr=1e-3\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss() #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, y):\n",
    "    \n",
    "    predicted = torch.max(preds.data, 1)[1]\n",
    "    batch_corr = (predicted == y).sum()\n",
    "    acc = batch_corr / len(y)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, loader_length):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train() #useful for batchnorm and dropout\n",
    "    \n",
    "    for i, (label, text, text_length) in enumerate(loader): \n",
    "        label = label.to(device) #(batch_size, )\n",
    "        text = text.to(device) #(batch_size, seq len)\n",
    "                \n",
    "        #predict\n",
    "        predictions = model(text, text_length).squeeze(1) #output by the fc is (batch_size, 1), thus need to remove this 1\n",
    "        \n",
    "        #calculate loss\n",
    "        loss = criterion(predictions, label)\n",
    "        acc  = accuracy(predictions, label)\n",
    "        \n",
    "        #backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "                        \n",
    "    return epoch_loss / loader_length, epoch_acc / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (label, text, text_length) in enumerate(loader): \n",
    "            label = label.to(device) #(batch_size, )\n",
    "            text  = text.to(device)  #(seq len, batch_size)\n",
    "\n",
    "            predictions = model(text, text_length).squeeze(1) \n",
    "            \n",
    "            loss = criterion(predictions, label)\n",
    "            acc  = accuracy(predictions, label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / loader_length, epoch_acc / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs      = 5\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, train_loader_length)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_accs.append(valid_acc)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!  Very high accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_accs, label = 'train acc')\n",
    "ax.plot(valid_accs, label = 'valid acc')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test on some random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"Google is now falling nonstop.  The price is really bad now.\"\n",
    "text = torch.tensor(text_pipeline(test_str)).to(device)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [x.item() for x in text]\n",
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[id2word[num] for num in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.reshape(1, -1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([text.size(1)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text, text_length):\n",
    "    with torch.no_grad():\n",
    "        output = model(text, text_length).squeeze(1)\n",
    "        predicted = torch.max(output.data, 1)[1]\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(text, text_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
