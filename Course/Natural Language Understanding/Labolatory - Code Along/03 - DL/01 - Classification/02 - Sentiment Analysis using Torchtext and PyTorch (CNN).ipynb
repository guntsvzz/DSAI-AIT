{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using Pytorch and Torchtext\n",
    "\n",
    "Why CNN? Why noy LSTM?\n",
    "- LSTM treats words as a sequence: I --> love --> sushi\n",
    "    - LSTM is just basically linear layers but a loop\n",
    "- CNN treats words as a n-grams: \"I love\", \"love sushi\"\n",
    "    - CNN is just basically linear layers + window\n",
    "\n",
    "For example :\n",
    "\n",
    "Chaky is making a sentiment analysis models.\n",
    "\n",
    "You can prepare a bigram window - CNN filter size of (height, width) ==> (1,2)\n",
    "\n",
    "Chaky is\n",
    "is making\n",
    "making a\n",
    "a sentiment\n",
    "etc.\n",
    "\n",
    "You can also play with stride....... e.g., stride = 2\n",
    "\n",
    "Chaky is\n",
    "making a \n",
    "\n",
    "You can also play with padding ...., eg., padding = 2\n",
    "\n",
    "padding Chaky\n",
    "Chaky is\n",
    ".....\n",
    "models padding\n",
    "\n",
    "Next big thing is very cool.... i.e., so which width we should use\n",
    "e.g., bi-gram(2), tri-grams(3), ??\n",
    "\n",
    "Answer: use all , 100 filters, 1 for uni-gram, 1-bigram\n",
    "\n",
    "Next thing - can CNN handles long-term relationships?\n",
    "\n",
    "Answer: yes!! by stacking more layers of CNN\n",
    "\n",
    "Chaky is \n",
    "making a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.14.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. ETL: Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment this if you are not using our department puffer\n",
    "# import os\n",
    "# os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "from torchtext.datasets import AG_NEWS\n",
    "train, test = AG_NEWS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShardingFilterIterDataPipe"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so this is a datapipe object; very similar to pytorch dataset version 2 which is better\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. EDA - simple investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's take a look at one example of train\n",
    "next(iter(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's change the unique label\n",
    "set([y for y, x in list(iter(train))])\n",
    "#{“World”, “Sports”, “Business”, “Sci/Tech”}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 120000 gonna just take up too much of our GPU, and also for the sake of learning, we gonna resize it.....  All `DataPipe` instance has a handy function called `random_split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "too_much, train, valid = train.random_split(total_length=train_size, weights = {\"too_much\": 0.7, \"smaller_train\": 0.2, \"valid\": 0.1}, seed=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = len(list(iter(valid)))\n",
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7600"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = len(list(iter(test)))\n",
    "test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "The first step is to decide which tokenizer we want to use, which depicts how we split our sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We', 'are', 'learning', 'torchtext', 'in', 'AIT', '!']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pip install spacy\n",
    "#python3 -m spacy download en_core_web_sm\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "tokens = tokenizer(\"We are learning torchtext in AIT!\")  #some test\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to integers (numeral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we use built in factory function <code>build_vocab_from_iterator</code> which accepts iterator that yield list or iterator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train), specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[509, 27, 9, 0, 9]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab(['here', 'is', 'a', 'unknownword', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab.get_itos()\n",
    "\n",
    "#print 159, for example\n",
    "mapping[509]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "vocab(['dddd', 'aaaa'])\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "vocab(['<pad>', '<bos>', '<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52828"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so we can 50k+ unique vocabularies!\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. FastText Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import FastText\n",
    "fast_vectors = FastText(language='simple') #small for easy training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_embedding = fast_vectors.get_vecs_by_tokens(vocab.get_itos()).to(device)\n",
    "# vocab.get_itos() returns a list of strings (tokens), where the token at the i'th position is what you get from doing vocab[token]\n",
    "# get_vecs_by_tokens gets the pre-trained vector for each string when given a list of strings\n",
    "# therefore pretrained_embedding is a fully \"aligned\" embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([52828, 300])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_embedding.shape   #we have X vocabs, each with a 300 fasttext embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Preparing the dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In torchtext, first thing before the batch iterator is to define how you want to process your text and label.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline  = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1 #turn {1, 2, 3, 4} to {0, 1, 2, 3} for pytorch training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at example how it looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[275, 4021, 8, 389, 574]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline(\"I love to play football\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pipeline('1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's make the batch iterator.  Here we create a function <code>collate_fn</code> that define how we want to create our batch.  **Since we are using CNN, we don't need the lengths which are needed for LSTM when we use pad_packed_sequence**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data   import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "pad_idx = vocab['<pad>'] #++<----making sure our embedding layer ignores pad\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "    #criterion expects float labels\n",
    "    return torch.tensor(label_list, dtype=torch.int64), pad_sequence(text_list, padding_value=pad_idx, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(valid, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test, batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader.  Note that {“World”, “Sports”, “Business”, “Sci/Tech”} maps to {0, 1, 2, 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, text in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label shape:  torch.Size([64])\n",
      "Text shape:  torch.Size([64, 163])\n"
     ]
    }
   ],
   "source": [
    "print(\"Label shape: \", label.shape) # (batch_size, )\n",
    "print(\"Text shape: \", text.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model\n",
    "\n",
    "The first major hurdle is visualizing how CNNs are used for text. Images are typically 2 dimensional (we'll ignore the fact that there is a third \"colour\" dimension for now) whereas text is 1 dimensional. \n",
    "\n",
    "Consider the 2 dimensional representation of the embedded sentence below:\n",
    "\n",
    "<img src = \"../figures/sentiment9.png\">\n",
    "\n",
    "We can then use a filter that is **[n x emb_dim]**. This will cover $n$ sequential words entirely, as their width will be `emb_dim` dimensions. \n",
    "\n",
    "Consider the image below, with our word vectors are represented in green. Here we have 4 words with 5 dimensional embeddings, creating a **[4x5]** \"image\" tensor. A filter that covers two words at a time (i.e. bi-grams) will be **[2x5]** filter, shown in yellow, and each element of the filter with have a _weight_ associated with it. \n",
    "\n",
    "The output of this filter (shown in red) will be a single real number that is the weighted sum of all elements covered by the filter.\n",
    "\n",
    "<img src = \"../figures/sentiment12_2.png\" width=500>\n",
    "\n",
    "The filter then moves \"down\" the image (or across the sentence) to cover the next bi-gram and another output (weighted sum) is calculated. \n",
    "\n",
    "<img src = \"../figures/sentiment13.png\">\n",
    "\n",
    "Finally, the filter moves down again and the final output for this filter is calculated.\n",
    "\n",
    "<img src = \"../figures/sentiment14.png\">\n",
    "\n",
    "In our case , our output will be a vector with number of elements equal to the height of the image (or length of the word) minus the height of the filter plus one, $4-2+1=3$ in this case.\n",
    "\n",
    "In our model, we will also have different sizes of filters, heights of 3, 4 and 5, with 100 of each of them. The intuition is that we will be looking for the occurence of different tri-grams, 4-grams and 5-grams that are relevant for classification.\n",
    "\n",
    "The next step in our model is to use *pooling* (specifically *max pooling*) on the output of the convolutional layers. Below an example of taking the maximum value (0.9) from the output of the convolutional layer on the example sentence (not shown is the activation function applied to the output of the convolutions).\n",
    "\n",
    "<img src = \"../figures/sentiment15.png\">\n",
    "\n",
    "The idea here is that the maximum value is the \"most important\" feature for prediction, which corresponds to the \"most important\" n-gram within the text. How do we know what the \"most important\" n-gram is? Luckily, we don't have to! Through backpropagation, the weights of the filters are changed so that whenever certain n-grams that are highly indicative of the labels, the output of the filter is a \"high\" value. This \"high\" value then passes through the max pooling layer if it is the maximum value in the output. \n",
    "\n",
    "Note: Another nice feature of the pooling layers is that they handle sentences of different lengths. The size of the output of the convolutional layer is dependent on the size of the input to it, and different batches contain sentences of different lengths. Without the max pooling layer the input to our linear layer would depend on the size of the input sentence (not what we want). One option to rectify this would be to trim/pad all sentences to the same length, however with the max pooling layer we always know the input to the linear layer will be the total number of filters.\n",
    "\n",
    "As our model has 100 filters of 3 different sizes, that means we have 300 different n-grams the model thinks are important. We concatenate these together into a single vector and pass them through a linear layer to predict the sentiment. We can think of the weights of this linear layer as \"weighting up the evidence\" from each of the 300 n-grams and making a final decision. \n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "We implement the convolutional layers with `nn.Conv2d`. The `in_channels` argument is the number of \"channels\" in your image going into the convolutional layer. In actual images this is usually 3 (one channel for each of the red, blue and green channels), however when using text we only have a single channel, the text itself. The `out_channels` is the number of filters and the `kernel_size` is the size of the filters. Each of our `kernel_size`s is going to be **[n x emb_dim]** where $n$ is the size of the n-grams.\n",
    "\n",
    "The second dimension of the input into a `nn.Conv2d` layer must be the channel dimension. As text technically does not have a channel dimension, we `unsqueeze` our tensor to create one. This matches with our `in_channels=1` in the initialization of our convolutional layers. \n",
    "\n",
    "We then pass the tensors through the convolutional and pooling layers, using the `ReLU` activation function after the convolutional layers. **Note**: there an exception to this if your sentence(s) are shorter than the largest filter used. You will then have to pad your sentences to the length of the largest filter. In our dataset there are no text smaller than 5 words long so we don't have to worry about that, but you will if you are using your own data.\n",
    "\n",
    "Finally, we perform dropout on the concatenated filter outputs and then pass them through a linear layer to make our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# batch_samples_RNN = (batch_size, seq_len, emb_dim)\n",
    "# batch_samples_CNN = (batch_size, 1, seq_len, emb_dim)\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, output_dim, dropout, n_filters):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "                        in_channels  = 1,\n",
    "                        out_channels = n_filters, #100\n",
    "                        kernel_size  = (3, emb_dim) #tri-grams\n",
    "                        )\n",
    "        self.conv4 = nn.Conv2d(\n",
    "                        in_channels  = 1,\n",
    "                        out_channels = n_filters, #100\n",
    "                        kernel_size  = (4, emb_dim) #quad-grams\n",
    "                        )\n",
    "        self.conv5 = nn.Conv2d(\n",
    "                        in_channels  = 1,\n",
    "                        out_channels = n_filters, #100\n",
    "                        kernel_size  = (5, emb_dim) #penta-grams\n",
    "                        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(n_filters * 3, output_dim)\n",
    "\n",
    "    def forward(self, text): \n",
    "        #text : [batch_size, seq_len]\n",
    "                \n",
    "        #embed the text\n",
    "        embedded = self.embedding(text)\n",
    "        #embedded : [batch_size, seq_len, emb_dim]\n",
    "        #reshape embedded into [batch_size, 1, seq_len, emb_dim] for CNN\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        #embedded : [batch_size, 1, seq_len, emb_dim]\n",
    "\n",
    "        #pass through conv2d\n",
    "        conv_fea3 = self.conv3(embedded) #conv_fea3: [batch_size, out_channels, seq_len*, 1]\n",
    "        conv_fea4 = self.conv4(embedded) #conv_fea4: [batch_size, out_channels, seq_len*, 1]\n",
    "        conv_fea5 = self.conv5(embedded) #conv_fea5: [batch_size, out_channels, seq_len*, 1]\n",
    "\n",
    "        #squeeze the last dimension\n",
    "        #because maxpool1d expects only 3d tensors, not 4d\n",
    "        conv_fea3 = conv_fea3.squeeze(3) #conv_fea3: [batch_size, out_channels, seq_len*]\n",
    "        conv_fea4 = conv_fea4.squeeze(3) #conv_fea4: [batch_size, out_channels, seq_len*]\n",
    "        conv_fea5 = conv_fea5.squeeze(3) #conv_fea5: [batch_size, out_channels, seq_len*]\n",
    "\n",
    "        #activation.....\n",
    "        conv_fea3 = F.relu(conv_fea3) #F is not learnable\n",
    "        conv_fea4 = F.relu(conv_fea4)\n",
    "        conv_fea5 = F.relu(conv_fea5)\n",
    "\n",
    "        #maxpooling - to find the most important words in that grams...\n",
    "        conv_fea3_max = F.max_pool1d(conv_fea3, conv_fea3.shape[2]) #conv_fea3_max: [batch_size, out_channels, 1]\n",
    "        conv_fea4_max = F.max_pool1d(conv_fea4, conv_fea4.shape[2]) #conv_fea4_max: [batch_size, out_channels, 1]\n",
    "        conv_fea5_max = F.max_pool1d(conv_fea5, conv_fea5.shape[2]) #conv_fea5_max: [batch_size, out_channels, 1]\n",
    "\n",
    "        #We are ready to put it into the linear layers!!!!\n",
    "        #1. since linear layer only takes 2d input, we can squeeze\n",
    "        conv_fea3_max = conv_fea3_max.squeeze(2) #conv_fea3_max: [batch_size, out_channels]\n",
    "        conv_fea4_max = conv_fea4_max.squeeze(2) #conv_fea4_max: [batch_size, out_channels]\n",
    "        conv_fea5_max = conv_fea5_max.squeeze(2) #conv_fea5_max: [batch_size, out_channels]\n",
    "        #2. concat all three features into one\n",
    "        final_conv_max = torch.cat((conv_fea3_max,conv_fea4_max,conv_fea5_max), dim=1)\n",
    "        #final_conv_max : [batch_size, out_channels * n_filters]\n",
    "        #3. dropout\n",
    "        final_conv_max = self.dropout(final_conv_max)\n",
    "\n",
    "        return self.fc(final_conv_max)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try whether the model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nput_dim = len(vocab)\n",
    "emb_dim = 300 #fasttext\n",
    "output_dim = 4 #four news\n",
    "dropout = 0.5\n",
    "n_filters = 100 \n",
    "model = CNN(nput_dim, emb_dim, output_dim, dropout, n_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "seq_len = 50\n",
    "\n",
    "text = torch.randint(0,1,(batch_size, seq_len))\n",
    "\n",
    "out = model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape #[batch_size, output_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4607,  0.0451,  0.4307,  0.1722],\n",
       "        [ 0.0155, -0.0737,  0.0913,  0.5674],\n",
       "        [-0.3784,  0.0939,  0.2795, -0.0277]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently the `CNN` model can only use 3 different sized filters, but we can actually improve the code of our model to make it more generic and take any number of filters.\n",
    "\n",
    "We do this by placing all of our convolutional layers in a  `nn.ModuleList`, a function used to hold a list of PyTorch `nn.Module`s. If we simply used a standard Python list, the modules within the list cannot be \"seen\" by any modules outside the list which will cause us some errors.\n",
    "\n",
    "We can now pass an arbitrary sized list of filter sizes and the list comprehension will create a convolutional layer for each of them. Then, in the `forward` method we iterate through the list applying each convolutional layer to get a list of convolutional outputs, which we also feed through the max pooling in a list comprehension before concatenating together and passing through the dropout and linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# batch_samples_RNN = (batch_size, seq_len, emb_dim)\n",
    "# batch_samples_CNN = (batch_size, 1, seq_len, emb_dim)\n",
    "\n",
    "class CNN(nn.Module):  #more elegant version\n",
    "    def __init__(self, input_dim, emb_dim, output_dim, dropout, n_filters,filter_sizes):\n",
    "\n",
    "        #filter_sizes = [3,4,5]\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
    "        self.conv = nn.ModuleList([\n",
    "                        nn.Conv2d(\n",
    "                            in_channels  = 1,\n",
    "                            out_channels = n_filters, #100\n",
    "                            kernel_size  = (fs, emb_dim) #tri-grams\n",
    "                            )\n",
    "                        for fs in filter_sizes\n",
    "                    ])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(n_filters * len(filter_sizes), output_dim)\n",
    "\n",
    "    def forward(self, text): \n",
    "        #text : [batch_size, seq_len]\n",
    "                \n",
    "        #embed the text\n",
    "        embedded = self.embedding(text)\n",
    "        #embedded : [batch_size, seq_len, emb_dim]\n",
    "        #reshape embedded into [batch_size, 1, seq_len, emb_dim] for CNN\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        #embedded : [batch_size, 1, seq_len, emb_dim]\n",
    "\n",
    "        conv_fea = [F.relu(conv(embedded)).squeeze(3) for conv in self.conv]\n",
    "\n",
    "        conv_fea_max = [F.max_pool1d(each_conv_fea, each_conv_fea.shape[2]).squeeze(2) for each_conv_fea in conv_fea]\n",
    "\n",
    "        #2. concat all three features into one\n",
    "        #3. dropout\n",
    "        final_conv_max = self.dropout(torch.cat(conv_fea_max, dim=1))\n",
    "        #final_conv_max : [batch_size, out_channels * n_filters]\n",
    "\n",
    "        return self.fc(final_conv_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try whether the model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "nput_dim = len(vocab)\n",
    "emb_dim = 300 #fasttext\n",
    "output_dim = 4 #four news\n",
    "dropout = 0.5\n",
    "n_filters = 100 \n",
    "filter_sizes = [3,4,5]\n",
    "model = CNN(nput_dim, emb_dim, output_dim, dropout, n_filters,filter_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "seq_len = 50\n",
    "\n",
    "text = torch.randint(0,1,(batch_size, seq_len))\n",
    "\n",
    "out = model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape #[batch_size, output_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3490,  0.1268, -0.4618,  0.0962],\n",
       "        [-0.8653, -0.1457, -0.2237, -0.3620],\n",
       "        [ 0.0450,  0.3625, -0.7145, -0.1203]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also implement the above model using 1-dimensional convolutional layers, where the embedding dimension is the \"depth\" of the filter and the number of tokens in the sentence is the width.\n",
    "\n",
    "<img src = \"../figures/conv1d.png\">\n",
    "\n",
    "We'll run our tests in this notebook using the 2-dimensional convolutional model, but leave the implementation for the 1-dimensional model below for anyone interested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# batch_samples_RNN = (batch_size, seq_len, emb_dim)\n",
    "# batch_samples_CNN = (batch_size, 1, seq_len, emb_dim)\n",
    "\n",
    "class CNN1d(nn.Module): \n",
    "    def __init__(self, input_dim, emb_dim, output_dim, dropout, n_filters,filter_sizes):\n",
    "\n",
    "        #filter_sizes = [3,4,5]\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
    "        self.conv = nn.ModuleList([\n",
    "                        nn.Conv1d(\n",
    "                            in_channels  = emb_dim,\n",
    "                            out_channels = n_filters, #100\n",
    "                            kernel_size  = fs #n-grams\n",
    "                            )\n",
    "                        for fs in filter_sizes\n",
    "                    ])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(n_filters * len(filter_sizes), output_dim)\n",
    "\n",
    "    def forward(self, text): \n",
    "        #text : [batch_size, seq_len]\n",
    "                \n",
    "        #embed the text\n",
    "        embedded = self.embedding(text)\n",
    "        #embedded : [batch_size, seq_len, emb_dim]\n",
    "        #reshape embedded into [batch_size, emb_dim, seq_len] for CNN1d\n",
    "        embedded = embedded.permute(0,2,1)\n",
    "        #embedded : [batch_size, emb_dim, seq_len]\n",
    "\n",
    "        conv_fea = [F.relu(conv(embedded)) for conv in self.conv]\n",
    "        #conv_fea : [batch_size, emb_dim, seq_len]\n",
    "\n",
    "        conv_fea_max = [F.max_pool1d(each_conv_fea, each_conv_fea.shape[2]).squeeze(2) for each_conv_fea in conv_fea]\n",
    "        #pooled_n = [batch size, n_filters]\n",
    "\n",
    "        #2. concat all three features into one\n",
    "        #3. dropout\n",
    "        final_conv_max = self.dropout(torch.cat(conv_fea_max, dim=1))\n",
    "        #final_conv_max : [batch_size, out_channels * n_filters]\n",
    "\n",
    "        return self.fc(final_conv_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try whether the model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "nput_dim = len(vocab)\n",
    "emb_dim = 300 #fasttext\n",
    "output_dim = 4 #four news\n",
    "dropout = 0.5\n",
    "n_filters = 100 \n",
    "filter_sizes = [3,4,5]\n",
    "model = CNN1d(nput_dim, emb_dim, output_dim, dropout, n_filters,filter_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "seq_len = 50\n",
    "\n",
    "text = torch.randint(0,1,(batch_size, seq_len))\n",
    "\n",
    "out = model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape #[batch_size, output_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2461, -0.1737, -0.3476,  0.4162],\n",
       "        [ 0.3721, -0.7201, -0.2970,  0.7478],\n",
       "        [ 0.6626, -0.6006,  0.0492,  0.5579]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explicitly initialize weights for better learning\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, (nn.Conv2d, nn.Conv2d)):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "            elif 'weight' in name:\n",
    "                nn.init.kaiming_normal_(param) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6409, -1.4227,  1.0356,  0.1386,  0.5899],\n",
       "        [ 0.1091, -0.1435,  0.1709,  1.1791, -0.2973],\n",
       "        [-0.2785,  0.7729, -0.7667,  0.3301,  0.1958]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.empty(3, 5)\n",
    "nn.init.kaiming_normal_(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim  = len(vocab)\n",
    "emb_dim    = 300\n",
    "output_dim = 4 #four classes\n",
    "\n",
    "#for cnn\n",
    "dropout = 0.5\n",
    "n_filters = 100\n",
    "filter_sizes = [3, 4, 5]\n",
    "\n",
    "model = CNN(input_dim, emb_dim, output_dim, dropout, n_filters, filter_sizes).to(device)\n",
    "model.apply(initialize_weights)\n",
    "model.embedding.weight.data = fast_embedding #**<------applied the fast text embedding as the initial weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15848400\n",
      " 90000\n",
      "   100\n",
      "120000\n",
      "   100\n",
      "150000\n",
      "   100\n",
      "  1200\n",
      "     4\n",
      "______\n",
      "16209904\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, a lot of the complexity resides in the embedding layer.  In the future, we shall use some pretrained embeddings to fix this complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr=1e-3\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss() #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, y):\n",
    "    \n",
    "    predicted = torch.max(preds.data, 1)[1]\n",
    "    batch_corr = (predicted == y).sum()\n",
    "    acc = batch_corr / len(y)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, loader_length):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train() #useful for batchnorm and dropout\n",
    "    \n",
    "    for i, (label, text) in enumerate(loader): \n",
    "        label = label.to(device) #(batch_size, )\n",
    "        text = text.to(device) #(batch_size, seq len)\n",
    "                \n",
    "        #predict\n",
    "        predictions = model(text).squeeze(1) #output by the fc is (batch_size, 1), thus need to remove this 1\n",
    "        \n",
    "        #calculate loss\n",
    "        loss = criterion(predictions, label)\n",
    "        acc  = accuracy(predictions, label)\n",
    "        \n",
    "        #backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "                        \n",
    "    return epoch_loss / loader_length, epoch_acc / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (label, text) in enumerate(loader): \n",
    "            label = label.to(device) #(batch_size, )\n",
    "            text  = text.to(device)  #(seq len, batch_size)\n",
    "\n",
    "            predictions = model(text).squeeze(1) \n",
    "            \n",
    "            loss = criterion(predictions, label)\n",
    "            acc  = accuracy(predictions, label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / loader_length, epoch_acc / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(valid_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs      = 5\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "valid_losses = []\n",
    "valid_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, train_loader_length)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    valid_losses.append(valid_loss)\n",
    "    valid_accs.append(valid_acc)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_accs, label = 'train acc')\n",
    "ax.plot(valid_accs, label = 'valid acc')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('acc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on some random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"Google is now falling nonstop.  The price is really bad now.\"\n",
    "text = torch.tensor(text_pipeline(test_str)).to(device)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [x.item() for x in text]\n",
    "text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[mapping[num] for num in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.reshape(1, -1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    with torch.no_grad():\n",
    "        output = model(text).squeeze(1)\n",
    "        predicted = torch.max(output.data, 1)[1]\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
