{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## DrQA \n",
    "\n",
    "This notebook implements model proposed in the paper: [Reading Wikipedia to Answer Open-Domain Questions](https://arxiv.org/abs/1704.00051) which is called DrQA by the authors. Specifically, DrQA is an end-to-end system for open domain question answering which involves an information retrieval system as well. This notebook however only explains the deep learning model proposed by them. This model is very similar to the one explained in [this](https://arxiv.org/abs/1606.02858) paper. The first authors in both the papers are also the same. The latter model is also known as \"Stanford Attentive Reader\" and is one of the models that is explained in Chris Manning's lecture on QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load SQuAD\n",
    "\n",
    "Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment this if you are not using AIT proxy...\n",
    "import os\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_json(path):\n",
    "    '''\n",
    "    Loads the JSON file of the Squad dataset.\n",
    "    Returns the json object of the dataset.\n",
    "    '''\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    print(\"Length of data: \", len(data['data']))\n",
    "    print(\"Data Keys: \", data['data'][0].keys())\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data:  442\n",
      "Data Keys:  dict_keys(['title', 'paragraphs'])\n",
      "Length of data:  48\n",
      "Data Keys:  dict_keys(['title', 'paragraphs'])\n"
     ]
    }
   ],
   "source": [
    "# load dataset json files\n",
    "train_data = load_json('data/squad_train.json')\n",
    "valid_data = load_json('data/squad_dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0 Title:  University_of_Notre_Dame\n"
     ]
    }
   ],
   "source": [
    "print(\"Example 0 Title: \", train_data['data'][0]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'qas': [{'answers': [{'answer_start': 515,\n",
       "     'text': 'Saint Bernadette Soubirous'}],\n",
       "   'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       "   'id': '5733be284776f41900661182'},\n",
       "  {'answers': [{'answer_start': 188, 'text': 'a copper statue of Christ'}],\n",
       "   'question': 'What is in front of the Notre Dame Main Building?',\n",
       "   'id': '5733be284776f4190066117f'},\n",
       "  {'answers': [{'answer_start': 279, 'text': 'the Main Building'}],\n",
       "   'question': 'The Basilica of the Sacred heart at Notre Dame is beside to which structure?',\n",
       "   'id': '5733be284776f41900661180'},\n",
       "  {'answers': [{'answer_start': 381,\n",
       "     'text': 'a Marian place of prayer and reflection'}],\n",
       "   'question': 'What is the Grotto at Notre Dame?',\n",
       "   'id': '5733be284776f41900661181'},\n",
       "  {'answers': [{'answer_start': 92,\n",
       "     'text': 'a golden statue of the Virgin Mary'}],\n",
       "   'question': 'What sits on top of the Main Building at Notre Dame?',\n",
       "   'id': '5733be284776f4190066117e'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example 0 Paragraph:\n",
    "train_data['data'][0]['paragraphs'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "### 2.1 Parse to dict\n",
    "\n",
    "Since SQuAD has an unique structure where there are many questions for one context, we have to construct a nice datasets, reflect each of these questions as one sample, but repeating the same context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(data:dict)->list:\n",
    "    \n",
    "    data = data['data']\n",
    "    qa_list = []\n",
    "\n",
    "    for paragraphs in data:\n",
    "\n",
    "        for para in paragraphs['paragraphs']:\n",
    "            context = para['context']\n",
    "\n",
    "            for qa in para['qas']:\n",
    "                \n",
    "                id = qa['id']\n",
    "                question = qa['question']\n",
    "                \n",
    "                for ans in qa['answers']:\n",
    "                    answer = ans['text']\n",
    "                    ans_start = ans['answer_start']\n",
    "                    ans_end = ans_start + len(answer)\n",
    "                    \n",
    "                    #one row of data\n",
    "                    qa_dict = {}\n",
    "                    qa_dict['id'] = id\n",
    "                    qa_dict['context'] = context\n",
    "                    qa_dict['question'] = question\n",
    "                    qa_dict['label'] = [ans_start, ans_end]\n",
    "                    qa_dict['answer'] = answer\n",
    "                    \n",
    "                    #append to a list of rows/dicts\n",
    "                    qa_list.append(qa_dict)    \n",
    "\n",
    "    return qa_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the json structure to return the data as a list of dictionaries\n",
    "train_list = parse_data(train_data)\n",
    "valid_list = parse_data(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train list len:  87599\n",
      "Valid list len:  34726\n"
     ]
    }
   ],
   "source": [
    "print('Train list len: ',len(train_list))\n",
    "print('Valid list len: ',len(valid_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimize the train_list and valid_list for easy debugging\n",
    "#uncomment this for toy training\n",
    "# train_list = train_list[:32]  #just enough for one batch\n",
    "# valid_list = valid_list[:32]\n",
    "train_list = train_list[:10000]\n",
    "valid_list = valid_list[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>[515, 541]</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>[188, 213]</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>[279, 296]</td>\n",
       "      <td>the Main Building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>[381, 420]</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>[92, 126]</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  5733be284776f41900661182   \n",
       "1  5733be284776f4190066117f   \n",
       "2  5733be284776f41900661180   \n",
       "3  5733be284776f41900661181   \n",
       "4  5733be284776f4190066117e   \n",
       "\n",
       "                                             context  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "3  Architecturally, the school has a Catholic cha...   \n",
       "4  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                            question       label  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...  [515, 541]   \n",
       "1  What is in front of the Notre Dame Main Building?  [188, 213]   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...  [279, 296]   \n",
       "3                  What is the Grotto at Notre Dame?  [381, 420]   \n",
       "4  What sits on top of the Main Building at Notre...   [92, 126]   \n",
       "\n",
       "                                    answer  \n",
       "0               Saint Bernadette Soubirous  \n",
       "1                a copper statue of Christ  \n",
       "2                        the Main Building  \n",
       "3  a Marian place of prayer and reflection  \n",
       "4       a golden statue of the Virgin Mary  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# converting the lists into dataframes for easy access\n",
    "train_df = pd.DataFrame(train_list)\n",
    "valid_df = pd.DataFrame(valid_list)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Numericalization (building vocabs)\n",
    "\n",
    "Next, we need to numericalize our dataset.\n",
    "\n",
    "Since our context and questions repeats in the df, let's first take only the unique context and question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_text_for_vocab(dfs:list):    \n",
    "    text = []\n",
    "    total = 0\n",
    "    for df in dfs:\n",
    "        unique_contexts = list(df.context.unique())\n",
    "        unique_questions = list(df.question.unique())\n",
    "        total += df.context.nunique() + df.question.nunique()\n",
    "        text.extend(unique_contexts + unique_questions)\n",
    "    \n",
    "    assert len(text) == total\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.8 ms, sys: 0 ns, total: 27.8 ms\n",
      "Wall time: 27.3 ms\n",
      "Number of unique sentences in dataset:  13712\n"
     ]
    }
   ],
   "source": [
    "# gather text to build vocabularies\n",
    "%time vocab_text = gather_text_for_vocab([train_df, valid_df])\n",
    "print(\"Number of unique sentences in dataset: \", len(vocab_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example\n",
    "vocab_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we gonna assign ids to every unique tokens.  Thus, before we assign, we use our favorite spaCy to help tokenize, and then we just assign the `ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use spacy to help tokenize\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def build_word_vocab(vocab_text):\n",
    "\n",
    "    words = []\n",
    "    for sent in vocab_text:\n",
    "        for word in nlp(sent, disable=['parser', 'ner']):  #disable so it's fast....\n",
    "            words.append(word.text)\n",
    "\n",
    "    word_counter = Counter(words)\n",
    "    word_vocab = sorted(word_counter, key=word_counter.get, reverse=True) #list of words sorted by frequency\n",
    "    print(f\"raw-vocab: {len(word_vocab)}\")\n",
    "    word_vocab.insert(0, '<unk>')\n",
    "    word_vocab.insert(1, '<pad>')\n",
    "    print(f\"vocab-length: {len(word_vocab)}\")\n",
    "    word2idx = {word:idx for idx, word in enumerate(word_vocab)}\n",
    "    print(f\"word2idx-length: {len(word2idx)}\")\n",
    "    idx2word = {v:k for k,v in word2idx.items()}\n",
    "    \n",
    "    return word2idx, idx2word, word_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw-vocab: 26420\n",
      "vocab-length: 26422\n",
      "word2idx-length: 26422\n",
      "CPU times: user 36.2 s, sys: 46.6 ms, total: 36.2 s\n",
      "Wall time: 36.2 s\n"
     ]
    }
   ],
   "source": [
    "# build word vocabulary\n",
    "%time word2idx, idx2word, word_vocab = build_word_vocab(vocab_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to convert our context and question into ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ids(text, word2idx):\n",
    "    \n",
    "    tokens = [w.text for w in nlp(text, disable=['parser','ner'])]\n",
    "    ids = [word2idx[word] for word in tokens]\n",
    "    \n",
    "    assert len(ids) == len(tokens)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.7 s, sys: 7.77 ms, total: 58.7 s\n",
      "Wall time: 58.7 s\n",
      "CPU times: user 27.8 s, sys: 7.9 ms, total: 27.8 s\n",
      "Wall time: 27.8 s\n",
      "CPU times: user 18 s, sys: 0 ns, total: 18 s\n",
      "Wall time: 18 s\n",
      "CPU times: user 8.87 s, sys: 3.98 ms, total: 8.88 s\n",
      "Wall time: 8.87 s\n"
     ]
    }
   ],
   "source": [
    "# numericalize context and questions for training and validation set\n",
    "%time train_df['context_ids'] = train_df.context.apply(convert_to_ids, word2idx=word2idx)\n",
    "%time valid_df['context_ids'] = valid_df.context.apply(convert_to_ids, word2idx=word2idx)\n",
    "\n",
    "%time train_df['question_ids'] = train_df.question.apply(convert_to_ids,  word2idx=word2idx)\n",
    "%time valid_df['question_ids'] = valid_df.question.apply(convert_to_ids,  word2idx=word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we remove any indices that have error.  Error here means the actual answer does not appear in the context, probably due to data entry errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_indices(df, idx2word):\n",
    "    \n",
    "    start_value_error, end_value_error, assert_error = test_indices(df, idx2word)\n",
    "    err_idx = start_value_error + end_value_error + assert_error\n",
    "    err_idx = set(err_idx)\n",
    "    print(f\"Number of error indices: {len(err_idx)}\")\n",
    "    \n",
    "    return err_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_indices(df, idx2word):\n",
    "   \n",
    "    start_value_error = []\n",
    "    end_value_error = []\n",
    "    assert_error = []\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        #get all answer tokens\n",
    "        answer_tokens = [w.text for w in nlp(row['answer'], disable=['parser','ner'])]\n",
    "\n",
    "        start_token = answer_tokens[0]\n",
    "        end_token = answer_tokens[-1]\n",
    "        \n",
    "        #get context tokens, and their start and end position\n",
    "        context_span  = [(word.idx, word.idx + len(word.text)) \n",
    "                         for word in nlp(row['context'], disable=['parser','ner'])]\n",
    "\n",
    "        #get starts from the first pair of the tuple, and ends from the second pair\n",
    "        starts, ends = zip(*context_span)\n",
    "\n",
    "        #ground truth indices\n",
    "        answer_start, answer_end = row['label']\n",
    "\n",
    "        try:\n",
    "            #try to find answer_start from starts\n",
    "            start_idx = starts.index(answer_start)\n",
    "        except:\n",
    "            start_value_error.append(index)\n",
    "        try:\n",
    "            #try to find answer_start from starts\n",
    "            end_idx  = ends.index(answer_end)\n",
    "        except:\n",
    "            end_value_error.append(index)\n",
    "\n",
    "        try:\n",
    "            #just to make sure that the idx2word convert back to the answer_token...\n",
    "            #otherwise, the ground truth cannot work.....\n",
    "            assert idx2word[row['context_ids'][start_idx]] == answer_tokens[0]\n",
    "            assert idx2word[row['context_ids'][end_idx]]   == answer_tokens[-1]\n",
    "        except:\n",
    "            assert_error.append(index)\n",
    "\n",
    "\n",
    "    return start_value_error, end_value_error, assert_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of error indices: 170\n",
      "Number of error indices: 99\n"
     ]
    }
   ],
   "source": [
    "# get indices with tokenization errors and drop those indices \n",
    "train_err = get_error_indices(train_df, idx2word)\n",
    "valid_err = get_error_indices(valid_df, idx2word)\n",
    "\n",
    "train_df.drop(train_err, inplace=True)\n",
    "valid_df.drop(valid_err, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we need to get the label answer based on our tokenized dataset.  That is, we should calculate the spans and then return a tuple of start and end positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_answer(row, idx2word):\n",
    "    \n",
    "    context_span = [(word.idx, word.idx + len(word.text)) for word in nlp(row.context, disable=['parser','ner'])]\n",
    "    starts, ends = zip(*context_span)\n",
    "    \n",
    "    #finding the spans\n",
    "    answer_start, answer_end = row.label\n",
    "    start_idx = starts.index(answer_start)\n",
    "    end_idx   = ends.index(answer_end)\n",
    "    \n",
    "    #double check\n",
    "    ans_toks = [w.text for w in nlp(row.answer,disable=['parser','ner'])]\n",
    "    ans_start = ans_toks[0]\n",
    "    ans_end  = ans_toks[-1]\n",
    "    assert idx2word[row.context_ids[start_idx]] == ans_start\n",
    "    assert idx2word[row.context_ids[end_idx]]   == ans_end\n",
    "    \n",
    "    return [start_idx, end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get start and end positions of answers from the context\n",
    "# this is basically the label for training QA models\n",
    "train_label_idx = train_df.apply(index_answer, axis=1, idx2word=idx2word)\n",
    "valid_label_idx = valid_df.apply(index_answer, axis=1, idx2word=idx2word)\n",
    "\n",
    "train_df['label_idx'] = train_label_idx\n",
    "valid_df['label_idx'] = valid_label_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Dump data to pickle files \n",
    "This ensures that we can directly access the preprocessed dataframe next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('drqastoi.pickle','wb') as handle:\n",
    "    pickle.dump(word2idx, handle)\n",
    "    \n",
    "train_df.to_pickle('drqatrain.pkl')\n",
    "valid_df.to_pickle('drqavalid.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Read data from pickle files\n",
    "\n",
    "You only need to run the preprocessing once. Some preprocessing functions can take upto 3 mins. Therefore, pickling preprocessed data can save a lot of time.\n",
    "Once the preprocessed files are saved, you can directly start from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('drqatrain.pkl')\n",
    "valid_df = pd.read_pickle('drqavalid.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing the Dataset/ Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset:\n",
    "    '''\n",
    "    -Divides the dataframe in batches.\n",
    "    -Pads the contexts and questions dynamically for each batch by padding \n",
    "     the examples to the maximum-length sequence in that batch.\n",
    "    -Calculates masks for context and question.\n",
    "    -Calculates spans for contexts.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data, batch_size):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        data = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]\n",
    "        self.data = data\n",
    "    \n",
    "    def get_span(self, text):\n",
    "        \n",
    "        text = nlp(text, disable=['parser','ner'])\n",
    "        span = [(w.idx, w.idx+len(w.text)) for w in text]\n",
    "\n",
    "        return span\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        Creates batches of data and yields them.\n",
    "        \n",
    "        Each yield comprises of:\n",
    "        :padded_context: padded tensor of contexts for each batch \n",
    "        :padded_question: padded tensor of questions for each batch \n",
    "        :context_mask & question_mask: zero-mask for question and context\n",
    "        :label: start and end index wrt context_ids\n",
    "        :context_text,answer_text: used while validation to calculate metrics\n",
    "        :context_spans: spans of context text\n",
    "        :ids: question_ids used in evaluation\n",
    "        '''\n",
    "        \n",
    "        for batch in self.data:\n",
    "                            \n",
    "            spans = []\n",
    "            context_text = []\n",
    "            answer_text = []\n",
    "            \n",
    "            max_context_len = max([len(ctx) for ctx in batch.context_ids])\n",
    "            padded_context = torch.LongTensor(len(batch), max_context_len).fill_(1)\n",
    "            \n",
    "            for ctx in batch.context:\n",
    "                context_text.append(ctx)\n",
    "                spans.append(self.get_span(ctx))\n",
    "            \n",
    "            for ans in batch.answer:\n",
    "                answer_text.append(ans)\n",
    "                \n",
    "            for i, ctx in enumerate(batch.context_ids):\n",
    "                padded_context[i, :len(ctx)] = torch.LongTensor(ctx)\n",
    "            \n",
    "            max_question_len = max([len(ques) for ques in batch.question_ids])\n",
    "            padded_question = torch.LongTensor(len(batch), max_question_len).fill_(1)\n",
    "            \n",
    "            for i, ques in enumerate(batch.question_ids):\n",
    "                padded_question[i,: len(ques)] = torch.LongTensor(ques)\n",
    "            \n",
    "            label = torch.LongTensor(list(batch.label_idx))\n",
    "            context_mask = torch.eq(padded_context, 1)\n",
    "            question_mask = torch.eq(padded_question, 1)\n",
    "            \n",
    "            ids = list(batch.id)  \n",
    "            \n",
    "            yield (padded_context, padded_question, context_mask, \n",
    "                   question_mask, label, context_text, answer_text, ids)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SquadDataset(train_df, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = SquadDataset(valid_df, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 253]),\n",
       " torch.Size([32, 19]),\n",
       " torch.Size([32, 253]),\n",
       " torch.Size([32, 19]),\n",
       " torch.Size([32, 2]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0].shape, a[1].shape, a[2].shape, a[3].shape, a[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[5][0]  #first sample of the batch (context_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Saint Bernadette Soubirous'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[6][0]  #answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([102, 104])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[4][0] #label of start and end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dive deep into the intricacies of the model, let's set up the notations. An input example during training is comprised of \n",
    "* a paragraph / context $p$ consisting of $l$ tokens { $p_{1}$, $p_{2}$,..., $p_{l}$ }\n",
    "* a question $q$ consisting of $m$ tokens { $q_{1}$, $q_{2}$,..., $q_{m}$ }\n",
    "* a start and and end position that comes from the context itself. More specifically, the start and end indices of the answer from the context  \n",
    "\n",
    "The following flowchart shows the flow of the model. It might not make sense now, but as we progress down the chart and build all the components, things will become clearer.\n",
    "\n",
    "\n",
    "<img src=\"images/drqaflow.PNG\" width=\"700\" height=\"800\"/>\n",
    "\n",
    "\n",
    "### Word Embedding\n",
    "\n",
    "The first transformation for both the question and the context tokens is that they are passed through an embedding layer initialized with pre-trained GloVe word vectors. 300-dimensional vectors version are used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_glove_matrix():\n",
    "    glove_dict = {}\n",
    "    with open(\"glove.6B.300d.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "            glove_dict[word] = vector\n",
    "    f.close()\n",
    "    \n",
    "    return glove_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dict = create_glove_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_embedding(glove_dict):\n",
    "    '''\n",
    "    Creates a weight matrix of the words that are common in the GloVe vocab and\n",
    "    the dataset's vocab. Initializes OOV words with a zero vector.\n",
    "    '''\n",
    "    weights_matrix = np.zeros((len(word_vocab), 300))\n",
    "    words_found = 0\n",
    "    for i, word in enumerate(word_vocab):\n",
    "        try:\n",
    "            weights_matrix[i] = glove_dict[word]\n",
    "            words_found += 1\n",
    "        except:\n",
    "            pass\n",
    "    return weights_matrix, words_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix, words_found = create_word_embedding(glove_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words found in glove vocab:  15768\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words found in glove vocab: \", words_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('drqaglove_vt.npy',weights_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align Question Embedding\n",
    "\n",
    "The paper has different encoding procedures for the context and the question. The context/paragraph encoding is more exhaustive and comprises of following additional features:\n",
    "\n",
    "* exact match : encodes a binary feature if $p$ can be exactly matched to one word in question in its original, lemma or lowercase form\n",
    "* token features : Includes POS, NER and TF of context tokens and\n",
    "* aligned question embedding ($f_{align}$) .  \n",
    "\n",
    "In this re-implementation we only implemented the aligned question embedding. The other features can be added easily but they do not affect the metrics by a large margin(~2).  \n",
    "$f_{align}$ has been formulated as shown below:\n",
    "\n",
    "$$ f_{align} = \\sum_{j}a_{i,j}E(q_{j}) $$ \n",
    "\n",
    "where $E()$ represents the glove embeddings and\n",
    "\n",
    "<img src=\"images/drqa1.PNG\" width=\"400\" height=\"200\"/>\n",
    "\n",
    "where $\\alpha()$ is a single dense layer with relu non-linearity. This transformation can be thought of as a projection to a new vector sub-space. The weights of the projection matrix will be learnt via backpropogation.\n",
    "These equations can be converted into code quite easily. Lets break this down into smaller chunks and understand what's going on actually. \n",
    "<img src=\"images/drqa2.PNG\" width=\"200\" height=\"150\"/>\n",
    "This is simply the product of projections of glove embeddings of the context and the question. Careful inspection of the equation for $a_{i,j}$ reveals that it is actually a softmax of the above product. The equations above depict everything at token level where $i$ represents a context token and $j$ represents a question token. Practically we usually vectorize our computations and deal with tensors directly.\n",
    "$f_{align}$ is a weighted representation of the question embeddings. $a_{i,j}$ represents the weights and hence a softmax function is necessary.  \n",
    "#### Intuition\n",
    "This feature enables the model to understand what portion of the context is more important or relevant with respect to the question. The products of projections taken at token level ensure a higher value when similar words from the question and context are multiplied. Quoting the paper,\n",
    "> *these features add soft alignments between similar but non-identical words (e.g., car and vehicle).* \n",
    "\n",
    "This is achieved via backpropation and training the weights of the dense layer. While this might seem a bit weird initially, we have to trust the process of backpropogation.   \n",
    "\n",
    "While implementing, we first calculate the projections of context and question vectors. We then use `torch.bmm` to calculate the product in the numerator of $a_{i,j}$, mask the product and then pass it through the softmax function to get $a_{i,j}$. Finally, we multiply this with the question embeddings. The output of this layer is an additional context embedding which is then concatenated with the glove embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlignQuestionEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim):        \n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, input_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, context, question, question_mask):\n",
    "        \n",
    "        # context = [bs, ctx_len, emb_dim]\n",
    "        # question = [bs, qtn_len, emb_dim]\n",
    "        # question_mask = [bs, qtn_len]\n",
    "    \n",
    "        ctx_ = self.linear(context)\n",
    "        ctx_ = self.relu(ctx_)\n",
    "        # ctx_ = [bs, ctx_len, emb_dim]\n",
    "        \n",
    "        qtn_ = self.linear(question)\n",
    "        qtn_ = self.relu(qtn_)\n",
    "        # qtn_ = [bs, qtn_len, emb_dim]\n",
    "        \n",
    "        qtn_transpose = qtn_.permute(0,2,1)\n",
    "        # qtn_transpose = [bs, emb_dim, qtn_len]\n",
    "        \n",
    "        align_scores = torch.bmm(ctx_, qtn_transpose)\n",
    "        # align_scores = [bs, ctx_len, qtn_len]\n",
    "        \n",
    "        qtn_mask = question_mask.unsqueeze(1).expand(align_scores.size())\n",
    "        # qtn_mask = [bs, 1, qtn_len] => [bs, ctx_len, qtn_len]\n",
    "        \n",
    "        # Fills elements of self tensor(align_scores) with value(-float(inf)) where mask is True. \n",
    "        # The shape of mask must be broadcastable with the shape of the underlying tensor.\n",
    "        align_scores = align_scores.masked_fill(qtn_mask == 1, -float('inf'))\n",
    "        # align_scores = [bs, ctx_len, qtn_len]\n",
    "        \n",
    "        align_scores_flat = align_scores.view(-1, question.size(1))\n",
    "        # align_scores = [bs*ctx_len, qtn_len]\n",
    "        \n",
    "        alpha = F.softmax(align_scores_flat, dim=1)\n",
    "        alpha = alpha.view(-1, context.shape[1], question.shape[1])\n",
    "        # alpha = [bs, ctx_len, qtn_len]\n",
    "        \n",
    "        align_embedding = torch.bmm(alpha, question)\n",
    "        # align = [bs, ctx_len, emb_dim]\n",
    "        \n",
    "        return align_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked BiLSTM\n",
    "\n",
    "The paragraph/context encoding which now has two features (glove and $f_{align}$) is then passed to a multilayer (3 layers) bidirectional LSTM. According to the paper,\n",
    "\n",
    "> *Speciﬁcally, we choose to use a multi-layer bidirectional long short-term memory network (LSTM), and take the concatenation of each layer’s hidden units in the end. *\n",
    "\n",
    "To achieve this functionality we cannot directly use the pytorch recurrent layers. Every recurrent layer in pytorch returns a tuple `[output, hidden]` where `output` holds the hidden states of all the timesteps from the __last layer only__. We need to access the hidden states of intermediate layers and then concatenate them at the end.\n",
    "The following figure illustrates this point in more detail.\n",
    "\n",
    "<img src=\"images/bilstm.png\" width=\"700\" height=\"600\"/>\n",
    "\n",
    "This figure shows a 3-layer bidirectional LSTM with an input sequence of size $n$. The green blocks denote the forward LSTMs and the blue blocks backward. Each block is labelled with the value that it calculates. The subscript denotes the time-step and the superscript denotes the depth or the layer-number.\n",
    "\n",
    "As highlighted in the diagram, we need the intermediate hidden states passed between the layers along with the final output. To create this in code, we create a `nn.ModuleList` and add 3 LSTM layers to it. The input size of the first layer remains the same but for subsequent LSTMs the input size must be twice the hidden size. This is because the `output` of the first LSTM will have the dimension of `[batch_size, seq_len, hidden_size*num_directions]` and `num_directions` is 2 in our case. In the forward method, we loop through the LSTMs, store the hidden states of each layer and finally return the concatenated output. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedBiLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstms = nn.ModuleList()\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            input_dim = input_dim if i == 0 else hidden_dim * 2\n",
    "            \n",
    "            self.lstms.append(nn.LSTM(input_dim, hidden_dim,\n",
    "                                      batch_first=True, bidirectional=True))\n",
    "           \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = [bs, seq_len, feature_dim]\n",
    "\n",
    "        outputs = [x]\n",
    "        for i in range(self.num_layers):\n",
    "\n",
    "            lstm_input = outputs[-1]\n",
    "            lstm_out = F.dropout(lstm_input, p=self.dropout)\n",
    "            lstm_out, (hidden, cell) = self.lstms[i](lstm_input)\n",
    "           \n",
    "            outputs.append(lstm_out)\n",
    "\n",
    "    \n",
    "        output = torch.cat(outputs[1:], dim=2)\n",
    "        # [bs, seq_len, num_layers*num_dir*hidden_dim]\n",
    "        \n",
    "        output = F.dropout(output, p=self.dropout)\n",
    "      \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Attention Layer\n",
    "\n",
    "The previous layers were majorly about encoding and representing the context. This layer is used to encode the question and is much simpler than the previous layers. The question tokens are first passed through the glove embedding layer, then passed through the bilstm layer and finally reach this layer. \n",
    "This layer is used to calculate the importance of each word in the question. This can be achieved by simply taking a softmax over the input. However to add more learning capacity to the model, the inputs are multiplied by a trainable weight vector $w$ and then passed through a softmax function.  \n",
    "This layer calculates the weights as \n",
    "<img src=\"images/drqab.PNG\" width=\"300\" height=\"300\"/>\n",
    "\n",
    "Essentially the layer is performing \"attention\" on inputs. The $w$ in code is characterized by a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearAttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        \n",
    "    def forward(self, question, question_mask):\n",
    "        \n",
    "        # question = [bs, qtn_len, input_dim] = [bs, qtn_len, bi_lstm_hid_dim]\n",
    "        # question_mask = [bs,  qtn_len]\n",
    "        \n",
    "        qtn = question.view(-1, question.shape[-1])\n",
    "        # qtn = [bs*qtn_len, hid_dim]\n",
    "        \n",
    "        attn_scores = self.linear(qtn)\n",
    "        # attn_scores = [bs*qtn_len, 1]\n",
    "        \n",
    "        attn_scores = attn_scores.view(question.shape[0], question.shape[1])\n",
    "        # attn_scores = [bs, qtn_len]\n",
    "        \n",
    "        attn_scores = attn_scores.masked_fill(question_mask == 1, -float('inf'))\n",
    "        \n",
    "        alpha = F.softmax(attn_scores, dim=1)\n",
    "        # alpha = [bs, qtn_len]\n",
    "        \n",
    "        return alpha\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function just multiplies the weights calculated in the previous layer by the outputs of the question bilstm layer. This allows the model to assign higher values to important words in each question.\n",
    "\n",
    "$$ q = \\sum_{j} b_{j} q_{j} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(x, weights):\n",
    "    # x = [bs, len, dim]\n",
    "    # weights = [bs, len]\n",
    "    \n",
    "    weights = weights.unsqueeze(1)\n",
    "    # weights = [bs, 1, len]\n",
    "    \n",
    "    w = weights.bmm(x).squeeze(1)\n",
    "    # w = [bs, 1, dim] => [bs, dim]\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "Recall that the attention mechanism was designed to do this: while decoding at any particular time step, encoder hidden states from all the time-steps are made available to the decoder. The decoder then can look back at the encoder hidden states or the source language and make a more informed prediction at a particular time-step. This alieviates the problem of all the information from source language being crammed into a single vector.  \n",
    "\n",
    "To illustrate this with equations, consider that the hidden states of the encoder RNN are represented by $H$ = {$h_{1}, h_{2}, h_{3},...,h_{t}$}. While decoding the token at position $t$, the input to the decoder unit is hidden state from previous unit $s_{t-1}$ and an attention vector which is a selective summary of the encoder hidden states and helps the decoder to pay more attention to a particular encoder state. \n",
    "The similarity between the encoder hidden states $H$ and the decoder hidden state so far $s_{t-1}$ is computed by,  \n",
    "$$ \\alpha = tanh (W [H ; s_{t-1}]) $$   \n",
    "\n",
    "$\\alpha$ is then passed through a softmax layer to obtain attention distribution such that $\\sum_{t} \\alpha_{t}$ = 1.\n",
    "The final step is calculating the attention vector by taking a weighted sum of the encoder hidden states,\n",
    "$$ \\sum_{t} \\alpha_{t} h_{t} $$\n",
    "\n",
    "The following diagram illustrates this process.  \n",
    " \n",
    "<img src=\"images/attnkj.PNG\" width=\"600\" height=\"100\"/>\n",
    "\n",
    "Here the encoder hidden states {$h_{1}, h_{2}, h_{3},...,h_{t}$} are commonly called the __*values*__ and the decoder hidden state $s_{t-1}$ is the __*query*__.  \n",
    "\n",
    "### A More General Take On Attention\n",
    "\n",
    "In general there are 3 steps when calculating the attention. Consider that values are represented by {$h_{1}, h_{2}, h_{3},..h_{n}$} and query is $s$. Then attention always involves,\n",
    "\n",
    "1. Calculating the energy $e$ or attention scores between these 2 vectors,\n",
    "$e$   $ \\epsilon$  $ R^{N} $\n",
    "2. Taking softmax to get an attention distribution $\\alpha$, $\\alpha$ $\\epsilon$ $R^{N}$\n",
    "\n",
    "$$ \\alpha = softmax(e)$$ \n",
    "$$ \\sum_{t}^{N} \\alpha_{t} = 1 $$\n",
    "\n",
    "3. Taking the weighted sum of the `values` by using $\\alpha$\n",
    "$$ a = \\sum_{t}^{N}\\alpha_{t}h_{t} $$\n",
    "\n",
    "\n",
    "Now there are different ways to calculate the energy between `query` and `values`. \n",
    "* **Basic Dot Product Attention**    \n",
    "$$ e_{t} = s^{T}h_{t}$$      \n",
    "* **Additive Attention**\n",
    "$$ e_{t} = v^{T} tanh (W [h_{t};s])$$  \n",
    "This is nothing but the Bahdanau attention.\n",
    "* **Scaled Dot Product Attention**\n",
    "$$ e_{t} = s^{T}h_{t}/\\sqrt n$$\n",
    "where $n$ is the model size. A modified version of this proposed in the Transformers paper by Vaswani et al. is now employed in almost every NLP system.\n",
    "\n",
    "* **Bilinear Attention**\n",
    "$$ e_{t} = s^{T} W h_{t}$$\n",
    "where $W$ is a trainable weight vector.\n",
    "This is the method used in this paper to predict the start and end position of the answer from the context.    \n",
    "\n",
    "\n",
    "To implement this layer, we characterise $W$ by a linear layer.\n",
    "First the linear layer is applied to the question, which is equivalent to the product $W.q$. This product is then multiplied by the context using `torch.bmm`.   \n",
    "Note that softmax is not taken over here to get the weights. This is taken care of when we calculate the loss using cross entropy. The following layer does not actually calculate the attention as a weighted sum. It just uses the bilinear term's representation to predict the span. However the intuition behind the bilinear term still remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearAttentionLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, context_dim, question_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(question_dim, context_dim)\n",
    "        \n",
    "    def forward(self, context, question, context_mask):\n",
    "        \n",
    "        # context = [bs, ctx_len, ctx_hid_dim] = [bs, ctx_len, hid_dim*6] = [bs, ctx_len, 768]\n",
    "        # question = [bs, qtn_hid_dim] = [bs, qtn_len, 768]\n",
    "        # context_mask = [bs, ctx_len]\n",
    "        \n",
    "        qtn_proj = self.linear(question)\n",
    "        # qtn_proj = [bs, ctx_hid_dim]\n",
    "        \n",
    "        qtn_proj = qtn_proj.unsqueeze(2)\n",
    "        # qtn_proj = [bs, ctx_hid_dim, 1]\n",
    "        \n",
    "        scores = context.bmm(qtn_proj)\n",
    "        # scores = [bs, ctx_len, 1]\n",
    "        \n",
    "        scores = scores.squeeze(2)\n",
    "        # scores = [bs, ctx_len]\n",
    "        \n",
    "        scores = scores.masked_fill(context_mask == 1, -float('inf'))\n",
    "        \n",
    "        #alpha = F.log_softmax(scores, dim=1)\n",
    "        # alpha = [bs, ctx_len]\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together\n",
    "\n",
    "The following module brings all the components discussed so far together. It takes in the context and question tokens as inputs and returns the start and end positions of the answer from the context.  \n",
    "\n",
    "<img src=\"images/drqaflow.PNG\" width=\"600\" height=\"600\"/>\n",
    "\n",
    "  \n",
    "Going down the flowchart, following steps are performed in sequence:  \n",
    "* The context and question tokens are passed through the Glove embedding layer. The glove embeddings are partially finetuned during training. According to the paper,  \n",
    "> *We keep most of the pre-trained word embeddings ﬁxed and only ﬁne-tune the 1000 most frequent question words because the representations of some key words such as what, how, which, many could be crucial for QA systems.*   \n",
    "\n",
    "In code, this is done by using hooks in pytorch. Hooks work as a callback functions and are executed after `forward` or `backward` function is called for a particular tensor. You should read more about this in their documentation.\n",
    "\n",
    "* Aligned question embedding is calculated for the context vector and concatenated (using `torch.cat`) to the context representation. If $d$ is the embedding dimension then context $\\epsilon$ $R^{2d}$ and question $\\epsilon$ $R^{d}$.\n",
    "* Context and question representations are then passed to bilstm layers to obtain tensors of dimension `[batch_size, seq_len, hidden_dim*6]` since the LSTM is bidirectional and there are 3 layers of it.\n",
    "* The embedded question is also passed through the linear attention layer and a weighted sum of its output is taken with the biLSTM output.\n",
    "* Both these representations are finally passed through the bilinear attention layer to predict the start and end position of the answer.   \n",
    "\n",
    "An intriguing point here is that the same set of weights are passed to the bilinear attention layers. Yet how do they predict different things. This is left over to the neural network to learn. Our loss function ensures that our objective is to predict different positions from the context. It is now the neural net's responsibility to learn different weights for each layer. It is sort of a \"black-box\" and we have to trust the process of backpropogation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentReader(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, embedding_dim, num_layers, num_directions, dropout, device):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        #self.embedding = self.get_glove_embedding()\n",
    "        \n",
    "        self.context_bilstm = StackedBiLSTM(embedding_dim * 2, hidden_dim, num_layers, dropout)\n",
    "        \n",
    "        self.question_bilstm = StackedBiLSTM(embedding_dim, hidden_dim, num_layers, dropout)\n",
    "        \n",
    "        self.glove_embedding = self.get_glove_embedding()\n",
    "        \n",
    "        def tune_embedding(grad, words=1000):\n",
    "            grad[words:] = 0\n",
    "            return grad\n",
    "        \n",
    "        self.glove_embedding.weight.register_hook(tune_embedding)\n",
    "        \n",
    "        self.align_embedding = AlignQuestionEmbedding(embedding_dim)\n",
    "        \n",
    "        self.linear_attn_question = LinearAttentionLayer(hidden_dim*num_layers*num_directions) \n",
    "        \n",
    "        self.bilinear_attn_start = BilinearAttentionLayer(hidden_dim*num_layers*num_directions, \n",
    "                                                          hidden_dim*num_layers*num_directions)\n",
    "        \n",
    "        self.bilinear_attn_end = BilinearAttentionLayer(hidden_dim*num_layers*num_directions,\n",
    "                                                        hidden_dim*num_layers*num_directions)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "   \n",
    "        \n",
    "    def get_glove_embedding(self):\n",
    "        \n",
    "        weights_matrix = np.load('drqaglove_vt.npy')\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape\n",
    "        embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix).to(self.device),freeze=False)\n",
    "\n",
    "        return embedding\n",
    "    \n",
    "    \n",
    "    def forward(self, context, question, context_mask, question_mask):\n",
    "       \n",
    "        # context = [bs, len_c]\n",
    "        # question = [bs, len_q]\n",
    "        # context_mask = [bs, len_c]\n",
    "        # question_mask = [bs, len_q]\n",
    "        \n",
    "        \n",
    "        ctx_embed = self.glove_embedding(context)\n",
    "        # ctx_embed = [bs, len_c, emb_dim]\n",
    "        \n",
    "        ques_embed = self.glove_embedding(question)\n",
    "        # ques_embed = [bs, len_q, emb_dim]\n",
    "        \n",
    "        ctx_embed = self.dropout(ctx_embed)\n",
    "     \n",
    "        ques_embed = self.dropout(ques_embed)\n",
    "             \n",
    "        align_embed = self.align_embedding(ctx_embed, ques_embed, question_mask)\n",
    "        # align_embed = [bs, len_c, emb_dim]  \n",
    "        \n",
    "        ctx_bilstm_input = torch.cat([ctx_embed, align_embed], dim=2)\n",
    "        # ctx_bilstm_input = [bs, len_c, emb_dim*2]\n",
    "                \n",
    "        ctx_outputs = self.context_bilstm(ctx_bilstm_input)\n",
    "        # ctx_outputs = [bs, len_c, hid_dim*layers*dir] = [bs, len_c, hid_dim*6]\n",
    "       \n",
    "        qtn_outputs = self.question_bilstm(ques_embed)\n",
    "        # qtn_outputs = [bs, len_q, hid_dim*6]\n",
    "    \n",
    "        qtn_weights = self.linear_attn_question(qtn_outputs, question_mask)\n",
    "        # qtn_weights = [bs, len_q]\n",
    "            \n",
    "        qtn_weighted = weighted_average(qtn_outputs, qtn_weights)\n",
    "        # qtn_weighted = [bs, hid_dim*6]\n",
    "        \n",
    "        start_scores = self.bilinear_attn_start(ctx_outputs, qtn_weighted, context_mask)\n",
    "        # start_scores = [bs, len_c]\n",
    "         \n",
    "        end_scores = self.bilinear_attn_end(ctx_outputs, qtn_weighted, context_mask)\n",
    "        # end_scores = [bs, len_c]\n",
    "        \n",
    "      \n",
    "        return start_scores, end_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Hyperparameters\n",
    "\n",
    "> *We use 3-layer bidirectional LSTMs with h = 128 hidden units for both paragraph and question encoding. Dropout with p = 0.3 is applied to word embeddings and all the hidden units of LSTMs. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "HIDDEN_DIM = 128\n",
    "EMB_DIM = 300\n",
    "NUM_LAYERS = 3\n",
    "NUM_DIRECTIONS = 2\n",
    "DROPOUT = 0.3\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = DocumentReader(HIDDEN_DIM,\n",
    "                       EMB_DIM, \n",
    "                       NUM_LAYERS, \n",
    "                       NUM_DIRECTIONS, \n",
    "                       DROPOUT, \n",
    "                       device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adamax(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 11,967,749 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    '''Returns the number of trainable parameters in the model.'''\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset):\n",
    "    '''\n",
    "    Trains the model.\n",
    "    '''\n",
    "    \n",
    "    print(\"Starting training ........\")\n",
    "    \n",
    "    train_loss = 0.\n",
    "    batch_count = 0\n",
    "    \n",
    "    # put the model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # iterate through training data\n",
    "    for batch in train_dataset:\n",
    "\n",
    "        if batch_count % 500 == 0:\n",
    "            print(f\"Starting batch: {batch_count}\")\n",
    "        batch_count += 1\n",
    "\n",
    "        context, question, context_mask, question_mask, label, ctx, ans, ids = batch\n",
    "        \n",
    "        # place the tensors on GPU\n",
    "        context, context_mask, question, question_mask, label = context.to(device), context_mask.to(device),\\\n",
    "                                    question.to(device), question_mask.to(device), label.to(device)\n",
    "        \n",
    "        # forward pass, get the predictions\n",
    "        preds = model(context, question, context_mask, question_mask)\n",
    "\n",
    "        start_pred, end_pred = preds\n",
    "        \n",
    "        # separate labels for start and end position\n",
    "        start_label, end_label = label[:,0], label[:,1]\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = F.cross_entropy(start_pred, start_label) + F.cross_entropy(end_pred, end_label)\n",
    "        \n",
    "        # backward pass, calculates the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "        \n",
    "        # update the gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # zero the gradients to prevent them from accumulating\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss/len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, valid_dataset):\n",
    "    '''\n",
    "    Performs validation.\n",
    "    '''\n",
    "    \n",
    "    print(\"Starting validation .........\")\n",
    "   \n",
    "    valid_loss = 0.\n",
    "\n",
    "    batch_count = 0\n",
    "    \n",
    "    f1, em = 0., 0.\n",
    "    \n",
    "    # puts the model in eval mode. Turns off dropout\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = {}\n",
    "    \n",
    "    for batch in valid_dataset:\n",
    "\n",
    "        if batch_count % 500 == 0:\n",
    "            print(f\"Starting batch {batch_count}\")\n",
    "        batch_count += 1\n",
    "\n",
    "        context, question, context_mask, question_mask, label, context_text, answers, ids = batch\n",
    "\n",
    "        context, context_mask, question, question_mask, label = context.to(device), context_mask.to(device),\\\n",
    "                                    question.to(device), question_mask.to(device), label.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            preds = model(context, question, context_mask, question_mask)\n",
    "\n",
    "            p1, p2 = preds\n",
    "            #p1, p2 = [bs, len_context]\n",
    "\n",
    "            y1, y2 = label[:,0], label[:,1]\n",
    "\n",
    "            loss = F.cross_entropy(p1, y1) + F.cross_entropy(p2, y2)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "            \n",
    "            # get the start and end index positions from the model preds\n",
    "            \n",
    "            batch_size, c_len = p1.size()\n",
    "            ls = nn.LogSoftmax(dim=1)\n",
    "            mask = (torch.ones(c_len, c_len) * float('-inf')).to(device).tril(-1).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            #mask = [bs, c_len, c_len]            \n",
    "            \n",
    "            score = (ls(p1).unsqueeze(2) + ls(p2).unsqueeze(1)) + mask\n",
    "            #score = [bs, len_context, 1] + [bs, 1, len_context] + mask\n",
    "            \n",
    "            #first, max along the p1 axis\n",
    "            score, s_idx = score.max(dim=1)\n",
    "            #s_idx = [bs, c_len]\n",
    "            #score = [bs, c_len]\n",
    "            \n",
    "            #then since the score now is left with [bs, len_context], we simply max again to get end_idx\n",
    "            score, e_idx = score.max(dim=1)\n",
    "            #s_idx = [bs]\n",
    "            #score = [bs]\n",
    "            \n",
    "            s_idx = torch.gather(s_idx, 1, e_idx.view(-1, 1)).squeeze()\n",
    "            \n",
    "            # stack predictions\n",
    "            for i in range(batch_size):\n",
    "                id = ids[i]\n",
    "                pred = context[i][s_idx[i]:e_idx[i]+1]\n",
    "                pred = ' '.join([idx2word[idx.item()] for idx in pred])\n",
    "                predictions[id] = pred\n",
    "            \n",
    "    em, f1 = evaluate(predictions)            \n",
    "    return valid_loss/len(valid_dataset), em, f1\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaning how we calculate the score, s_idx, and e_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [-inf, 0., 0., 0., 0.],\n",
      "         [-inf, -inf, 0., 0., 0.],\n",
      "         [-inf, -inf, -inf, 0., 0.],\n",
      "         [-inf, -inf, -inf, -inf, 0.]]])\n",
      "torch.Size([1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "#in case you don't understand what this line does\n",
    "#it basically block all impossible scores where end_idx > start_idx\n",
    "mask = (torch.ones(5, 5) * float('-inf')).tril(-1).unsqueeze(0).expand(1, -1, -1)\n",
    "print(mask)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****\n",
      "p1 shape:  torch.Size([1, 5])\n",
      "p1_unsqueezed=tensor([[[ 2],\n",
      "         [ 3],\n",
      "         [ 2],\n",
      "         [10],\n",
      "         [ 5]]])\n",
      "p1_unsqueezed.shape=torch.Size([1, 5, 1])\n",
      "*****\n",
      "p2 shape:  torch.Size([1, 5])\n",
      "p2_unsqueezed=tensor([[[ 1,  3,  2,  5, 10]]])\n",
      "torch.Size([1, 1, 5])\n",
      "*****\n",
      "score=tensor([[[ 3.,  5.,  4.,  7., 12.],\n",
      "         [-inf,  6.,  5.,  8., 13.],\n",
      "         [-inf, -inf,  4.,  7., 12.],\n",
      "         [-inf, -inf, -inf, 15., 20.],\n",
      "         [-inf, -inf, -inf, -inf, 15.]]])\n",
      "*****\n",
      "score=tensor([[ 3.,  6.,  5., 15., 20.]])\n",
      "s_idx=tensor([[0, 1, 1, 3, 3]])\n",
      "*****\n",
      "score=tensor([20.])\n",
      "e_idx=tensor([4])\n",
      "*****\n",
      "s_idx=tensor(3)\n"
     ]
    }
   ],
   "source": [
    "#let's say the context here is\n",
    "# AIT is at Pathum Thani\n",
    "#let's say the answer is Pathum Thani\n",
    "\n",
    "print(\"*\" * 5)\n",
    "p1 = torch.tensor([[2, 3, 2, 10, 5]])\n",
    "print(\"p1 shape: \", p1.shape)\n",
    "p1_unsqueezed = p1.unsqueeze(2)\n",
    "print(f\"{p1_unsqueezed=}\")\n",
    "print(f\"{p1_unsqueezed.shape=}\")\n",
    "\n",
    "print(\"*\" * 5)\n",
    "p2 = torch.tensor([[1, 3, 2, 5, 10]])\n",
    "print(\"p2 shape: \", p2.shape)\n",
    "p2_unsqueezed = p2.unsqueeze(1)\n",
    "print(f\"{p2_unsqueezed=}\")\n",
    "print(f\"{p2_unsqueezed.shape}\")\n",
    "\n",
    "print(\"*\" * 5)\n",
    "score = p1_unsqueezed + p2_unsqueezed + mask\n",
    "'''\n",
    "[6 3 2 10 6     [6 3 2 6 10    [0 0 0 0 0\n",
    " 6 3 2 10 6      6 3 2 6 10     - 0 0 0 0\n",
    " 6 3 2 10 6   +  6 3 2 6 10  +  - - 0 0 0\n",
    " 6 3 2 10 6      6 3 2 6 10     - - - 0 0\n",
    " 6 3 2 10 6]     6 3 2 6 10]    - - - - 0]\n",
    "'''\n",
    "print(f\"{score=}\")\n",
    "\n",
    "print(\"*\" * 5)\n",
    "#first, max along the p1 axis\n",
    "score, s_idx = score.max(dim=1)\n",
    "print(f\"{score=}\")\n",
    "print(f\"{s_idx=}\")\n",
    "\n",
    "print(\"*\" * 5)\n",
    "#first, max along the p2 axis\n",
    "score, e_idx = score.max(dim=1)\n",
    "print(f\"{score=}\")\n",
    "print(f\"{e_idx=}\")\n",
    "\n",
    "print(\"*\" * 5)\n",
    "#torch.gather(input, dim, index)\n",
    "s_idx = torch.gather(s_idx, 1, e_idx.view(-1, 1)).squeeze()\n",
    "print(f\"{s_idx=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions):\n",
    "    '''\n",
    "    Gets a dictionary of predictions with question_id as key\n",
    "    and prediction as value. The validation dataset has multiple \n",
    "    answers for a single question. Hence we compare our prediction\n",
    "    with all the answers and choose the one that gives us\n",
    "    the maximum metric (em or f1). \n",
    "    This method first parses the JSON file, gets all the answers\n",
    "    for a given id and then passes the list of answers and the \n",
    "    predictions to calculate em, f1.\n",
    "    \n",
    "    \n",
    "    :param dict predictions\n",
    "    Returns\n",
    "    : exact_match: 1 if the prediction and ground truth \n",
    "      match exactly, 0 otherwise.\n",
    "    : f1_score: \n",
    "    '''\n",
    "    with open('./data/squad_dev.json','r',encoding='utf-8') as f:\n",
    "        dataset = json.load(f)\n",
    "        \n",
    "    dataset = dataset['data']\n",
    "    f1 = exact_match = total = 0\n",
    "    for article in dataset:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                total += 1\n",
    "                if qa['id'] not in predictions:\n",
    "                    continue\n",
    "                \n",
    "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n",
    "                \n",
    "                prediction = predictions[qa['id']]\n",
    "                \n",
    "                exact_match += metric_max_over_ground_truths(\n",
    "                    exact_match_score, prediction, ground_truths)\n",
    "                \n",
    "                f1 += metric_max_over_ground_truths(\n",
    "                    f1_score, prediction, ground_truths)\n",
    "                \n",
    "    \n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "    \n",
    "    return exact_match, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re\n",
    "\n",
    "def normalize_answer(s):\n",
    "    '''\n",
    "    Performs a series of cleaning steps on the ground truth and \n",
    "    predicted answer.\n",
    "    '''\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    '''\n",
    "    Returns maximum value of metrics for predicition by model against\n",
    "    multiple ground truths.\n",
    "    \n",
    "    :param func metric_fn: can be 'exact_match_score' or 'f1_score'\n",
    "    :param str prediction: predicted answer span by the model\n",
    "    :param list ground_truths: list of ground truths against which\n",
    "                               metrics are calculated. Maximum values of \n",
    "                               metrics are chosen.\n",
    "                            \n",
    "    \n",
    "    '''\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "        \n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    '''\n",
    "    Returns f1 score of two strings.\n",
    "    '''\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    '''\n",
    "    Returns exact_match_score of two strings.\n",
    "    '''\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    '''\n",
    "    Helper function to record epoch time.\n",
    "    '''\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting validation .........\n",
      "Starting batch 0\n",
      "Epoch train loss : 7.143351714332383| Time: 1m 34s\n",
      "Epoch valid loss: 6.428433150440068\n",
      "Epoch EM: 2.866603595080416\n",
      "Epoch F1: 4.1844099085305135\n",
      "====================================================================================\n",
      "Epoch 2\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting validation .........\n",
      "Starting batch 0\n",
      "Epoch train loss : 5.625602439626471| Time: 1m 33s\n",
      "Epoch valid loss: 5.678070783615112\n",
      "Epoch EM: 4.2951750236518444\n",
      "Epoch F1: 5.7572854482205775\n",
      "====================================================================================\n",
      "Epoch 3\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting validation .........\n",
      "Starting batch 0\n",
      "Epoch train loss : 4.895061938019542| Time: 1m 33s\n",
      "Epoch valid loss: 5.235319673240959\n",
      "Epoch EM: 5.10879848628193\n",
      "Epoch F1: 6.621075770924116\n",
      "====================================================================================\n",
      "Epoch 4\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting validation .........\n",
      "Starting batch 0\n",
      "Epoch train loss : 4.362497874668667| Time: 1m 34s\n",
      "Epoch valid loss: 5.074764686745483\n",
      "Epoch EM: 5.373699148533586\n",
      "Epoch F1: 6.813977891061937\n",
      "====================================================================================\n",
      "Epoch 5\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting validation .........\n",
      "Starting batch 0\n",
      "Epoch train loss : 3.9603334696261916| Time: 1m 34s\n",
      "Epoch valid loss: 5.118560362171817\n",
      "Epoch EM: 5.534531693472091\n",
      "Epoch F1: 7.056964219619483\n",
      "====================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "ems = []\n",
    "f1s = []\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_dataset)\n",
    "    valid_loss, em, f1 = valid(model, valid_dataset)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    ems.append(em)\n",
    "    f1s.append(f1)\n",
    "    \n",
    "    print(f\"Epoch train loss : {train_loss}| Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"Epoch valid loss: {valid_loss}\")\n",
    "    print(f\"Epoch EM: {em}\")\n",
    "    print(f\"Epoch F1: {f1}\")\n",
    "    print(\"====================================================================================\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Papers read/referenced\n",
    "    1. https://arxiv.org/abs/1704.00051\n",
    "    2. https://arxiv.org/abs/1606.02858\n",
    "    3. https://arxiv.org/abs/1409.0473\n",
    "* Other helpful links\n",
    "    1. https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html\n",
    "    2. https://github.com/facebookresearch/DrQA\n",
    "    3. https://github.com/hitvoice/DrQA. Special thanks to [Runqi Yang](https://github.com/hitvoice) who helped me clarify some doubts with respect to preprocessing the SQUAD dataset.\n",
    "    4. https://towardsdatascience.com/the-definitive-guide-to-bidaf-part-3-attention-92352bbdcb07. Good introduction to attention.\n",
    "    5. https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/lectures/lecture10.pdf. The attention section of this notebook is largely inspired and derived from these slides.\n",
    "* Following links are related to debugging neural nets. Something on which I was stuck for quite some time during training these models.\n",
    "    1. https://datascience.stackexchange.com/questions/410/choosing-a-learning-rate\n",
    "    2. https://www.jeremyjordan.me/nn-learning-rate/\n",
    "    3. https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0\n",
    "    4. https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n",
    "    5. https://towardsdatascience.com/checklist-for-debugging-neural-networks-d8b2a9434f21\n",
    "    6. https://arxiv.org/abs/1708.07120\n",
    "    7. https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6 (default, Nov 17 2020, 08:01:36) \n[Clang 12.0.0 (clang-1200.0.32.21)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "714d3f4db9a58ba7d2f2a9a4fffe577af3df8551aebd380095064812e2e0a6a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
