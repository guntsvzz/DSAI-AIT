{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## BiDAF\n",
    "\n",
    "This notebook implements one of the most important papers in NLP literature: [BiDAF](https://arxiv.org/abs/1611.01603) or Bidirectional Attention Flow for Machine Comprehension. The key issue that this paper tries to address is that of *early summarization* in all the earlier approches that use attention mechanisms. The attention mechanisms until then were used to obtain a fixed-size summarization of given values and query. This, according to the authors leads to early summarization and loss of information. Moreover, previously, attention was only calculated in only one direction. To improve upon these issues, the authors propose a *hierarchical, multi-stage network*.   \n",
    "> *Our attention layer is not used to summarize the context paragraph into a ﬁxed-size vector. Instead, the attention is computed for every time step, and the attended vector at each time step, along with the representations from previous layers, is allowed to ﬂow through to the subsequent modeling layer. *\n",
    "\n",
    "Most loading and preprocessing is the same, except that **BiDAF also uses character embeddings**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load SQuAD\n",
    "\n",
    "No changes here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment this if you are not using AIT proxy...\n",
    "import os\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_json(path):\n",
    "    '''\n",
    "    Loads the JSON file of the Squad dataset.\n",
    "    Returns the json object of the dataset.\n",
    "    '''\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    print(\"Length of data: \", len(data['data']))\n",
    "    print(\"Data Keys: \", data['data'][0].keys())\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data:  442\n",
      "Data Keys:  dict_keys(['title', 'paragraphs'])\n",
      "Length of data:  48\n",
      "Data Keys:  dict_keys(['title', 'paragraphs'])\n"
     ]
    }
   ],
   "source": [
    "# load dataset json files\n",
    "train_data = load_json('data/squad_train.json')\n",
    "valid_data = load_json('data/squad_dev.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "### 2.1 Parse to dict\n",
    "\n",
    "No changes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(data:dict)->list:\n",
    "    \n",
    "    data = data['data']\n",
    "    qa_list = []\n",
    "\n",
    "    for paragraphs in data:\n",
    "\n",
    "        for para in paragraphs['paragraphs']:\n",
    "            context = para['context']\n",
    "\n",
    "            for qa in para['qas']:\n",
    "                \n",
    "                id = qa['id']\n",
    "                question = qa['question']\n",
    "                \n",
    "                for ans in qa['answers']:\n",
    "                    answer = ans['text']\n",
    "                    ans_start = ans['answer_start']\n",
    "                    ans_end = ans_start + len(answer)\n",
    "                    \n",
    "                    #one row of data\n",
    "                    qa_dict = {}\n",
    "                    qa_dict['id'] = id\n",
    "                    qa_dict['context'] = context\n",
    "                    qa_dict['question'] = question\n",
    "                    qa_dict['label'] = [ans_start, ans_end]\n",
    "                    qa_dict['answer'] = answer\n",
    "                    \n",
    "                    #append to a list of rows/dicts\n",
    "                    qa_list.append(qa_dict)    \n",
    "\n",
    "    return qa_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the json structure to return the data as a list of dictionaries\n",
    "train_list = parse_data(train_data)\n",
    "valid_list = parse_data(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train list len:  87599\n",
      "Valid list len:  34726\n"
     ]
    }
   ],
   "source": [
    "print('Train list len: ',len(train_list))\n",
    "print('Valid list len: ',len(valid_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#minimize the train_list and valid_list for easy debugging\n",
    "#uncomment this for toy training\n",
    "# train_list = train_list[:32]  #just enough for one batch\n",
    "# valid_list = valid_list[:32]\n",
    "train_list = train_list[:10000]\n",
    "valid_list = valid_list[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>[515, 541]</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>[188, 213]</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>[279, 296]</td>\n",
       "      <td>the Main Building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>[381, 420]</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>[92, 126]</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  5733be284776f41900661182   \n",
       "1  5733be284776f4190066117f   \n",
       "2  5733be284776f41900661180   \n",
       "3  5733be284776f41900661181   \n",
       "4  5733be284776f4190066117e   \n",
       "\n",
       "                                             context  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "3  Architecturally, the school has a Catholic cha...   \n",
       "4  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                            question       label  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...  [515, 541]   \n",
       "1  What is in front of the Notre Dame Main Building?  [188, 213]   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...  [279, 296]   \n",
       "3                  What is the Grotto at Notre Dame?  [381, 420]   \n",
       "4  What sits on top of the Main Building at Notre...   [92, 126]   \n",
       "\n",
       "                                    answer  \n",
       "0               Saint Bernadette Soubirous  \n",
       "1                a copper statue of Christ  \n",
       "2                        the Main Building  \n",
       "3  a Marian place of prayer and reflection  \n",
       "4       a golden statue of the Virgin Mary  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# converting the lists into dataframes for easy access\n",
    "train_df = pd.DataFrame(train_list)\n",
    "valid_df = pd.DataFrame(valid_list)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    \n",
    "    def to_lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    df.context = df.context.apply(to_lower)\n",
    "    df.question = df.question.apply(to_lower)\n",
    "    df.answer = df.answer.apply(to_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_df(train_df)\n",
    "preprocess_df(valid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Numericalization (building vocabs)\n",
    "\n",
    "The part the changes is that here we shall build **character vocabs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_text_for_vocab(dfs:list):    \n",
    "    text = []\n",
    "    total = 0\n",
    "    for df in dfs:\n",
    "        unique_contexts = list(df.context.unique())\n",
    "        unique_questions = list(df.question.unique())\n",
    "        total += df.context.nunique() + df.question.nunique()\n",
    "        text.extend(unique_contexts + unique_questions)\n",
    "    \n",
    "    assert len(text) == total\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.8 ms, sys: 4.31 ms, total: 38.1 ms\n",
      "Wall time: 36.8 ms\n",
      "Number of unique sentences in dataset:  13706\n"
     ]
    }
   ],
   "source": [
    "# gather text to build vocabularies\n",
    "%time vocab_text = gather_text_for_vocab([train_df, valid_df])\n",
    "print(\"Number of unique sentences in dataset: \", len(vocab_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'architecturally, the school has a catholic character. atop the main building\\'s gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \"venite ad me omnes\". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive (and in a direct line that connects through 3 statues and the gold dome), is a simple, modern stone statue of mary.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example\n",
    "vocab_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use spacy to help tokenize\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def build_word_vocab(vocab_text):\n",
    "\n",
    "    words = []\n",
    "    for sent in vocab_text:\n",
    "        for word in nlp(sent, disable=['parser', 'ner']):  #disable so it's fast....\n",
    "            words.append(word.text)\n",
    "\n",
    "    word_counter = Counter(words)\n",
    "    word_vocab = sorted(word_counter, key=word_counter.get, reverse=True) #list of words sorted by frequency\n",
    "    print(f\"raw-vocab: {len(word_vocab)}\")\n",
    "    word_vocab.insert(0, '<unk>')\n",
    "    word_vocab.insert(1, '<pad>')\n",
    "    print(f\"vocab-length: {len(word_vocab)}\")\n",
    "    word2idx = {word:idx for idx, word in enumerate(word_vocab)}\n",
    "    print(f\"word2idx-length: {len(word2idx)}\")\n",
    "    idx2word = {v:k for k,v in word2idx.items()}\n",
    "    \n",
    "    return word2idx, idx2word, word_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw-vocab: 23598\n",
      "vocab-length: 23600\n",
      "word2idx-length: 23600\n",
      "CPU times: user 41.3 s, sys: 51.3 ms, total: 41.4 s\n",
      "Wall time: 41.3 s\n"
     ]
    }
   ],
   "source": [
    "# build word vocabulary\n",
    "%time word2idx, idx2word, word_vocab = build_word_vocab(vocab_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we need to build vocab for character as well**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_char_vocab(vocab_text):\n",
    "    '''\n",
    "    Builds a character-level vocabulary from the given text.\n",
    "    \n",
    "    :param list vocab_text: list of contexts and questions\n",
    "    :returns \n",
    "        dict char2idx: character to index mapping of words\n",
    "        list char_vocab: list of characters sorted by frequency\n",
    "    '''\n",
    "    \n",
    "    chars = []\n",
    "    for sent in vocab_text:\n",
    "        for ch in sent:\n",
    "            chars.append(ch)\n",
    "\n",
    "    char_counter = Counter(chars)\n",
    "    char_vocab = sorted(char_counter, key=char_counter.get, reverse=True)\n",
    "    print(f\"raw-char-vocab: {len(char_vocab)}\")\n",
    "    high_freq_char = [char for char, count in char_counter.items() if count>=20]\n",
    "    char_vocab = list(set(char_vocab).intersection(set(high_freq_char)))\n",
    "    print(f\"char-vocab-intersect: {len(char_vocab)}\")\n",
    "    char_vocab.insert(0,'<unk>')\n",
    "    char_vocab.insert(1,'<pad>')\n",
    "    char2idx = {char:idx for idx, char in enumerate(char_vocab)}\n",
    "    print(f\"char2idx-length: {len(char2idx)}\")\n",
    "    \n",
    "    return char2idx, char_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw-char-vocab: 362\n",
      "char-vocab-intersect: 76\n",
      "char2idx-length: 78\n",
      "CPU times: user 207 ms, sys: 7.92 ms, total: 214 ms\n",
      "Wall time: 214 ms\n"
     ]
    }
   ],
   "source": [
    "%time char2idx, char_vocab = build_char_vocab(vocab_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to convert our context and question into ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ids(text, word2idx):\n",
    "    \n",
    "    tokens = [w.text for w in nlp(text, disable=['parser','ner'])]\n",
    "    ids = [word2idx[word] for word in tokens]\n",
    "    \n",
    "    assert len(ids) == len(tokens)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 6s, sys: 11.4 ms, total: 1min 6s\n",
      "Wall time: 1min 6s\n",
      "CPU times: user 31.3 s, sys: 0 ns, total: 31.3 s\n",
      "Wall time: 31.3 s\n",
      "CPU times: user 20.9 s, sys: 0 ns, total: 20.9 s\n",
      "Wall time: 20.9 s\n",
      "CPU times: user 10.4 s, sys: 3.78 ms, total: 10.4 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "# numericalize context and questions for training and validation set\n",
    "%time train_df['context_ids'] = train_df.context.apply(convert_to_ids, word2idx=word2idx)\n",
    "%time valid_df['context_ids'] = valid_df.context.apply(convert_to_ids, word2idx=word2idx)\n",
    "\n",
    "%time train_df['question_ids'] = train_df.question.apply(convert_to_ids,  word2idx=word2idx)\n",
    "%time valid_df['question_ids'] = valid_df.question.apply(convert_to_ids,  word2idx=word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we remove any indices that have error.  Error here means the actual answer does not appear in the context, probably due to data entry errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_indices(df, idx2word):\n",
    "    \n",
    "    start_value_error, end_value_error, assert_error = test_indices(df, idx2word)\n",
    "    err_idx = start_value_error + end_value_error + assert_error\n",
    "    err_idx = set(err_idx)\n",
    "    print(f\"Number of error indices: {len(err_idx)}\")\n",
    "    \n",
    "    return err_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_indices(df, idx2word):\n",
    "   \n",
    "    start_value_error = []\n",
    "    end_value_error = []\n",
    "    assert_error = []\n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        #get all answer tokens\n",
    "        answer_tokens = [w.text for w in nlp(row['answer'], disable=['parser','ner'])]\n",
    "\n",
    "        start_token = answer_tokens[0]\n",
    "        end_token = answer_tokens[-1]\n",
    "        \n",
    "        #get context tokens, and their start and end position\n",
    "        context_span  = [(word.idx, word.idx + len(word.text)) \n",
    "                         for word in nlp(row['context'], disable=['parser','ner'])]\n",
    "\n",
    "        #get starts from the first pair of the tuple, and ends from the second pair\n",
    "        starts, ends = zip(*context_span)\n",
    "\n",
    "        #ground truth indices\n",
    "        answer_start, answer_end = row['label']\n",
    "\n",
    "        try:\n",
    "            #try to find answer_start from starts\n",
    "            start_idx = starts.index(answer_start)\n",
    "        except:\n",
    "            start_value_error.append(index)\n",
    "        try:\n",
    "            #try to find answer_start from starts\n",
    "            end_idx  = ends.index(answer_end)\n",
    "        except:\n",
    "            end_value_error.append(index)\n",
    "\n",
    "        try:\n",
    "            #just to make sure that the idx2word convert back to the answer_token...\n",
    "            #otherwise, the ground truth cannot work.....\n",
    "            assert idx2word[row['context_ids'][start_idx]] == answer_tokens[0]\n",
    "            assert idx2word[row['context_ids'][end_idx]]   == answer_tokens[-1]\n",
    "        except:\n",
    "            assert_error.append(index)\n",
    "\n",
    "\n",
    "    return start_value_error, end_value_error, assert_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of error indices: 169\n",
      "Number of error indices: 95\n"
     ]
    }
   ],
   "source": [
    "# get indices with tokenization errors and drop those indices \n",
    "train_err = get_error_indices(train_df, idx2word)\n",
    "valid_err = get_error_indices(valid_df, idx2word)\n",
    "\n",
    "train_df.drop(train_err, inplace=True)\n",
    "valid_df.drop(valid_err, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to last part, we need to get the label answer based on our tokenized dataset.  That is, we should calculate the spans and then return a tuple of start and end positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_answer(row, idx2word):\n",
    "    \n",
    "    context_span = [(word.idx, word.idx + len(word.text)) for word in nlp(row.context, disable=['parser','ner'])]\n",
    "    starts, ends = zip(*context_span)\n",
    "    \n",
    "    #finding the spans\n",
    "    answer_start, answer_end = row.label\n",
    "    start_idx = starts.index(answer_start)\n",
    "    end_idx   = ends.index(answer_end)\n",
    "    \n",
    "    #double check\n",
    "    ans_toks = [w.text for w in nlp(row.answer,disable=['parser','ner'])]\n",
    "    ans_start = ans_toks[0]\n",
    "    ans_end  = ans_toks[-1]\n",
    "    assert idx2word[row.context_ids[start_idx]] == ans_start\n",
    "    assert idx2word[row.context_ids[end_idx]]   == ans_end\n",
    "    \n",
    "    return [start_idx, end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get start and end positions of answers from the context\n",
    "# this is basically the label for training QA models\n",
    "train_label_idx = train_df.apply(index_answer, axis=1, idx2word=idx2word)\n",
    "valid_label_idx = valid_df.apply(index_answer, axis=1, idx2word=idx2word)\n",
    "\n",
    "train_df['label_idx'] = train_label_idx\n",
    "valid_df['label_idx'] = valid_label_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Dump data to pickle files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# dump to pickle files\n",
    "train_df.to_pickle('bidaftrain.pkl')\n",
    "valid_df.to_pickle('bidafvalid.pkl')\n",
    "\n",
    "with open('bidafw2id.pickle','wb') as handle:\n",
    "    pickle.dump(word2idx, handle)\n",
    "\n",
    "with open('bidafc2id.pickle','wb') as handle:\n",
    "    pickle.dump(char2idx, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Read data from pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from pickle files\n",
    "train_df = pd.read_pickle('bidaftrain.pkl')\n",
    "valid_df = pd.read_pickle('bidafvalid.pkl')\n",
    "\n",
    "with open('bidafw2id.pickle','rb') as handle:\n",
    "    word2idx = pickle.load(handle)\n",
    "with open('bidafc2id.pickle','rb') as handle:\n",
    "    char2idx = pickle.load(handle)\n",
    "\n",
    "idx2word = {v:k for k,v in word2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Preparing Dataloader/Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset:\n",
    "    '''\n",
    "    - Creates batches dynamically by padding to the length of largest example\n",
    "      in a given batch.\n",
    "    - Calulates character vectors for contexts and question.\n",
    "    - Returns tensors for training.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data, batch_size):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        data = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]\n",
    "        self.data = data\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def make_char_vector(self, max_sent_len, max_word_len, sentence):\n",
    "        \n",
    "        char_vec = torch.ones(max_sent_len, max_word_len).type(torch.LongTensor)\n",
    "        \n",
    "        for i, word in enumerate(nlp(sentence, disable=['parser', 'ner'])):\n",
    "            for j, ch in enumerate(word.text):\n",
    "                char_vec[i][j] = char2idx.get(ch, 0)  #vector of character ids, e.g., the ==> 67, 42, 41 (but will be later padded to equal max length)\n",
    "        \n",
    "        return char_vec    \n",
    "    \n",
    "    def get_span(self, text):\n",
    "        \n",
    "        text = nlp(text, disable=['parser','ner'])\n",
    "        span = [(w.idx, w.idx+len(w.text)) for w in text]\n",
    "\n",
    "        return span\n",
    "\n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        Creates batches of data and yields them.\n",
    "        \n",
    "        Each yield comprises of:\n",
    "        :padded_context: padded tensor of contexts for each batch \n",
    "        :padded_question: padded tensor of questions for each batch \n",
    "        :char_ctx & ques_ctx: character-level ids for context and question\n",
    "        :label: start and end index wrt context_ids\n",
    "        :context_text,answer_text: used while validation to calculate metrics\n",
    "        :ids: question_ids for evaluation\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        for batch in self.data:\n",
    "            \n",
    "            spans = []\n",
    "            ctx_text = []\n",
    "            answer_text = []\n",
    "            \n",
    "            max_context_len = max([len(ctx) for ctx in batch.context_ids])\n",
    "            padded_context = torch.LongTensor(len(batch), max_context_len).fill_(1)\n",
    "            \n",
    "            for ctx in batch.context:\n",
    "                ctx_text.append(ctx)\n",
    "                spans.append(self.get_span(ctx))\n",
    "            \n",
    "            for ans in batch.answer:\n",
    "                answer_text.append(ans)\n",
    "            \n",
    "            for i, ctx in enumerate(batch.context_ids):\n",
    "                padded_context[i, :len(ctx)] = torch.LongTensor(ctx)\n",
    "                \n",
    "            max_question_len = max([len(ques) for ques in batch.question_ids])\n",
    "            padded_question = torch.LongTensor(len(batch), max_question_len).fill_(1)\n",
    "               \n",
    "            max_word_ctx = 0\n",
    "            for context in batch.context:\n",
    "                for word in nlp(context, disable=['parser','ner']):\n",
    "                    if len(word.text) > max_word_ctx:\n",
    "                        max_word_ctx = len(word.text)\n",
    "            \n",
    "            char_ctx = torch.ones(len(batch), max_context_len, max_word_ctx).type(torch.LongTensor)\n",
    "            for i, context in enumerate(batch.context):\n",
    "                char_ctx[i] = self.make_char_vector(max_context_len, max_word_ctx, context)\n",
    "            \n",
    "            for i, ques in enumerate(batch.question_ids):\n",
    "                padded_question[i, :len(ques)] = torch.LongTensor(ques)\n",
    "                \n",
    "            max_word_ques = 0\n",
    "            for question in batch.question:\n",
    "                for word in nlp(question, disable=['parser','ner']):\n",
    "                    if len(word.text) > max_word_ques:\n",
    "                        max_word_ques = len(word.text)\n",
    "            \n",
    "            char_ques = torch.ones(len(batch), max_question_len, max_word_ques).type(torch.LongTensor)\n",
    "            for i, question in enumerate(batch.question):\n",
    "                char_ques[i] = self.make_char_vector(max_question_len, max_word_ques, question)\n",
    "            \n",
    "            ids = list(batch.id)  \n",
    "            label = torch.LongTensor(list(batch.label_idx))\n",
    "            \n",
    "            yield (padded_context, padded_question, char_ctx, char_ques, label, ctx_text, answer_text, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SquadDataset(train_df, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = SquadDataset(valid_df, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'architecturally, the school has a catholic character. atop the main building\\'s gold dome is a golden statue of the virgin mary. immediately in front of the main building and facing it, is a copper statue of christ with arms upraised with the legend \"venite ad me omnes\". next to the main building is the basilica of the sacred heart. immediately behind the basilica is the grotto, a marian place of prayer and reflection. it is a replica of the grotto at lourdes, france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858. at the end of the main drive (and in a direct line that connects through 3 statues and the gold dome), is a simple, modern stone statue of mary.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#actual context\n",
    "a[5][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' architecturally , the school has a catholic character . atop the main building \\'s gold dome is a golden statue of the virgin mary . immediately in front of the main building and facing it , is a copper statue of christ with arms upraised with the legend \" venite ad me omnes \" . next to the main building is the basilica of the sacred heart . immediately behind the basilica is the grotto , a marian place of prayer and reflection . it is a replica of the grotto at lourdes , france where the virgin mary reputedly appeared to saint bernadette soubirous in 1858 . at the end of the main drive ( and in a direct line that connects through 3 statues and the gold dome ) , is a simple , modern stone statue of mary . <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#padded context\n",
    "string = ''\n",
    "for id in a[0][0]:\n",
    "    id = id.item()\n",
    "    string = string + \" \" + (idx2word[id])\n",
    "    \n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([60, 26, 13, 23, 58, 19, 66, 13, 19, 17, 26, 60,  3,  3, 73])\n"
     ]
    }
   ],
   "source": [
    "#padded context character\n",
    "string = ''\n",
    "for id in a[2][0]:\n",
    "    print(id)\n",
    "    break\n",
    "    #e.g., first vector refers to achitecturally\n",
    "    #etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([253, 15])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[2][0].shape  #word, char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BiDAF Model\n",
    "\n",
    "<img src = \"images/bidaf.png\" width=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Word Embed Layer\n",
    "\n",
    "Since the paper uses 100d version of glove, we gonna use as well.  (not sure why...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_glove_matrix():\n",
    "    glove_dict = {}\n",
    "    with open(\"glove.6B.100d.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "            glove_dict[word] = vector\n",
    "    f.close()\n",
    "    \n",
    "    return glove_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dict = create_glove_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_embedding(glove_dict):\n",
    "    '''\n",
    "    Creates a weight matrix of the words that are common in the GloVe vocab and\n",
    "    the dataset's vocab. Initializes OOV words with a zero vector.\n",
    "    '''\n",
    "    weights_matrix = np.zeros((len(word_vocab), 100))\n",
    "    words_found = 0\n",
    "    for i, word in enumerate(word_vocab):\n",
    "        try:\n",
    "            weights_matrix[i] = glove_dict[word]\n",
    "            words_found += 1\n",
    "        except:\n",
    "            pass\n",
    "    return weights_matrix, words_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_matrix, words_found = create_word_embedding(glove_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words found in glove vocab:  21386\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words found in glove vocab: \", words_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('bidafglove_vt.npy',weights_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Character Embed Layer\n",
    "\n",
    "A character embedding is calculated for each context and query word. This is done by using convolutions. The intuition is simple over here. We first pass each word through an embedding layer to get a fixed size vector. Let the embedding dimension be $d$.\n",
    "Let $C$ represent a matrix representation of word of length $l$. Therefore $C$ is a matrix with dimensions $d$ x $l$.\n",
    "<img src=\"images/charemb1.PNG\" width=\"500\" height=\"300\"/>\n",
    "\n",
    "Let $H$ represent a convolutional filter with dimensions $d$ x $w$, where $d$ is the embedding dimension and $w$ is the width or the window size of the filter.   \n",
    "The weights of this filter are randomly initialized and learnt parallelly via backpropogation. We convolve this filter $H$ over our word representation $C$ as shown below. \n",
    "<img src=\"images/charemb2.PNG\" width=\"500\" height=\"400\"/>  \n",
    "\n",
    "The convolution operation is simply the inner product of the filter $H$ and matrix $C$. The convolution operations can be visualized as follows:\n",
    "<img src=\"images/charemb3.PNG\" width=\"900\" height=\"700\"/>    \n",
    "The result of the above operation is a feature vector. A single filter is usually associated with a unique feature that it captures from the image/matrix. To get the most representative value related to the feature, we perform max pooling over the dimension of this vector.\n",
    "<img src=\"images/charemb4.PNG\" width=\"500\" height=\"300\"/>     \n",
    "\n",
    "The above process was described for a single filter. This same process is repeated with $N$ number of filters. Each of these filters captures a different property of word. In an image, for example, if one filter captures the edges, another filter will capture the texture and another one the shapes in the image and so on. $N$ is also the size of the desired character embedding. In this paper authors have trained the model with $N$ = 100.  \n",
    "\n",
    "<!-- The implementation of this layer is fairly straightforward.  The input to this layer is of dimension `[batch_size, seq_len, word_len]` where `seq_len` and `word_len` are the lengths of largest sequence and word respectively within a given batch . We first embed the character tokens into a fixed size vector using an embedding layer. This gives a vector of dimension `[batch_size, seq_len, word_len, emb_dim]`.   \n",
    "We then convert this tensor into a format that closely resembles an image, of type [ $N$, $C_{in}$, $H_{in}$, $W_{in}$]. The number of input channels, $C_{in}$ would be 1 and the output channels would be the desired embedding size which is 100. This is then passed through the convolution layer which gives an output of shape [ $N$, $C_{out}$, $H_{out}$, $W_{out}$]. Here,\n",
    "\n",
    "<img src=\"images/conv.PNG\" width=\"600\" height=\"500\"/>  \n",
    "\n",
    "If `padding` = [0,0], `kernel_size` (or filter_size) = [$H_{in}$, $w$], `dilation` = [1,1], `stride` = [1,1] (as visible in images above), then,   \n",
    "$H_{out}$ = 1, and $W_{out}$ = $W_{in}$ - $w$ - 1.\n",
    "\n",
    "Since $H_{out}$ = 1, we squeeze that dimension and perform max pooling with a kernel_size = $L_{in}$. The value of $L_{in}$ =  $W_{in}$ - $w$ - 1.\n",
    "\n",
    "<img src=\"images/maxpool.PNG\" width=\"600\" height=\"500\"/>     \n",
    "If the kernel size = $L_{in}$, we get $L_{out}$ = 1 if other values are default. This dimension is again squeezed to finally give us a tensor of dimension   `[batch_size, seq_len, output_channels (or 100)]`. \n",
    " -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterEmbeddingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, char_vocab_dim, char_emb_dim, num_output_channels, kernel_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.char_emb_dim = char_emb_dim\n",
    "        \n",
    "        self.char_embedding = nn.Embedding(char_vocab_dim, char_emb_dim, padding_idx=1)\n",
    "        \n",
    "        self.char_convolution = nn.Conv2d(in_channels=1, out_channels=100, kernel_size=kernel_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = [bs, seq_len, word_len]\n",
    "        # returns : [batch_size, seq_len, num_output_channels]\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        x = self.dropout(self.char_embedding(x))\n",
    "        # x = [bs, seq_len, word_len, char_emb_dim]\n",
    "        \n",
    "        # following three operations manipulate x in such a way that\n",
    "        # it closely resembles an image. this format is important before \n",
    "        # we perform convolution on the character embeddings.\n",
    "        \n",
    "        x = x.permute(0,1,3,2)\n",
    "        # x = [bs, seq_len, char_emb_dim, word_len]\n",
    "        \n",
    "        x = x.view(-1, self.char_emb_dim, x.shape[3])\n",
    "        # x = [bs*seq_len, char_emb_dim, word_len]  #make char to height and word_len to width\n",
    "        \n",
    "        x = x.unsqueeze(1)\n",
    "        # x = [bs*seq_len, 1, char_emb_dim, word_len]\n",
    "        \n",
    "        # x is now in a format that can be accepted by a conv layer. \n",
    "        # think of the tensor above in terms of an image of dimension\n",
    "        # (N, C_in, H_in, W_in).\n",
    "        \n",
    "        x = self.relu(self.char_convolution(x))\n",
    "        # x = [bs*seq_len, out_channels, H_out, W_out]\n",
    "        \n",
    "        x = x.squeeze()\n",
    "        # x = [bs*seq_len, out_channels, W_out]\n",
    "                \n",
    "        x = F.max_pool1d(x, x.shape[2]).squeeze()\n",
    "        # x = [bs*seq_len, out_channels, 1] => [bs*seq_len, out_channels]\n",
    "        \n",
    "        x = x.view(batch_size, -1, x.shape[-1])\n",
    "        # x = [bs, seq_len, out_channels]\n",
    "        # x = [bs, seq_len, features] = [bs, seq_len, 100]\n",
    "        \n",
    "        \n",
    "        return x        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Highway Networks\n",
    "\n",
    "Highway networks were originally introduced to ease the training of deep neural networks. While researchers had cracked the code for optimizing shallow neural networks, training *deep* networks was still a challenging task owing to problems such as vanishing gradients etc. **Note that highway networks and residual network are basically for the same problem; residual is just a special case of highway networks with weight of identity**.\n",
    "\n",
    "This paper takes the key idea of learned gating mechanism from LSTMs which process information internally through a sequence of learned gates. The purpose of this layer is to *learn* to pass relevant information from the input. A highway network is a series of feed-forward or linear layers with a gating mechanism. The gating is implemented by using a sigmoid function which decides what amount of information should be transformed and what should be passed as it is.   \n",
    "\n",
    "A plain feed-forward layer is associated with a linear transform $H$ parameterized by ($W_{H}, b_{H}$), such that for input $x$, the output $y$ is  \n",
    "\n",
    "$$ y = g(W_{H}.x + b_{H})$$\n",
    "where $g$ is a non-linear activation.  \n",
    "For highway networks, two additional linear transforms are defined viz. $T$ ($W_{T},b_{T}$) and $C$ ($W_{C}$,$b_{C}$).\n",
    "Then,    \n",
    "  \n",
    "$$ y = T(x) . H(x) + x . C(x) $$ \n",
    "> *We refer to T as the transform gate and C as the carry gate, since they express how much of the output is produced by\n",
    "transforming the input and carrying it, respectively. For simplicity, in this paper we set C = 1 − T. *\n",
    "\n",
    "$$ y = T(x) . H(x) + x . (1 - T(x)) $$  \n",
    "  \n",
    "$$ y = T(x) . g(W_{H}.x + b_{H}) + x . (1 - T(x)) $$  \n",
    "where $T(x)$ = $\\sigma$ ($W_{T}$ . $x$ + $b_{T}$) and $g$ is relu activation.  \n",
    "\n",
    "The input to this layer is the concatenation of word and character embeddings of each word. To implement this we use `nn.ModuleList` to add multiple linear layers. This is done for the gate layer as well as for a normal linear transform. In code the `flow_layer` is the same as linear transform $H$ discussed above and `gate_layer` is $T$. In the forward method we loop through each layer and compute the output according to the highway equation described above.   \n",
    "  \n",
    "The output of this layer for context is $X$ $\\epsilon$ $R^{\\ d \\ X \\ T}$ and for query is $Q$ $\\epsilon$ $R^{\\ d \\ X \\ J}$, where $d$ is hidden size of the LSTM, $T$ is the context length, $J$ is the query length.  \n",
    "\n",
    "The structure discussed so far is a recurring pattern in many NLP systems. Although this might be out of favor now with the advent of transformers and large pretrained language models, you will find this pattern in many NLP systems before transformers came into being. The idea behind this is that adding highway layers enables the network to make more efficient use of character embeddings. If a particular word is not found in the pretrained word vector vocabulary (OOV word), it will most likely be initialized with a zero vector. It then makes much more sense to look at the character embedding of that word rather than the word embedding. The soft gating mechanism in highway layers helps the model to achieve this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighwayNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, num_layers=2):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.flow_layer = nn.ModuleList([nn.Linear(input_dim, input_dim) for _ in range(num_layers)])\n",
    "        self.gate_layer = nn.ModuleList([nn.Linear(input_dim, input_dim) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            flow_value = F.relu(self.flow_layer[i](x))\n",
    "            gate_value = torch.sigmoid(self.gate_layer[i](x))\n",
    "            \n",
    "            x = gate_value * flow_value + (1-gate_value) * x\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Contextual Embed Layer\n",
    "\n",
    "This layer is the final embedding layer in the model.\n",
    "> *Bi-Directional Attention Flow (BIDAF) network, a hierarchical multi-stage architecture for modeling the representations ofthe context paragraph at different levels of granularity. BIDAF includes character-level, word-level, and contextual embeddings*\n",
    "\n",
    "The output of highway layers is passed to a bidirection LSTM to model the temporal features of the text. This is done for both, the context and the query.   \n",
    ">  *Utilizes contextual cues from surrounding words to reﬁne the embedding of the words*\n",
    "\n",
    "The output of this layer for context is called as $H$ $\\epsilon$ $R^{\\ 2d \\ X \\ T}$ and for query it is named as $U$ $\\epsilon$ $R^{\\ 2d \\  X \\  J}$. The $2d$ is because of the features from both forward and backward LSTMs. In code, we simply use the outputs of the LSTM layer and ignore the hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualEmbeddingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.highway_net = HighwayNetwork(input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = [bs, seq_len, input_dim] = [bs, seq_len, emb_dim*2]\n",
    "        # the input is the concatenation of word and characeter embeddings\n",
    "        # for the sequence.\n",
    "        \n",
    "        highway_out = self.highway_net(x)\n",
    "        # highway_out = [bs, seq_len, input_dim]\n",
    "        \n",
    "        outputs, _ = self.lstm(highway_out)\n",
    "        # outputs = [bs, seq_len, emb_dim*2]\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Attention Flow Layer\n",
    "\n",
    "The output of the previous layer: contextual representation of context $H$ and query $U$ are passed on to this layer. Until now processing of the context and the query has been independent of each other. This layer, however, is responsible for fusing and linking the context and query representations.   \n",
    "\n",
    "This layer calculates attention in two directions: from context to query and from query to context. Attention vectors for these calculations are derived from a common matrix which is called as the similarity matrix and is denoted by $S$ $\\epsilon$ $R^{\\ T \\ X \\ J}$. $T$ is the length of the context (number of words/tokens) and $J$ is the length of the query for each training example. The similarity matrix is computed by,\n",
    "$$ S_{tj} = \\alpha\\ (H_{:t}, U_{:j}) $$  \n",
    "\n",
    "where $S_{tj}$ $\\epsilon$ $R$.  \n",
    "$S_{tj}$ is a single float value that determines the similarity between the $t$-th context word and $j$-th query word.\n",
    "$\\alpha$ is a trainable function that encodes the similarity between two input vectors $H_{:t}$ and $U_{:j}$.  \n",
    "$H_{:t}$ is the contextual representation of the $t$-th context word and $U_{:j}$ is the contextual representation of the $j$-th query word. \n",
    "<img src=\"images/simimat.PNG\" width=\"500\" height=\"500\"/>     \n",
    "\n",
    "\n",
    "\n",
    "The trainable function is defined as,  \n",
    "\n",
    "$$ \\alpha \\ (h, u) = w_{(S)}^{T}\\ [h\\ ;\\ u \\ ; \\ h \\circ u]  $$\n",
    "where $;$ denotes concatenation and $\\circ$ denotes an element wise product.    \n",
    "$w_{T}^{S}$ is a trainable weight matrix of dimension $6d$. This is because $H$ $\\epsilon$ $R^{\\ 2d \\ X \\ T}$ and $U$ $\\epsilon$ $R^{\\ 2d \\ X \\ J}$, and we are concatenating 3 such vectors.  \n",
    "\n",
    "In code, all the computations above are performed directly by operating on matrices/ tensors. We first `repeat` the contextual representations of context and query along appropriate dimensions to get two tensors of shape `[batch_size, ctx_len, query_len, emb_dim*2 (d*2)]`. $w_{(S)}^T$ is characterized by a linear layer called `similarity_weight`. We concat the tensors along the last dimension and pass it through the linear layer. \n",
    "\n",
    "### Context-to-Query Attention\n",
    "\n",
    "> *. Context-to-query (C2Q) attention signiﬁes which query words are most relevant to each context word.*\n",
    " \n",
    "Let $a_{t}$ represent the vector that encodes the attention paid on each query word by the $t$-th context word.   \n",
    "$$ \\sum_{j}a_{tj} = 1  $$  \n",
    "$$ a_{t} = softmax(S_{:t}) $$\n",
    "where $a_{t}$ $\\epsilon$ $R^{J}$   \n",
    "This can be visualized as,\n",
    "<img src=\"images/c2q.PNG\" width=\"700\" height=\"750\"/>     \n",
    "\n",
    "Subsequently, each attended query vector is calculated by,\n",
    "$$ \\overline U_{:t} = \\sum_{j} a_{tj} U_{:j} $$\n",
    "\n",
    "This vector indicates the most important word in the query with respect to context.  \n",
    "\n",
    "In code, again vectorizing operations into tensors, this simply involves taking a softmax of the similarity matrix across the last dimension, i.e across the columns and multiplying this with $U$. The shape of similarity matrix is `[batch_size, ctx_len, query_len]`. The shape of $U$ is `[batch_size, query_len, emb_dim*2]` . Hence performing matrix multiplication by using `torch.bmm` would give us a tensor of shape `[batch_size, ctx_len, emb_dim*2 ]`. Therefore, $\\overline U$ will be $2d$ X $T$\n",
    " matrix.\n",
    "\n",
    "\n",
    "### Query-to-Context Attention\n",
    "\n",
    ">  *Query-to-context (Q2C) attention signiﬁes which context words have the closest similarity to one of the query words and are hence critical for answering the query.*\n",
    "\n",
    "The attention vector here is calculated as,  \n",
    "$$ b = softmax \\ (max_{col} \\ (S)) \\ \\epsilon R^{T} $$  \n",
    "where $max_{col}$ performs the maximum function across the column. The attended context vector is then calculated as\n",
    "\n",
    "$$ \\overline h = \\sum_{t} b_{t} H_{:t}$$\n",
    "\n",
    "The above equations can be visualized as,\n",
    "\n",
    "<img src=\"images/q2c.PNG\" width=\"800\" height=\"900\"/>\n",
    "\n",
    "The above figure helps us in understanding that for each training example we'll get a single $T$ - dimensional vector. This vector will then be multiplied by the contextual representation $H$ to get a single $2d$ vector.  To get the matrix $\\overline H$, we need to tile/repeat this vector $T$ times to get a $2d$-by-$T$ representation.\n",
    "This is unlike the previous attention we calculated where we directly got a $2d$-by-$T$ matrix.\n",
    "\n",
    "The implementation for this part is straightforward and very similar to the explanation above. We first perform maximum across columns using `torch.max`. This gives a tensor of shape `[batch_size, ctx_len]`. This is then passed through a softmax to get weights that add up to 1. We then insert an extra dimension in this tensor and multiply it with $H$. This gives a tensor of shape `[batch_size, 1, emb_dim*2]`. This is exactly what we have discussed above. This tensor corresponds to $\\overline h$. We then `repeat` this $T$ times to get $\\overline H$ of shape `[batch_size, ctx_len, emb_dim*2]`\n",
    "\n",
    "\n",
    "### 4.6 Combining attention vectors and contextual embeddings\n",
    "\n",
    "> *Finally, the contextual embeddings and the attention vectors are combined together to yield G, where each column vector can be considered as the query-aware representation of each context word.*\n",
    "\n",
    "$G$ is defined as,\n",
    "\n",
    "$$ G_{:t} = \\beta \\ (\\ H_{:t}\\ ;\\ \\overline U_{:t} \\ ;\\ \\overline H_{:t}  \\ )$$\n",
    "\n",
    "where $\\beta$ is a trainable function.  \n",
    "$$ \\beta (h, \\overline u, \\overline h) = [h\\ ;\\ \\overline u\\ ;\\ h \\circ \\overline u\\ ;\\ h \\circ \\overline h\\ ] $$\n",
    "\n",
    "This concatenation gives a $8d$ vector. Hence the trainable weight here should have an input dimension of $8d$. However here we don't involve a trainable weight because according to the authors,\n",
    "\n",
    "> *While the β function can be an arbitrary trainable neural network, such as multi-layer perceptron, a simple concatenation as following still shows good performance in our experiments.*\n",
    "\n",
    "In code, this simply involves a single line of concatenating tensors using `torch.cat` to yeild $G$. This tensor holds the query-aware representation of the context words.\n",
    "\n",
    "\n",
    "### 4.7 Modeling Layer\n",
    "\n",
    "$G$ is then passed on to this layer. This layer is responsible for capturing temporal features interactions among the context words. This is done using a bidirectional LSTM. The difference between this layer and the contextual layer, both of which involve an LSTM layer is that here we have a *query-aware* representation of the context while in the contextual layer, encoding of the context and query was independent.  \n",
    "Using an LSTM layer with a hidden size of $d$ we get a $2d$-by-$T$ output called $M$.  \n",
    "\n",
    "> *Each column vector of M is expected to contain contextual information about the word with respect to the entire context paragraph and the query.*\n",
    "\n",
    "\n",
    "### 4.8 Output Layer\n",
    "\n",
    "The start index $p^{1}$ of the answer span is calculated by,\n",
    "\n",
    "<img src=\"images/p1.PNG\" width=\"300\" height=\"300\"/>\n",
    "\n",
    "where $w_{(p^{1})}$ is a $10d$ trainable weight vector which corresponds to a linear layer in code called `output_start`.  \n",
    "To predict the end of the answer span, $M$ is once again passed to a bidirectional LSTM to get $M^{2}$ which again is a $2d$-by-$T$ matrix. The end is then computed as,  \n",
    "\n",
    "<img src=\"images/p2.PNG\" width=\"300\" height=\"300\"/>  \n",
    "\n",
    "\n",
    "$w_{(p^{2})}$ is again a $10d$ trainable weight vector and corresponds to the linear layer `output_end`. \n",
    "\n",
    "In code we don't explicitly calculate softmax since this is taken care of while calculating the losses during training. \n",
    "\n",
    "> *The output layer is application-speciﬁc. The modular nature of BIDAF allows us to easily swap out the output layer based on the task, with the rest of the architecture remaining exactly the same. *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiDAF(nn.Module):\n",
    "    \n",
    "    def __init__(self, char_vocab_dim, emb_dim, char_emb_dim, num_output_channels, \n",
    "                 kernel_size, ctx_hidden_dim, device):\n",
    "        '''\n",
    "        char_vocab_dim = len(char2idx)\n",
    "        emb_dim = 100\n",
    "        char_emb_dim = 8\n",
    "        num_output_chanels = 100\n",
    "        kernel_size = (8,5)\n",
    "        ctx_hidden_dim = 100\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.word_embedding = self.get_glove_embedding()\n",
    "        \n",
    "        self.character_embedding = CharacterEmbeddingLayer(char_vocab_dim, char_emb_dim, \n",
    "                                                      num_output_channels, kernel_size)\n",
    "        \n",
    "        self.contextual_embedding = ContextualEmbeddingLayer(emb_dim*2, ctx_hidden_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        self.similarity_weight = nn.Linear(emb_dim*6, 1, bias=False)\n",
    "        \n",
    "        self.modeling_lstm = nn.LSTM(emb_dim*8, emb_dim, bidirectional=True, num_layers=2, batch_first=True, dropout=0.2)\n",
    "        \n",
    "        self.output_start = nn.Linear(emb_dim*10, 1, bias=False)\n",
    "        \n",
    "        self.output_end = nn.Linear(emb_dim*10, 1, bias=False)\n",
    "        \n",
    "        self.end_lstm = nn.LSTM(emb_dim*2, emb_dim, bidirectional=True, batch_first=True)\n",
    "        \n",
    "    \n",
    "    def get_glove_embedding(self):\n",
    "        \n",
    "        weights_matrix = np.load('bidafglove_vt.npy')\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape\n",
    "        embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix).to(self.device),freeze=True)\n",
    "\n",
    "        return embedding\n",
    "        \n",
    "    def forward(self, ctx, ques, char_ctx, char_ques):\n",
    "        # ctx = [bs, ctx_len]\n",
    "        # ques = [bs, ques_len]\n",
    "        # char_ctx  = [bs, ctx_len, ctx_word_len]  #word_len refer to word with highest amount of chars; while len is seq len\n",
    "        # char_ques = [bs, ques_len, ques_word_len]\n",
    "        \n",
    "        ctx_len = ctx.shape[1]\n",
    "        ques_len = ques.shape[1]\n",
    "        \n",
    "        ## 1. Character embed layer\n",
    "        \n",
    "        ctx_char_embed = self.character_embedding(char_ctx)\n",
    "        # ctx_char_embed =  [bs, ctx_len, emb_dim]\n",
    "        \n",
    "        ques_char_embed = self.character_embedding(char_ques)\n",
    "        # ques_char_embed = [bs, ques_len, emb_dim]\n",
    "        \n",
    "        ## 2. Character embed layer\n",
    "        \n",
    "        ctx_word_embed = self.word_embedding(ctx)\n",
    "        # ctx_word_embed = [bs, ctx_len, emb_dim]\n",
    "        \n",
    "        ques_word_embed = self.word_embedding(ques)\n",
    "        # ques_word_embed = [bs, ques_len, emb_dim]\n",
    "        \n",
    "        ## 3. Contextual embed layer\n",
    "        \n",
    "        ctx_contextual_inp = torch.cat([ctx_word_embed, ctx_char_embed],dim=2)\n",
    "        # [bs, ctx_len, emb_dim*2]\n",
    "        \n",
    "        ques_contextual_inp = torch.cat([ques_word_embed, ques_char_embed],dim=2)\n",
    "        # [bs, ques_len, emb_dim*2]\n",
    "        \n",
    "        ctx_contextual_emb = self.contextual_embedding(ctx_contextual_inp)\n",
    "        # [bs, ctx_len, emb_dim*2]\n",
    "        \n",
    "        ques_contextual_emb = self.contextual_embedding(ques_contextual_inp)\n",
    "        # [bs, ques_len, emb_dim*2]\n",
    "        \n",
    "        \n",
    "        ## 4. Attention flow layer\n",
    "        \n",
    "        ### CREATE SIMILARITY MATRIX\n",
    "        \n",
    "        ctx_ = ctx_contextual_emb.unsqueeze(2).repeat(1,1,ques_len,1)\n",
    "        # [bs, ctx_len, 1, emb_dim*2] => [bs, ctx_len, ques_len, emb_dim*2]\n",
    "        \n",
    "        ques_ = ques_contextual_emb.unsqueeze(1).repeat(1,ctx_len,1,1)\n",
    "        # [bs, 1, ques_len, emb_dim*2] => [bs, ctx_len, ques_len, emb_dim*2]\n",
    "        \n",
    "        elementwise_prod = torch.mul(ctx_, ques_)\n",
    "        # [bs, ctx_len, ques_len, emb_dim*2]\n",
    "        \n",
    "        alpha = torch.cat([ctx_, ques_, elementwise_prod], dim=3)\n",
    "        # [bs, ctx_len, ques_len, emb_dim*6]\n",
    "        \n",
    "        similarity_matrix = self.similarity_weight(alpha).view(-1, ctx_len, ques_len)\n",
    "        # [bs, ctx_len, ques_len]\n",
    "        \n",
    "        \n",
    "        ### CALCULATE CONTEXT2QUERY ATTENTION\n",
    "        \n",
    "        a = F.softmax(similarity_matrix, dim=-1)\n",
    "        # [bs, ctx_len, ques_len]\n",
    "        \n",
    "        c2q = torch.bmm(a, ques_contextual_emb)\n",
    "        # [bs] ([ctx_len, ques_len] X [ques_len, emb_dim*2]) => [bs, ctx_len, emb_dim*2]\n",
    "        \n",
    "        ### CALCULATE QUERY2CONTEXT ATTENTION\n",
    "        \n",
    "        b = F.softmax(torch.max(similarity_matrix,2)[0], dim=-1)\n",
    "        # [bs, ctx_len]\n",
    "        \n",
    "        b = b.unsqueeze(1)\n",
    "        # [bs, 1, ctx_len]\n",
    "        \n",
    "        q2c = torch.bmm(b, ctx_contextual_emb)\n",
    "        # [bs] ([bs, 1, ctx_len] X [bs, ctx_len, emb_dim*2]) => [bs, 1, emb_dim*2]\n",
    "        \n",
    "        q2c = q2c.repeat(1, ctx_len, 1)\n",
    "        # [bs, ctx_len, emb_dim*2]\n",
    "        \n",
    "        ### QUERY AWARE REPRESENTATION\n",
    "        \n",
    "        G = torch.cat([ctx_contextual_emb, c2q, \n",
    "                       torch.mul(ctx_contextual_emb,c2q), \n",
    "                       torch.mul(ctx_contextual_emb, q2c)], dim=2)\n",
    "        \n",
    "        # [bs, ctx_len, emb_dim*8]\n",
    "        \n",
    "        \n",
    "        ## MODELING LAYER\n",
    "        \n",
    "        M, _ = self.modeling_lstm(G)\n",
    "        # [bs, ctx_len, emb_dim*2]\n",
    "        \n",
    "        ## OUTPUT LAYER\n",
    "        \n",
    "        M2, _ = self.end_lstm(M)\n",
    "        \n",
    "        # START PREDICTION\n",
    "        \n",
    "        p1 = self.output_start(torch.cat([G,M], dim=2))\n",
    "        # [bs, ctx_len, 1]\n",
    "        \n",
    "        p1 = p1.squeeze()\n",
    "        # [bs, ctx_len]\n",
    "        \n",
    "        #p1 = F.softmax(p1, dim=-1)\n",
    "        \n",
    "        # END PREDICTION\n",
    "        \n",
    "        p2 = self.output_end(torch.cat([G, M2], dim=2)).squeeze()\n",
    "        # [bs, ctx_len, 1] => [bs, ctx_len]\n",
    "        \n",
    "        #p2 = F.softmax(p2, dim=-1)\n",
    "        \n",
    "        \n",
    "        return p1, p2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_VOCAB_DIM = len(char2idx)\n",
    "EMB_DIM = 100\n",
    "CHAR_EMB_DIM = 8\n",
    "NUM_OUTPUT_CHANNELS = 100\n",
    "KERNEL_SIZE = (8,5)\n",
    "HIDDEN_DIM = 100\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = BiDAF(CHAR_VOCAB_DIM, \n",
    "              EMB_DIM, \n",
    "              CHAR_EMB_DIM, \n",
    "              NUM_OUTPUT_CHANNELS, \n",
    "              KERNEL_SIZE, \n",
    "              HIDDEN_DIM, \n",
    "              device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "optimizer = optim.Adadelta(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset):\n",
    "    print(\"Starting training ........\")\n",
    "   \n",
    "    train_loss = 0.\n",
    "    batch_count = 0\n",
    "    model.train()\n",
    "    for batch in train_dataset:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        if batch_count % 500 == 0:\n",
    "            print(f\"Starting batch: {batch_count}\")\n",
    "        batch_count += 1\n",
    "        \n",
    "        context, question, char_ctx, char_ques, label, ctx_text, ans, ids = batch\n",
    "\n",
    "        context, question, char_ctx, char_ques, label = context.to(device), question.to(device),\\\n",
    "                                   char_ctx.to(device), char_ques.to(device), label.to(device)\n",
    "\n",
    "\n",
    "        preds = model(context, question, char_ctx, char_ques)\n",
    "\n",
    "        start_pred, end_pred = preds\n",
    "\n",
    "        s_idx, e_idx = label[:,0], label[:,1]\n",
    "\n",
    "        loss = F.cross_entropy(start_pred, s_idx) + F.cross_entropy(end_pred, e_idx)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss/len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, valid_dataset):\n",
    "    \n",
    "    print(\"Starting validation .........\")\n",
    "   \n",
    "    valid_loss = 0.\n",
    "\n",
    "    batch_count = 0\n",
    "    \n",
    "    f1, em = 0., 0.\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "    predictions = {}\n",
    "    \n",
    "    for batch in valid_dataset:\n",
    "\n",
    "        if batch_count % 500 == 0:\n",
    "            print(f\"Starting batch {batch_count}\")\n",
    "        batch_count += 1\n",
    "\n",
    "        context, question, char_ctx, char_ques, label, ctx, answers, ids = batch\n",
    "\n",
    "        context, question, char_ctx, char_ques, label = context.to(device), question.to(device),\\\n",
    "                                   char_ctx.to(device), char_ques.to(device), label.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            s_idx, e_idx = label[:,0], label[:,1]\n",
    "\n",
    "            preds = model(context, question, char_ctx, char_ques)\n",
    "\n",
    "            p1, p2 = preds\n",
    "            \n",
    "            loss = F.cross_entropy(p1, s_idx) + F.cross_entropy(p2, e_idx)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            batch_size, c_len = p1.size()\n",
    "            ls = nn.LogSoftmax(dim=1)\n",
    "            mask = (torch.ones(c_len, c_len) * float('-inf')).to(device).tril(-1).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            score = (ls(p1).unsqueeze(2) + ls(p2).unsqueeze(1)) + mask\n",
    "            score, s_idx = score.max(dim=1)\n",
    "            score, e_idx = score.max(dim=1)\n",
    "            s_idx = torch.gather(s_idx, 1, e_idx.view(-1, 1)).squeeze()\n",
    "            \n",
    "           \n",
    "            for i in range(batch_size):\n",
    "                id = ids[i]\n",
    "                pred = context[i][s_idx[i]:e_idx[i]+1]\n",
    "                pred = ' '.join([idx2word[idx.item()] for idx in pred])\n",
    "                predictions[id] = pred\n",
    "            \n",
    "\n",
    "    \n",
    "    em, f1 = evaluate(predictions)\n",
    "    return valid_loss/len(valid_dataset), em, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions):\n",
    "    '''\n",
    "    Gets a dictionary of predictions with question_id as key\n",
    "    and prediction as value. The validation dataset has multiple \n",
    "    answers for a single question. Hence we compare our prediction\n",
    "    with all the answers and choose the one that gives us\n",
    "    the maximum metric (em or f1). \n",
    "    This method first parses the JSON file, gets all the answers\n",
    "    for a given id and then passes the list of answers and the \n",
    "    predictions to calculate em, f1.\n",
    "    \n",
    "    \n",
    "    :param dict predictions\n",
    "    Returns\n",
    "    : exact_match: 1 if the prediction and ground truth \n",
    "      match exactly, 0 otherwise.\n",
    "    : f1_score: \n",
    "    '''\n",
    "    with open('./data/squad_dev.json','r',encoding='utf-8') as f:\n",
    "        dataset = json.load(f)\n",
    "        \n",
    "    dataset = dataset['data']\n",
    "    f1 = exact_match = total = 0\n",
    "    for article in dataset:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                total += 1\n",
    "                if qa['id'] not in predictions:\n",
    "                    continue\n",
    "                \n",
    "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n",
    "                \n",
    "                prediction = predictions[qa['id']]\n",
    "                \n",
    "                exact_match += metric_max_over_ground_truths(\n",
    "                    exact_match_score, prediction, ground_truths)\n",
    "                \n",
    "                f1 += metric_max_over_ground_truths(\n",
    "                    f1_score, prediction, ground_truths)\n",
    "                \n",
    "    \n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "    \n",
    "    return exact_match, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, re\n",
    "\n",
    "def normalize_answer(s):\n",
    "    '''\n",
    "    Performs a series of cleaning steps on the ground truth and \n",
    "    predicted answer.\n",
    "    '''\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    '''\n",
    "    Returns maximum value of metrics for predicition by model against\n",
    "    multiple ground truths.\n",
    "    \n",
    "    :param func metric_fn: can be 'exact_match_score' or 'f1_score'\n",
    "    :param str prediction: predicted answer span by the model\n",
    "    :param list ground_truths: list of ground truths against which\n",
    "                               metrics are calculated. Maximum values of \n",
    "                               metrics are chosen.\n",
    "                            \n",
    "    \n",
    "    '''\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "        \n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    '''\n",
    "    Returns f1 score of two strings.\n",
    "    '''\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    '''\n",
    "    Returns exact_match_score of two strings.\n",
    "    '''\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    '''\n",
    "    Helper function to record epoch time.\n",
    "    '''\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting validation .........\n",
      "Starting batch 0\n",
      "Epoch train loss : 7.454526586455058| Time: 7m 16s\n",
      "Epoch valid loss: 7.222504965257179\n",
      "Epoch EM: 1.6177861873226111\n",
      "Epoch F1: 2.5711593728006346\n",
      "====================================================================================\n",
      "Epoch 2\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting validation .........\n",
      "Starting batch 0\n",
      "Epoch train loss : 6.092980009559693| Time: 7m 13s\n",
      "Epoch valid loss: 6.06355271393779\n",
      "Epoch EM: 2.8855250709555347\n",
      "Epoch F1: 4.181444067679884\n",
      "====================================================================================\n",
      "Epoch 3\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting validation .........\n",
      "Starting batch 0\n",
      "Epoch train loss : 5.307723025965497| Time: 7m 17s\n",
      "Epoch valid loss: 5.7401202920982035\n",
      "Epoch EM: 3.3301797540208136\n",
      "Epoch F1: 4.614917714077235\n",
      "====================================================================================\n",
      "Epoch 4\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting validation .........\n",
      "Starting batch 0\n",
      "Epoch train loss : 4.752350188270817| Time: 6m 42s\n",
      "Epoch valid loss: 5.599349497972171\n",
      "Epoch EM: 3.793755912961211\n",
      "Epoch F1: 5.182660864340517\n",
      "====================================================================================\n",
      "Epoch 5\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "Starting batch: 500\n",
      "Starting validation .........\n",
      "Starting batch 0\n",
      "Epoch train loss : 4.2630822914402655| Time: 6m 9s\n",
      "Epoch valid loss: 5.662473886719744\n",
      "Epoch EM: 4.134342478713339\n",
      "Epoch F1: 5.400549034676259\n",
      "====================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "ems = []\n",
    "f1s = []\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_dataset)\n",
    "    valid_loss, em, f1 = valid(model, valid_dataset)\n",
    "\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': valid_loss,\n",
    "            'em':em,\n",
    "            'f1':f1,\n",
    "            }, 'bidaf_run4_{}.pth'.format(epoch))\n",
    "    \n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    ems.append(em)\n",
    "    f1s.append(f1)\n",
    "\n",
    "    print(f\"Epoch train loss : {train_loss}| Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"Epoch valid loss: {valid_loss}\")\n",
    "    print(f\"Epoch EM: {em}\")\n",
    "    print(f\"Epoch F1: {f1}\")\n",
    "    print(\"====================================================================================\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* Papers read/referred:\n",
    "    1. BiDAF: https://arxiv.org/abs/1611.01603\n",
    "    2. Convolutional Neural Networks for Sentence Classification: https://arxiv.org/abs/1408.5882\n",
    "    3. Highway Networks: https://arxiv.org/abs/1505.00387\n",
    "* Other helpful links:\n",
    "    1. https://nlp.seas.harvard.edu/slides/aaai16.pdf. A great resource for character embeddings. The figures in the character embedding section are taken from here.\n",
    "    2. https://towardsdatascience.com/the-definitive-guide-to-bi-directional-attention-flow-d0e96e9e666b. A great series of blogs to understand BiDAF.\n",
    "    Some of the following repos might be out of date.\n",
    "    3. https://github.com/allenai/bi-att-flow\n",
    "    4. https://github.com/galsang\n",
    "    5. https://github.com/jojonki/BiDAF/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
