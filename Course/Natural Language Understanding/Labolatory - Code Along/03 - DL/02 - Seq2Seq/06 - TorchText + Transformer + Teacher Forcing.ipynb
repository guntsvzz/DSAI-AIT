{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "In this notebook we will be implementing a (slightly modified version) of the Transformer model from the [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper.\n",
    "\n",
    "<img src = \"figures/transformer1.png\" >\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Similar to the Convolutional Sequence-to-Sequence model, the Transformer does not use any recurrence. It also does not use any convolutional layers. Instead the model is entirely made up of linear layers, attention mechanisms and normalization. \n",
    "\n",
    "As of January 2020, Transformers are the dominant architecture in NLP and are used to achieve state-of-the-art results for many tasks and it appears as if they will be for the near future. \n",
    "\n",
    "The differences between the implementation in this notebook and the paper are:\n",
    "- we use a learned positional encoding instead of a static one\n",
    "- we use the standard Adam optimizer with a static learning rate instead of one with warm-up and cool-down steps\n",
    "- we do not use label smoothing\n",
    "\n",
    "We make all of these changes as they closely follow BERT's set-up and the majority of Transformer variants use a similar set-up.\n",
    "\n",
    "Note: The model expects data to be fed in with the batch dimension first, so we use `batch_first = True`. \n",
    "\n",
    "This tutorial is edited from https://github.com/bentrevett.\n",
    "\n",
    "Feel free to skip to the model part, since everything is similar to part 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random, math, time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA RTX A6000'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0+cu117'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.14.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. ETL: Loading the dataset\n",
    "\n",
    "**Note**: Here I chose to translate English to German, simply it is easier for myself, since I don't understand German so it is difficult for me to imagine a sentence during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment this if you are not using our department puffer\n",
    "import os\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "SRC_LANGUAGE = 'en'\n",
    "TRG_LANGUAGE = 'de'\n",
    "\n",
    "train = Multi30k(split=('train'), language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShardingFilterIterDataPipe"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so this is a datapipe object; very similar to pytorch dataset version 2 which is better\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. EDA - simple investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Two young, White males are outside near many bushes.',\n",
       " 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's take a look at one example of train\n",
    "sample = next(iter(train))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29001"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 29001 is plenty,, we gonna call `random_split` to train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = train.random_split(total_length=train_size, weights = {\"train\": 0.7, \"val\": 0.2, \"test\": 0.1}, seed=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20301"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5800"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = len(list(iter(val)))\n",
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2900"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = len(list(iter(test)))\n",
    "test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "**Note**: the models must first be downloaded using the following on the command line: \n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download de_core_news_sm\n",
    "```\n",
    "\n",
    "First, since we have two languages, let's create some constants to represent that.  Also, let's create two dicts: one for holding our tokenizers and one for holding all the vocabs with assigned numbers for each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[TRG_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  Two young, White males are outside near many bushes.\n",
      "Tokenization:  ['Two', 'young', ',', 'White', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the english part\n",
    "print(\"Sentence: \", sample[0])\n",
    "print(\"Tokenization: \", token_transform[SRC_LANGUAGE](sample[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to tokenize our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "# here data can be `train` or `val` or `test`\n",
    "def yield_tokens(data, language):\n",
    "    language_index = {SRC_LANGUAGE: 0, TRG_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample[language_index[language]]) #either first or second index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we tokenize, let's define some special symbols so our neural network understand the embeddings of these symbols, namely the unknown, the padding, the start of sentence, and end of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text to integers (Numericalization)\n",
    "\n",
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we use built in factory function <code>build_vocab_from_iterator</code> which accepts iterator that yield list or iterator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    # Create torchtext's Vocab object \n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train, ln), \n",
    "                                                    min_freq=2,   #if not, everything will be treated as UNK\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True) #indicates whether to insert symbols at the beginning or at the end                                            \n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1891, 10, 4, 0, 4]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[SRC_LANGUAGE](['here', 'is', 'a', 'unknownword', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[SRC_LANGUAGE].get_itos()\n",
    "\n",
    "#print 1816, for example\n",
    "mapping[1891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "mapping[0]\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5174"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for src_sample, trg_sample in batch:\n",
    "        processed_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first = True) #<----need this because we use linear layers mostly\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first = True)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(val, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test, batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for en, _, de in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English shape:  torch.Size([64, 27])\n",
      "German shape:  torch.Size([64, 24])\n"
     ]
    }
   ],
   "source": [
    "print(\"English shape: \", en.shape)  # (batch_size, seq len)\n",
    "print(\"German shape: \", de.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model\n",
    "\n",
    "### Encoder\n",
    "\n",
    "Similar to the ConvSeq2Seq model, the Transformer's encoder does not attempt to compress the entire source sentence, $X = (x_1, ... ,x_n)$, into a single context vector, $z$. Instead it produces a sequence of context vectors, $Z = (z_1, ... , z_n)$. So, if our input sequence was 5 tokens long we would have $Z = (z_1, z_2, z_3, z_4, z_5)$. Why do we call this a sequence of context vectors and not a sequence of hidden states? A hidden state at time $t$ in an RNN has only seen tokens $x_t$ and all the tokens before it. However, each context vector here has seen all tokens at all positions within the input sequence.\n",
    "\n",
    "<img src=\"figures/transformer-encoder.png\" >\n",
    "\n",
    "First, the tokens are passed through a standard embedding layer. Next, as the model has no recurrent it has no idea about the order of the tokens within the sequence. We solve this by using a second embedding layer called a *positional embedding layer*. This is a standard embedding layer where the input is not the token itself but the position of the token within the sequence, starting with the first token, the `<sos>` (start of sequence) token, in position 0. The position embedding has a \"vocabulary\" size of 100, which means our model can accept sentences up to 100 tokens long. This can be increased if we want to handle longer sentences.\n",
    "\n",
    "The original Transformer implementation from the Attention is All You Need paper does not learn positional embeddings. Instead it uses a fixed static embedding. Modern Transformer architectures, like BERT, use positional embeddings instead, hence we have decided to use them in these tutorials. Check out [this](http://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding) section to read more about the positional embeddings used in the original Transformer model.\n",
    "\n",
    "Next, the token and positional embeddings are elementwise summed together to get a vector which contains information about the token and also its position with in the sequence. However, before they are summed, the token embeddings are multiplied by a scaling factor which is $\\sqrt{d_{model}}$, where $d_{model}$ is the hidden dimension size, `hid_dim`. This supposedly reduces variance in the embeddings and the model is difficult to train reliably without this scaling factor. Dropout is then applied to the combined embeddings.\n",
    "\n",
    "The combined embeddings are then passed through $N$ *encoder layers* to get $Z$, which is then output and can be used by the decoder.\n",
    "\n",
    "The source mask, `src_mask`, is simply the same shape as the source sentence but has a value of 1 when the token in the source sentence is not a `<pad>` token and 0 when it is a `<pad>` token. This is used in the encoder layers to mask the multi-head attention mechanisms, which are used to calculate and apply attention over the source sentence, so the model does not pay attention to `<pad>` tokens, which contain no useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, device, max_length = 100):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([EncoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim,\n",
    "                                                  dropout, \n",
    "                                                  device) \n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos = [batch size, src len]\n",
    "        \n",
    "        src = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        #src = [batch size, src len, hid dim]\n",
    "            \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer\n",
    "\n",
    "The encoder layers are where all of the \"meat\" of the encoder is contained. We first pass the source sentence and its mask into the *multi-head attention layer*, then perform dropout on it, apply a residual connection and pass it through a [Layer Normalization](https://arxiv.org/abs/1607.06450) layer. We then pass it through a *position-wise feedforward* layer and then, again, apply dropout, a residual connection and then layer normalization to get the output of this layer which is fed into the next layer. The parameters are not shared between layers. \n",
    "\n",
    "The mutli head attention layer is used by the encoder layer to attend to the source sentence, i.e. it is calculating and applying attention over itself instead of another sequence, hence we call it *self attention*.\n",
    "\n",
    "The gist is that it normalizes the values of the features, i.e. across the hidden dimension, so each feature has a mean of 0 and a standard deviation of 1. This allows neural networks with a larger number of layers, like the Transformer, to be trained easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim,  \n",
    "                 dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len] \n",
    "                \n",
    "        #self attention\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _src = self.positionwise_feedforward(src)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutli Head Attention Layer\n",
    "\n",
    "One of the key, novel concepts introduced by the Transformer paper is the *multi-head attention layer*. \n",
    "\n",
    "<img src = \"figures/transformer-attention.png\" >\n",
    "\n",
    "Attention can be though of as *queries*, *keys* and *values* - where the query is used with the key to get an attention vector (usually the output of a *softmax* operation and has all values between 0 and 1 which sum to 1) which is then used to get a weighted sum of the values.\n",
    "\n",
    "The Transformer uses *scaled dot-product attention*, where the query and key are combined by taking the dot product between them, then applying the softmax operation and scaling by $d_k$ before finally then multiplying by the value. $d_k$ is the *head dimension*, `head_dim`, which we will shortly explain further.\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ \n",
    "\n",
    "This is similar to standard *dot product attention* but is scaled by $d_k$, which the paper states is used to stop the results of the dot products growing large, causing gradients to become too small.\n",
    "\n",
    "However, the scaled dot-product attention isn't simply applied to the queries, keys and values. Instead of doing a single attention application the queries, keys and values have their `hid_dim` split into $h$ *heads* and the scaled dot-product attention is calculated over all heads in parallel. This means instead of paying attention to one concept per attention application, we pay attention to $h$. We then re-combine the heads into their `hid_dim` shape, thus each `hid_dim` is potentially paying attention to $h$ different concepts.\n",
    "\n",
    "$$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1,...,\\text{head}_h)W^O $$\n",
    "\n",
    "$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $$\n",
    "\n",
    "$W^O$ is the linear layer applied at the end of the multi-head attention layer, `fc`. $W^Q, W^K, W^V$ are the linear layers `fc_q`, `fc_k` and `fc_v`.\n",
    "\n",
    "Walking through the module, first we calculate $QW^Q$, $KW^K$ and $VW^V$ with the linear layers, `fc_q`, `fc_k` and `fc_v`, to give us `Q`, `K` and `V`. Next, we split the `hid_dim` of the query, key and value into `n_heads` using `.view` and correctly permute them so they can be multiplied together. We then calculate the `energy` (the un-normalized attention) by multiplying `Q` and `K` together and scaling it by the square root of `head_dim`, which is calulated as `hid_dim // n_heads`. We then mask the energy so we do not pay attention over any elements of the sequeuence we shouldn't, then apply the softmax and dropout. We then apply the attention to the value heads, `V`, before combining the `n_heads` together. Finally, we multiply this $W^O$, represented by `fc_o`. \n",
    "\n",
    "Note that in our implementation the lengths of the keys and values are always the same, thus when matrix multiplying the output of the softmax, `attention`, with `V` we will always have valid dimension sizes for matrix multiplication. This multiplication is carried out using `torch.matmul` which, when both tensors are >2-dimensional, does a batched matrix multiplication over the last two dimensions of each tensor. This will be a `[query len, key len] x [value len, head dim]` batched matrix multiplication over the batch size and each head which provides the `[batch size, n heads, query len, head dim]` result.\n",
    "\n",
    "One thing that looks strange at first is that dropout is applied directly to the attention. This means that our attention vector will most probably not sum to 1 and we may pay full attention to a token but the attention over that token is set to 0 by dropout. This is never explained, or even mentioned, in the paper however is used by the [official implementation](https://github.com/tensorflow/tensor2tensor/) and every Transformer implementation since, [including BERT](https://github.com/google-research/bert/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert hid_dim % n_heads == 0\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "        \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        #Q = [batch size, query len, hid dim]\n",
    "        #K = [batch size, key len, hid dim]\n",
    "        #V = [batch size, value len, hid dim]\n",
    "                \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        #Q = [batch size, n heads, query len, head dim]\n",
    "        #K = [batch size, n heads, key len, head dim]\n",
    "        #V = [batch size, n heads, value len, head dim]\n",
    "                \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        #energy = [batch size, n heads, query len, key len]\n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        #attention = [batch size, n heads, query len, key len]\n",
    "                \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #x = [batch size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        #x = [batch size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch size, query len, hid dim]\n",
    "        \n",
    "        return x, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feedforward Layer\n",
    "\n",
    "The other main block inside the encoder layer is the *position-wise feedforward layer* This is relatively simple compared to the multi-head attention layer. The input is transformed from `hid_dim` to `pf_dim`, where `pf_dim` is usually a lot larger than `hid_dim`. The original Transformer used a `hid_dim` of 512 and a `pf_dim` of 2048. The ReLU activation function and dropout are applied before it is transformed back into a `hid_dim` representation. \n",
    "\n",
    "Why is this used? Unfortunately, it is never explained in the paper.\n",
    "\n",
    "BERT uses the [GELU](https://arxiv.org/abs/1606.08415) activation function, which can be used by simply switching `torch.relu` for `F.gelu`. Why did they use GELU? Again, it is never explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
    "        #x = [batch size, seq len, pf dim]\n",
    "        \n",
    "        x = self.fc_2(x)\n",
    "        #x = [batch size, seq len, hid dim]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decoder\n",
    "\n",
    "The objective of the decoder is to take the encoded representation of the source sentence, $Z$, and convert it into predicted tokens in the target sentence, $\\hat{Y}$. We then compare $\\hat{Y}$ with the actual tokens in the target sentence, $Y$, to calculate our loss, which will be used to calculate the gradients of our parameters and then use our optimizer to update our weights in order to improve our predictions. \n",
    "\n",
    "<img src = \"figures/transformer-decoder.png\" >\n",
    "\n",
    "The decoder is similar to encoder, however it now has two multi-head attention layers. A *masked multi-head attention layer* over the target sequence, and a multi-head attention layer which uses the decoder representation as the query and the encoder representation as the key and value.\n",
    "\n",
    "The decoder uses positional embeddings and combines - via an elementwise sum - them with the scaled embedded target tokens, followed by dropout. Again, our positional encodings have a \"vocabulary\" of 100, which means they can accept sequences up to 100 tokens long. This can be increased if desired.\n",
    "\n",
    "The combined embeddings are then passed through the $N$ decoder layers, along with the encoded source, `enc_src`, and the source and target masks. Note that the number of layers in the encoder does not have to be equal to the number of layers in the decoder, even though they are both denoted by $N$.\n",
    "\n",
    "The decoder representation after the $N^{th}$ layer is then passed through a linear layer, `fc_out`. In PyTorch, the softmax operation is contained within our loss function, so we do not explicitly need to use a softmax layer here.\n",
    "\n",
    "As well as using the source mask, as we did in the encoder to prevent our model attending to `<pad>` tokens, we also use a target mask. This will be explained further in the `Seq2Seq` model which encapsulates both the encoder and decoder, but the gist of it is that it performs a similar operation as the decoder padding in the convolutional sequence-to-sequence model. As we are processing all of the target tokens at once in parallel we need a method of stopping the decoder from \"cheating\" by simply \"looking\" at what the next token in the target sequence is and outputting it. \n",
    "\n",
    "Our decoder layer also outputs the normalized attention values so we can later plot them to see what our model is actually paying attention to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, device,max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(hid_dim, \n",
    "                                                  n_heads, \n",
    "                                                  pf_dim, \n",
    "                                                  dropout, \n",
    "                                                  device)\n",
    "                                     for _ in range(n_layers)])\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "                \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)          \n",
    "        #pos = [batch size, trg len]\n",
    "            \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        #output = [batch size, trg len, output dim]\n",
    "            \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer\n",
    "\n",
    "As mentioned previously, the decoder layer is similar to the encoder layer except that it now has two multi-head attention layers, `self_attention` and `encoder_attention`. \n",
    "\n",
    "The first performs self-attention, as in the encoder, by using the decoder representation so far as the query, key and value. This is followed by dropout, residual connection and layer normalization. This `self_attention` layer uses the target sequence mask, `trg_mask`, in order to prevent the decoder from \"cheating\" by paying attention to tokens that are \"ahead\" of the one it is currently processing as it processes all tokens in the target sentence in parallel.\n",
    "\n",
    "The second is how we actually feed the encoded source sentence, `enc_src`, into our decoder. In this multi-head attention layer the queries are the decoder representations and the keys and values are the encoder representations. Here, the source mask, `src_mask` is used to prevent the multi-head attention layer from attending to `<pad>` tokens within the source sentence. This is then followed by the dropout, residual connection and layer normalization layers. \n",
    "\n",
    "Finally, we pass this through the position-wise feedforward layer and yet another sequence of dropout, residual connection and layer normalization.\n",
    "\n",
    "The decoder layer isn't introducing any new concepts, just using the same set of layers as the encoder in a slightly different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        #self attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "            \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "            \n",
    "        #encoder attention\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))            \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together (become Seq2Seq!)\n",
    "\n",
    "Finally, we have the `Seq2Seq` module which encapsulates the encoder and decoder, as well as handling the creation of the masks.\n",
    "\n",
    "The source mask is created by checking where the source sequence is not equal to a `<pad>` token. It is 1 where the token is not a `<pad>` token and 0 when it is. It is then unsqueezed so it can be correctly broadcast when applying the mask to the `energy`, which of shape `[batch size, n heads, seq len, seq len]`.\n",
    "\n",
    "The target mask is slightly more complicated. First, we create a mask for the `<pad>` tokens, as we did for the source mask. Next, we create a \"subsequent\" mask, `trg_sub_mask`, using `torch.tril`. This creates a diagonal matrix where the elements above the diagonal will be zero and the elements below the diagonal will be set to whatever the input tensor is. In this case, the input tensor will be a tensor filled with ones. So this means our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "This shows what each target token (row) is allowed to look at (column). The first target token has a mask of `[1, 0, 0, 0, 0]` which means it can only look at the first target token. The second target token has a mask of `[1, 1, 0, 0, 0]` which it means it can look at both the first and second target tokens. \n",
    "\n",
    "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "After the masks are created, they used with the encoder and decoder along with the source and target sentences to get our predicted target sentence, `output`, along with the decoder's attention over the source sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Training\n",
    "\n",
    "We can now define our encoder and decoders. This model is significantly smaller than Transformers used in research today, but is able to be run on a single GPU quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper does not mention which weight initialization scheme was used, however Xavier uniform seems to be common amongst Transformer models, so we use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab_transform[SRC_LANGUAGE])\n",
    "OUTPUT_DIM = len(vocab_transform[TRG_LANGUAGE])\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(5174, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(6433, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=6433, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "hid_dim = 256\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "enc_heads = 8\n",
    "dec_heads = 8\n",
    "enc_pf_dim = 512\n",
    "dec_pf_dim = 512\n",
    "enc_dropout = 0.1\n",
    "dec_dropout = 0.1\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "enc = Encoder(input_dim, \n",
    "              hid_dim, \n",
    "              enc_layers, \n",
    "              enc_heads, \n",
    "              enc_pf_dim, \n",
    "              enc_dropout, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(output_dim, \n",
    "              hid_dim, \n",
    "              dec_layers, \n",
    "              dec_heads, \n",
    "              dec_pf_dim, \n",
    "              enc_dropout, \n",
    "              device)\n",
    "\n",
    "model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1324544\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "1646848\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "1646848\n",
      "  6433\n",
      "______\n",
      "8629537\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer used in the original Transformer paper uses Adam with a learning rate that has a \"warm-up\" and then a \"cool-down\" period. BERT and other Transformer models use Adam with a fixed learning rate, so we will implement that. Check [this](http://nlp.seas.harvard.edu/2018/04/03/attention.html#optimizer) link for more details about the original Transformer's learning rate schedule.\n",
    "\n",
    "Note that the learning rate needs to be lower than the default used by Adam or else learning is unstable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.0005\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll define our training loop. This is the exact same as the one used in the previous tutorial.\n",
    "\n",
    "As we want our model to predict the `<eos>` token but not have it be an input into our model we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_len, trg in loader:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg    = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.reshape(-1, output_dim)\n",
    "        trg = trg[:,1:].reshape(-1)\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg    = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an optimizer or a clip value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_len, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!\n",
    "\n",
    "**Note: similar to CNN, this model always has a teacher forcing ratio of 1, i.e. it will always use the ground truth next token from the target sequence (this is simply because CNN do everything in parallel so we cannot have the next token). This means we cannot compare perplexity values against the previous models when they are using a teacher forcing ratio that is not 1. To understand this, try run previous tutorials with teaching forcing ratio of 1, you will get very low perplexity.  **   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(valid_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 14s\n",
      "\tTrain Loss: 4.127 | Train PPL:  61.985\n",
      "\t Val. Loss: 3.051 |  Val. PPL:  21.134\n",
      "Epoch: 02 | Time: 0m 13s\n",
      "\tTrain Loss: 2.824 | Train PPL:  16.848\n",
      "\t Val. Loss: 2.322 |  Val. PPL:  10.201\n",
      "Epoch: 03 | Time: 0m 13s\n",
      "\tTrain Loss: 2.276 | Train PPL:   9.736\n",
      "\t Val. Loss: 1.869 |  Val. PPL:   6.481\n",
      "Epoch: 04 | Time: 0m 13s\n",
      "\tTrain Loss: 1.936 | Train PPL:   6.934\n",
      "\t Val. Loss: 1.573 |  Val. PPL:   4.820\n",
      "Epoch: 05 | Time: 0m 13s\n",
      "\tTrain Loss: 1.689 | Train PPL:   5.415\n",
      "\t Val. Loss: 1.374 |  Val. PPL:   3.952\n",
      "Epoch: 06 | Time: 0m 13s\n",
      "\tTrain Loss: 1.508 | Train PPL:   4.516\n",
      "\t Val. Loss: 1.193 |  Val. PPL:   3.295\n",
      "Epoch: 07 | Time: 0m 13s\n",
      "\tTrain Loss: 1.361 | Train PPL:   3.899\n",
      "\t Val. Loss: 1.043 |  Val. PPL:   2.837\n",
      "Epoch: 08 | Time: 0m 13s\n",
      "\tTrain Loss: 1.240 | Train PPL:   3.456\n",
      "\t Val. Loss: 0.941 |  Val. PPL:   2.563\n",
      "Epoch: 09 | Time: 0m 13s\n",
      "\tTrain Loss: 1.135 | Train PPL:   3.113\n",
      "\t Val. Loss: 0.844 |  Val. PPL:   2.325\n",
      "Epoch: 10 | Time: 0m 13s\n",
      "\tTrain Loss: 1.050 | Train PPL:   2.859\n",
      "\t Val. Loss: 0.773 |  Val. PPL:   2.167\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 10\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.766 | Test PPL:   2.152 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "test_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on some random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Two young, White males are outside near many bushes.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,   19,   25,   15, 1069,  842,   17,   56,   84,  331, 1623,    5,\n",
       "           3], device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = text_transform[SRC_LANGUAGE](sample[0]).to(device)\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,   21,   83,  262,   32,   89,   22,   91,    7,   16,  115,    0,\n",
       "        2893,    4,    3], device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_text = text_transform[TRG_LANGUAGE](sample[1]).to(device)\n",
    "trg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_text.reshape(1, -1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = trg_text.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 13]), torch.Size([1, 15]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, attentions = model(src_text, trg_text) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 6433])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape #batch_size, trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since batch size is 1, we just take off that dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 6433])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall remove the first token since it's zeroes anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 6433])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output[1:]\n",
    "output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just take the top token with highest probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = output.argmax(1) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 83, 262,  32,   9,  22,  91, 453,  16, 115,  24,  24,   4,   3,   4],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mapping of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junge\n",
      "weiße\n",
      "Männer\n",
      ",\n",
      "im\n",
      "Freien\n",
      "nahe\n",
      "der\n",
      "Nähe\n",
      "von\n",
      "von\n",
      ".\n",
      "<eos>\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention\n",
    "\n",
    "Let's display the attentions to understand how the source text links with the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 15, 13])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 8 heads, we can look at just 1 head for sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 13])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = attentions[0, 0, :, :]\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'Two',\n",
       " 'young',\n",
       " ',',\n",
       " 'White',\n",
       " 'males',\n",
       " 'are',\n",
       " 'outside',\n",
       " 'near',\n",
       " 'many',\n",
       " 'bushes',\n",
       " '.',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](sample[0]) + ['<eos>']\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'junge',\n",
       " 'weiße',\n",
       " 'Männer',\n",
       " ',',\n",
       " 'im',\n",
       " 'Freien',\n",
       " 'nahe',\n",
       " 'der',\n",
       " 'Nähe',\n",
       " 'von',\n",
       " 'von',\n",
       " '.',\n",
       " '<eos>',\n",
       " '.']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2597910/59549304.py:17: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(x_ticks, rotation=45)\n",
      "/tmp/ipykernel_2597910/59549304.py:18: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(y_ticks)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(src_tokens, trg_tokens, attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Nice!  As you can see, it's fast like CNN due to parallezibility and perform like LSTM+attention."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
