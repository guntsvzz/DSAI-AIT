{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "## Part 5: TorchText + CNN + Teaching Forcing\n",
    "\n",
    "In this notebook we'll be implementing the [Convolutional Sequence to Sequence Learning](https://arxiv.org/abs/1705.03122) model. \n",
    "\n",
    "<img src=\"figures/convseq2seq0.png\" width=\"600\">\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This model is drastically different to the previous models used in these tutorials. There are no recurrent components used at all. Instead it makes use of convolutional layers, typically used for image processing.\n",
    "\n",
    "In short, a convolutional layer uses *filters*. These filters have a *width* (and also a *height* in images, but usually not text). If a filter has a width of 3, then it can see 3 consecutive tokens. Each convolutional layer has many of these filters (1024 in this tutorial). Each filter will slide across the sequence, from beginning to the end, looking at all 3 consectuive tokens at a time. The idea is that each of these 1024 filters will learn to extract a different feature from the text. The result of this feature extraction will then be used by the model - potentially as input to another convolutional layer. This can then all be used to extract features from the source sentence to translate it into the target language.\n",
    "\n",
    "**Note: in this notebook we are using CNNs which expect the batch dimension to be first. We tell TorchText to have batches be <code>[batch size, sequence length]</code> by setting <code>batch_first</code> = True** in the <code>collate_batch</code>\n",
    "\n",
    "The figures are from https://github.com/bentrevett."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random, math, time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA RTX A6000'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0+cu117'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.14.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. ETL: Loading the dataset\n",
    "\n",
    "**Note**: Here I chose to translate English to German, simply it is easier for myself, since I don't understand German so it is difficult for me to imagine a sentence during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment this if you are not using our department puffer\n",
    "import os\n",
    "os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "SRC_LANGUAGE = 'en'\n",
    "TRG_LANGUAGE = 'de'\n",
    "\n",
    "train = Multi30k(split=('train'), language_pair=(SRC_LANGUAGE, TRG_LANGUAGE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShardingFilterIterDataPipe"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so this is a datapipe object; very similar to pytorch dataset version 2 which is better\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. EDA - simple investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Two young, White males are outside near many bushes.',\n",
       " 'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's take a look at one example of train\n",
    "sample = next(iter(train))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29001"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 29001 is plenty,, we gonna call `random_split` to train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = train.random_split(total_length=train_size, weights = {\"train\": 0.7, \"val\": 0.2, \"test\": 0.1}, seed=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20301"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5800"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = len(list(iter(val)))\n",
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2900"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = len(list(iter(test)))\n",
    "test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "**Note**: the models must first be downloaded using the following on the command line: \n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download de_core_news_sm\n",
    "```\n",
    "\n",
    "First, since we have two languages, let's create some constants to represent that.  Also, let's create two dicts: one for holding our tokenizers and one for holding all the vocabs with assigned numbers for each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[TRG_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  Two young, White males are outside near many bushes.\n",
      "Tokenization:  ['Two', 'young', ',', 'White', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the english part\n",
    "print(\"Sentence: \", sample[0])\n",
    "print(\"Tokenization: \", token_transform[SRC_LANGUAGE](sample[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to tokenize our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "# here data can be `train` or `val` or `test`\n",
    "def yield_tokens(data, language):\n",
    "    language_index = {SRC_LANGUAGE: 0, TRG_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample[language_index[language]]) #either first or second index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we tokenize, let's define some special symbols so our neural network understand the embeddings of these symbols, namely the unknown, the padding, the start of sentence, and end of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text to integers (Numericalization)\n",
    "\n",
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we use built in factory function <code>build_vocab_from_iterator</code> which accepts iterator that yield list or iterator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    # Create torchtext's Vocab object \n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train, ln), \n",
    "                                                    min_freq=2,   #if not, everything will be treated as UNK\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True) #indicates whether to insert symbols at the beginning or at the end                                            \n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1891, 10, 4, 0, 4]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[SRC_LANGUAGE](['here', 'is', 'a', 'unknownword', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[SRC_LANGUAGE].get_itos()\n",
    "\n",
    "#print 1816, for example\n",
    "mapping[1891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "mapping[0]\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5174"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>pad_sequence</code> which has `batch_first = True` since `conv` requires `batch` as the first dimension, unlike `rnn` which has `src_len` as first dimension by default.   We also remove `src_len` since we no longer needs to packed our sequences for rnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, trg_batch = [], []\n",
    "    for src_sample, trg_sample in batch:\n",
    "        processed_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
    "        \n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    return src_batch, trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(val, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test, batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for en, de in train_loader:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"English shape: \", en.shape)  # (batch_size, seq len)\n",
    "# print(\"German shape: \", de.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Design the Model\n",
    "\n",
    "Next up is building the model. As before, the model is made of an encoder and decoder. The encoder *encodes* the input sentence, in the source language, into a *context vector*. The decoder *decodes* the context vector to produce the output sentence in the target language.\n",
    "\n",
    "### Encoder\n",
    "\n",
    "The image below shows the result of an input sentence - *zwei menschen fechten.* - being passed through the encoder.\n",
    "\n",
    "<img src = \"figures/convseq2seq1.png\">\n",
    "\n",
    "First, the token is passed through a *token embedding layer* - which is standard for neural networks in natural language processing. However, as there are no recurrent connections in this model it has no idea about the order of the tokens within a sequence. To rectify this we have a second embedding layer, the *positional embedding layer*. This is a standard embedding layer where the input is not the token itself but the position of the token within the sequence - starting with the first token, the `<sos>` (start of sequence) token, in position 0.\n",
    "\n",
    "Next, the token and positional embeddings are elementwise summed together to get a vector which contains information about the token and also its position with in the sequence - which we simply call the *embedding vector*. This is followed by a linear layer which transforms the embedding vector into a vector with the required hidden dimension size. \n",
    "\n",
    "The next step is to pass this hidden vector into $N$ *convolutional blocks*. This is where the \"magic\" happens in this model and we will detail the contents of the convolutional blocks shortly. After passing through the convolutional blocks, the vector is then fed through another linear layer to transform it back from the hidden dimension size into the embedding dimension size. This is our *conved* vector - and we have one of these per token in the input sequence. \n",
    "\n",
    "Finally, the conved vector is elementwise summed with the embedding vector via a residual connection to get a *combined* vector for each token. Again, there is a combined vector for each token in the input sequence.\n",
    "\n",
    "**Note**:  Residual connection or so called skip connections are popularized in Resnet, which states that by allowing input to skip some layers, it prevents the model to overfit the deeper it goes since the model may figure out that the deeper layers are not necessary.\n",
    "\n",
    "### Convolutional Blocks\n",
    "\n",
    "So, how do these convolutional blocks work? The below image shows 2 convolutional blocks with a single filter (blue) that is sliding across the tokens within the sequence. In the actual implementation we will have 10 convolutional blocks with 1024 filters in each block.\n",
    "\n",
    "<img src = \"figures/convseq2seq2.png\" >\n",
    "\n",
    "First, the input sentence is padded to make the length of the sentence coming into the convolutional blocks to equal the length of it coming out of the convolutional blocks. Without padding, the length of the sequence coming out of a convolutional layer will be `filter_size - 1` shorter than the sequence entering the convolutional layer. For example, if we had a filter size of 3, the sequence will be 2 elements shorter. Thus, we pad the sentence with one padding element on each side. We can calculate the amount of padding on each side by simply doing `(filter_size - 1)/2` for odd sized filters - we will not cover even sized filters in this tutorial.\n",
    "\n",
    "These filters are designed so the output hidden dimension of them is twice the input hidden dimension. **Why do we double the size of the hidden dimension leaving the convolutional filter?** This is because we are using a special activation function called **gated linear units** (GLU). GLUs have gating mechanisms (similar to LSTMs and GRUs) contained within the activation function and actually half the size of the hidden dimension - whereas usually activation functions keep the hidden dimensions the same size.\n",
    "\n",
    "After passing through the GLU activation the hidden dimension size for each token is the same as it was when it entered the convolutional blocks. It is now elementwise summed with its own vector before it was passed through the convolutional layer. \n",
    "\n",
    "This concludes a single convolutional block. Subsequent blocks take the output of the previous block and perform the same steps. Each block has their own parameters, they are not shared between blocks. The output of the last block goes back to the main encoder - where it is fed through a linear layer to get the conved output and then elementwise summed with the embedding of the token to get the combined output.\n",
    "\n",
    "### Encoder Implementation\n",
    "\n",
    "To keep the implementation simple, we only allow for odd sized kernels. This allows padding to be added equally to both sides of the source sequence.\n",
    "\n",
    "The `scale` variable is used at the residual connection by the authors to \"ensure that the variance throughout the network does not change dramatically\". The performance of the model seems to vary wildly using different seeds if this is not used.\n",
    "\n",
    "The positional embedding is initialized to have a \"vocabulary\" of 100. This means it can handle sequences up to 100 elements long, indexed from 0 to 99. This can be increased if used on a dataset with longer sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, \n",
    "                 kernel_size, dropout, device, max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert kernel_size % 2 == 1, \"Kernel size must be odd!\"\n",
    "        \n",
    "        self.device = device\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
    "                                              out_channels = 2 * hid_dim, \n",
    "                                              kernel_size = kernel_size, \n",
    "                                              padding = (kernel_size - 1) // 2)\n",
    "                                    for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        batch_size = src.shape[0]\n",
    "        src_len = src.shape[1]\n",
    "        \n",
    "        #create position tensor\n",
    "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        #pos = [0, 1, 2, 3, ..., src len - 1]\n",
    "        #pos = [batch size, src len]\n",
    "        \n",
    "        #embed tokens and positions\n",
    "                \n",
    "        tok_embedded = self.tok_embedding(src)\n",
    "        pos_embedded = self.pos_embedding(pos)\n",
    "        \n",
    "        #tok_embedded = pos_embedded = [batch size, src len, emb dim]\n",
    "        \n",
    "        #combine embeddings by elementwise summing\n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
    "        #embedded = [batch size, src len, emb dim]\n",
    "        \n",
    "        #pass embedded through linear layer to convert from emb dim to hid dim\n",
    "        conv_input = self.emb2hid(embedded)\n",
    "        #conv_input = [batch size, src len, hid dim]\n",
    "        \n",
    "        #permute for convolutional layer\n",
    "        conv_input = conv_input.permute(0, 2, 1) \n",
    "        #conv_input = [batch size, hid dim, src len]\n",
    "        \n",
    "        #begin convolutional blocks...\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "        \n",
    "            #pass through convolutional layer\n",
    "            conved = conv(self.dropout(conv_input))\n",
    "            #conved = [batch size, 2 * hid dim, src len]\n",
    "\n",
    "            #pass through GLU activation function\n",
    "            conved = F.glu(conved, dim = 1)\n",
    "            #conved = [batch size, hid dim, src len]\n",
    "            \n",
    "            #apply residual connection\n",
    "            conved = (conved + conv_input) * self.scale\n",
    "            #conved = [batch size, hid dim, src len]\n",
    "            \n",
    "            #set conv_input to conved for next loop iteration\n",
    "            conv_input = conved\n",
    "        \n",
    "        #...end convolutional blocks\n",
    "        \n",
    "        #permute and convert back to emb dim\n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
    "        #conved = [batch size, src len, emb dim]\n",
    "        \n",
    "        #elementwise sum output (conved) and input (embedded) to be used for attention\n",
    "        combined = (conved + embedded) * self.scale\n",
    "        #combined = [batch size, src len, emb dim]\n",
    "        \n",
    "        return conved, combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decoder\n",
    "\n",
    "The decoder takes in the actual target sentence and tries to predict it. This model differs from the recurrent neural network models previously detailed in these tutorials as it predicts all tokens within the target sentence in **parallel**, **no decoding loop.**\n",
    "\n",
    "The decoder is similar to the encoder, with a few changes to both the main model and the convolutional blocks inside the model.\n",
    "\n",
    "<img src = \"figures/convseq2seq3.png\" >\n",
    "\n",
    "First, the embeddings do not have a residual connection that connects after the convolutional blocks and the transformation. Instead the embeddings are fed into the convolutional blocks to be used as residual connections there.\n",
    "\n",
    "Second, to feed the decoder information from the encoder, the encoder conved and combined outputs are used - again, within the convolutional blocks. \n",
    "\n",
    "Finally, the output of the decoder is a linear layer from embedding dimension to output dimension. This is used make a prediction about what the next word in the translation should be.\n",
    "\n",
    "### Decoder Convolutional Blocks\n",
    "\n",
    "Again, these are similar to the convolutional blocks within the encoder, with a few changes.\n",
    "\n",
    "<img src = \"figures/convseq2seq4.png\" >\n",
    "\n",
    "First, the padding. Instead of padding equally on each side to ensure the length of the sentence stays the same throughout, we only pad at the beginning of the sentence. As we are processing all of the targets simultaneously in parallel, and not sequentially, we need a method of only allowing the filters translating token $i$ to only look at tokens before word $i$. If they were allowed to look at token $i+1$ (the token they should be outputting), the model will simply learn to output the next word in the sequence by directly copying it, without actually learning how to translate.\n",
    "\n",
    "Let's see what happens if we **incorrectly** padded equally on each side, like we do in the encoder.\n",
    "\n",
    "<img src = \"figures/convseq2seq5.png\" >\n",
    "\n",
    "The filter at the first position, which is trying use the first word in the sequence, `<sos>` to predict the second word, `two`, can now directly see the word `two`. This is the same for every position, the word the model trying to predict is the second element covered by the filter. Thus, the filters can learn to simply copy the second word at each position allowing for perfect translation without actually learning how to translate.\n",
    "\n",
    "Second, after the GLU activation and before the residual connection, the block calculates and applies attention - using the encoded representations and the embedding of the current word. **Note**: we only show the connections to the rightmost token, but they are actually connected to all tokens - this was done for clarity. Each token input uses their own, and only their own, embedding for their own attention calculation.\n",
    "\n",
    "The attention is calculated by first using a linear layer to change the hidden dimension to the same size as the embedding dimension. Then the embedding summed via a residual connection. This combination then has the standard attention calculation applied by finding how much it \"matches\" with the *encoded conved* and then this is applied by getting a weighted sum over the *encoded combined*. This is then projected back up to the hidden dimenson size and a residual connection to the initial input to the attention layer is applied.\n",
    "\n",
    "Why do they calculate attention first with the encoded conved and then use it to calculate the weighted sum over the encoded combined? The paper argues that the encoded conved is good for getting a larger context over the encoded sequence, whereas the encoded combined has more information about the specific token and is thus therefore more useful for makng a prediction.\n",
    "\n",
    "### Decoder Impementation\n",
    "\n",
    "As we only pad on one side the decoder is allowed to use both odd and even sized padding. Again, the `scale` is used to reduce variance throughout the model and the position embedding is initialized to have a \"vocabulary\" of 100.\n",
    "\n",
    "This model takes in the encoder representations in its `forward` method and both are passed to the `calculate_attention` method which calculates and applies attention. It also returns the actual attention values, but we are not currently using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, kernel_size, dropout, \n",
    "                 trg_pad_idx, device, max_length = 100):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.kernel_size = kernel_size\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
    "        \n",
    "        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
    "        \n",
    "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        \n",
    "        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\n",
    "        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
    "                                              out_channels = 2 * hid_dim, \n",
    "                                              kernel_size = kernel_size)\n",
    "                                    for _ in range(n_layers)])\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "      \n",
    "    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\n",
    "        \n",
    "        #embedded = [batch size, trg len, emb dim]\n",
    "        #conved = [batch size, hid dim, trg len]\n",
    "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\n",
    "        \n",
    "        #permute and convert back to emb dim\n",
    "        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\n",
    "        #conved_emb = [batch size, trg len, emb dim]\n",
    "        \n",
    "        combined = (conved_emb + embedded) * self.scale\n",
    "        #combined = [batch size, trg len, emb dim]\n",
    "                \n",
    "        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\n",
    "        #energy = [batch size, trg len, src len]\n",
    "        \n",
    "        attention = F.softmax(energy, dim=2)\n",
    "        #attention = [batch size, trg len, src len]\n",
    "            \n",
    "        attended_encoding = torch.matmul(attention, encoder_combined)\n",
    "        #attended_encoding = [batch size, trg len, emd dim]\n",
    "        \n",
    "        #convert from emb dim -> hid dim\n",
    "        attended_encoding = self.attn_emb2hid(attended_encoding)\n",
    "        #attended_encoding = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #apply residual connection\n",
    "        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\n",
    "        #attended_combined = [batch size, hid dim, trg len]\n",
    "        \n",
    "        return attention, attended_combined\n",
    "        \n",
    "    def forward(self, trg, encoder_conved, encoder_combined):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #encoder_conved = encoder_combined = [batch size, src len, emb dim]\n",
    "                \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "            \n",
    "        #create position tensor\n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        \n",
    "        #pos = [batch size, trg len]\n",
    "        \n",
    "        #embed tokens and positions\n",
    "        tok_embedded = self.tok_embedding(trg)\n",
    "        pos_embedded = self.pos_embedding(pos)\n",
    "        #tok_embedded = [batch size, trg len, emb dim]\n",
    "        #pos_embedded = [batch size, trg len, emb dim]\n",
    "        \n",
    "        #combine embeddings by elementwise summing\n",
    "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
    "        #embedded = [batch size, trg len, emb dim]\n",
    "        \n",
    "        #pass embedded through linear layer to go through emb dim -> hid dim\n",
    "        conv_input = self.emb2hid(embedded)\n",
    "        #conv_input = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #permute for convolutional layer\n",
    "        conv_input = conv_input.permute(0, 2, 1) \n",
    "        #conv_input = [batch size, hid dim, trg len]\n",
    "        \n",
    "        batch_size = conv_input.shape[0]\n",
    "        hid_dim = conv_input.shape[1]\n",
    "        \n",
    "        for i, conv in enumerate(self.convs):\n",
    "        \n",
    "            #apply dropout\n",
    "            conv_input = self.dropout(conv_input)\n",
    "        \n",
    "            #need to pad so decoder can't \"cheat\"\n",
    "            padding = torch.zeros(batch_size, \n",
    "                                  hid_dim, \n",
    "                                  self.kernel_size - 1).fill_(self.trg_pad_idx).to(self.device)\n",
    "                \n",
    "            padded_conv_input = torch.cat((padding, conv_input), dim = 2)\n",
    "            #padded_conv_input = [batch size, hid dim, trg len + kernel size - 1]\n",
    "        \n",
    "            #pass through convolutional layer\n",
    "            conved = conv(padded_conv_input)\n",
    "            #conved = [batch size, 2 * hid dim, trg len]\n",
    "            \n",
    "            #pass through GLU activation function\n",
    "            conved = F.glu(conved, dim = 1)\n",
    "            #conved = [batch size, hid dim, trg len]\n",
    "            \n",
    "            #calculate attention\n",
    "            attention, conved = self.calculate_attention(embedded, conved, encoder_conved, encoder_combined)\n",
    "            #attention = [batch size, trg len, src len]\n",
    "            \n",
    "            #apply residual connection\n",
    "            conved = (conved + conv_input) * self.scale\n",
    "            #conved = [batch size, hid dim, trg len]\n",
    "            \n",
    "            #set conv_input to conved for next loop iteration\n",
    "            conv_input = conved\n",
    "            \n",
    "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
    "        #conved = [batch size, trg len, emb dim]\n",
    "            \n",
    "        output = self.fc_out(self.dropout(conved))\n",
    "        #output = [batch size, trg len, output dim]\n",
    "            \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together (become Seq2Seq!)\n",
    "\n",
    "The encapsulating `Seq2Seq` module is a lot different from recurrent neural network methods used in previous notebooks, especially in the decoding. \n",
    "\n",
    "Our `trg` has the `<eos>` element sliced off of the end of the sequence. This is because we do not input the `<eos>` token into the decoder.\n",
    "\n",
    "The encoding is similar, insert the source sequence and receive a \"context vector\". However, here we have two context vectors per word in the source sequence, `encoder_conved` and `encoder_combined`. \n",
    "\n",
    "As the decoding is done in parallel we do not need a decoding loop. All of the target sequence is input into the decoder at once and the padding is used to ensure each convolutional filter in the decoder can only see the current and previous tokens in the sequence as it slides across the sentence.\n",
    "\n",
    "This also, however, means we cannot do teacher forcing using this model. We do not have a loop in which we can choose whether to input the predicted token or the actual token in the sequence as everything is predicted in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len - 1] (<eos> token sliced off the end)\n",
    "           \n",
    "        # calculate z^u (encoder_conved) and (z^u + e) (encoder_combined)\n",
    "        # encoder_conved is output from final encoder conv. block\n",
    "        # encoder_combined is encoder_conved plus (elementwise) src embedding plus \n",
    "        # positional embeddings \n",
    "        encoder_conved, encoder_combined = self.encoder(src)\n",
    "        # encoder_conved = [batch size, src len, emb dim]\n",
    "        # encoder_combined = [batch size, src len, emb dim]\n",
    "        \n",
    "        # calculate predictions of next words\n",
    "        # output is a batch of predictions for each word in the trg sentence\n",
    "        # attention a batch of attention scores across the src sentence for \n",
    "        # each word in the trg sentence\n",
    "        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\n",
    "        # output    = [batch size, trg len - 1, output dim]\n",
    "        # attention = [batch size, trg len - 1, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "emb_dim     = 256  \n",
    "hid_dim     = 512  # each conv. layer has 2 * hid_dim filters because of GLU\n",
    "enc_layers  = 10\n",
    "dec_layers  = 10\n",
    "enc_kernel_size = 3 # must be odd!\n",
    "dec_kernel_size = 3 # can be even or odd\n",
    "enc_dropout     = 0.25\n",
    "dec_dropout     = 0.25\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "enc  = Encoder(input_dim,  emb_dim,  hid_dim, enc_layers, enc_kernel_size, enc_dropout, device)\n",
    "dec  = Decoder(output_dim, emb_dim,  hid_dim, dec_layers, dec_kernel_size, dec_dropout, TRG_PAD_IDX, device)\n",
    "\n",
    "model = Seq2Seq(enc, dec).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1324544\n",
      " 25600\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "1572864\n",
      "  1024\n",
      "1572864\n",
      "  1024\n",
      "1572864\n",
      "  1024\n",
      "1572864\n",
      "  1024\n",
      "1572864\n",
      "  1024\n",
      "1572864\n",
      "  1024\n",
      "1572864\n",
      "  1024\n",
      "1572864\n",
      "  1024\n",
      "1572864\n",
      "  1024\n",
      "1572864\n",
      "  1024\n",
      "1646848\n",
      " 25600\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "131072\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "1646848\n",
      "  6433\n",
      "1572864\n",
      "  1024\n",
      "1572864\n",
      "  1024\n",
      "1572864\n",
      "  1024\n",
      "1572864\n",
      "  1024\n",
      "1572864\n",
      "  1024\n",
      "1572864\n",
      "  1024\n",
      "1572864\n",
      "  1024\n",
      "1572864\n",
      "  1024\n",
      "1572864\n",
      "  1024\n",
      "1572864\n",
      "  1024\n",
      "______\n",
      "36942369\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss function calculates the average loss per token, however by passing the index of the `<pad>` token as the `ignore_index` argument we ignore the loss whenever the target token is a padding token. \n",
    "\n",
    "**Note**: I apply some scheduler, due to exploding gradients.  There are many types as follow:\n",
    "\n",
    "<img src = \"figures/scheduler.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the training loop for the model.\n",
    "\n",
    "We handle the sequences a little differently than previous tutorials. For all models we never put the `<eos>` into the decoder. This is handled in the RNN models by the having the decoder loop not reach having the `<eos>` as an input to the decoder. In this model, we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, trg in loader:\n",
    "            \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        output, _ = model(src, trg[:,:-1])\n",
    "        \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg    = [batch size, trg len]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output.reshape(-1, output_dim)\n",
    "        trg = trg[:,1:].reshape(-1)\n",
    "        \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg    = [batch size * trg len - 1]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #clip the gradients to prevent them from exploding\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation loop is the same as the training loop, just without the gradient calculations and parameter updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "        \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg    = [batch size, trg len]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.reshape(-1, output_dim)\n",
    "            trg = trg[:,1:].reshape(-1)\n",
    "\n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg    = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(valid_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we have reduced the `CLIP` value from 1 to 0.1 in order to train this model more reliably. With higher `CLIP` values, the gradient occasionally explodes.\n",
    "\n",
    "Although we have almost twice as many parameters as the attention based RNN model, it actually takes around half the time as the standard version and about the same time as the packed padded sequences version. This is due to all calculations being done in parallel using the convolutional filters instead of sequentially using RNNs. \n",
    "\n",
    "**Note: this model always has a teacher forcing ratio of 1, i.e. it will always use the ground truth next token from the target sequence (this is simply because CNN do everything in parallel so we cannot have the next token). This means we cannot compare perplexity values against the previous models when they are using a teacher forcing ratio that is not 1. To understand this, try run previous tutorials with teaching forcing ratio of 1, you will get very low perplexity.  In this aspect, it is better to compare BLEU.**  ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 19s\n",
      "\tTrain Loss: 4.296 | Train PPL:  73.389\n",
      "\t Val. Loss: 3.224 |  Val. PPL:  25.129\n",
      "Epoch: 02 | Time: 0m 18s\n",
      "\tTrain Loss: 3.286 | Train PPL:  26.735\n",
      "\t Val. Loss: 2.739 |  Val. PPL:  15.470\n",
      "Epoch: 03 | Time: 0m 18s\n",
      "\tTrain Loss: 2.891 | Train PPL:  18.008\n",
      "\t Val. Loss: 2.482 |  Val. PPL:  11.969\n",
      "Epoch: 04 | Time: 0m 18s\n",
      "\tTrain Loss: 2.757 | Train PPL:  15.745\n",
      "\t Val. Loss: 2.395 |  Val. PPL:  10.967\n",
      "Epoch: 05 | Time: 0m 18s\n",
      "\tTrain Loss: 2.698 | Train PPL:  14.852\n",
      "\t Val. Loss: 2.348 |  Val. PPL:  10.460\n",
      "Epoch: 06 | Time: 0m 18s\n",
      "\tTrain Loss: 2.684 | Train PPL:  14.644\n",
      "\t Val. Loss: 2.354 |  Val. PPL:  10.532\n",
      "Epoch: 07 | Time: 0m 18s\n",
      "\tTrain Loss: 2.672 | Train PPL:  14.474\n",
      "\t Val. Loss: 2.350 |  Val. PPL:  10.484\n",
      "Epoch: 08 | Time: 0m 18s\n",
      "\tTrain Loss: 2.679 | Train PPL:  14.570\n",
      "\t Val. Loss: 2.354 |  Val. PPL:  10.526\n",
      "Epoch: 09 | Time: 0m 18s\n",
      "\tTrain Loss: 2.674 | Train PPL:  14.494\n",
      "\t Val. Loss: 2.376 |  Val. PPL:  10.759\n",
      "Epoch: 10 | Time: 0m 18s\n",
      "\tTrain Loss: 2.679 | Train PPL:  14.577\n",
      "\t Val. Loss: 2.362 |  Val. PPL:  10.617\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 10\n",
    "clip       = 0.1\n",
    "\n",
    "save_path = f'models/{model.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    \n",
    "    valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "            \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 2.372 | Test PPL:  10.719 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "test_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on some random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Two young, White males are outside near many bushes.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,   19,   25,   15, 1069,  842,   17,   56,   84,  331, 1623,    5,\n",
       "           3], device='cuda:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = text_transform[SRC_LANGUAGE](sample[0]).to(device)\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,   21,   83,  262,   32,   89,   22,   91,    7,   16,  115,    0,\n",
       "        2893,    4,    3], device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_text = text_transform[TRG_LANGUAGE](sample[1]).to(device)\n",
    "trg_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we need to make sure we use `batch_first` scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_text.reshape(1, -1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = trg_text.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 13]), torch.Size([1, 15]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, attentions = model(src_text, trg_text) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 6433])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape #batch_size, trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since batch size is 1, we just take off that dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 6433])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall remove the first token since it's zeroes anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 6433])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output[1:]\n",
    "output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just take the top token with highest probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = output.argmax(1) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 83,   9,   9,  25,   7,  91,   8, 403, 115,   4,   4,   4,   3,  21],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mapping of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junge\n",
      ",\n",
      ",\n",
      "sich\n",
      "in\n",
      "Freien\n",
      "und\n",
      "vielen\n",
      "Nähe\n",
      ".\n",
      ".\n",
      ".\n",
      "<eos>\n",
      "Zwei\n"
     ]
    }
   ],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention\n",
    "\n",
    "Let's display the attentions to understand how the source text links with the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 13])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'Two',\n",
       " 'young',\n",
       " ',',\n",
       " 'White',\n",
       " 'males',\n",
       " 'are',\n",
       " 'outside',\n",
       " 'near',\n",
       " 'many',\n",
       " 'bushes',\n",
       " '.',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](sample[0]) + ['<eos>']\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'junge',\n",
       " ',',\n",
       " ',',\n",
       " 'sich',\n",
       " 'in',\n",
       " 'Freien',\n",
       " 'und',\n",
       " 'vielen',\n",
       " 'Nähe',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " '<eos>',\n",
       " 'Zwei']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(0).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2597046/3332828533.py:17: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels(x_ticks, rotation=45)\n",
      "/tmp/ipykernel_2597046/3332828533.py:18: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels(y_ticks)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(src_tokens, trg_tokens, attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Nice!  CNN is very fast, thanks to its parallel operation.  To make sure LSTM can be parallelized, we will learn Transformers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6 (default, Nov 17 2020, 08:01:36) \n[Clang 12.0.0 (clang-1200.0.32.21)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "714d3f4db9a58ba7d2f2a9a4fffe577af3df8551aebd380095064812e2e0a6a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
