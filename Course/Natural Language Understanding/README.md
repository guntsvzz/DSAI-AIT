#  AT82.05 Artificial Intelligence: Natural Language Understanding (NLU)
```
As we have mentioned in the class, read just one ACL / EMNLP / NIPS paper.   You can select any paper you like.

Append a table in a Github repo.

Since this is your first week, we shall warm up and give you two weeks for your first paper.

PS:  Note that you may not completely understand everything, but just try your best.
   
Once you reach the mark of 50+ papers, you will start to understand and become superman.

Enjoy reading.

Point criteria:
0:  Not done / copy
1:  Ok
2:  With good, clear, precise, personal explanations in own words
```

## This is read paper
- Thai Nested Named Entity Recognition - 2022 [click](./Assignment/paper/00%20-%20Thai%20Nested%20Named%20Entity%20Recognition%20Corpus.md)
- Quality Controlled Paraphrase Generation - 2021 [click](./Assignment/paper/04%20-%20Quality%20Controlled%20Paraphrase%20Generation.md)

## Classical Paper
- A Fast and Accurate Dependency Parser using Neural Networks - 2014 [click](./Assignment/paper/03%20-%20A%20Fast%20and%20Accurate%20Dependency%20Parser%20using%20Neural%20Networks.md)
- Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies - 2016 
- On the difficulty of training recurrent neural networks - 2013
- Long short-term memory, Hochreiter and Schmidhuber - 1997
- Learning to Forget: Continual Prediction with LSTM, Gers - 2000
- Learning Phrase Representations using RNN Encoderâ€“Decoder for Statistical Machine Translation - 2014
- Massive Exploration of Neural Machine Translation Architectures - 2017
- Fully Character-Level Neural Machine Translation without Explicit Segmentation - 2017
- Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models - 2016
- Enriching Word Vectors with Subword Information - 2017
- Semi-supervised sequence tagging with bidirectional language models - 2017
- Deep contextualized word representations - 2018