{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTML Lab 11: GNNs\n",
    "\n",
    "Today we explore GNNs, in particular [Graph Convolutional Networks by Kipf and Welling (2017)](https://arxiv.org/abs/1609.02907).\n",
    "\n",
    "This tutorial is based on the [PyTorch Geometric tutorial for GCN](https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html).\n",
    "\n",
    "## Setup\n",
    "\n",
    "PyG only seems to work with recent versions of PyTorch, so you may need to upgrade:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 123012\n",
    "name = 'Todsavad Tangtortan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade torch\n",
    "# !pip install torch_geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be sure to restart your kernel if you're doing this in Jupyter.\n",
    "\n",
    "## Our first graph\n",
    "\n",
    "OK, with that set up, let's use PyG to build a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch_geometric\n",
    "torch_geometric.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import os\n",
    "\n",
    "# os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "    \n",
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                           [1, 0, 2, 1]], dtype=torch.long)\n",
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3, 1], edge_index=[2, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that this defines a graph with three nodes and two edges. The edge index\n",
    "is in so-called [COO format](https://pytorch.org/docs/stable/sparse.html#sparse-coo-docs) (a format for representing sparse matrices)\n",
    "and in this case is a $2\\times 2|E|$ tensor.\n",
    "\n",
    "The indices are indexing the set of nodes, in the range 0 to $N-1$.\n",
    "\n",
    "The following is equivalent. Note that each undirected edge is denoted by two pairs of indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor([[0, 1],\n",
    "                           [1, 0],\n",
    "                           [1, 2],\n",
    "                           [2, 1]], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor `x` is providing the *features* of the nodes in the graph, in this case a scalar for each node. As we know,\n",
    "node features can be vectors as well, but all nodes must have the same feature dimensionality.\n",
    "\n",
    "We can check the validity of our graph with the `validate()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.validate(raise_on_error=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try some other attributes and:\n",
    "- `x`\n",
    "- `edge_index`\n",
    "- `edge_attr`\n",
    "- `num_nodes`\n",
    "- `num_edges`\n",
    "- `num_node_features`\n",
    "- `has_isolated_nodes()`\n",
    "- `has_self_loops()`\n",
    "- `is_directed()`\n",
    "\n",
    "And you can move a graph to a GPU just like a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "data = data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyG datasets (Cora)\n",
    "\n",
    "Let's load the Cora dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "dataset = Planetoid(root='./data/Cora', name='Cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset[0]\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a few of the dataset's methods:\n",
    "- `len(dataset)`\n",
    "- `dataset.num_classes`\n",
    "- `dataset.num_node_features`\n",
    "- `dataset[0]`\n",
    "\n",
    "From this we see that Cora is a single graph.\n",
    "We will only be doing transduction with this dataset.\n",
    "After assigning the single graph to a variable such as `data`,\n",
    "explore its characteristics:\n",
    "- `data.is_undirected()`\n",
    "- `data.train_mask.sum().item()`\n",
    "- `data.val_mask.sum().item()`\n",
    "- `data.test_mask.sum().item()`\n",
    "\n",
    "As this graph is mainly used to test semi-supervised learning algorithms, we see that the validation and test data are more numerous than the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batches\n",
    "\n",
    "How can we train on multiple examples in parallel? PyG puts multiple graphs together with block-diagonal adjacency matrices and concatenated features.\n",
    "See the [tutorial section on mini-batches](https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html#mini-batches) for more\n",
    "information.\n",
    "\n",
    "For a GCN on Cora, however, we don't need fancy data loaders or parallel mini-batches, as we're dealing with just one graph.\n",
    "\n",
    "## Define a GCN\n",
    "\n",
    "So let's define a GCN for Cora, then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn as nn\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self,dataset):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.normalize(x)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving the model to a GPU and training is the same as for any other PyTorch model. The \"data\" is treated as a single example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 50\n",
      "Epoch : 100\n",
      "Epoch : 150\n",
      "Epoch : 200\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(dataset).to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch+1) % 50 == 0: \n",
    "        print(f'Epoch : {epoch+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we an evaluate on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8170\n"
     ]
    }
   ],
   "source": [
    "model.eval() #No dropout\n",
    "pred = model(data).argmax(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Implement early stopping using the validation set for the Cora GCN.\n",
    "2. Reproduce the rest of the results from Kipf and Welling with your GCN.\n",
    "3. Implement the [Graph Attention Network](https://arxiv.org/abs/1710.10903) and reproduce their results on the datasets available via PyG.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Early Stopping \n",
    "Implement early stopping using the validation set for the Cora GCN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def valid(model,data):\n",
    "    model.eval() #No dropout\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.val_mask], data.y[data.val_mask])\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "def test(model,data):\n",
    "    model.eval() #No dropout\n",
    "    pred = model(data).argmax(dim=1)\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "    acc = int(correct) / int(data.test_mask.sum())\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, monitor='val_loss', patience=10, mode='min', delta=0.00001):\n",
    "        self.monitor = monitor\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, epoch, current_score):\n",
    "        if self.mode == 'min':\n",
    "            current_score *= -1  # flip the sign for minimization\n",
    "        if self.best_score is None:\n",
    "            self.best_score = current_score\n",
    "        elif current_score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = current_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(dataset).to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "num_epochs = 200\n",
    "early_stopping = EarlyStopping(patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010 Train Loss: 1.5452 Val Loss: 1.6988\n",
      "Epoch: 020 Train Loss: 1.0170 Val Loss: 1.3865\n",
      "Epoch: 030 Train Loss: 0.6126 Val Loss: 1.0893\n",
      "Epoch: 040 Train Loss: 0.3385 Val Loss: 0.9202\n",
      "Epoch: 050 Train Loss: 0.2685 Val Loss: 0.8397\n",
      "Epoch: 060 Train Loss: 0.2027 Val Loss: 0.8114\n",
      "Epoch: 070 Train Loss: 0.1924 Val Loss: 0.7849\n",
      "Epoch: 080 Train Loss: 0.1634 Val Loss: 0.7695\n",
      "Epoch: 090 Train Loss: 0.1485 Val Loss: 0.7542\n",
      "Epoch: 100 Train Loss: 0.1606 Val Loss: 0.7391\n",
      "Epoch: 110 Train Loss: 0.1282 Val Loss: 0.7420\n",
      "We are at epoch: 110\n",
      "Epoch: 111 Train Loss: 0.1061 Val Loss: 0.7409\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, data)\n",
    "    val_loss  = valid(model, data)\n",
    "    test_acc = test(model,data)\n",
    "    \n",
    "    early_stopping(epoch, val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"We are at epoch:\", epoch)\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "            'Train Loss: {:.4f}'.format(train_loss.item()),\n",
    "            'Val Loss: {:.4f}'.format(val_loss.item()*-1))\n",
    "        break\n",
    "              \n",
    "    if (epoch+1) % 10 == 0: \n",
    "        # print(f'Epoch : {epoch+1:03d}, Loss: {val_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "            'Train Loss: {:.4f}'.format(train_loss.item()),\n",
    "            'Val Loss: {:.4f}'.format(val_loss.item()*-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8150\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "pred = model(data).argmax(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kipf and Welling\n",
    "Reproduce the rest of the results from Kipf and Welling with your GCN."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the following sets of hyperparameters for\n",
    "- Cora 0.5 (dropout rate), 5 · 10^−4\n",
    "(L2 regularization) and 16 (number of hidden units)\n",
    "- NELL: 0.1 (dropout rate), 1 · 10^−5\n",
    "(L2 regularization) and 64 (number of hidden\n",
    "units).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 torch_geometric --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "# !pip install torch-scatter torch-sparse -f https://data.pyg.org/whl/torch-1.13.0+cu117.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.datasets import NELL\n",
    "\n",
    "dataset_cora = Planetoid(root='./data/Cora', name='Cora')\n",
    "# dataset_Citeseer = Planetoid(root='./data/Citeseer', name='Citeseer')\n",
    "# dataset_Pubmed = Planetoid(root='./data/Pubmed', name='Pubmed')\n",
    "dataset_NELL = NELL(root='./data/NELL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(model):\n",
    "    if isinstance(model, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform(model.weight)\n",
    "        model.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data = dataset_cora[0].to(device)\n",
    "\n",
    "model = GCN(dataset_cora).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "num_epochs = 200\n",
    "early_stopping = EarlyStopping(patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010 Train Loss: 1.5303 Val Loss: 1.6895\n",
      "Epoch: 020 Train Loss: 1.0476 Val Loss: 1.3734\n",
      "Epoch: 030 Train Loss: 0.5806 Val Loss: 1.0790\n",
      "Epoch: 040 Train Loss: 0.3818 Val Loss: 0.9020\n",
      "Epoch: 050 Train Loss: 0.2339 Val Loss: 0.8162\n",
      "Epoch: 060 Train Loss: 0.1933 Val Loss: 0.7873\n",
      "Epoch: 070 Train Loss: 0.1571 Val Loss: 0.7653\n",
      "Epoch: 080 Train Loss: 0.1482 Val Loss: 0.7530\n",
      "Epoch: 090 Train Loss: 0.1430 Val Loss: 0.7410\n",
      "Epoch: 100 Train Loss: 0.1310 Val Loss: 0.7386\n",
      "Epoch: 110 Train Loss: 0.1166 Val Loss: 0.7275\n",
      "Epoch: 120 Train Loss: 0.1172 Val Loss: 0.7265\n",
      "We are at epoch: 123\n",
      "Epoch: 124 Train Loss: 0.1099 Val Loss: 0.7249\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, data)\n",
    "    val_loss  = valid(model, data)\n",
    "    # test_acc = test(model,data)\n",
    "    \n",
    "    early_stopping(epoch, val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"We are at epoch:\", epoch)\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "            'Train Loss: {:.4f}'.format(train_loss.item()),\n",
    "            'Val Loss: {:.4f}'.format(val_loss.item()*-1))\n",
    "        break\n",
    "              \n",
    "    if (epoch+1) % 10 == 0: \n",
    "        # print(f'Epoch : {epoch+1:03d}, Loss: {val_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "            'Train Loss: {:.4f}'.format(train_loss.item()),\n",
    "            'Val Loss: {:.4f}'.format(val_loss.item()*-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8130\n"
     ]
    }
   ],
   "source": [
    "def accuracy(data):\n",
    "    model.eval() #No dropout\n",
    "    pred = model(data).argmax(dim=1)\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "    acc = int(correct) / int(data.test_mask.sum())\n",
    "    print(f'Accuracy: {acc:.4f}')\n",
    "\n",
    "accuracy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn as nn\n",
    "\n",
    "class GCN_NELL(torch.nn.Module):\n",
    "    def __init__(self,dataset):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, 64)\n",
    "        self.conv2 = GCNConv(64, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x.to_dense(), data.edge_index.to_dense()\n",
    "\n",
    "        x = F.normalize(x)\n",
    "        \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training, p=0.1)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning** \n",
    "\n",
    "*Memory requirement* In the current setup with full-batch gradient descent, memory requirement\n",
    "grows linearly in the size of the dataset. We have shown that for large graphs that do not fit in GPU\n",
    "memory, training on CPU can still be a viable option. \n",
    "\n",
    "Mini-batch stochastic gradient descent can\n",
    "alleviate this issue. The procedure of generating mini-batches, however, should take into account the\n",
    "number of layers in the GCN model, as the Kth-order neighborhood for a GCN with K layers has to\n",
    "be stored in memory for an exact procedure. For very large and densely connected graph datasets,\n",
    "further approximations might be necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'\n",
    "\n",
    "data = dataset_NELL[0].to(device)\n",
    "\n",
    "model = GCN_NELL(dataset_NELL).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "num_epochs = 100\n",
    "early_stopping = EarlyStopping(patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010 Train Loss: 4.6822 Val Loss: 4.7744\n",
      "Epoch: 020 Train Loss: 3.6778 Val Loss: 4.3350\n",
      "Epoch: 030 Train Loss: 2.4306 Val Loss: 3.8665\n",
      "Epoch: 040 Train Loss: 1.2967 Val Loss: 3.3216\n",
      "Epoch: 050 Train Loss: 0.7001 Val Loss: 2.9760\n",
      "Epoch: 060 Train Loss: 0.4625 Val Loss: 2.8736\n",
      "Epoch: 070 Train Loss: 0.3803 Val Loss: 2.7510\n",
      "Epoch: 080 Train Loss: 0.3105 Val Loss: 2.6610\n",
      "Epoch: 090 Train Loss: 0.2754 Val Loss: 2.5940\n",
      "Epoch: 100 Train Loss: 0.2468 Val Loss: 2.5692\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, data)\n",
    "    val_loss  = valid(model, data)\n",
    "    # test_acc = test(model,data)\n",
    "    \n",
    "    early_stopping(epoch, val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"We are at epoch:\", epoch)\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "            'Train Loss: {:.4f}'.format(train_loss.item()),\n",
    "            'Val Loss: {:.4f}'.format(val_loss.item()*-1))\n",
    "        break\n",
    "              \n",
    "    if (epoch+1) % 10 == 0: \n",
    "        # print(f'Epoch : {epoch+1:03d}, Loss: {val_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "            'Train Loss: {:.4f}'.format(train_loss.item()),\n",
    "            'Val Loss: {:.4f}'.format(val_loss.item()*-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4861\n"
     ]
    }
   ],
   "source": [
    "def accuracy(data):\n",
    "    model.eval() #No dropout\n",
    "    pred = model(data).argmax(dim=1)\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "    acc = int(correct) / int(data.test_mask.sum())\n",
    "    print(f'Accuracy: {acc:.4f}')\n",
    "\n",
    "accuracy(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GAT via PyG\n",
    "Implement the [Graph Attention Network](https://arxiv.org/abs/1710.10903) and reproduce their results on the datasets available via PyG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transductive learning** \n",
    "For the transductive learning tasks, we apply a two-layer GAT model. \n",
    "Its architectural hyperparameters have been optimized on the Cora dataset.\n",
    "\n",
    "The 1st layer : K = 8 attention heads, 8 features each (a total of 64 features), followed by an exponential linear unit (ELU)nonlinearity. \n",
    "\n",
    "The 2nd layer is used for classification: a single attention head that computes C features followed by a softmax activation. \n",
    "\n",
    "During training, we apply L2 regularization with λ = 0.0005. Furthermore, dropout with p = 0.6 is applied to both layers’ inputs, as well as to the normalized attention coefficients (critically, this means that at each training iteration, each node is exposed to a stochastically sampled neighborhood). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GAT_Transductive(torch.nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.head = 8\n",
    "        self.feature = 8 \n",
    "        self.conv1 = GATConv(in_channels = dataset.num_node_features, \n",
    "                             out_channels = self.feature, \n",
    "                             heads = self.head, \n",
    "                             dropout=0.6)\n",
    "        self.conv2 = GATConv(in_channels = self.feature * self.head, \n",
    "                             out_channels = dataset.num_classes, \n",
    "                             heads=1, \n",
    "                             concat=True,\n",
    "                             dropout=0.6)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        x = F.normalize(x)\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        # x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "dataset_cora = Planetoid(root='./data/Cora', name='Cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "data = dataset_cora[0].to(device)\n",
    "\n",
    "model = GAT_Transductive(dataset_cora).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "num_epochs = 100\n",
    "early_stopping = EarlyStopping(patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def valid(model,data):\n",
    "    model.eval() #No dropout\n",
    "    out = model(data)\n",
    "    loss = F.cross_entropy(out[data.val_mask], data.y[data.val_mask])\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "def test(model,data):\n",
    "    model.eval() #No dropout\n",
    "    pred = model(data).argmax(dim=1)\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "    acc = int(correct) / int(data.test_mask.sum())\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010 Train Loss: 1.5202 Val Loss: 1.6461\n",
      "Epoch: 020 Train Loss: 0.9906 Val Loss: 1.2806\n",
      "Epoch: 030 Train Loss: 0.7572 Val Loss: 0.9640\n",
      "Epoch: 040 Train Loss: 0.5484 Val Loss: 0.7996\n",
      "Epoch: 050 Train Loss: 0.5442 Val Loss: 0.7287\n",
      "Epoch: 060 Train Loss: 0.4993 Val Loss: 0.6829\n",
      "Epoch: 070 Train Loss: 0.5078 Val Loss: 0.6785\n",
      "Epoch: 080 Train Loss: 0.3625 Val Loss: 0.6598\n",
      "Epoch: 090 Train Loss: 0.4113 Val Loss: 0.6464\n",
      "Epoch: 100 Train Loss: 0.4098 Val Loss: 0.6457\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, data)\n",
    "    val_loss  = valid(model, data)\n",
    "    # test_acc = test(model,data)\n",
    "    \n",
    "    early_stopping(epoch, val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"We are at epoch:\", epoch)\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "            'Train Loss: {:.4f}'.format(train_loss.item()),\n",
    "            'Val Loss: {:.4f}'.format(val_loss.item()*-1))\n",
    "        break\n",
    "              \n",
    "    if (epoch+1) % 10 == 0: \n",
    "        # print(f'Epoch : {epoch+1:03d}, Loss: {val_loss:.4f}, Test Acc: {test_acc:.4f}')\n",
    "        print('Epoch: {:03d}'.format(epoch+1),\n",
    "            'Train Loss: {:.4f}'.format(train_loss.item()),\n",
    "            'Val Loss: {:.4f}'.format(val_loss.item()*-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we an evaluate on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8230\n"
     ]
    }
   ],
   "source": [
    "def accuracy(data):\n",
    "    model.eval() #No dropout\n",
    "    pred = model(data).argmax(dim=1)\n",
    "    correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "    acc = int(correct) / int(data.test_mask.sum())\n",
    "    print(f'Accuracy: {acc:.4f}')\n",
    "\n",
    "accuracy(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "|                       | Cora        | NELL    |\n",
    "|-----------------------|-------------|---------|\n",
    "| GCN (paper)           | 81.5 %      | 66.0  % |\n",
    "| GCN early stopping    | 81.5 %      |         |\n",
    "| GCN (Kipf and Welling)| 81.3 %      | 48.61 % |\n",
    "| GAT (paper)           | 83.0 ± 0.7% |         |\n",
    "| GAT (own)             | 82.3 %      |         |\n",
    "\n",
    "After changing Hyperparameter following, in GCN experiment, as Kipf and Welling, our implementational accuracy of Cora is same as paper. In contrast, NELL Dataset's accuracy cannot reach the performance as GCN paper. \n",
    "\n",
    "In GAT experiment, it was followed configuraiton as paper attempt. Thus, it return performance equally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
