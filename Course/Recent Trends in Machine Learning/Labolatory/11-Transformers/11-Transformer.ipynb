{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 11: Transformers\n",
    "\n",
    "In today's lab, we will learn how to create a transformer from scratch, then\n",
    "we'll take a look at ViT (the visual transformer). Some of the material in this\n",
    "lab comes from the following online sources:\n",
    "\n",
    "- https://medium.com/the-dl/transformers-from-scratch-in-pytorch-8777e346ca51\n",
    "- https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec\n",
    "- https://towardsdatascience.com/implementing-visualttransformer-in-pytorch-184f9f16f632\n",
    "- https://github.com/lucidrains/vit-pytorch#vision-transformer---pytorch\n",
    "- https://medium.com/mlearning-ai/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c\n",
    "\n",
    "<img src=\"img/optimus_prime.jpg\" title=\"Transformer\" style=\"width: 600px;\" />\n",
    "\n",
    "The above photo needs a credit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers and machine learning trends\n",
    "\n",
    "Before the arrival of transformers, CNNs were most often used in the visual domain, while RNNs like LSTMs were most often used in NLP.\n",
    "There were many attempts at crossover, without much real success. Neither approach seemed capable of dealing with very large complex\n",
    "natural language datasets effectively.\n",
    "\n",
    "In 2017, the Transformer was introduced. \"Attention is all you need\" has been cited more than 38,000 times.\n",
    "\n",
    "The main concept in a Transformer is self-attention, which replaces the sequential processing of RNNs and the local\n",
    "processing of CNNs with the ability to adaptively extract arbitrary relationships between different elements of its input,\n",
    "output, and memory state.\n",
    "\n",
    "## Transformer architecture\n",
    "\n",
    "We will use [Frank Odom's implementation of the Transformer in PyTorch](https://github.com/fkodom/transformer-from-scratch/tree/main/src).\n",
    "\n",
    "The architecture of the transformer looks like this:\n",
    "\n",
    "<img src=\"img/Transformer.png\" title=\"Transformer\" style=\"width: 600px;\" />\n",
    "\n",
    "Here is a summary of the Transformer's details and mathematics:\n",
    "\n",
    "<img src=\"img/SummaryTransformer.PNG\" title=\"Transformer Details\" style=\"width: 1000px;\" />\n",
    "\n",
    "There are several processes that we need to implement in the model. We go one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "Before Transformers, the standard model for sequence-to-sequence learning was seq2seq, which combines an RNN for encoding with\n",
    "an RNN for decoding. The encoder processes the input and retains important information in a sequence or block of memory,\n",
    "while the decoder extracts the important information from the memory in order to produce an output.\n",
    "\n",
    "One problem with seq2seq is that some information may be lost while processing a long sequence.\n",
    "Attention allows us to focus on specific inputs directly.\n",
    "\n",
    "An attention-based decoder, when we want to produce the output token at a target position, will calculate an attention score\n",
    "with the encoder's memory at each input position. A high score for a particular encoder position indicates that it is more important\n",
    "than another position. We essentially use the decoder's input to select which encoder output(s) should be used to calculate the\n",
    "current decoder output. Given decoder input $q$ (the *query*) and encoder outputs $p_i$, the attention operation calculates dot\n",
    "products between $q$ and each $p_i$. The dot products give the similarity of each pair. The dot products are softmaxed to get\n",
    "positive weights summing to 1, and the weighted average $r$ is calculated as\n",
    "\n",
    "$$r = \\sum_i \\frac{e^{p_i\\cdot q}}{\\sum_j e^{p_j\\cdot q}}p_i .$$\n",
    "\n",
    "We can think of $r$ as an adaptively selected combination of the inputs most relevant to producing an output.\n",
    "\n",
    "### Multi-head self attention\n",
    "\n",
    "Transformers use a specific type of attention mechanism, referred to as multi-head self attention.\n",
    "This is the most important part of the model. An illustration from the paper is shown below.\n",
    "\n",
    "<img src=\"img/MultiHeadAttention.png\" title=\"Transformer\" style=\"width: 600px;\" />\n",
    "\n",
    "The multi-head attention layer is described as:\n",
    "\n",
    "$$\\text{Attention}(Q,K,V)=\\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "$Q$, $K$, and $V$ are batches of matrices, each with shape <code>(batch_size, seq_length, num_features)</code>.\n",
    "When we are talking about *self* attention, each of the three matrices in\n",
    "each batch is just a separate linear projection of the same input $\\bar{h}_t^{l-1}$.\n",
    "\n",
    "Multiplying the query $Q$ with the key $K$ arrays results in a <code>(batch_size, seq_length, seq_length)</code> array,\n",
    "which tells us roughly how important each element in the sequence is to each other element in the sequence. These dot\n",
    "products are converted to normalized weights using a softmax across rows, so that each row of weights sums to one.\n",
    "Finally, the weight matrix attention is applied to the value ($V$) array using matrix multiplication. We thus get,\n",
    "for each token in the input sequence, a weighted average of the rows of $V$, each of which corresponds to one of the\n",
    "elements in the input sequence.\n",
    "\n",
    "Here is code for the scaled dot-product operation that is part of a multi-head attention layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as f\n",
    "from torch import Tensor, nn\n",
    "\n",
    "def scaled_dot_product_attention(query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "    # MatMul operations are translated to torch.bmm in PyTorch\n",
    "    temp = query.bmm(key.transpose(1, 2))\n",
    "    scale = query.size(-1) ** 0.5\n",
    "    softmax = f.softmax(temp / scale, dim=-1)\n",
    "    return softmax.bmm(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multi-head attention module is composed of several identical\n",
    "*attention head* modules.\n",
    "Each attention head contains three linear transformations for $Q$, $K$, and $V$ and combines them using scaled dot-product attention.\n",
    "Note that this attention head could be used for self attention or another type of attention such as decoder-to-encoder attention, since\n",
    "we keep $Q$, $K$, and $V$ separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, dim_in: int, dim_q: int, dim_k: int):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(dim_in, dim_q)\n",
    "        self.k = nn.Linear(dim_in, dim_k)\n",
    "        self.v = nn.Linear(dim_in, dim_k)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "        return scaled_dot_product_attention(self.q(query), self.k(key), self.v(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple attention heads can be combined with the output concatenation and linear transformation to construct a multi-head attention layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, dim_in: int, dim_q: int, dim_k: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(dim_in, dim_q, dim_k) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(num_heads * dim_k, dim_in)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "        return self.linear(\n",
    "            torch.cat([h(query, key, value) for h in self.heads], dim=-1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each attention head computes its own transformation of the query, key, and value arrays,\n",
    "and then applies scaled dot-product attention. Conceptually, this means each head can attend to a different part of the input sequence, independent of the others. Increasing the number of attention heads allows the model to pay attention to more parts of the sequence at\n",
    "once, which makes the model more powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "To complete the transformer encoder, we need another component, the *position encoder*.\n",
    "The <code>MultiHeadAttention</code> class we just write has no trainable components that depend on a token's position\n",
    "in the sequence (axis 1 of the input tensor). Meaning all of the weight matrices we have seen so far\n",
    "*perform the same calculation for every input position*; that is, we don't have any position-dependent weights.\n",
    "All of the operations so far operate over the *feature dimension* (axis 2). This is good in that the model is compatible with any sequence\n",
    "length. But without *any* information about position, our model is going to be unable to differentiate between different orderings of\n",
    "the input -- we'll get the same result regardless of the order of the tokens in the input.\n",
    "\n",
    "Since order matters (\"Ridgemont was in the store\" has a different\n",
    "meaning from \"The store was in Ridgemont\"), we need some way to provide the model with information about tokens' positions in the input sequence.\n",
    "Whatever strategy we use should provide information about the relative position of data points in the input sequences.\n",
    "In the Transformer, positional information is encoded using trigonometric functions in a constant 2D matrix $PE$:\n",
    "\n",
    "$$PE_{(pos,2i)}=\\sin (\\frac{pos}{10000^{2i/d_{model}}})$$\n",
    "$$PE_{(pos,2i+1)}=\\cos (\\frac{pos}{10000^{2i/d_{model}}}),$$\n",
    "\n",
    "where $pos$ refers to a position in the input sentence sequence and $i$ refers to the position along the embedding vector dimension.\n",
    "This matrix is *added* to the matrix consisting of the embeddings of each of the input tokens:\n",
    "\n",
    "<img src=\"img/positionalencoder.png\" title=\"Positional Encoder\" style=\"width: 400px;\" />\n",
    "\n",
    "Position encoding can implemented as follows (put this in `utils.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding(seq_len: int, dim_model: int, device: torch.device = torch.device(\"cpu\")) -> Tensor:\n",
    "    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n",
    "    dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
    "    phase = pos / (1e4 ** (dim // dim_model))\n",
    "\n",
    "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These sinusoidal encodings allow us to work with arbirary length sequences because the sine and cosine functions are periodic in the range\n",
    "$[-1, 1]$. One hope is that if during inference we are provided with an input sequence longer than any found during training.\n",
    "The position encodings of the last elements in the sequence would be different from anything the model has seen before, but with the\n",
    "periodic sine/cosine encoding, there will still be some similar structure, with the new encodings being very similar to neighboring encodings the model has seen before. For this reason, despite the fact that learned embeddings appeared to perform equally as well, the authors chose\n",
    "this fixed sinusoidal encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The complete encoder\n",
    "\n",
    "The transformer uses an encoder-decoder architecture. The encoder processes the input sequence and returns a sequence of\n",
    "feature vectors or memory vectors, while the decoder outputs a prediction of the target sequence,\n",
    "incorporating information from the encoder memory.\n",
    "\n",
    "First, let's complete the transformer layer with the two-layer feed forward network. Put this in `utils.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(dim_input: int = 512, dim_feedforward: int = 2048) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim_input, dim_feedforward),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(dim_feedforward, dim_input),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a residual module to encapsulate the feed forward network or attention\n",
    "model along with the common dropout and LayerNorm operations (also in `utils.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, *tensors: Tensor) -> Tensor:\n",
    "        # Assume that the \"query\" tensor is given first, so we can compute the\n",
    "        # residual.  This matches the signature of 'MultiHeadAttention'.\n",
    "        return self.norm(tensors[0] + self.dropout(self.sublayer(*tensors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the complete encoder! Put this in `encoder.py`. First, the encoder layer\n",
    "module, which comprised a self attention residual block followed by a fully connected residual block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_q = dim_k = max(dim_model // num_heads, 1)\n",
    "        self.attention = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.feed_forward = Residual(\n",
    "            feed_forward(dim_model, dim_feedforward),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        src = self.attention(src, src, src)\n",
    "        return self.feed_forward(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the Transformer encoder just encapsulates several transformer encoder layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int = 6,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerEncoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        seq_len, dimension = src.size(1), src.size(2)\n",
    "        src += position_encoding(seq_len, dimension)\n",
    "        for layer in self.layers:\n",
    "            src = layer(src)\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The decoder\n",
    "\n",
    "The decoder module is quite similar to the encoder, with just a few small differences:\n",
    "- The decoder accepts two inputs (the target sequence and the encoder memory), rather than one input.\n",
    "- There are two multi-head attention modules per layer (the target sequence self-attention module and the decoder-encoder attention module) rather than just one.\n",
    "- The second multi-head attention module, rather than strict self attention, expects the encoder memory as $K$ and $V$.\n",
    "- Since accessing future elements of the target sequence would be \"cheating,\" we need to mask out future elements of the input target sequence.\n",
    "\n",
    "First, we have the decoder version of the transformer layer and the decoder module itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_q = dim_k = max(dim_model // num_heads, 1)\n",
    "        self.attention_1 = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.attention_2 = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.feed_forward = Residual(\n",
    "            feed_forward(dim_model, dim_feedforward),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
    "        tgt = self.attention_1(tgt, tgt, tgt)\n",
    "        tgt = self.attention_2(tgt, memory, memory)\n",
    "        return self.feed_forward(tgt)\n",
    "\n",
    "    \n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int = 6,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerDecoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.linear = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
    "        seq_len, dimension = tgt.size(1), tgt.size(2)\n",
    "        tgt += position_encoding(seq_len, dimension)\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory)\n",
    "\n",
    "        return torch.softmax(self.linear(tgt), dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is not, as of yet, any masked attention implementation here!\n",
    "Making this version of the Transformer work in practice would require at least that.\n",
    "\n",
    "### Putting it together\n",
    "\n",
    "Now we can put the encoder and decoder together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_encoder_layers: int = 6,\n",
    "        num_decoder_layers: int = 6,\n",
    "        dim_model: int = 512, \n",
    "        num_heads: int = 6, \n",
    "        dim_feedforward: int = 2048, \n",
    "        dropout: float = 0.1, \n",
    "        activation: nn.Module = nn.ReLU(),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(\n",
    "            num_layers=num_encoder_layers,\n",
    "            dim_model=dim_model,\n",
    "            num_heads=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.decoder = TransformerDecoder(\n",
    "            num_layers=num_decoder_layers,\n",
    "            dim_model=dim_model,\n",
    "            num_heads=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor) -> Tensor:\n",
    "        return self.decoder(tgt, self.encoder(src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s create a simple test, as a sanity check for our implementation. We can construct random tensors for the input and target sequences, check that our model executes without errors, and confirm that the output tensor has the correct shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 512])\n"
     ]
    }
   ],
   "source": [
    "src = torch.rand(64, 32, 512)\n",
    "tgt = torch.rand(64, 16, 512)\n",
    "out = Transformer()(src, tgt)\n",
    "print(out.shape)\n",
    "# torch.Size([64, 16, 512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could try implementing masked attention and training this Transformer model on a\n",
    "sequence-to-sequence problem. However, to understand masking, you might first find\n",
    "the [PyTorch Transformer tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n",
    "useful. Note that this model is only a Transformer encoder for language modeling, but it uses\n",
    "masking in the encoder's self attention module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Transformer (ViT)\n",
    "\n",
    "The Vision Transformer (ViT) is a transformer targeted at vision processing tasks. It has achieved state-of-the-art performance in image classification and (with some modification) other tasks. The ViT concept for image classification is as follows:\n",
    "\n",
    "<img src=\"img/vit.gif\" title=\"ViT\" />\n",
    "\n",
    "### How does ViT work?\n",
    "\n",
    "The steps of ViT are as follows:\n",
    "\n",
    "1. Split input image into patches\n",
    "2. Flatten the patches\n",
    "3. Produce linear embeddings from the flattened patches\n",
    "4. Add position embeddings\n",
    "5. Feed the sequence preceeded by a `[class]` token as input to a standard transformer encoder\n",
    "6. Pretrain the model to ouptut image labels for the `[class]` token (fully supervised on a huge dataset such as ImageNet-22K)\n",
    "7. Fine-tune on the downstream dataset for the specific image classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT architecture\n",
    "\n",
    "ViT is a Transformer encoder. In detail, it looks like this:\n",
    "\n",
    "<img src=\"img/ViTArchitecture.png\" title=\"ViT architecture\" />\n",
    "\n",
    "In the figure we see four main parts:\n",
    "<ol style=\"list-style-type:lower-alpha\">\n",
    "    <li> The high-level architecture of the model.</li>\n",
    "    <li> The Transformer module.</li>\n",
    "    <li> The multiscale self-attention (MSA) head.</li>\n",
    "    <li> An individual self-attention (SA) head.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start\n",
    "\n",
    "Let's do a small scale implementation with the MNIST dataset. The\n",
    "code here is based on [Brian Pulfer's paper reimplementation repository](https://github.com/BrianPulfer/PapersReimplementations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./../datasets/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730d960970b2404c8cc37c2628f09c67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9912422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./../datasets/MNIST/raw/train-images-idx3-ubyte.gz to ./../datasets/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./../datasets/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c11389a41ee4307a5ba9a0db3ca5bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28881.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./../datasets/MNIST/raw/train-labels-idx1-ubyte.gz to ./../datasets/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./../datasets/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1b8a51f207e40a7a969cf8d62443e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1648877.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./../datasets/MNIST/raw/t10k-images-idx3-ubyte.gz to ./../datasets/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./../datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1284ab224724824ae8b10b87a947161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4542.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./../datasets/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./../datasets/MNIST/raw\n",
      "\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mdailey/.local/lib/python3.8/site-packages/torchvision/datasets/mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "transform = ToTensor()\n",
    "\n",
    "train_set = MNIST(root='./../datasets', train=True, download=True, transform=transform)\n",
    "test_set = MNIST(root='./../datasets', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=16)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test functions\n",
    "\n",
    "Next, let's create the train and test functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ViT_classify(model, optimizer, N_EPOCHS, train_loader, device=\"cpu\"):\n",
    "    criterion = CrossEntropyLoss()\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        train_loss = 0.0\n",
    "        for batch in train_loader:\n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y) / len(x)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")\n",
    "        \n",
    "def test_ViT_classify(model, optimizer, test_loader):\n",
    "    criterion = CrossEntropyLoss()\n",
    "    correct, total = 0, 0\n",
    "    test_loss = 0.0\n",
    "    for batch in test_loader:\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y) / len(x)\n",
    "        test_loss += loss\n",
    "\n",
    "        correct += torch.sum(torch.argmax(y_hat, dim=1) == y).item()\n",
    "        total += len(x)\n",
    "    print(f\"Test loss: {test_loss:.2f}\")\n",
    "    print(f\"Test accuracy: {correct / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-head Self Attention (MSA) Model\n",
    "\n",
    "As with the basic transformer above, to build the ViT model, we need to create a MSA module and put it\n",
    "together with the other elements.\n",
    "\n",
    "For a single image, self attention means that each patch's representation\n",
    "is updated based on its input token's similarity with those of the other patches.\n",
    "As before, we perform a linear mapping of each patch to three distinct vectors $q$, $k$, and $v$ (query, key, value).\n",
    "\n",
    "For each patch, we need to compute the dot product of its $q$ vector with all of the $k$ vectors, divide by the square root of the dimension\n",
    "of the vectors, then apply softmax to the result. The resulting matrix is called the matrix of attention cues.\n",
    "We multiply the attention cues with the $v$ vectors associated with the different input tokens and sum them all up.\n",
    "\n",
    "The input for each patch is transformed to a new value based on its similarity (after the linear mapping to $q$, $k$, and $v$) with other patches.\n",
    "\n",
    "However, the whole procedure is carried out $H$ times on $H$ sub-vectors of our current 8-dimensional patches, where $H$ is the number of heads.\n",
    "\n",
    "Once all results are obtained, they are concatenated together then passed through a linear layer.\n",
    "\n",
    "The MSA model looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSA(nn.Module):\n",
    "    def __init__(self, d, n_heads=2):\n",
    "        super(MSA, self).__init__()\n",
    "        self.d = d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
    "\n",
    "        d_head = int(d / n_heads)\n",
    "        self.q_mappings = [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
    "        self.k_mappings = [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
    "        self.v_mappings = [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
    "        self.d_head = d_head\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        # Sequences has shape (N, seq_length, token_dim)\n",
    "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "        result = []\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            for head in range(self.n_heads):\n",
    "                q_mapping = self.q_mappings[head]\n",
    "                k_mapping = self.k_mappings[head]\n",
    "                v_mapping = self.v_mappings[head]\n",
    "\n",
    "                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "                seq_result.append(attention @ v)\n",
    "            result.append(torch.hstack(seq_result))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: for each head, we create distinct Q, K, and V mapping functions (square matrices of size 4x4 in our example).\n",
    "\n",
    "Since our inputs will be sequences of size (N, 50, 8), and we only use 2 heads, we will at some point have an (N, 50, 2, 4) tensor, use a nn.Linear(4, 4) module on it, and then come back, after concatenation, to an (N, 50, 8) tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position encoding\n",
    "\n",
    "The position encoding allows the model to understand where each patch is in the original image. While it is theoretically possible to learn\n",
    "such positional embeddings, the original Vaswani et al. Transformer uses a fixed position embedding representation that adds\n",
    "low-frequency values to the first dimension and higher-frequency values to the later dimensions, resulting in a code that is\n",
    "more similar for nearby tokens than far away tokens. For each token, we add to its j-th coordinate the value\n",
    "\n",
    "$$ p_{i,j} =\n",
    "\\left\\{\\begin{matrix}\n",
    "\\sin (\\frac{i}{10000^{j/d_{embdim}}})\\\\ \n",
    "\\cos (\\frac{i}{10000^{j/d_{embdim}}})\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "\n",
    "We can visualize the position encoding matrix thusly:\n",
    "\n",
    "<img src=\"img/peimages.png\" title=\"\" style=\"width: 800px;\" />\n",
    "\n",
    "Here is an implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length, d, device=\"cpu\"):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT Model\n",
    "\n",
    "Create the ViT model as below. The explaination is later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, input_shape, n_patches=7, hidden_d=8, n_heads=2, out_d=10):\n",
    "        # Super constructor\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        # Input and patches sizes\n",
    "        self.input_shape = input_shape\n",
    "        self.n_patches = n_patches\n",
    "        self.n_heads = n_heads\n",
    "        assert input_shape[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        assert input_shape[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        self.patch_size = (input_shape[1] / n_patches, input_shape[2] / n_patches)\n",
    "        self.hidden_d = hidden_d\n",
    "\n",
    "        # 1) Linear mapper\n",
    "        self.input_d = int(input_shape[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
    "\n",
    "        # 2) Classification token\n",
    "        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "\n",
    "        # 3) Positional embedding\n",
    "        # (In forward method)\n",
    "\n",
    "        # 4a) Layer normalization 1\n",
    "        self.ln1 = nn.LayerNorm((self.n_patches ** 2 + 1, self.hidden_d))\n",
    "\n",
    "        # 4b) Multi-head Self Attention (MSA) and classification token\n",
    "        self.msa = MSA(self.hidden_d, n_heads)\n",
    "\n",
    "        # 5a) Layer normalization 2\n",
    "        self.ln2 = nn.LayerNorm((self.n_patches ** 2 + 1, self.hidden_d))\n",
    "\n",
    "        # 5b) Encoder MLP\n",
    "        self.enc_mlp = nn.Sequential(\n",
    "            nn.Linear(self.hidden_d, self.hidden_d),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 6) Classification MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.hidden_d, out_d),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Dividing images into patches\n",
    "        n, c, w, h = images.shape\n",
    "        patches = images.reshape(n, self.n_patches ** 2, self.input_d)\n",
    "\n",
    "        # Running linear layer for tokenization\n",
    "        tokens = self.linear_mapper(patches)\n",
    "\n",
    "        # Adding classification token to the tokens\n",
    "        tokens = torch.stack([torch.vstack((self.class_token, tokens[i])) for i in range(len(tokens))])\n",
    "\n",
    "        # Adding positional embedding\n",
    "        tokens += get_positional_embeddings(self.n_patches ** 2 + 1, self.hidden_d, device).repeat(n, 1, 1)\n",
    "\n",
    "        # TRANSFORMER ENCODER BEGINS ###################################\n",
    "        # NOTICE: MULTIPLE ENCODER BLOCKS CAN BE STACKED TOGETHER ######\n",
    "        # Running Layer Normalization, MSA and residual connection\n",
    "        self.msa(self.ln1(tokens.to(\"cpu\")).to(device))\n",
    "        out = tokens + self.msa(self.ln1(tokens))\n",
    "\n",
    "        # Running Layer Normalization, MLP and residual connection\n",
    "        out = out + self.enc_mlp(self.ln2(out))\n",
    "        # TRANSFORMER ENCODER ENDS   ###################################\n",
    "\n",
    "        # Getting the classification token only\n",
    "        out = out[:, 0]\n",
    "\n",
    "        return self.mlp(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Patchifying and the linear mapping\n",
    "\n",
    "The transformer encoder was developed with sequence data in mind, such as English sentences. However, an image is not a sequence. Thus, we break it into multiple sub-images and map each sub-image to a vector.\n",
    "\n",
    "We do so by simply reshaping our input, which has size $(N, C, H, W)$ (in our example $(N, 1, 28, 28)$), to size (N, #Patches, Patch dimensionality), where the dimensionality of a patch is adjusted accordingly.\n",
    "\n",
    "In MNIST, we break each $(1, 28, 28)$ into 7x7 patches (hence, each of size 4x4). That is, we are going to obtain 7x7=49 sub-images out of a single image.\n",
    "\n",
    "$$(N,1,28,28) \\rightarrow (N,P\\times P, H \\times C/P  \\times W \\times C/P) \\rightarrow (N, 7\\times 7, 4\\times 4) \\rightarrow (N, 49, 16)$$\n",
    "\n",
    "<img src=\"img/patch.png\" title=\"an image is split into patches\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Adding the classification token\n",
    "\n",
    "When information about all other tokens will be present here, we will be able to classify the image using only this special token. The initial value of the special token (the one fed to the transformer encoder) is a parameter of the model that needs to be learned.\n",
    "\n",
    "We can now add a parameter to our model and convert our (N, 49, 8) tokens tensor to an (N, 50, 8) tensor (we add the special token to each sequence).\n",
    "\n",
    "Passing from (N,49,8) → (N,50,8) is probably sub-optimal. Also, notice that the classification token is put as the first token of each sequence. This will be important to keep in mind when we will then retrieve the classification token to feed to the final MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Positional encoding\n",
    "\n",
    "See above, as we mentioned.\n",
    "\n",
    "#### Step 4: LN, MSA, and Residual Connection\n",
    "\n",
    "The step is to apply layer normalization to the tokens, then apply MSA, and add a residual connection (add the input we had before applying LN).\n",
    "- **Layer normalization** is a popular block that, given an input, subtracts its mean and divides by the standard deviation.\n",
    "- **MSA**: same as the vanilla transformer.\n",
    "- **A residual connection** consists in just adding the original input to the result of some computation. This, intuitively, allows a network to become more powerful while also preserving the set of possible functions that the model can approximate.\n",
    "\n",
    "The residual connection is added at the original (N, 50, 8) tensor to the (N, 50, 8) obtained after LN and MSA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: LN, MLP, and Residual Connection\n",
    "All that is left to the transformer encoder is just a simple residual connection between what we already have and what we get after passing the current tensor through another LN and an MLP.\n",
    "\n",
    "#### Step 6: Classification MLP\n",
    "Finally, we can extract just the classification token (first token) out of our N sequences, and use each token to get N classifications.\n",
    "Since we decided that each token is an 8-dimensional vector, and since we have 10 possible digits, we can implement the classification MLP as a simple 8x10 matrix, activated with the SoftMax function.\n",
    "\n",
    "The output of our model is now an (N, 10) tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print('Using device', device)\n",
    "# We haven't gotten CUDA too work yet -- the kernels always die!\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT((1, 28, 28), n_patches=7, hidden_d=20, n_heads=2, out_d=10)\n",
    "model = model.to(device)\n",
    "\n",
    "N_EPOCHS = 5\n",
    "LR = 0.01\n",
    "optimizer = Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 loss: 389.51\n",
      "Epoch 2/5 loss: 366.17\n",
      "Epoch 3/5 loss: 363.27\n",
      "Epoch 4/5 loss: 361.59\n",
      "Epoch 5/5 loss: 360.39\n"
     ]
    }
   ],
   "source": [
    "train_ViT_classify(model, optimizer, N_EPOCHS, train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 59.81\n",
      "Test accuracy: 93.07%\n"
     ]
    }
   ],
   "source": [
    "test_ViT_classify(model, optimizer, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing accuracy is over 90%. Our implementation is done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch ViT\n",
    "\n",
    "[Here](https://github.com/lucidrains/vit-pytorch#vision-transformer---pytorch) is the link of the full version of ViT using pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vit-pytorch\n",
      "  Downloading vit_pytorch-0.29.0-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 1.6 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.6 in /home/alisa/anaconda3/lib/python3.8/site-packages (from vit-pytorch) (1.8.0+cu111)\n",
      "Collecting einops>=0.4.1\n",
      "  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: torchvision in /home/alisa/anaconda3/lib/python3.8/site-packages (from vit-pytorch) (0.9.0+cu111)\n",
      "Requirement already satisfied: numpy in /home/alisa/.local/lib/python3.8/site-packages (from torch>=1.6->vit-pytorch) (1.21.4)\n",
      "Requirement already satisfied: typing-extensions in /home/alisa/.local/lib/python3.8/site-packages (from torch>=1.6->vit-pytorch) (3.10.0.2)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/alisa/anaconda3/lib/python3.8/site-packages (from torchvision->vit-pytorch) (7.2.0)\n",
      "Installing collected packages: einops, vit-pytorch\n",
      "Successfully installed einops-0.4.1 vit-pytorch-0.29.0\n"
     ]
    }
   ],
   "source": [
    "!pip install vit-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT Pytorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vit_pytorch import ViT\n",
    "\n",
    "v = ViT(\n",
    "    image_size = 256,\n",
    "    patch_size = 32,\n",
    "    num_classes = 1000,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "\n",
    "img = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "preds = v(img) # (1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation also contains a distillable ViT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "from vit_pytorch.distill import DistillableViT, DistillWrapper\n",
    "\n",
    "teacher = resnet50(pretrained = True)\n",
    "\n",
    "v = DistillableViT(\n",
    "    image_size = 256,\n",
    "    patch_size = 32,\n",
    "    num_classes = 1000,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "\n",
    "distiller = DistillWrapper(\n",
    "    student = v,\n",
    "    teacher = teacher,\n",
    "    temperature = 3,           # temperature of distillation\n",
    "    alpha = 0.5,               # trade between main loss and distillation loss\n",
    "    hard = False               # whether to use soft or hard distillation\n",
    ")\n",
    "\n",
    "img = torch.randn(2, 3, 256, 256)\n",
    "labels = torch.randint(0, 1000, (2,))\n",
    "\n",
    "loss = distiller(img, labels)\n",
    "loss.backward()\n",
    "\n",
    "# after lots of training above ...\n",
    "\n",
    "pred = v(img) # (2, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do on your own\n",
    "\n",
    "If we can manage the resources, let's try pre-training a ViT on ImageNet 1K then fine tuning it on another dataset such as CIFAR-10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
