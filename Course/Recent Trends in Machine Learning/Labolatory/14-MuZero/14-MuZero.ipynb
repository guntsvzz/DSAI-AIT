{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 14: MuZero\n",
    "\n",
    "In this lab, we experiment with a reinforcement learning method claimed to be a prototype of AGI (Artificial General Intelligence): MuZero.\n",
    "\n",
    "Material in this lab comes from several sources:\n",
    "- https://www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules\n",
    "- https://github.com/werner-duvaud/muzero-general\n",
    "- https://medium.com/applied-data-science/how-to-build-your-own-muzero-in-python-f77d5718061a\n",
    "- https://arxiv.org/src/1911.08265v1/anc/pseudocode.py\n",
    "- https://github.com/suragnair/alpha-zero-general\n",
    "- https://arxiv.org/pdf/1911.08265.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial General Intelligence (AGI)\n",
    "\n",
    "AI methods that solve a single problem such as classification, object detection, or text translation are called \"Narrow AI\" methods.\n",
    "Systems that employ narrow AI cannot switch to a new task without modification by an external actor. AI methods that can adapt to new\n",
    "problems in the same way humans are capable of are called \"General AI\" methods.\n",
    "\n",
    "*Artificial general intelligence* (AGI) is the capability of learning or understanding any intellectual task that humans can. AGI is\n",
    "some AI researchers' ultimate goal and a favorite topic for science fiction writers and prognosticators about the future (try one of\n",
    "our favorite \"future science\" books, [Life 3.0 by Max Tegmark](https://www.amazon.com/Life-3-0-Being-Artificial-Intelligence/dp/1101946598)).\n",
    "\n",
    "\n",
    "A [2020 survey by the Global Catastrophic Risk Institute](https://gcrinstitute.org/papers/055_agi-2020.pdf) identifies 72 active AGI R&D projects spread across 37 countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MuZero\n",
    "\n",
    "We've read about DeepMind's 2016 AlphaGo system, which was the first program that could play Go at a master level.\n",
    "In 2018, DeepMind released AlphaZero, which could learn to play Go from scratch without relying on example games by humans.\n",
    "AlphaZero could master three games rather than just one: Go, chess and shogi. In 2020, DeepMind introduced MuZero, a\n",
    "step toward more general-purpose algorithms that learns any game without being told the rules.\n",
    "\n",
    "Here is [a preprint of DeepMind's MuZero paper on arXiv](https://arxiv.org/abs/1911.08265).\n",
    "\n",
    "Recall that most of the RL methods we've looked at so far have been *model free* meaning they don't try to predict\n",
    "the consequences of an action. This is sensible, because it dramatically simplifies the learning algorithm to something\n",
    "tractable.\n",
    "\n",
    "It's clear, however, that humans do some planning when getting ready to perform some task. With some introspection,\n",
    "you will probably realize that when we plan how to perform a task, we usually mentally practice our steps, visualizing\n",
    "what the world/environment is going to look like as we take the steps in our plan.\n",
    "\n",
    "But doing this prediction obviously requires some model of the world/environment! In the MDP formulation of RL, the model is a\n",
    "distribution $p(s' \\mid s, a)$. To even begin to think about this distibution, we need to know what are $\\mathcal{S}$\n",
    "and $\\mathcal{A}$. How could it be possible to build a system that can learn any task that, like humans, predicts the\n",
    "consequences of its actions, without hard coding the structure of the world $\\mathcal{S}$ and what it can do $\\mathcal{A}$?\n",
    "\n",
    "This is the clever contribution of MuZero. It eschews the model-free RL architectures we have mainly adopted thus\n",
    "far, instead building the model from scratch as it learns, then using that model to decide what actions are best for the task\n",
    "at hand in a way similar to the lookahead search of AlphaGo and AlphaZero.\n",
    "\n",
    "With this approach, MuZero set set a new state of the art result on the Atari benchmark, while also matching AlphaZero\n",
    "in Go, chess and shogi.\n",
    "\n",
    "Here's a [summary from DeepMind](https://www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules)\n",
    "of the relative advances represented by AlphaGo, AlphaZero, and MuZero:\n",
    "<img src=\"img/alphago_summary.jpeg\" title=\"alphago_summary\" style=\"width: 800px;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MuZero vs. AlphaZero\n",
    "\n",
    "AlphaZero can in principle learn any game but requires knowledge of the rules of how pieces move and which moves are legal.\n",
    "This would obviously be troublesome in environments like Atari. To get AlphaZero to learn Atari, you would have to model the\n",
    "effects of each action on the environment explicitly and hard code them into the game. You would basically be reimplementing\n",
    "the game itself!\n",
    "\n",
    "MuZero uses much less knowledge. We only need to tell it what moves are legal in the current position and when the game is over.\n",
    "\n",
    "During exploration, MuZero, through its model, has to learn the rules of the game implicitly, like when you are first learning\n",
    "chess by playing against your older brother and he only stops you from making illegal moves while he crushes you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MuZero models\n",
    "\n",
    "Predicting the consequences of our actions without replicating the entire environment in our heads is a difficult task\n",
    "that we are pretty good at. MuZero approximates this capability by modeling just the aspects of the environment that are\n",
    "important to the decision making process. Three elements of the environment are critical to planning:\n",
    "\n",
    "- Value: how good is the current state?\n",
    "- Reward: how good was the last action?\n",
    "- Dynamics: what will happen if I take a particular action?\n",
    "\n",
    "The MuZero policy utilizes all of the elements, represented by\n",
    "deep neural networks, to understand what happens when it takes certain actions and to plan accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudocode\n",
    "\n",
    "Alongside the [MuZero paper](https://arxiv.org/pdf/1911.08265.pdf), DeepMind have released [Python pseudocode]((https://arxiv.org/src/1911.08265v1/anc/pseudocode.py)) detailing the interactions between each part of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lint as: python3\n",
    "\"\"\"Pseudocode description of the MuZero algorithm.\"\"\"\n",
    "# pylint: disable=unused-argument\n",
    "# pylint: disable=missing-docstring\n",
    "# pylint: disable=g-explicit-length-test\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import google_type_annotations\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import typing\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "##########################\n",
    "####### Helpers ##########\n",
    "\n",
    "MAXIMUM_FLOAT_VALUE = float('inf')\n",
    "\n",
    "KnownBounds = collections.namedtuple('KnownBounds', ['min', 'max'])\n",
    "\n",
    "\n",
    "class MinMaxStats(object):\n",
    "    \"\"\"A class that holds the min-max values of the tree.\"\"\"\n",
    "\n",
    "    def __init__(self, known_bounds: Optional[KnownBounds]):\n",
    "        self.maximum = known_bounds.max if known_bounds else -MAXIMUM_FLOAT_VALUE\n",
    "        self.minimum = known_bounds.min if known_bounds else MAXIMUM_FLOAT_VALUE\n",
    "\n",
    "    def update(self, value: float):\n",
    "        self.maximum = max(self.maximum, value)\n",
    "        self.minimum = min(self.minimum, value)\n",
    "\n",
    "    def normalize(self, value: float) -> float:\n",
    "        if self.maximum > self.minimum:\n",
    "            # We normalize only when we have set the maximum and minimum values.\n",
    "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
    "        return value\n",
    "\n",
    "\n",
    "class MuZeroConfig(object):\n",
    "\n",
    "    def __init__(self,\n",
    "                 action_space_size: int,\n",
    "                 max_moves: int,\n",
    "                 discount: float,\n",
    "                 dirichlet_alpha: float,\n",
    "                 num_simulations: int,\n",
    "                 batch_size: int,\n",
    "                 td_steps: int,\n",
    "                 num_actors: int,\n",
    "                 lr_init: float,\n",
    "                 lr_decay_steps: float,\n",
    "                 visit_softmax_temperature_fn,\n",
    "                 known_bounds: Optional[KnownBounds] = None):\n",
    "        \n",
    "        ### Self-Play\n",
    "        self.action_space_size = action_space_size\n",
    "        self.num_actors = num_actors\n",
    "\n",
    "        self.visit_softmax_temperature_fn = visit_softmax_temperature_fn\n",
    "        self.max_moves = max_moves\n",
    "        self.num_simulations = num_simulations\n",
    "        self.discount = discount\n",
    "\n",
    "        # Root prior exploration noise.\n",
    "        self.root_dirichlet_alpha = dirichlet_alpha\n",
    "        self.root_exploration_fraction = 0.25\n",
    "\n",
    "        # UCB formula\n",
    "        self.pb_c_base = 19652\n",
    "        self.pb_c_init = 1.25\n",
    "\n",
    "        # If we already have some information about which values occur in the\n",
    "        # environment, we can use them to initialize the rescaling.\n",
    "        # This is not strictly necessary, but establishes identical behaviour to\n",
    "        # AlphaZero in board games.\n",
    "        self.known_bounds = known_bounds\n",
    "\n",
    "        ### Training\n",
    "        self.training_steps = int(1000e3)\n",
    "        self.checkpoint_interval = int(1e3)\n",
    "        self.window_size = int(1e6)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_unroll_steps = 5\n",
    "        self.td_steps = td_steps\n",
    "\n",
    "        self.weight_decay = 1e-4\n",
    "        self.momentum = 0.9\n",
    "\n",
    "        # Exponential learning rate schedule\n",
    "        self.lr_init = lr_init\n",
    "        self.lr_decay_rate = 0.1\n",
    "        self.lr_decay_steps = lr_decay_steps\n",
    "\n",
    "    def new_game(self):\n",
    "        return Game(self.action_space_size, self.discount)\n",
    "\n",
    "\n",
    "def make_board_game_config(action_space_size: int, max_moves: int,\n",
    "                           dirichlet_alpha: float,\n",
    "                           lr_init: float) -> MuZeroConfig:\n",
    "\n",
    "    def visit_softmax_temperature(num_moves, training_steps):\n",
    "        if num_moves < 30:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0  # Play according to the max.\n",
    "\n",
    "    return MuZeroConfig(\n",
    "            action_space_size=action_space_size,\n",
    "            max_moves=max_moves,\n",
    "            discount=1.0,\n",
    "            dirichlet_alpha=dirichlet_alpha,\n",
    "            num_simulations=800,\n",
    "            batch_size=2048,\n",
    "            td_steps=max_moves,  # Always use Monte Carlo return.\n",
    "            num_actors=3000,\n",
    "            lr_init=lr_init,\n",
    "            lr_decay_steps=400e3,\n",
    "            visit_softmax_temperature_fn=visit_softmax_temperature,\n",
    "            known_bounds=KnownBounds(-1, 1))\n",
    "\n",
    "\n",
    "def make_go_config() -> MuZeroConfig:\n",
    "    return make_board_game_config(\n",
    "            action_space_size=362, max_moves=722, dirichlet_alpha=0.03, lr_init=0.01)\n",
    "\n",
    "\n",
    "def make_chess_config() -> MuZeroConfig:\n",
    "    return make_board_game_config(\n",
    "            action_space_size=4672, max_moves=512, dirichlet_alpha=0.3, lr_init=0.1)\n",
    "\n",
    "\n",
    "def make_shogi_config() -> MuZeroConfig:\n",
    "    return make_board_game_config(\n",
    "            action_space_size=11259, max_moves=512, dirichlet_alpha=0.15, lr_init=0.1)\n",
    "\n",
    "\n",
    "def make_atari_config() -> MuZeroConfig:\n",
    "\n",
    "    def visit_softmax_temperature(num_moves, training_steps):\n",
    "        if training_steps < 500e3:\n",
    "            return 1.0\n",
    "        elif training_steps < 750e3:\n",
    "            return 0.5\n",
    "        else:\n",
    "            return 0.25\n",
    "\n",
    "        return MuZeroConfig(\n",
    "                action_space_size=18,\n",
    "                max_moves=27000,  # Half an hour at action repeat 4.\n",
    "                discount=0.997,\n",
    "                dirichlet_alpha=0.25,\n",
    "                num_simulations=50,\n",
    "                batch_size=1024,\n",
    "                td_steps=10,\n",
    "                num_actors=350,\n",
    "                lr_init=0.05,\n",
    "                lr_decay_steps=350e3,\n",
    "                visit_softmax_temperature_fn=visit_softmax_temperature)\n",
    "\n",
    "\n",
    "class Action(object):\n",
    "\n",
    "    def __init__(self, index: int):\n",
    "        self.index = index\n",
    "\n",
    "    def __hash__(self):\n",
    "        return self.index\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.index == other.index\n",
    "\n",
    "    def __gt__(self, other):\n",
    "        return self.index > other.index\n",
    "\n",
    "\n",
    "class Player(object):\n",
    "    pass\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "\n",
    "    def __init__(self, prior: float):\n",
    "        self.visit_count = 0\n",
    "        self.to_play = -1\n",
    "        self.prior = prior\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.hidden_state = None\n",
    "        self.reward = 0\n",
    "\n",
    "    def expanded(self) -> bool:\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def value(self) -> float:\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visit_count\n",
    "\n",
    "\n",
    "class ActionHistory(object):\n",
    "    \"\"\"Simple history container used inside the search.\n",
    "\n",
    "    Only used to keep track of the actions executed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, history: List[Action], action_space_size: int):\n",
    "        self.history = list(history)\n",
    "        self.action_space_size = action_space_size\n",
    "\n",
    "    def clone(self):\n",
    "        return ActionHistory(self.history, self.action_space_size)\n",
    "\n",
    "    def add_action(self, action: Action):\n",
    "        self.history.append(action)\n",
    "\n",
    "    def last_action(self) -> Action:\n",
    "        return self.history[-1]\n",
    "\n",
    "    def action_space(self) -> List[Action]:\n",
    "        return [Action(i) for i in range(self.action_space_size)]\n",
    "\n",
    "    def to_play(self) -> Player:\n",
    "        return Player()\n",
    "\n",
    "\n",
    "class Environment(object):\n",
    "    \"\"\"The environment MuZero is interacting with.\"\"\"\n",
    "\n",
    "    def step(self, action):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Game(object):\n",
    "    \"\"\"A single episode of interaction with the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, action_space_size: int, discount: float):\n",
    "        self.environment = Environment()  # Game specific environment.\n",
    "        self.history = []\n",
    "        self.rewards = []\n",
    "        self.child_visits = []\n",
    "        self.root_values = []\n",
    "        self.action_space_size = action_space_size\n",
    "        self.discount = discount\n",
    "\n",
    "    def terminal(self) -> bool:\n",
    "        # Game specific termination rules.\n",
    "        pass\n",
    "\n",
    "    def legal_actions(self) -> List[Action]:\n",
    "        # Game specific calculation of legal actions.\n",
    "        return []\n",
    "\n",
    "    def apply(self, action: Action):\n",
    "        reward = self.environment.step(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.history.append(action)\n",
    "\n",
    "    def store_search_statistics(self, root: Node):\n",
    "        sum_visits = sum(child.visit_count for child in root.children.values())\n",
    "        action_space = (Action(index) for index in range(self.action_space_size))\n",
    "        self.child_visits.append([\n",
    "            root.children[a].visit_count / sum_visits if a in root.children else 0\n",
    "            for a in action_space\n",
    "        ])\n",
    "        self.root_values.append(root.value())\n",
    "\n",
    "    def make_image(self, state_index: int):\n",
    "        # Game specific feature planes.\n",
    "        return []\n",
    "\n",
    "    def make_target(self, state_index: int, num_unroll_steps: int, td_steps: int,\n",
    "                    to_play: Player):\n",
    "        # The value target is the discounted root value of the search tree N steps\n",
    "        # into the future, plus the discounted sum of all rewards until then.\n",
    "        targets = []\n",
    "        for current_index in range(state_index, state_index + num_unroll_steps + 1):\n",
    "            bootstrap_index = current_index + td_steps\n",
    "            if bootstrap_index < len(self.root_values):\n",
    "                value = self.root_values[bootstrap_index] * self.discount**td_steps\n",
    "            else:\n",
    "                value = 0\n",
    "\n",
    "            for i, reward in enumerate(self.rewards[current_index:bootstrap_index]):\n",
    "                value += reward * self.discount**i  # pytype: disable=unsupported-operands\n",
    "\n",
    "            if current_index < len(self.root_values):\n",
    "                targets.append((value, self.rewards[current_index],\n",
    "                               self.child_visits[current_index]))\n",
    "            else:\n",
    "                # States past the end of games are treated as absorbing states.\n",
    "                targets.append((0, 0, []))\n",
    "        return targets\n",
    "\n",
    "    def to_play(self) -> Player:\n",
    "        return Player()\n",
    "\n",
    "    def action_history(self) -> ActionHistory:\n",
    "        return ActionHistory(self.history, self.action_space_size)\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, config: MuZeroConfig):\n",
    "        self.window_size = config.window_size\n",
    "        self.batch_size = config.batch_size\n",
    "        self.buffer = []\n",
    "\n",
    "    def save_game(self, game):\n",
    "        if len(self.buffer) > self.window_size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(game)\n",
    "\n",
    "    def sample_batch(self, num_unroll_steps: int, td_steps: int):\n",
    "        games = [self.sample_game() for _ in range(self.batch_size)]\n",
    "        game_pos = [(g, self.sample_position(g)) for g in games]\n",
    "        return [(g.make_image(i), g.history[i:i + num_unroll_steps],\n",
    "                 g.make_target(i, num_unroll_steps, td_steps, g.to_play()))\n",
    "                for (g, i) in game_pos]\n",
    "\n",
    "    def sample_game(self) -> Game:\n",
    "        # Sample game from buffer either uniformly or according to some priority.\n",
    "        return self.buffer[0]\n",
    "\n",
    "    def sample_position(self, game) -> int:\n",
    "        # Sample position from game either uniformly or according to some priority.\n",
    "        return -1\n",
    "\n",
    "\n",
    "class NetworkOutput(typing.NamedTuple):\n",
    "    value: float\n",
    "    reward: float\n",
    "    policy_logits: Dict[Action, float]\n",
    "    hidden_state: List[float]\n",
    "\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def initial_inference(self, image) -> NetworkOutput:\n",
    "        # representation + prediction function\n",
    "        return NetworkOutput(0, 0, {}, [])\n",
    "\n",
    "    def recurrent_inference(self, hidden_state, action) -> NetworkOutput:\n",
    "        # dynamics + prediction function\n",
    "        return NetworkOutput(0, 0, {}, [])\n",
    "\n",
    "    def get_weights(self):\n",
    "        # Returns the weights of this network.\n",
    "        return []\n",
    "\n",
    "    def training_steps(self) -> int:\n",
    "        # How many steps / batches the network has been trained for.\n",
    "        return 0\n",
    "\n",
    "\n",
    "class SharedStorage(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._networks = {}\n",
    "\n",
    "    def latest_network(self) -> Network:\n",
    "        if self._networks:\n",
    "            return self._networks[max(self._networks.keys())]\n",
    "        else:\n",
    "            # policy -> uniform, value -> 0, reward -> 0\n",
    "            return make_uniform_network()\n",
    "\n",
    "    def save_network(self, step: int, network: Network):\n",
    "        self._networks[step] = network\n",
    "\n",
    "\n",
    "##### End Helpers ########\n",
    "##########################\n",
    "\n",
    "\n",
    "# MuZero training is split into two independent parts: Network training and\n",
    "# self-play data generation.\n",
    "# These two parts only communicate by transferring the latest network checkpoint\n",
    "# from the training to the self-play, and the finished games from the self-play\n",
    "# to the training.\n",
    "def muzero(config: MuZeroConfig):\n",
    "    storage = SharedStorage()\n",
    "    replay_buffer = ReplayBuffer(config)\n",
    "\n",
    "    for _ in range(config.num_actors):\n",
    "        launch_job(run_selfplay, config, storage, replay_buffer)\n",
    "\n",
    "    train_network(config, storage, replay_buffer)\n",
    "\n",
    "    return storage.latest_network()\n",
    "\n",
    "\n",
    "##################################\n",
    "####### Part 1: Self-Play ########\n",
    "\n",
    "\n",
    "# Each self-play job is independent of all others; it takes the latest network\n",
    "# snapshot, produces a game and makes it available to the training job by\n",
    "# writing it to a shared replay buffer.\n",
    "def run_selfplay(config: MuZeroConfig, storage: SharedStorage,\n",
    "                 replay_buffer: ReplayBuffer):\n",
    "    while True:\n",
    "        network = storage.latest_network()\n",
    "        game = play_game(config, network)\n",
    "        replay_buffer.save_game(game)\n",
    "\n",
    "\n",
    "# Each game is produced by starting at the initial board position, then\n",
    "# repeatedly executing a Monte Carlo Tree Search to generate moves until the end\n",
    "# of the game is reached.\n",
    "def play_game(config: MuZeroConfig, network: Network) -> Game:\n",
    "    game = config.new_game()\n",
    "\n",
    "    while not game.terminal() and len(game.history) < config.max_moves:\n",
    "        # At the root of the search tree we use the representation function to\n",
    "        # obtain a hidden state given the current observation.\n",
    "        root = Node(0)\n",
    "        current_observation = game.make_image(-1)\n",
    "        expand_node(root, game.to_play(), game.legal_actions(),\n",
    "                    network.initial_inference(current_observation))\n",
    "        add_exploration_noise(config, root)\n",
    "\n",
    "        # We then run a Monte Carlo Tree Search using only action sequences and the\n",
    "        # model learned by the network.\n",
    "        run_mcts(config, root, game.action_history(), network)\n",
    "        action = select_action(config, len(game.history), root, network)\n",
    "        game.apply(action)\n",
    "        game.store_search_statistics(root)\n",
    "    return game\n",
    "\n",
    "\n",
    "# Core Monte Carlo Tree Search algorithm.\n",
    "# To decide on an action, we run N simulations, always starting at the root of\n",
    "# the search tree and traversing the tree according to the UCB formula until we\n",
    "# reach a leaf node.\n",
    "def run_mcts(config: MuZeroConfig, root: Node, action_history: ActionHistory,\n",
    "             network: Network):\n",
    "    min_max_stats = MinMaxStats(config.known_bounds)\n",
    "\n",
    "    for _ in range(config.num_simulations):\n",
    "        history = action_history.clone()\n",
    "        node = root\n",
    "        search_path = [node]\n",
    "\n",
    "        while node.expanded():\n",
    "            action, node = select_child(config, node, min_max_stats)\n",
    "            history.add_action(action)\n",
    "            search_path.append(node)\n",
    "\n",
    "        # Inside the search tree we use the dynamics function to obtain the next\n",
    "        # hidden state given an action and the previous hidden state.\n",
    "        parent = search_path[-2]\n",
    "        network_output = network.recurrent_inference(parent.hidden_state,\n",
    "                                                     history.last_action())\n",
    "        expand_node(node, history.to_play(), history.action_space(), network_output)\n",
    "\n",
    "        backpropagate(search_path, network_output.value, history.to_play(),\n",
    "                      config.discount, min_max_stats)\n",
    "\n",
    "\n",
    "def select_action(config: MuZeroConfig, num_moves: int, node: Node,\n",
    "                  network: Network):\n",
    "    visit_counts = [\n",
    "        (child.visit_count, action) for action, child in node.children.items()\n",
    "    ]\n",
    "    t = config.visit_softmax_temperature_fn(\n",
    "        num_moves=num_moves, training_steps=network.training_steps())\n",
    "    _, action = softmax_sample(visit_counts, t)\n",
    "    return action\n",
    "\n",
    "\n",
    "# Select the child with the highest UCB score.\n",
    "def select_child(config: MuZeroConfig, node: Node,\n",
    "                 min_max_stats: MinMaxStats):\n",
    "    _, action, child = max(\n",
    "        (ucb_score(config, node, child, min_max_stats), action,\n",
    "         child) for action, child in node.children.items())\n",
    "    return action, child\n",
    "\n",
    "\n",
    "# The score for a node is based on its value, plus an exploration bonus based on\n",
    "# the prior.\n",
    "def ucb_score(config: MuZeroConfig, parent: Node, child: Node,\n",
    "              min_max_stats: MinMaxStats) -> float:\n",
    "    pb_c = math.log((parent.visit_count + config.pb_c_base + 1) /\n",
    "                  config.pb_c_base) + config.pb_c_init\n",
    "    pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "\n",
    "    prior_score = pb_c * child.prior\n",
    "    value_score = min_max_stats.normalize(child.value())\n",
    "    return prior_score + value_score\n",
    "\n",
    "\n",
    "# We expand a node using the value, reward and policy prediction obtained from\n",
    "# the neural network.\n",
    "def expand_node(node: Node, to_play: Player, actions: List[Action],\n",
    "                network_output: NetworkOutput):\n",
    "    node.to_play = to_play\n",
    "    node.hidden_state = network_output.hidden_state\n",
    "    node.reward = network_output.reward\n",
    "    policy = {a: math.exp(network_output.policy_logits[a]) for a in actions}\n",
    "    policy_sum = sum(policy.values())\n",
    "    for action, p in policy.items():\n",
    "        node.children[action] = Node(p / policy_sum)\n",
    "\n",
    "\n",
    "# At the end of a simulation, we propagate the evaluation all the way up the\n",
    "# tree to the root.\n",
    "def backpropagate(search_path: List[Node], value: float, to_play: Player,\n",
    "                  discount: float, min_max_stats: MinMaxStats):\n",
    "    for node in search_path:\n",
    "        node.value_sum += value if node.to_play == to_play else -value\n",
    "        node.visit_count += 1\n",
    "        min_max_stats.update(node.value())\n",
    "\n",
    "        value = node.reward + discount * value\n",
    "\n",
    "\n",
    "# At the start of each search, we add dirichlet noise to the prior of the root\n",
    "# to encourage the search to explore new actions.\n",
    "def add_exploration_noise(config: MuZeroConfig, node: Node):\n",
    "    actions = list(node.children.keys())\n",
    "    noise = numpy.random.dirichlet([config.root_dirichlet_alpha] * len(actions))\n",
    "    frac = config.root_exploration_fraction\n",
    "    for a, n in zip(actions, noise):\n",
    "        node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
    "\n",
    "\n",
    "######### End Self-Play ##########\n",
    "##################################\n",
    "\n",
    "##################################\n",
    "####### Part 2: Training #########\n",
    "\n",
    "\n",
    "def train_network(config: MuZeroConfig, storage: SharedStorage,\n",
    "                  replay_buffer: ReplayBuffer):\n",
    "    network = Network()\n",
    "    learning_rate = config.lr_init * config.lr_decay_rate**(\n",
    "        tf.train.get_global_step() / config.lr_decay_steps)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, config.momentum)\n",
    "\n",
    "    for i in range(config.training_steps):\n",
    "        if i % config.checkpoint_interval == 0:\n",
    "            storage.save_network(i, network)\n",
    "        batch = replay_buffer.sample_batch(config.num_unroll_steps, config.td_steps)\n",
    "        update_weights(optimizer, network, batch, config.weight_decay)\n",
    "    storage.save_network(config.training_steps, network)\n",
    "\n",
    "\n",
    "def update_weights(optimizer: tf.train.Optimizer, network: Network, batch,\n",
    "                   weight_decay: float):\n",
    "    loss = 0\n",
    "    for image, actions, targets in batch:\n",
    "        # Initial step, from the real observation.\n",
    "        value, reward, policy_logits, hidden_state = network.initial_inference(image)\n",
    "        predictions = [(1.0, value, reward, policy_logits)]\n",
    "\n",
    "        # Recurrent steps, from action and previous hidden state.\n",
    "        for action in actions:\n",
    "            value, reward, policy_logits, hidden_state = network.recurrent_inference(\n",
    "                hidden_state, action)\n",
    "            predictions.append((1.0 / len(actions), value, reward, policy_logits))\n",
    "\n",
    "            hidden_state = tf.scale_gradient(hidden_state, 0.5)\n",
    "\n",
    "        for prediction, target in zip(predictions, targets):\n",
    "            gradient_scale, value, reward, policy_logits = prediction\n",
    "            target_value, target_reward, target_policy = target\n",
    "\n",
    "            l = (\n",
    "                scalar_loss(value, target_value) +\n",
    "                scalar_loss(reward, target_reward) +\n",
    "                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=policy_logits, labels=target_policy))\n",
    "\n",
    "            loss += tf.scale_gradient(l, gradient_scale)\n",
    "\n",
    "    for weights in network.get_weights():\n",
    "        loss += weight_decay * tf.nn.l2_loss(weights)\n",
    "\n",
    "    optimizer.minimize(loss)\n",
    "\n",
    "\n",
    "def scalar_loss(prediction, target) -> float:\n",
    "    # MSE in board games, cross entropy between categorical values in Atari.\n",
    "    return -1\n",
    "\n",
    "######### End Training ###########\n",
    "##################################\n",
    "\n",
    "################################################################################\n",
    "############################# End of pseudocode ################################\n",
    "################################################################################\n",
    "\n",
    "\n",
    "# Stubs to make the typechecker happy.\n",
    "def softmax_sample(distribution, temperature: float):\n",
    "    return 0, 0\n",
    "\n",
    "\n",
    "def launch_job(f, *args):\n",
    "    f(*args)\n",
    "\n",
    "\n",
    "def make_uniform_network():\n",
    "    return Network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Muzero function\n",
    "\n",
    "We'll take a look at the essential parts of the pseudocode.\n",
    "The `muzero` function provides an overview of the entire process.\n",
    "\n",
    "<img src=\"img/muzero_sum.png\" title=\"muzero sum\" style=\"width: 800px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def muzero(config: MuZeroConfig):\n",
    "    storage = SharedStorage()\n",
    "    replay_buffer = ReplayBuffer(config)\n",
    "\n",
    "    for _ in range(config.num_actors):\n",
    "        launch_job(run_selfplay, config, storage, replay_buffer)\n",
    "\n",
    "    train_network(config, storage, replay_buffer)\n",
    "\n",
    "    return storage.latest_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entry point function muzero is passed a `MuZeroConfig` object, which stores important information\n",
    "about parameter settings such as the `action_space_size` (number of possible actions) and `num_actors`\n",
    "(the number of parallel game simulations to run).\n",
    "\n",
    "There are two independent parts to the MuZero algorithm, self-play (creating game data) and training\n",
    "(producing improved versions of the neural network models). The SharedStorage and ReplayBuffer objects\n",
    "can be accessed by both halves of the algorithm in order to store neural network versions and game data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shared Storage and the Replay Buffer\n",
    "\n",
    "The SharedStorage object contains methods for saving a version of the neural network and retrieving the latest neural network from the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedStorage(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self._networks = {}\n",
    "\n",
    "    def latest_network(self) -> Network:\n",
    "        if self._networks:\n",
    "            return self._networks[max(self._networks.keys())]\n",
    "        else:\n",
    "            # policy -> uniform, value -> 0, reward -> 0\n",
    "            return make_uniform_network()\n",
    "\n",
    "    def save_network(self, step: int, network: Network):\n",
    "        self._networks[step] = network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay buffer\n",
    "\n",
    "The ReplayBuffer stores data from previous games. The ReplayBuffer class contains a sample_batch method to sample a batch of observations from the buffer.\n",
    "\n",
    "The default batch_size of MuZero for chess is 2048. This number of games are selected from the buffer and one position is chosen from each.\n",
    "\n",
    "A single batch is a list of tuples, where each tuple consists of three elements:\n",
    "- g.make_image(i) — the observation at the chosen position\n",
    "- g.history[i:i + num_unroll_steps] — a list of the next num_unroll_steps actions taken after the chosen position (if they exist)\n",
    "- g.make_target(i, num_unroll_steps, td_steps, g.to_play() — a list of the targets that will be used to train the neural networks. Specifically, this is a list of tuples:target_value, target_reward and target_policy.\n",
    "\n",
    "For each observation in the batch, we will be ‘unrolling’ the position num_unroll_steps into the future using the actions provided. For the initial position, we will use the initial_inference function to predict the value, reward and policy and compare these to the target value, target reward and target policy. For subsequent actions, we will use the recurrent_inference function to predict the value, reward and policy and compare to the target value, target reward and target policy. This way, all three networks are utilised in the predictive process and therefore the weights in all three networks will be updated.\n",
    "\n",
    "During training, the model is unrolled alongside the collected experience, at each step predicting the previously saved information: the value function v predicts the sum of observed rewards (u), the policy estimate (p) predicts the previous search outcome (π), the reward estimate r predicts the last observed reward (u). This process is the overall of recurrent_inference.\n",
    "\n",
    "<img src=\"img/unroll.gif\" title=\"\" style=\"width: 800px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, config: MuZeroConfig):\n",
    "        self.window_size = config.window_size\n",
    "        self.batch_size = config.batch_size\n",
    "        self.buffer = []\n",
    "\n",
    "    def save_game(self, game):\n",
    "        if len(self.buffer) > self.window_size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(game)\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>window_size</code> parameter limits the maximum number of games stored in the buffer. In MuZero, this is set to the latest 1,000,000 games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-play (run_selfplay)\n",
    "\n",
    "MuZero launches <code>num_actors</code> parallel game environments, that run independently.\n",
    "\n",
    "The method plays thousands of games against itself. In the process, the games are saved\n",
    "to a buffer, and then training utilizes the data from those games.\n",
    "This step is the same as AlphaZero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_selfplay(config: MuZeroConfig, storage: SharedStorage,\n",
    "                 replay_buffer: ReplayBuffer):\n",
    "    while True:\n",
    "        network = storage.latest_network()\n",
    "        game = play_game(config, network)\n",
    "        replay_buffer.save_game(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo tree search (MCTS)\n",
    "\n",
    "All of the algorithms from AlphaGo to MuZero use Monte Carlo Tree Search (MCTS) to select the next best move.\n",
    "We \"play out\" future scenarios from the current position, evaluate the resulting states with a value network, and\n",
    "after a period of tree search, choose the action that maximises the future expected value.\n",
    "\n",
    "The figure below illustrates Monte Carlo Tree Search in MuZero.\n",
    "Starting at the current state, we use a *representation function* $h$ to map from the observation to an embedding of\n",
    "the state, which is denoted $s_0$. Using the *dynamics function* $g$ and the *prediction function* $f$,\n",
    "we can build a search tree over future states reachable from $s_0$ based on our past experience.\n",
    "\n",
    "<img src=\"img/mcts.gif\" title=\"MCTS\" style=\"width: 800px;\" />\n",
    "\n",
    "The diagram below shows a comparison between the MCTS processes in AlphaZero and MuZero. We see that\n",
    "while AlphaZero has only one function, an estimator of the value function called the prediction network, which relies\n",
    "on a game-specific state representation obviating $h$ and the game rulels obviating $g$. \n",
    "\n",
    "<img src=\"img/alphagovsmuzero.png\" title=\"AlphaZero VS MuZero\" style=\"width: 800px;\" />\n",
    "\n",
    "MuZero, on the other hand, since it doesn't know the rules of the game, has to create $g$ from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on the AlphaZero and MuZero networks\n",
    "\n",
    "AlphaZero has just one neural network:\n",
    "\n",
    "1. $f: \\mathcal{S} \\rightarrow \\mathbb{R}^{|\\mathcal{A}|} \\times \\mathbb{R}$ outputs a probability distribution over the actions indicating their optimality, along\n",
    "   with the estimated value of state $s$.\n",
    "\n",
    "The prediction is made every time MCTS hits an unexplored leaf node, so that it can immediately assign an estimated value to the new position and a probability to each subsequent action. The values are backfilled up the tree, back to the root node, so that after many simulations, the root node has a good idea of the future value of the current state, having explored many different possible futures.\n",
    "\n",
    "As already discussed, MuZero uses three neural networks. Although the\n",
    "actual state of the environment is unknown, we model it as a simple vector of reals, i.e., $\\mathcal{S} = \\mathbb{R}^d$.\n",
    "\n",
    "1. Represenation $h: \\mathcal{O} \\rightarrow \\mathcal{S}$. Calculates an embedding of the observation intended to serve as a proxy for the actual state of the environment. Since\n",
    "   $h$ is learned, in\n",
    "   practice, the embedding should encode the attributes of the observation of the environment that are most useful for predicting eventual rewards.\n",
    "2. Prediction $f$: Same as $f$ in AlphaZero.\n",
    "3. Dynamics $g: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathcal{S} \\times \\mathbb{R}$. Maps current state $s$ and chosen action $a$ to the new\n",
    "   state $s'$ and immediate reward $r$.\n",
    "\n",
    "<img src=\"img/muzero-network.png\" title=\"MuZero Network\" style=\"width: 800px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "MuZero uses the experience it collects when interacting with the environment to train its neural networks.\n",
    "This experience includes the observations and rewards from the environment.\n",
    "\n",
    "The process of using these networks is visualized in the following diagram.\n",
    "\n",
    "<img src=\"img/experience.gif\" title=\"\" style=\"width: 800px;\" />\n",
    "\n",
    "In terms of the pseudocode, there are two key inference functions used to move through the MCTS tree making predictions:\n",
    "- `initial_inference` for the current state. Calls $h$ followed by $f$.\n",
    "- `recurrent_inference` for moving between states inside the MCTS tree. Calls $g$ followed by $f$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkOutput(typing.NamedTuple):\n",
    "    value: float\n",
    "    reward: float\n",
    "    policy_logits: Dict[Action, float]\n",
    "    hidden_state: List[float]\n",
    "\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def initial_inference(self, image) -> NetworkOutput:\n",
    "        # representation + prediction function\n",
    "        return NetworkOutput(0, 0, {}, [])\n",
    "\n",
    "    def recurrent_inference(self, hidden_state, action) -> NetworkOutput:\n",
    "        # dynamics + prediction function\n",
    "        return NetworkOutput(0, 0, {}, [])\n",
    "\n",
    "    def get_weights(self):\n",
    "        # Returns the weights of this network.\n",
    "        return []\n",
    "\n",
    "    def training_steps(self) -> int:\n",
    "        # How many steps / batches the network has been trained for.\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing a game\n",
    "\n",
    "A game is a loop. The game ends when a terminal condition is met or the maximum number of moves is reached.\n",
    "\n",
    "When a new game is started, MCTS must be started over at the root node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(config: MuZeroConfig, network: Network) -> Game:\n",
    "    game = config.new_game()\n",
    "\n",
    "    while not game.terminal() and len(game.history) < config.max_moves:\n",
    "        # At the root of the search tree we use the representation function to\n",
    "        # obtain a hidden state given the current observation.\n",
    "        root = Node(0)\n",
    "        current_observation = game.make_image(-1)\n",
    "        expand_node(root, game.to_play(), game.legal_actions(),\n",
    "                    network.initial_inference(current_observation))\n",
    "        add_exploration_noise(config, root)\n",
    "\n",
    "        # We then run a Monte Carlo Tree Search using only action sequences and the\n",
    "        # model learned by the network.\n",
    "        run_mcts(config, root, game.action_history(), network)\n",
    "        action = select_action(config, len(game.history), root, network)\n",
    "        game.apply(action)\n",
    "        game.store_search_statistics(root)\n",
    "    return game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Node stores key statistics relating to the number of times it has been visited <code>visit_count</code>, whose turn it is <code>to_play</code>, the predicted prior probability of choosing the action that leads to this node prior, the backfilled value sum of the node <code>node_sum</code>, its child nodes children, the hidden state it corresponds to <code>hidden_state</code> and the predicted reward received by moving to this node reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "\n",
    "    def __init__(self, prior: float):\n",
    "        self.visit_count = 0\n",
    "        self.to_play = -1\n",
    "        self.prior = prior\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.hidden_state = None\n",
    "        self.reward = 0\n",
    "\n",
    "    def expanded(self) -> bool:\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def value(self) -> float:\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visit_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we request the game to return the current observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_observation = game.make_image(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we expand the root node using the known legal actions provided by the game and the inference about the current observation provided by the `initial_inference` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_node(node: Node, to_play: Player, actions: List[Action],\n",
    "                network_output: NetworkOutput):\n",
    "    node.to_play = to_play\n",
    "    node.hidden_state = network_output.hidden_state\n",
    "    node.reward = network_output.reward\n",
    "    policy = {a: math.exp(network_output.policy_logits[a]) for a in actions}\n",
    "    policy_sum = sum(policy.values())\n",
    "    for action, p in policy.items():\n",
    "        node.children[action] = Node(p / policy_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_node(root, game.to_play(), game.legal_actions(), network.initial_inference(current_observation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add exploration noise to the root node actions, to ensure that MCTS explores a range of possible actions\n",
    "rather than only exploring the action which it currently believes to be optimal. For chess, we use `root_dirichlet_alpha = 0.3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_exploration_noise(config: MuZeroConfig, node: Node):\n",
    "    actions = list(node.children.keys())\n",
    "    noise = numpy.random.dirichlet([config.root_dirichlet_alpha] * len(actions))\n",
    "    frac = config.root_exploration_fraction\n",
    "    for a, n in zip(actions, noise):\n",
    "        node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_exploration_noise(config, root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCTS run function\n",
    "\n",
    "As MuZero has no knowledge of the environment rules, it also has no knowledge of the bounds on the rewards that it may receive throughout the learning process. The MinMaxStats object is created to store information on the current minimum and maximum rewards encountered so that MuZero can normalise its value output accordingly. Alternatively, this can also be initialised with known bounds for a game such as chess (-1, 1).\n",
    "\n",
    "The main MCTS loop iterates over num_simulations, where one simulation is a pass through the MCTS tree until a leaf node (i.e. unexplored node) is reached and subsequent backpropagation.\n",
    "1. The <code>history</code> is initialized with the list of actions taken from the start of the game. The current node is an initialize node, so it is the root node, and currently the search path has only one node.\n",
    "2. MuZero first traverses down the MCTS tree, always selecting the action with the highest UCB (Upper Confidence Bound) score.\n",
    "3. The UCB score is a measure that balances the estimated value of the action Q(s,a)with a exploration bonus based on the prior probability of selecting the action P(s,a) and the number of times the action has already been selected N(s,a).\n",
    "\n",
    "$$a^k=\\text{arg} \\max_a[Q(s,a) + P(s,a) \\cdot \\frac{\\sqrt{\\sum_b N(s,b)}}{1+N(s,a)}(c_1 + \\log (\\frac{\\sum_b N(s,b)+c_2+1}{c_2}))]$$\n",
    "\n",
    "4. the recurrent_inference function is called on the parent of the leaf node, in order to obtain the predicted reward and new hidden state (from the dynamics network) and policy and value of the new hidden state (from the prediction network).\n",
    "5. the value predicted by the network is back-propagated up the tree, along the search path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_child(config: MuZeroConfig, node: Node,\n",
    "                 min_max_stats: MinMaxStats):\n",
    "    _, action, child = max(\n",
    "        (ucb_score(config, node, child, min_max_stats), action,\n",
    "         child) for action, child in node.children.items())\n",
    "    return action, child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(search_path: List[Node], value: float, to_play: Player,\n",
    "                  discount: float, min_max_stats: MinMaxStats):\n",
    "    for node in search_path:\n",
    "        node.value_sum += value if node.to_play == to_play else -value\n",
    "        node.visit_count += 1\n",
    "        min_max_stats.update(node.value())\n",
    "\n",
    "        value = node.reward + discount * value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mcts(config: MuZeroConfig, root: Node, action_history: ActionHistory,\n",
    "             network: Network):\n",
    "    min_max_stats = MinMaxStats(config.known_bounds)\n",
    "\n",
    "    for _ in range(config.num_simulations):\n",
    "        history = action_history.clone()\n",
    "        node = root\n",
    "        search_path = [node]\n",
    "\n",
    "        while node.expanded():\n",
    "            action, node = select_child(config, node, min_max_stats)\n",
    "            history.add_action(action)\n",
    "            search_path.append(node)\n",
    "\n",
    "        # Inside the search tree we use the dynamics function to obtain the next\n",
    "        # hidden state given an action and the previous hidden state.\n",
    "        parent = search_path[-2]\n",
    "        network_output = network.recurrent_inference(parent.hidden_state,\n",
    "                                                     history.last_action())\n",
    "        expand_node(node, history.to_play(), history.action_space(), network_output)\n",
    "\n",
    "        backpropagate(search_path, network_output.value, history.to_play(),\n",
    "                      config.discount, min_max_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mcts(config, root, game.action_history(), network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After num_simulations passes through the tree, the process stops and an action is chosen based on the number of times each child node of the root has been visited\n",
    "\n",
    "For the first 30 moves, the temperate of the softmax is set to 1, meaning that the probability of selection for each action is proportional to the number of times it has been visited. From the 30th move onwards, the action with the maximum number of visits is selected.\n",
    "\n",
    "$$p_\\alpha = \\frac{N(\\alpha)^{1/T}}{\\sum_b N(b)^{1/T}}$$\n",
    "\n",
    "Though the number of visits may feel a strange metric on which to select the final action, it isn’t really, as the UCB selection criteria within the MCTS process is designed to eventually spend more time exploring actions that it feels are truly high value opportunities, once it has sufficiently explored the alternatives early on in the process.\n",
    "\n",
    "The chosen action is then applied to the true environment and relevant values are appended to the following lists in the gameobject.\n",
    "- game.rewards — a list of true rewards received at each turn of the game\n",
    "- game.history — a list of actions taken at each turn of the game\n",
    "- game.child_visits — a list of action probability distributions from the root node at each turn of the game\n",
    "- game.root_values — a list of values of the root node at each turn of the game\n",
    "\n",
    "All of the game data (rewards, history, child_visits, root_values) is saved to the replay buffer and the actor is then free to start a new game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(config: MuZeroConfig, num_moves: int, node: Node,\n",
    "                  network: Network):\n",
    "    visit_counts = [\n",
    "        (child.visit_count, action) for action, child in node.children.items()\n",
    "    ]\n",
    "    t = config.visit_softmax_temperature_fn(\n",
    "        num_moves=num_moves, training_steps=network.training_steps())\n",
    "    _, action = softmax_sample(visit_counts, t)\n",
    "    return action\n",
    "\n",
    "def visit_softmax_temperature(num_moves, training_steps):\n",
    "    if num_moves < 30:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0  # Play according to the max."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training\n",
    "\n",
    "It first creates a new Network object (that stores randomly initialised instances of MuZero’s three neural networks) and sets the learning rate to decay based on the number of training steps that have been completed. We also create the gradient descent optimiser that will calculate the magnitude and direction of the weight updates at each training step.\n",
    "\n",
    "The last part of this function simply loops over training_steps (=1,000,000 in the paper, for chess). At each step, it samples a batch of positions from the replay buffer and uses them to update the networks, which is saved to storage every checkpoint_interval batches (=1000).\n",
    "\n",
    "There are therefore two finals parts we need to cover — how MuZero creates a batch of training data and how it uses this to update the weights of the three neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(config: MuZeroConfig, storage: SharedStorage,\n",
    "                  replay_buffer: ReplayBuffer):\n",
    "    network = Network()\n",
    "    learning_rate = config.lr_init * config.lr_decay_rate**(\n",
    "        tf.train.get_global_step() / config.lr_decay_steps)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, config.momentum)\n",
    "\n",
    "    for i in range(config.training_steps):\n",
    "        if i % config.checkpoint_interval == 0:\n",
    "            storage.save_network(i, network)\n",
    "        batch = replay_buffer.sample_batch(config.num_unroll_steps, config.td_steps)\n",
    "        update_weights(optimizer, network, batch, config.weight_decay)\n",
    "    storage.save_network(config.training_steps, network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MuZero loss function\n",
    "\n",
    "The loss function of MuZero is shown as:\n",
    "\n",
    "$$ \\mathcal{L}_t(\\theta) = \\sum_{k=0}^K \\mathcal{L}^r(u_{t+k}, r_t^k)+\\mathcal{L}^v (z_{t+k},v_t^k) + \\mathcal{L}^p (\\pi_{t+k},p_t^k + c||\\theta||^2 $$\n",
    "\n",
    "$K$ is the <code>num_unroll_steps</code> variable. There are three losses we are trying to minimise:\n",
    "1. The difference between the predicted reward $k$ steps ahead of turn $t$ ($r$) and the actual reward ($u$)\n",
    "2. The difference between the predicted value $k$ steps ahead of turn $t$ ($v$) and the TD target value ($z$)\n",
    "3. The difference between the predicted policy $k$ steps ahead of turn $t$ ($p$) and the MCTS policy($\\pi$)\n",
    "\n",
    "These losses are summed over the rollout to generate the loss for a given position in the batch. There is also a regularisation term to penalise large weights in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(optimizer: tf.train.Optimizer, network: Network, batch,\n",
    "                   weight_decay: float):\n",
    "    loss = 0\n",
    "    for image, actions, targets in batch:\n",
    "        # Initial step, from the real observation.\n",
    "        value, reward, policy_logits, hidden_state = network.initial_inference(\n",
    "            image)\n",
    "        predictions = [(1.0, value, reward, policy_logits)]\n",
    "\n",
    "    # Recurrent steps, from action and previous hidden state.\n",
    "      for action in actions:\n",
    "          value, reward, policy_logits, hidden_state = network.recurrent_inference(\n",
    "              hidden_state, action)\n",
    "          predictions.append((1.0 / len(actions), value, reward, policy_logits))\n",
    "\n",
    "          hidden_state = tf.scale_gradient(hidden_state, 0.5)\n",
    "\n",
    "      for prediction, target in zip(predictions, targets):\n",
    "          gradient_scale, value, reward, policy_logits = prediction\n",
    "          target_value, target_reward, target_policy = target\n",
    "\n",
    "          l = (\n",
    "              scalar_loss(value, target_value) +\n",
    "              scalar_loss(reward, target_reward) +\n",
    "              tf.nn.softmax_cross_entropy_with_logits(\n",
    "                  logits=policy_logits, labels=target_policy))\n",
    "\n",
    "          loss += tf.scale_gradient(l, gradient_scale)\n",
    "\n",
    "    for weights in network.get_weights():\n",
    "        loss += weight_decay * tf.nn.l2_loss(weights)\n",
    "\n",
    "    optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In lab practice (MuZero)\n",
    "\n",
    "Because MuZero uses a lot of GPU to run, we won't ask you to do so. Download the sample code from\n",
    "[this github respository](https://github.com/werner-duvaud/muzero-general) and open it in visual studio code.\n",
    "You can also run in Google Colab,\n",
    "following from [this link](https://stackoverflow.com/questions/51194303/how-to-run-a-python-script-in-a-py-file-from-a-google-colab-notebook)\n",
    "\n",
    "The code from the link is the same concept as pseudo code, but it has been written to support full running in Gym environments.\n",
    "The code supports multi-GPU running, and we cannot modify it to run on only one GPU for the short time, so it is not recommended to run in the shared server.\n",
    "\n",
    "The programs that you can run in this program (without modification) are:\n",
    "- Cartpole (Tested with the fully connected network)\n",
    "- Lunar Lander (Tested in deterministic mode with the fully connected network)\n",
    "- Gridworld (Tested with the fully connected network)\n",
    "- Tic-tac-toe (Tested with the fully connected network and the residual network)\n",
    "- Connect4 (Slightly tested with the residual network)\n",
    "- Gomoku\n",
    "- Twenty-One / Blackjack (Tested with the residual network)\n",
    "- Atari Breakout\n",
    "\n",
    "**Note**: you may need to install some extra requirements as:\n",
    "- gym[classic_control]\n",
    "- nevergrad\n",
    "- numpy\n",
    "- ray\n",
    "- seaborn\n",
    "- tensorboard\n",
    "- torch\n",
    "\n",
    "### Running MuZero\n",
    "\n",
    "When you want to run the MuZero, run is as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python muzero.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are command line functions that you can input the number to select the games and load pretrained weight, training and playing the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running Tensorboard to show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir ./results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/tensorboard.png\" title=\"\" style=\"width: 800px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check CPU/GPU/RAM in used\n",
    "\n",
    "When running the program, you can check PC performance by opening localhost:8265"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/ray_dashboard.png\" title=\"\" style=\"width: 800px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In lab practice 2 (AlphaZero)\n",
    "\n",
    "The AlphaZero is more light weight and more friendly to try! You can run on any PC without any problems.\n",
    "\n",
    "Please download the code from [github respository](https://github.com/suragnair/alpha-zero-general)\n",
    "\n",
    "Before running, install a library named <code>coloredlogs</code>. This library will allow you to show the text color in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install coloredlogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running AlphaZero\n",
    "\n",
    "Same as MuZero, to run the AlphaZero in **Othello** game can run by the command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To modify the selected GPU, for example from slot 0 to 1, open the code file name <code>NNet.py</code>. Search all the file and change from <code>xxx.cuda()</code> to <code>xxx.to(\"cuda:1\")</code>\n",
    "\n",
    "Here is the all code which is changed from automatic cuda to \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('../../')\n",
    "from utils import *\n",
    "from NeuralNet import NeuralNet\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from .OthelloNNet import OthelloNNet as onnet\n",
    "\n",
    "args = dotdict({\n",
    "    'lr': 0.001,\n",
    "    'dropout': 0.3,\n",
    "    'epochs': 10,\n",
    "    'batch_size': 64,\n",
    "    'cuda': torch.cuda.is_available(),\n",
    "    'num_channels': 512,\n",
    "})\n",
    "\n",
    "\n",
    "class NNetWrapper(NeuralNet):\n",
    "    def __init__(self, game):\n",
    "        self.nnet = onnet(game, args)\n",
    "        self.board_x, self.board_y = game.getBoardSize()\n",
    "        self.action_size = game.getActionSize()\n",
    "\n",
    "        if args.cuda:\n",
    "            # self.nnet.cuda()\n",
    "            self.nnet.to(\"cuda:1\")\n",
    "\n",
    "    def train(self, examples):\n",
    "        \"\"\"\n",
    "        examples: list of examples, each example is of form (board, pi, v)\n",
    "        \"\"\"\n",
    "        optimizer = optim.Adam(self.nnet.parameters())\n",
    "\n",
    "        for epoch in range(args.epochs):\n",
    "            print('EPOCH ::: ' + str(epoch + 1))\n",
    "            self.nnet.train()\n",
    "            pi_losses = AverageMeter()\n",
    "            v_losses = AverageMeter()\n",
    "\n",
    "            batch_count = int(len(examples) / args.batch_size)\n",
    "\n",
    "            t = tqdm(range(batch_count), desc='Training Net')\n",
    "            for _ in t:\n",
    "                sample_ids = np.random.randint(len(examples), size=args.batch_size)\n",
    "                boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))\n",
    "                boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n",
    "                target_pis = torch.FloatTensor(np.array(pis))\n",
    "                target_vs = torch.FloatTensor(np.array(vs).astype(np.float64))\n",
    "\n",
    "                # predict\n",
    "                if args.cuda:\n",
    "                    # boards, target_pis, target_vs = boards.contiguous().cuda(), target_pis.contiguous().cuda(), target_vs.contiguous().cuda()\n",
    "                    boards, target_pis, target_vs = boards.contiguous().to(\"cuda:1\"), target_pis.contiguous().to(\"cuda:1\"), target_vs.contiguous().to(\"cuda:1\")\n",
    "\n",
    "                # compute output\n",
    "                out_pi, out_v = self.nnet(boards)\n",
    "                l_pi = self.loss_pi(target_pis, out_pi)\n",
    "                l_v = self.loss_v(target_vs, out_v)\n",
    "                total_loss = l_pi + l_v\n",
    "\n",
    "                # record loss\n",
    "                pi_losses.update(l_pi.item(), boards.size(0))\n",
    "                v_losses.update(l_v.item(), boards.size(0))\n",
    "                t.set_postfix(Loss_pi=pi_losses, Loss_v=v_losses)\n",
    "\n",
    "                # compute gradient and do SGD step\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    def predict(self, board):\n",
    "        \"\"\"\n",
    "        board: np array with board\n",
    "        \"\"\"\n",
    "        # timing\n",
    "        start = time.time()\n",
    "\n",
    "        # preparing input\n",
    "        board = torch.FloatTensor(board.astype(np.float64))\n",
    "        # if args.cuda: board = board.contiguous().cuda()\n",
    "        if args.cuda: board = board.contiguous().to(\"cuda:1\")\n",
    "        board = board.view(1, self.board_x, self.board_y)\n",
    "        self.nnet.eval()\n",
    "        with torch.no_grad():\n",
    "            pi, v = self.nnet(board)\n",
    "\n",
    "        # print('PREDICTION TIME TAKEN : {0:03f}'.format(time.time()-start))\n",
    "        return torch.exp(pi).data.cpu().numpy()[0], v.data.cpu().numpy()[0]\n",
    "\n",
    "    def loss_pi(self, targets, outputs):\n",
    "        return -torch.sum(targets * outputs) / targets.size()[0]\n",
    "\n",
    "    def loss_v(self, targets, outputs):\n",
    "        return torch.sum((targets - outputs.view(-1)) ** 2) / targets.size()[0]\n",
    "\n",
    "    def save_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if not os.path.exists(folder):\n",
    "            print(\"Checkpoint Directory does not exist! Making directory {}\".format(folder))\n",
    "            os.mkdir(folder)\n",
    "        else:\n",
    "            print(\"Checkpoint Directory exists! \")\n",
    "        torch.save({\n",
    "            'state_dict': self.nnet.state_dict(),\n",
    "        }, filepath)\n",
    "\n",
    "    def load_checkpoint(self, folder='checkpoint', filename='checkpoint.pth.tar'):\n",
    "        # https://github.com/pytorch/examples/blob/master/imagenet/main.py#L98\n",
    "        filepath = os.path.join(folder, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            raise (\"No model in path {}\".format(filepath))\n",
    "        map_location = None if args.cuda else 'cpu'\n",
    "        checkpoint = torch.load(filepath, map_location=map_location)\n",
    "        self.nnet.load_state_dict(checkpoint['state_dict'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change the game, go to <code>main.py</code> and change the library above. In current version, the pytorch game support has only one game, othello. If you have keras or tensorflow, you can try connect4 and gobang (Go game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import coloredlogs\n",
    "\n",
    "from Coach import Coach\n",
    "from othello.OthelloGame import OthelloGame as Game        # Change here\n",
    "from othello.pytorch.NNet import NNetWrapper as nn         # Change here\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do on your own\n",
    "\n",
    "- Add a self play simulator that runs with a fixed model for n-loops.\n",
    "- Try a different game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
