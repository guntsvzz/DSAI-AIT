{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 09: Variational Autoencoders\n",
    "\n",
    "In this lab, we will learn how to build and train a variational autoencoder following [Kingma and Welling (2014)](https://arxiv.org/pdf/1312.6114.pdf).\n",
    "\n",
    "The application the authors consider is a generative model for the MNIST digits dataset that uses a spherical Gaussian for the latent space.\n",
    "\n",
    "We'll first look at the official VAE PyTorch example code (https://github.com/pytorch/examples/tree/master/vae).\n",
    "They follow Kingma and Welling in that the encoder $q_\\phi(\\mathbf{x})$ and the decoder $p_\\theta(\\mathbf{z})$ are both fully-connected neural networks.\n",
    "However, in the original paper, the authors use sigmoid hidden units and the AdaGrad optimizer. In the PyTorch VAE example, the implementers find that it\n",
    "converges much faster with ReLU hidden units and the Adam optimizer.\n",
    "\n",
    "After that, we'll explore the use of convolutional layers rather than fully-connected layers for the encoder and decoder, for MNIST, AIT ICT Faces, and CelebA.\n",
    "\n",
    "## Deep generative models overview\n",
    "\n",
    "Deep generative models have 2 major families:\n",
    " - Generative Adversarial Networks (GANs)\n",
    " - Variational Autoencoders (VAEs)\n",
    "\n",
    "Both models can produce highly realistic content such as images, text documents, and sounds. The difference\n",
    "is that GANs use two networks (generator and discriminator) that play a minimax game to optimize performance, whereas\n",
    "the VAE is an autoencoder in which the distribution at the bottlneck layer is regularised during training in order to\n",
    "ensure that the latent space has good properties (smoothness), enabling the decoder to act as a regularized generator\n",
    "that can generate novel data consistent with the training data set. The term \"variational\" comes from the close relationship\n",
    "between regularisation and the variational inference method in statistics.\n",
    "\n",
    "Here's the idea of the GAN's structure from [Towards Data Science](https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73):\n",
    "\n",
    "<img src=\"img/gans.jpg\" title=\"VAE\" style=\"width: 640px;\" />\n",
    "\n",
    "and here's the idea of the VAE's structure (with convolutional encoder and decoder):\n",
    "\n",
    "<img src=\"img/vae.jpg\" title=\"VAE\" style=\"width: 640px;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder in an ordinary autoencoder maps the input to a low-dimensional repesentation at a \"bottleneck\" layer that preserves as much information as possible that will be\n",
    "useful for reconstructing the input from the encoding. If we use a single fully connected linear layer as the bottleneck without any nonlinearity or other hidden layers,\n",
    "use a single fully connected linear layer for the decoder, and\n",
    "train the model to minimize L2 reconstruction error, it is a well-known result in the neural network literature that we obtain the same result as Principal Components\n",
    "Analysis (PCA), a linear statistical dimensionality reduction technique more than 100 years old!\n",
    "\n",
    "The autoencoding process, when thought of as a compression process, can be lossless or lossy:\n",
    "\n",
    "<img src=\"img/encodedecode.png\" title=\"Encoder-Decoder\" style=\"width: 1080px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More on principal components analysis (PCA)\n",
    "\n",
    "Assume $n_d$ is the dimensionality of the initial (decoded) space and that $n_e$ is the dimensionality of the representation at the bottleneck.\n",
    "PCA constructs $n_e$ independent features as linear combinations of the input $n_d$ features. Since each feature is a linear combination of the input features,\n",
    "we can envision the weights for each feature as a vector in the input space and the dot product operation as a projection operation. PCA will find the\n",
    "projections of the data that are as well aligned as possible with the input data distribution. With L2 loss at the ouptut of the decoder,\n",
    "we see that PCA is performing a type of linear regression. Minimizing reconstruction error given a fixed size representation at the bottleneck will require an\n",
    "efficient representation at the bottleneck.\n",
    "\n",
    "<img src=\"img/pca.png\" title=\"PCA\" style=\"width: 1080px;\" />\n",
    "\n",
    "In the case of linear dimensionality reduction, the optimal encoding is to take the $n_e$ eigenvectors of the data's covariance matrix corresponding to the\n",
    "$n_e$ largest eigenvectors of that covariance matrix. The encoder's weight matrix is composed of these $n_e$ eignevectors, and the decoder's weight matrix\n",
    "is composed of the same weight vector transposed:\n",
    "\n",
    "<img src=\"img/eigen.png\" title=\"Eigen\" style=\"width: 1080px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder\n",
    "\n",
    "Now we extend the linear PCA concept to a situation in which we would like to obtain a more efficient representation at the bottleneck than can be obtained by PCA.\n",
    "In place of linear projections, we have two neural networks acting as encoder and decoder. Data are fed to the autoencoder which compresses then decompresses the\n",
    "input, and the decoded output is compared with the input. Errors are backpropagated. Hopefully, the encoder will find a representation at the bottleneck that can\n",
    "be efficiently decoded to minimize reconstruction error.\n",
    "The optimization problem looks like this:\n",
    "\n",
    "<img src=\"img/dimen-reduc-eq.png\" title=\"Equation\" style=\"width: 250px;\" />\n",
    "\n",
    "where\n",
    "\n",
    "<img src=\"img/where.png\" title=\"Equation\" style=\"width: 100px;\" />\n",
    "\n",
    "is L2 loss:\n",
    "\n",
    "<img src=\"img/autoencoder.png\" title=\"Autoencoder\" style=\"width: 640px;\" />\n",
    "\n",
    "Ideally, we will obtain an extremely efficient encoding that is also\n",
    "meaningful for the input data set:\n",
    "\n",
    "<img src=\"img/autoencoderwithpca.png\" title=\"Object classifier\" style=\"width: 1080;\" />\n",
    "\n",
    "\n",
    "<img src=\"img/autoencoderconcept.png\" title=\"Autoencoder concept\" style=\"width: 1080px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational autoencoders (VAEs)\n",
    "\n",
    "Now, let's discuss how the VAE overcomes some issues with the ordinariy autoencoder.\n",
    "A VAE is trained with regularization to avoid overfitting and ensure that the latent space\n",
    "has good properties such as smoothness that encourage generation of novel samples consistent\n",
    "with the input distriution. Instead of encoding an input as a single point in the latent space\n",
    "each input is mapped to a *distribution* over the latent space. The model is then trained as follows:\n",
    "\n",
    "1. The input is encoded, resulting in a distribution over the latent space\n",
    "2. A point from the latent space is sampled from that distribution\n",
    "3. The sampled point is decoded and the reconstruction error is computed\n",
    "4. The reconstruction error is backpropagated through the network\n",
    "\n",
    "Here's the idea, visually:\n",
    "\n",
    "<img src=\"img/diffAE_VAEs.png\" title=\"Difference between autoencoder and VAEs\" style=\"width: 1080;\" />\n",
    "\n",
    "We would ordinarily (but not necessarily) choose the encoded distribution to be multivariate Gaussian, meaning the encoder should produce a mean vector and covariance\n",
    "matrix for each input. Putting together the output loss function (reconstruction error) with the regularization at the latent layer will give us an encoder and decoder\n",
    "that are both performant in terms of reconstruction error and well organized at the latent layer. Since we have to reconstruct the input accurately from any of the\n",
    "samples it might generate at the latent layer, we should obtain smooth behavior in which interpolation of latent vectors yield realistic and smoothly varying samples\n",
    "at the output. Obtaining this result means \"simply\" penalizing the Kulback-Leibler divergence between the latent distribution and a standardized Gaussian.\n",
    "\n",
    "<img src=\"img/VAEs.png\" title=\"VAEs loss function\" style=\"width: 640;\" />\n",
    "\n",
    "Note that we cannot push the encoded distribution for every input to the standard normal distribution exactly, as we would not then be able to reconstruct the input!\n",
    "But simply penalizing the KL divergence on each iteration will tend to move the marginal distribution over the latent space for the whole training set\n",
    "toward a well organized normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of VAEs\n",
    "\n",
    "OK, let's summarize. The encoder maps the input to the parameters of a distribution:\n",
    "\n",
    "<img src=\"img/encoderVAE.png\" title=\"Encoder of VAEs\" style=\"width: 640\" />\n",
    "\n",
    "The decoder maps an element of the latent space to an element of the data space:\n",
    "\n",
    "<img src=\"img/decoderVAE.png\" title=\"Decoder of VAEs\" style=\"width: 640\" />\n",
    "\n",
    "The reparameterization trick introduced by the authors\n",
    "isolates what we cannot backpropagate through (sampling from a Gaussian with a particular mean and covariance) from\n",
    "the production of the parameters of the distribution, which can be backpropagated through:\n",
    "\n",
    "<img src=\"img/backpropVAE.png\" title=\"Trick\" style=\"width: 640\" />\n",
    "\n",
    "Putting this together we have the full VAE in all of its glory:\n",
    "\n",
    "<img src=\"img/FullVAE.png\" title=\"Full VAE\" style=\"width: 640\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE example\n",
    "Here is a slightly modified version of the PyTorch VAE example. Try it out and take a look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 100\n",
    "seed = 1\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = './torch_data/VGAN/MNIST/dataset' #you can use old downloaded dataset, I use from VGAN\n",
    "batch_size=128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST(root=out_dir, download=True, train=True, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST(root=out_dir, train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create class VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # for encoder\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)\n",
    "        self.fc22 = nn.Linear(400, 20)\n",
    "\n",
    "        # for decoder\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # 0.5 for square root (variance to standard deviation)\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function for VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)))\n",
    "\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      recon_batch.view(batch_size, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the VAE model and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 547.322205\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 188.789932\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 153.061295\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 141.650803\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 130.378632\n",
      "====> Epoch: 1 Average loss: 165.1544\n",
      "====> Test set loss: 127.5979\n",
      "save image: results/sample_1.png\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 128.828674\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 126.597290\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 124.658905\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 126.276428\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 118.374466\n",
      "====> Epoch: 2 Average loss: 121.6115\n",
      "====> Test set loss: 115.6949\n",
      "save image: results/sample_2.png\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 118.661194\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 116.210960\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 113.593597\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 113.495743\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 113.915268\n",
      "====> Epoch: 3 Average loss: 114.5948\n",
      "====> Test set loss: 112.0315\n",
      "save image: results/sample_3.png\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 109.332825\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 114.727570\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 110.114212\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 113.194878\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 111.528122\n",
      "====> Epoch: 4 Average loss: 111.7513\n",
      "====> Test set loss: 109.8421\n",
      "save image: results/sample_4.png\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 114.680054\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 110.418350\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 113.392952\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 108.467697\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 109.514168\n",
      "====> Epoch: 5 Average loss: 109.9936\n",
      "====> Test set loss: 108.6544\n",
      "save image: results/sample_5.png\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 109.998466\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 112.268257\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 112.104698\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 114.072044\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 106.073227\n",
      "====> Epoch: 6 Average loss: 108.8494\n",
      "====> Test set loss: 107.5799\n",
      "save image: results/sample_6.png\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 107.388756\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 107.662781\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 107.508560\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 106.574051\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 107.352753\n",
      "====> Epoch: 7 Average loss: 108.0283\n",
      "====> Test set loss: 107.4122\n",
      "save image: results/sample_7.png\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 105.725372\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 108.277092\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 110.150177\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 108.045601\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 107.789276\n",
      "====> Epoch: 8 Average loss: 107.3584\n",
      "====> Test set loss: 106.5449\n",
      "save image: results/sample_8.png\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 108.137848\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 108.918716\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 103.535446\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 106.205811\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 105.473221\n",
      "====> Epoch: 9 Average loss: 106.8405\n",
      "====> Test set loss: 106.3312\n",
      "save image: results/sample_9.png\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 107.600082\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 105.015251\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 106.488159\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 109.903778\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 104.651505\n",
      "====> Epoch: 10 Average loss: 106.4092\n",
      "====> Test set loss: 105.6986\n",
      "save image: results/sample_10.png\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 102.507812\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 102.779099\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 107.848343\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 104.481735\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 105.037865\n",
      "====> Epoch: 11 Average loss: 106.0427\n",
      "====> Test set loss: 105.4486\n",
      "save image: results/sample_11.png\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 104.755676\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 103.436623\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 103.951599\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 105.730629\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 105.855347\n",
      "====> Epoch: 12 Average loss: 105.7249\n",
      "====> Test set loss: 105.2530\n",
      "save image: results/sample_12.png\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 104.656120\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 106.102020\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 105.138077\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 107.540176\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 101.804695\n",
      "====> Epoch: 13 Average loss: 105.4799\n",
      "====> Test set loss: 105.2220\n",
      "save image: results/sample_13.png\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 103.797470\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 105.732254\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 106.332962\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 103.356323\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 103.924225\n",
      "====> Epoch: 14 Average loss: 105.2526\n",
      "====> Test set loss: 104.8680\n",
      "save image: results/sample_14.png\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 102.936417\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 106.363831\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 105.318436\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 104.509781\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 103.462219\n",
      "====> Epoch: 15 Average loss: 104.9784\n",
      "====> Test set loss: 104.6740\n",
      "save image: results/sample_15.png\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 106.405762\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 103.068573\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 103.813744\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 102.857697\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 106.666924\n",
      "====> Epoch: 16 Average loss: 104.8375\n",
      "====> Test set loss: 104.6237\n",
      "save image: results/sample_16.png\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 107.054710\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 105.854118\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 105.918640\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 102.777679\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 109.074562\n",
      "====> Epoch: 17 Average loss: 104.6270\n",
      "====> Test set loss: 104.5443\n",
      "save image: results/sample_17.png\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 102.507248\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 103.522003\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 106.050591\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 105.737244\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 104.041458\n",
      "====> Epoch: 18 Average loss: 104.4422\n",
      "====> Test set loss: 104.1669\n",
      "save image: results/sample_18.png\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 101.907494\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 108.702087\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 102.574005\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 100.687698\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 106.975327\n",
      "====> Epoch: 19 Average loss: 104.2679\n",
      "====> Test set loss: 104.1470\n",
      "save image: results/sample_19.png\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 106.011055\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 104.743073\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 99.014854\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 102.747284\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 104.936905\n",
      "====> Epoch: 20 Average loss: 104.1829\n",
      "====> Test set loss: 103.9117\n",
      "save image: results/sample_20.png\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 103.944611\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 104.532852\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 107.971771\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 105.647812\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 104.510796\n",
      "====> Epoch: 21 Average loss: 103.9925\n",
      "====> Test set loss: 103.8328\n",
      "save image: results/sample_21.png\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 101.891693\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 101.301117\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 102.945549\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 103.284988\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 103.592224\n",
      "====> Epoch: 22 Average loss: 103.8791\n",
      "====> Test set loss: 103.6460\n",
      "save image: results/sample_22.png\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 107.456009\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 105.963005\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 105.665787\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 106.459198\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 103.456421\n",
      "====> Epoch: 23 Average loss: 103.7355\n",
      "====> Test set loss: 103.5653\n",
      "save image: results/sample_23.png\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 101.297043\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 108.734047\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 101.335190\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 103.466782\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 108.422653\n",
      "====> Epoch: 24 Average loss: 103.6533\n",
      "====> Test set loss: 103.5041\n",
      "save image: results/sample_24.png\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 102.100800\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 104.824081\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 108.458626\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 99.499611\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 101.588348\n",
      "====> Epoch: 25 Average loss: 103.5317\n",
      "====> Test set loss: 103.4536\n",
      "save image: results/sample_25.png\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 102.185379\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 102.459839\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 105.500458\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 102.460968\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 102.615952\n",
      "====> Epoch: 26 Average loss: 103.4480\n",
      "====> Test set loss: 103.4593\n",
      "save image: results/sample_26.png\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 102.862648\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 103.492920\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 102.794151\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 104.446190\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 99.532715\n",
      "====> Epoch: 27 Average loss: 103.3462\n",
      "====> Test set loss: 103.2161\n",
      "save image: results/sample_27.png\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 102.276939\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 102.532852\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 106.239563\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 102.071426\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 106.302628\n",
      "====> Epoch: 28 Average loss: 103.2476\n",
      "====> Test set loss: 103.0989\n",
      "save image: results/sample_28.png\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 106.549110\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 106.366486\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 102.155640\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 102.852585\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 101.567551\n",
      "====> Epoch: 29 Average loss: 103.1654\n",
      "====> Test set loss: 103.1369\n",
      "save image: results/sample_29.png\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 103.572777\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 99.842247\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 105.473038\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 106.760078\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 102.153763\n",
      "====> Epoch: 30 Average loss: 103.0911\n",
      "====> Test set loss: 103.0549\n",
      "save image: results/sample_30.png\n",
      "Train Epoch: 31 [0/60000 (0%)]\tLoss: 103.022507\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 103.373802\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 101.472305\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 99.202118\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 100.484680\n",
      "====> Epoch: 31 Average loss: 103.0327\n",
      "====> Test set loss: 103.0792\n",
      "save image: results/sample_31.png\n",
      "Train Epoch: 32 [0/60000 (0%)]\tLoss: 104.372406\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 106.679535\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 104.487656\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 101.894501\n",
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 101.053017\n",
      "====> Epoch: 32 Average loss: 102.8788\n",
      "====> Test set loss: 102.9204\n",
      "save image: results/sample_32.png\n",
      "Train Epoch: 33 [0/60000 (0%)]\tLoss: 101.480331\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 104.617157\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 98.967896\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 104.504761\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 103.104225\n",
      "====> Epoch: 33 Average loss: 102.8419\n",
      "====> Test set loss: 103.0638\n",
      "save image: results/sample_33.png\n",
      "Train Epoch: 34 [0/60000 (0%)]\tLoss: 98.627548\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 102.435326\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 102.493423\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 102.241966\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 103.844330\n",
      "====> Epoch: 34 Average loss: 102.7698\n",
      "====> Test set loss: 102.7539\n",
      "save image: results/sample_34.png\n",
      "Train Epoch: 35 [0/60000 (0%)]\tLoss: 100.908936\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 103.694244\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 104.274696\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 103.290413\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 103.964775\n",
      "====> Epoch: 35 Average loss: 102.7456\n",
      "====> Test set loss: 102.7630\n",
      "save image: results/sample_35.png\n",
      "Train Epoch: 36 [0/60000 (0%)]\tLoss: 103.666985\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 102.039223\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 103.814102\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 101.723518\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 97.379974\n",
      "====> Epoch: 36 Average loss: 102.6676\n",
      "====> Test set loss: 102.6123\n",
      "save image: results/sample_36.png\n",
      "Train Epoch: 37 [0/60000 (0%)]\tLoss: 104.765488\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 103.305771\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 99.402260\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 104.395592\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 99.821884\n",
      "====> Epoch: 37 Average loss: 102.5913\n",
      "====> Test set loss: 102.5776\n",
      "save image: results/sample_37.png\n",
      "Train Epoch: 38 [0/60000 (0%)]\tLoss: 101.691620\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 102.019356\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 102.365051\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 99.345413\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 103.367767\n",
      "====> Epoch: 38 Average loss: 102.5170\n",
      "====> Test set loss: 102.5772\n",
      "save image: results/sample_38.png\n",
      "Train Epoch: 39 [0/60000 (0%)]\tLoss: 101.423988\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 100.488686\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 100.159317\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 102.528694\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 104.271515\n",
      "====> Epoch: 39 Average loss: 102.4955\n",
      "====> Test set loss: 102.7277\n",
      "save image: results/sample_39.png\n",
      "Train Epoch: 40 [0/60000 (0%)]\tLoss: 104.893723\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 105.543106\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 100.690331\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 101.541725\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 99.038300\n",
      "====> Epoch: 40 Average loss: 102.3902\n",
      "====> Test set loss: 102.6026\n",
      "save image: results/sample_40.png\n",
      "Train Epoch: 41 [0/60000 (0%)]\tLoss: 100.111984\n",
      "Train Epoch: 41 [12800/60000 (21%)]\tLoss: 105.371857\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 100.869980\n",
      "Train Epoch: 41 [38400/60000 (64%)]\tLoss: 103.094810\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 101.304230\n",
      "====> Epoch: 41 Average loss: 102.3802\n",
      "====> Test set loss: 102.3791\n",
      "save image: results/sample_41.png\n",
      "Train Epoch: 42 [0/60000 (0%)]\tLoss: 104.786774\n",
      "Train Epoch: 42 [12800/60000 (21%)]\tLoss: 101.302116\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 99.064629\n",
      "Train Epoch: 42 [38400/60000 (64%)]\tLoss: 100.519730\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 104.721130\n",
      "====> Epoch: 42 Average loss: 102.2311\n",
      "====> Test set loss: 102.4653\n",
      "save image: results/sample_42.png\n",
      "Train Epoch: 43 [0/60000 (0%)]\tLoss: 99.749046\n",
      "Train Epoch: 43 [12800/60000 (21%)]\tLoss: 99.040878\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 102.483170\n",
      "Train Epoch: 43 [38400/60000 (64%)]\tLoss: 102.180923\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 101.719177\n",
      "====> Epoch: 43 Average loss: 102.2334\n",
      "====> Test set loss: 102.5383\n",
      "save image: results/sample_43.png\n",
      "Train Epoch: 44 [0/60000 (0%)]\tLoss: 99.901131\n",
      "Train Epoch: 44 [12800/60000 (21%)]\tLoss: 104.085632\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 101.852417\n",
      "Train Epoch: 44 [38400/60000 (64%)]\tLoss: 100.038025\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 104.833405\n",
      "====> Epoch: 44 Average loss: 102.2206\n",
      "====> Test set loss: 102.4184\n",
      "save image: results/sample_44.png\n",
      "Train Epoch: 45 [0/60000 (0%)]\tLoss: 101.905838\n",
      "Train Epoch: 45 [12800/60000 (21%)]\tLoss: 100.743835\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 102.978584\n",
      "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 100.596489\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 102.997826\n",
      "====> Epoch: 45 Average loss: 102.1497\n",
      "====> Test set loss: 102.4483\n",
      "save image: results/sample_45.png\n",
      "Train Epoch: 46 [0/60000 (0%)]\tLoss: 102.993851\n",
      "Train Epoch: 46 [12800/60000 (21%)]\tLoss: 104.292633\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 100.641113\n",
      "Train Epoch: 46 [38400/60000 (64%)]\tLoss: 102.327164\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 103.152115\n",
      "====> Epoch: 46 Average loss: 102.1531\n",
      "====> Test set loss: 102.2475\n",
      "save image: results/sample_46.png\n",
      "Train Epoch: 47 [0/60000 (0%)]\tLoss: 100.006668\n",
      "Train Epoch: 47 [12800/60000 (21%)]\tLoss: 103.498917\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 105.542542\n",
      "Train Epoch: 47 [38400/60000 (64%)]\tLoss: 102.762810\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 99.589172\n",
      "====> Epoch: 47 Average loss: 102.0775\n",
      "====> Test set loss: 102.3077\n",
      "save image: results/sample_47.png\n",
      "Train Epoch: 48 [0/60000 (0%)]\tLoss: 103.365791\n",
      "Train Epoch: 48 [12800/60000 (21%)]\tLoss: 100.216103\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 98.865005\n",
      "Train Epoch: 48 [38400/60000 (64%)]\tLoss: 104.735924\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 100.888008\n",
      "====> Epoch: 48 Average loss: 101.9861\n",
      "====> Test set loss: 102.2626\n",
      "save image: results/sample_48.png\n",
      "Train Epoch: 49 [0/60000 (0%)]\tLoss: 104.982162\n",
      "Train Epoch: 49 [12800/60000 (21%)]\tLoss: 106.811211\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 105.201866\n",
      "Train Epoch: 49 [38400/60000 (64%)]\tLoss: 102.306374\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 104.912872\n",
      "====> Epoch: 49 Average loss: 101.9262\n",
      "====> Test set loss: 102.1513\n",
      "save image: results/sample_49.png\n",
      "Train Epoch: 50 [0/60000 (0%)]\tLoss: 102.617935\n",
      "Train Epoch: 50 [12800/60000 (21%)]\tLoss: 100.042946\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 103.734573\n",
      "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 102.477196\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 105.295593\n",
      "====> Epoch: 50 Average loss: 101.8779\n",
      "====> Test set loss: 102.0593\n",
      "save image: results/sample_50.png\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p results\n",
    "epochs = 50\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    with torch.no_grad():\n",
    "        sample = torch.randn(64, 20).to(device)\n",
    "        sample = model.decode(sample).cpu()\n",
    "        print(\"save image: \" + 'results/sample_' + str(epoch) + '.png')\n",
    "        save_image(sample.view(64, 1, 28, 28), 'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example result after 1 epoch:\n",
    "\n",
    "<img src=\"img/sample_1.png\" title=\"1 epoch\" style=\"width: 640\" />\n",
    "\n",
    "And here is the corresponding result after 50 epochs:\n",
    "\n",
    "<img src=\"img/sample_50.png\" title=\"50 epochs\" style=\"width: 640\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## In-lab exercise\n",
    "\n",
    "Set up visdom to visualize the progress of train and test loss during training, and also visualize the test reconstructions and reconstructed samples from the latent space.\n",
    "\n",
    "Experiment with number of epochs, batch size, learning rate, etc. to get the best results you can.\n",
    "\n",
    "## In-lab/at home exercise\n",
    "\n",
    "Repeat the MNIST experiment with AIT ICT Faces and the small subset of CelebA. Use convolutional layers rather than fully connected layers for the encoder and decoder, keeping the prior over the latent representation as a spherical Gaussian. Does it work with such a small dataset? If not, you may consider using a larger subset of CelebA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
