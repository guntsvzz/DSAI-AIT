{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTML Lab 3: ResNet, SeNet and GradCam\n",
    "\n",
    "NAME = \"Todsavad Tangtortan\"\n",
    "ID = \"123012\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab report\n",
    "\n",
    "As usual, write up an introduction, methods, results, and conclusion based on your work as a Jupyter notebook, export to PDF,\n",
    "and submit in the Google Classroom before next week's lab."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, we can see that the additional parameters accelerate learning of the training set without\n",
    "causing any degredation on the validation set and in fact improving validation set performance early\n",
    "on.\n",
    "\n",
    "## Independent exercises (Please modify)\n",
    "\n",
    "1. Get your Docker container running on the GPU server and get VSCode talking to your Docker container as in previous labs.\n",
    "2. Set up the ResNet and ResSENet classes with supporting functions using a good structure (one class per file) as in Lab 02.\n",
    "   Fix any warnings pylint finds in our sample code. :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet Model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# %%\n",
    "class BasicBlock(nn.Module):\n",
    "    '''\n",
    "    BasicBlock: Simple residual block with two conv layers\n",
    "    '''\n",
    "    EXPANSION = 1\n",
    "    def __init__(self, in_planes, out_planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_planes)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # If output size is not equal to input size, reshape it with 1x1 convolution\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# %%\n",
    "class BottleneckBlock(nn.Module):\n",
    "    '''\n",
    "    BottleneckBlock: More powerful residual block with three convs, used for Resnet50 and up\n",
    "    '''\n",
    "    EXPANSION = 4\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.EXPANSION * planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.EXPANSION * planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # If the output size is not equal to input size, reshape it with 1x1 convolution\n",
    "        if stride != 1 or in_planes != self.EXPANSION * planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.EXPANSION * planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.EXPANSION * planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# %%\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_planes = 64\n",
    "        # Initial convolution\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding = 1) # added maxpool layer to math the paper\n",
    "\n",
    "        # Residual blocks\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        # FC layer = 1 layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.linear = nn.Linear(512 * block.EXPANSION, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.EXPANSION\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn1(self.conv1(x))\n",
    "        out = self.maxpool(out)\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        \n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "# %%\n",
    "def ResNet18(num_classes = 10):\n",
    "    '''\n",
    "    First conv layer: 1\n",
    "    4 residual blocks with two sets of two convolutions each: 2*2 + 2*2 + 2*2 + 2*2 = 16 conv layers\n",
    "    last FC layer: 1\n",
    "    Total layers: 1+16+1 = 18\n",
    "    '''\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "\n",
    "def ResNet34(num_classes):\n",
    "    '''\n",
    "    First conv layer: 1\n",
    "    4 residual blocks with [3, 4, 6, 3] sets of two convolutions each: 3*2 + 4*2 + 6*2 + 3*2 = 32\n",
    "    last FC layer: 1\n",
    "    Total layers: 1+32+1 = 34\n",
    "    '''\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "\n",
    "def ResNet50(num_classes = 10):\n",
    "    '''\n",
    "    First conv layer: 1\n",
    "    4 residual blocks with [3, 4, 6, 3] sets of three convolutions each: 3*3 + 4*3 + 6*3 + 3*3 = 48\n",
    "    last FC layer: 1\n",
    "    Total layers: 1+48+1 = 50\n",
    "    '''\n",
    "    return ResNet(BottleneckBlock, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "\n",
    "def ResNet101(num_classes = 10):\n",
    "    '''\n",
    "    First conv layer: 1\n",
    "    4 residual blocks with [3, 4, 23, 3] sets of three convolutions each: 3*3 + 4*3 + 23*3 + 3*3 = 99\n",
    "    last FC layer: 1\n",
    "    Total layers: 1+99+1 = 101\n",
    "    '''\n",
    "    return ResNet(BottleneckBlock, [3, 4, 23, 3], num_classes)\n",
    "\n",
    "\n",
    "def ResNet152(num_classes = 10):\n",
    "    '''\n",
    "    First conv layer: 1\n",
    "    4 residual blocks with [3, 8, 36, 3] sets of three convolutions each: 3*3 + 8*3 + 36*3 + 3*3 = 150\n",
    "    last FC layer: 1\n",
    "    Total layers: 1+150+1 = 152\n",
    "    '''\n",
    "    return ResNet(BottleneckBlock, [3, 8, 36, 3], num_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For ResNet Model, there are 2 points which edit to match with the architecture described in the paper.\n",
    "- Change the initial convolution kernel size to 7\n",
    "- Add MaxPooling layer after BatchNorm2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResSENet Model\n",
    "import torch.nn as nn\n",
    "from resnet_model import ResNet\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class ResidualSEBasicBlock(nn.Module):\n",
    "    '''\n",
    "    ResidualSEBasicBlock: Standard two-convolution residual block with an SE Module between the\n",
    "                          second convolution and the identity addition\n",
    "    '''\n",
    "    EXPANSION = 1\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, stride=1, reduction=16):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_planes)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.se = SELayer(out_planes, reduction)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # If output size is not equal to input size, reshape it with a 1x1 conv\n",
    "        if stride != 1 or in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.EXPANSION * out_planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.se(out)              # se net add here\n",
    "        out += self.shortcut(x)         # shortcut just plus it!!!\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResSENet18(num_classes = 10):\n",
    "    return ResNet(ResidualSEBasicBlock, [2, 2, 2, 2], num_classes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Switch to 256x256 scaling and 224x224 cropping for CIFAR as standardized for ImageNet. Modify the implementation to use\n",
    "   the same convolutions (e.g., initial 7x7) and other layer settings found in the ResNet paper. Find competitive hyperparameters for\n",
    "   ResNet18 and ResSENet18 you can and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow augmentation transform for training set, no augementation for val/test set\n",
    "\n",
    "'''\n",
    "train_preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "eval_preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For data augmentation, chaning to 256x256 scaling and 224x224 cropping for CIFAR which were the standard augmentation for ImageNet\n",
    "- For the training set, using RandomCrop and RandomHorizontalFlip\n",
    "- For the validation and test set, using only CenterCrop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traning ResNet SGD learning rate : 0.01, monemtum : 0.9\n",
    "'''\n",
    "Epoch 24/24\n",
    "----------\n",
    "train Loss: 0.2126 Acc: 0.9251\n",
    "Epoch time taken:  173.42196822166443\n",
    "val Loss: 0.3928 Acc: 0.8750\n",
    "Epoch time taken:  190.02183413505554\n",
    "\n",
    "Training complete in 79m 32s\n",
    "Best val Acc: 0.875000\n",
    "'''\n",
    "#Testing ResNet SGD learning rate : 0.01, monemtum : 0.9\n",
    "'''\n",
    "test Loss: 0.3880 Acc: 0.8797\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traning ResNet SGD learning rate : 0.01, monemtum : 0.9, weight_decay : 0.0001\n",
    "'''\n",
    "Epoch 24/24\n",
    "----------\n",
    "train Loss: 0.1829 Acc: 0.9366\n",
    "Epoch time taken:  181.1041021347046\n",
    "val Loss: 0.3871 Acc: 0.8755\n",
    "Epoch time taken:  198.00470638275146\n",
    "\n",
    "Training complete in 81m 29s\n",
    "Best val Acc: 0.888900\n",
    "'''\n",
    "#Testing ResNet SGD learning rate : 0.01, monemtum : 0.9, weight_decay : 0.0001\n",
    "'''\n",
    "test Loss: 0.3754 Acc: 0.8812\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traning ResNet Adam learning rate 0.01\n",
    "'''\n",
    "Epoch 24/24\n",
    "----------\n",
    "train Loss: 0.1957 Acc: 0.9325\n",
    "Epoch time taken:  176.92339634895325\n",
    "val Loss: 0.4305 Acc: 0.8660\n",
    "Epoch time taken:  193.23867511749268\n",
    "\n",
    "Training complete in 81m 18s\n",
    "Best val Acc: 0.872500\n",
    "'''\n",
    "#Testing ResNet Adam learning rate 0.01\n",
    "'''\n",
    "test Loss: 0.4264 Acc: 0.8676\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traning ResNet Adam learning rate : 0.01, weight_decay : 0.0005\n",
    "'''\n",
    "\n",
    "'''\n",
    "#Testing ResNet Adam learning rrate : 0.01, weight_decay : 0.0005\n",
    "'''\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traning ResSENet SGD learning rate : 0.01, monemtum : 0.9\n",
    "'''\n",
    "train Loss: 0.1222 Acc: 0.9571\n",
    "Epoch time taken:  199.08523273468018\n",
    "val Loss: 0.3332 Acc: 0.9021\n",
    "Epoch time taken:  219.11097955703735\n",
    "\n",
    "Training complete in 89m 34s\n",
    "Best val Acc: 0.902100\n",
    "'''\n",
    "#Testing ResSENet SGD learning rate : 0.01, monemtum : 0.9\n",
    "'''\n",
    "test Loss: 0.3506 Acc: 0.8953\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traning ResSENet SGD learning rate : 0.01, monemtum : 0.9, weight_decay : 0.0001\n",
    "'''\n",
    "Epoch 24/24\n",
    "----------\n",
    "train Loss: 0.3188 Acc: 0.8931\n",
    "Epoch time taken:  201.43707418441772\n",
    "val Loss: 0.4106 Acc: 0.8578\n",
    "Epoch time taken:  221.6597032546997\n",
    "\n",
    "Training complete in 105m 46s\n",
    "Best val Acc: 0.874200\n",
    "'''\n",
    "#Testing ResSENet SGD learning rate : 0.01, monemtum : 0.9, weight_decay : 0.0001\n",
    "'''\n",
    "test Loss: 0.3875 Acc: 0.8729\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traning ResSENet Adam learning rate : 0.01\n",
    "'''\n",
    "Epoch 24/24\n",
    "----------\n",
    "train Loss: 0.2641 Acc: 0.9076\n",
    "Epoch time taken:  194.07701754570007\n",
    "val Loss: 0.4524 Acc: 0.8606\n",
    "Epoch time taken:  211.7615306377411\n",
    "\n",
    "Training complete in 88m 19s\n",
    "Best val Acc: 0.861500\n",
    "'''\n",
    "#Testing ResSENet Adam learning rate : 0.01\n",
    "'''\n",
    "test Loss: 0.4540 Acc: 0.8554\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traning ResSENet Adam learning rate : 0.01, weight_decay : 0.0001\n",
    "'''\n",
    "Epoch 24/24\n",
    "----------\n",
    "train Loss: 1.3009 Acc: 0.5383\n",
    "Epoch time taken:  196.39878392219543\n",
    "val Loss: 1.7016 Acc: 0.4149\n",
    "Epoch time taken:  214.19287991523743\n",
    "\n",
    "Training complete in 88m 60s\n",
    "Best val Acc: 0.532500\n",
    "'''\n",
    "#Testing ResSENet Adam learning rate : 0.01, weight_decay : 0.0001\n",
    "'''\n",
    "test Loss: 1.2968 Acc: 0.5280\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model    | Optimizer | Weight Decay | Train Loss | Train Acc | Val Loss | Val Acc | Test Loss | Test Acc | Time Taken |\n",
    "|----------|-----------|--------------|------------|-----------|----------|---------|-----------|----------|------------|\n",
    "| ResNet   | SGD       |              | 0.2126     | 92.51 %   | 0.3928   | 87.50 % | 0.388     | 87.97 %  | 79m 32s    |\n",
    "| ResNet   | SGD       | &check;      | 0.1826     | 93.66 %   | 0.3871   | 87.55 % | 0.3754    | 88.12 %  | 81m 29s    |\n",
    "| ResNet   | Adam      |              | 0.1957     | 93.25 %   | 0.4305   | 87.25 % | 0.4264    | 86.76 %  | 81m 18s    |\n",
    "| ResNet   | Adam      | &check;      | 1.0032     | 64.92 %   | 0.9674   | 65.47 % | 0.9832    | 66.28 %  | 80m 10s    |\n",
    "| ResSENet | SGD       |              | 0.1222     | 95.71 %   | 0.3332   | 90.21 % | 0.3506    | 89.53 %  | 89m 34s    |\n",
    "| ResSENet | SGD       | &check;      | 0.3188     | 89.31 %   | 0.4106   | 8578    | 0.3875    | 87.29 %  | 105m 46s   |\n",
    "| ResSENet | Adam      |              | 0.2641     | 90.76 %   | 0.4525   | 86.06 % | 0.4540    | 85.54 %  | 88m 19s    |\n",
    "| ResSENet | Adam      | &check;      | 1.3009     | 53.83 %   | 1.7016   | 53.25 % | 1.2968    | 52.80 %  | 88m 60s    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data(val_acc_history, loss_acc_history, val_acc_history2, loss_acc_history2):\n",
    "    plt.plot(loss_acc_history, label = 'ResNet18')\n",
    "    plt.plot(loss_acc_history2, label = 'ResSENet18')\n",
    "    plt.title('Training loss over time')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.plot(val_acc_history, label = 'ResNet18')\n",
    "    plt.plot(val_acc_history2, label = 'ResSENet18')\n",
    "    plt.title('Validation accuracy over time')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "accResNet = np.load('./plot/resnet18SGDdecay_val_acc_history.npy', allow_pickle=True)\n",
    "lossResNet = np.load('./plot/resnet18SGDdecay_loss_history.npy', allow_pickle=True)\n",
    "\n",
    "accResSENet = np.load('./plot/resnetSE18SGDdecay_val_acc_history.npy', allow_pickle=True)\n",
    "lossResSENet = np.load('./plot/resnetSE18SGDdecay_loss_history.npy', allow_pickle=True)\n",
    "\n",
    "plot_data(accResNet, lossResNet, accResSENet, lossResSENet)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. As practice in transfer learning for very small datasets, take your best ResSENet18 from the exercise above and fine tune it\n",
    "   on the [Chihuahua vs. Muffins Challenge](https://www.topbots.com/chihuahua-muffin-searching-best-computer-vision-api/?amp).\n",
    "   You only have 16 examples from each category, so to get a reasonable validation accuracy estimate,\n",
    "   use 8-fold cross validation for this experiment. In particular,\n",
    "   fine tune your model 8 different times, holding out two examples from each category as the validation set each time, report\n",
    "   the average accuracy as the expected accuracy of your model, then train a final fine-tuned\n",
    "   model on all of the data with the hyperparameters found during cross validation.\n",
    "   Lastly, test the final model on a few muffin and chihuahua images you find online that are not identical to the examples\n",
    "   in the meme. Have fun with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader,Subset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# Set device to GPU or CPU\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "dataset = datasets.ImageFolder(root='./chihuahua_muffin/', transform=train_preprocess)\n",
    "dataloaders = DataLoader(dataset, batch_size = 16) # get all 16 images into 1 batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1\n",
      "----------\n",
      "train Loss: 2.2454 Acc: 0.2857\n",
      "Epoch time taken:  0.17180728912353516\n",
      "val Loss: 84122.5078 Acc: 0.5000\n",
      "Epoch time taken:  0.18587923049926758\n",
      "\n",
      "Epoch 1/1\n",
      "----------\n",
      "train Loss: 1.4903 Acc: 0.2857\n",
      "Epoch time taken:  0.17571091651916504\n",
      "val Loss: 660.5890 Acc: 0.5000\n",
      "Epoch time taken:  0.1918191909790039\n",
      "\n",
      "Training complete in 0m 1s\n",
      "Best val Acc: 0.500000\n",
      "[tensor(0.5000, device='cuda:0', dtype=torch.float64), tensor(0.5000, device='cuda:0', dtype=torch.float64)]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mprint\u001b[39m(val_acc_history)\n\u001b[1;32m     39\u001b[0m \u001b[39m# break\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m ax[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mplot(np\u001b[39m.\u001b[39marange(\u001b[39m25\u001b[39m),np\u001b[39m.\u001b[39;49marray(val_acc_history),label \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval acc model\u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m ax[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mplot(np\u001b[39m.\u001b[39marange(\u001b[39m25\u001b[39m),np\u001b[39m.\u001b[39marray(loss_acc_history),label \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval acc model\u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m ax[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mset_xlabel(\u001b[39m\"\u001b[39m\u001b[39mEpochs\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:678\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m__array__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    677\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 678\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    679\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    680\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from train_model import train_model\n",
    "from senet_model import ResSENet18\n",
    "import numpy as np\n",
    "\n",
    "folds = 8\n",
    "skf = StratifiedKFold(n_splits=folds, shuffle=True)\n",
    "fig,ax = plt.subplots(1,2,sharex=True,figsize=(20,5))\n",
    "\n",
    "model_acc = 0\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(dataset, dataset.targets)):\n",
    "      batch_size = 4\n",
    "      train = Subset(dataset, train_index)\n",
    "      val = Subset(dataset, val_index)\n",
    "      \n",
    "      train_loader = DataLoader(train, batch_size=batch_size, \n",
    "                                                shuffle=True, num_workers=0, \n",
    "                                                pin_memory=False)\n",
    "      val_loader = DataLoader(val, batch_size=batch_size, \n",
    "                                                shuffle=True, num_workers=0, \n",
    "                                                pin_memory=False)\n",
    "\n",
    "      dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "      \n",
    "      #There are only 2 classes\n",
    "      model = ResSENet18()\n",
    "      model.linear = nn.Linear(512,2)\n",
    "      model.eval()\n",
    "      model.to(device)\n",
    "\n",
    "      dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "      criterion = nn.CrossEntropyLoss().to(device)\n",
    "      optimizer =  optim.Adam(model.parameters(), lr = 0.005 + 0.005*2)\n",
    "\n",
    "      bestmodel, val_acc_history, loss_acc_history = train_model(model, dataloaders, criterion, optimizer, device, 2, 'train_se_chimuffin')\n",
    "\n",
    "      print(val_acc_history)\n",
    "      # break\n",
    "      ax[0].plot(np.arange(25),np.array(val_acc_history),label = f\"val acc model{fold}\")\n",
    "      ax[1].plot(np.arange(25),np.array(loss_acc_history),label = f\"val acc model{fold}\")\n",
    "      ax[0].set_xlabel(\"Epochs\")\n",
    "      ax[1].set_xlabel(\"Epochs\")\n",
    "      ax[0].set_ylabel(\"Accuracy\")\n",
    "      ax[1].set_ylabel(\"Loss\")    \n",
    "      ax[0].set_title(f\"Accuracy vs Epochs of model{fold}\")\n",
    "      ax[1].set_title(f\"Loss vs Epochs of model{fold}\")\n",
    "      ax[0].legend()\n",
    "      ax[1].legend()\n",
    "      ax[0].grid(True)\n",
    "      ax[1].grid(True)    \n",
    "      # model_acc = model_acc + sum(val_acc_history)/len(val_acc_history)\n",
    "\n",
    "plt.show()\n",
    "# print(f'Average accuracy of model: {model_acc/8}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Find 20 different images that have multiple classes of CIFAR dataset classes. Show the result in Grad-CAM using your trained model as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 26 2021, 20:14:08) \n[GCC 9.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
