{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTML Lab 04: YOLO\n",
    "\n",
    "In this lab, we'll explore a fascinating use of image classification deep neural networks to peform a different\n",
    "task: object detection.\n",
    "\n",
    "Credits: parts of this lab are based on other authors' code and blog posts:\n",
    "\n",
    "- YOLO v3 in PyTorch: [tutorial code](https://github.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch),\n",
    "  [blog](https://blog.paperspace.com/how-to-implement-a-yolo-object-detector-in-pytorch/)\n",
    "\n",
    "- YOLO v4 in PyTorch: Chaichan Poonperm (AIT Ph.D. student)\n",
    "\n",
    "\n",
    "## Object Detection\n",
    "\n",
    "If you could go back in time to the 1990s, there were no cameras that could find faces in a photograph, and no\n",
    "researcher had a way to count dogs in a video in real time. Everyone had to count the dogs manually.\n",
    "Times were very tough.\n",
    "\n",
    "The Holy Grail of computer vision research at the time was real time face detection. If we could find faces\n",
    "in images fast enough, we could build systems that interact more naturally with human beings. But nobody had\n",
    "a solution.\n",
    "\n",
    "Things changed when Viola and Jones introduced the first real time face detector, the Haar-like cascade, at\n",
    "the end of the 1990s.\n",
    "This technique swept a detection window over the input image at multiple sizes, and subjected each local patch\n",
    "to a cascade of simple rough classifiers. Each patch that made it to the end of the cascade of classifiers was\n",
    "treated as a positive detection. After a set of candidate patches were identified, there would be a cleanup\n",
    "stage when neighboring detections are clustered into isolated detections.\n",
    "\n",
    "This method and one cousin, the HOG detector, which was slower but a little more accurate, dominated during the 2000s\n",
    "and on into the 2010s. These methods worked well enough when trained carefully on the specific environment they were\n",
    "used in, but usually couldn't be transfer to a new environment.\n",
    "\n",
    "With the introduction of AlexNet and the amazing advances in image classification, we could follow the direction\n",
    "of R-CNN, to use a region proposal algorithm followed by a deep learning classifier to do object detection VERY slowly\n",
    "but much more accurately than the old real time methods.\n",
    "\n",
    "## What is YOLO?\n",
    "\n",
    "However, it wasn't until YOLO that we had a deep learning model for object detection that could run in real time.\n",
    "It took some clever insight to realize that everything, from feature extraction to bounding box estimation, could\n",
    "actually be done with a single neural network that could be trained end-to-end to detect objects.\n",
    "\n",
    "YOLO (You Only Look Once) uses only convolutional layers. This makes it a \"fully convolutional network\" or FCN.\n",
    "\n",
    "YOLOv3 has 75 convolutional layers, with skip connections and upsampling layers. No pooling is used, but there is a convolutional\n",
    "layer with stride 2 used for downsampling. Strided convoution rather than pooling was used to prevent loss of fine-grained detailed\n",
    "information about the precise location of low-level features that would otherwise occur with pooling.\n",
    "\n",
    "Normally, the output of a convolutional layer is a feature map. Applying convolutional layers to a possible detection window or\n",
    "region of interest (ROI) in the image then classifying the ROI's feature map is a reasonable method for detection prediction that\n",
    "is used in Fast R-CNN and Faster R-CNN.\n",
    "However, the innovation of YOLO was to use the feature map directly to predict bounding boxes and, for each bounding box, to\n",
    "predict whether or not an object is at the center of the bounding box. Finally, a classifier is used for each bounding box\n",
    "to indicate the content of the bounding box.\n",
    "\n",
    "## YOLO v3 from \"scratch\"\n",
    "\n",
    "Early versions of YOLO were very fast but not nearly as accurate as their slower cousins. YOLO v3 included many of the\n",
    "tricks and techniques used by other models, such as multiscale analysis, and it achieved both high accuracy and fast inference.\n",
    "\n",
    "Here we'll experiment with building up the YOLO v3 model in PyTorch. However, we won't train it ourselves, as that would\n",
    "require days of training; instead, we'll\n",
    "grab the weights for our PyTorch YOLO v3 from the original Darknet model by Joseph Redmon and friends.\n",
    "\n",
    "### Ground Truth Bounding Boxes\n",
    "\n",
    "Here is how we present example images and corresponding object bounding boxes to the model.\n",
    "\n",
    "The input image is divided into grid cells. The number of cells depends on the number of convolutional layers\n",
    "and the stride of each of those convolutional layers. For example, if we use a 416$\\times$416 input image size,\n",
    "and we apply 5 conv layers with a stride of 2 each (for a total downsampling factor of 32), we end up with a 13$\\times$13\n",
    "feature map, each corresponding to a region in the original image of size 32$\\times$32 pixels.\n",
    "\n",
    "A ground truth box has a center (x and y position), a width, and a height. Normally the ground truth boxes would be\n",
    "provided by a human annotator.\n",
    "\n",
    "Each ground truth box's center must lie in some grid cell in the original image. Consider this example from the YOLO paper:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/yolo05.png\" title=\"GroundTruthBox\" style=\"width: 400px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid is represented by the black lines. The ground truth bounding box for the object is the yellow rectangle. The center\n",
    "of this bounding box happens to be within the red-outlined grid cell.\n",
    "\n",
    "The grid cell containing the center of a ground truth bounding box is given the responsibility during training to try to predict\n",
    "the presence of the object.\n",
    "\n",
    "In order to indicate the presence of the given object, the model outputs several parameters for a given candidate object:\n",
    " - $(t_x, t_y, t_w, t_h)$ indicate the box's location and size. During training, the targets for these outputs are the actual ground truth box parameters.\n",
    " - $p_o$ is an \"objectness\" score that indicates the likelihood that an object exists in the given bounding box. This output uses a sigmoid function.\n",
    "   During training, the target for $p_o$ is set to 1 for the center grid cell (the red grid cell), and it is set to 0 for the the neighboring grid cells.\n",
    " - $(p_1, p_2, \\ldots, p_n)$ are class confidence scores. They indicate the probability of the detected object belonging to a particular class. The targets,\n",
    "   obviously, are set to 1 for the ground truth object class and 0 for other classes during training.\n",
    "\n",
    "### Anchor Boxes\n",
    "\n",
    "One problem that would occur in YOLO if you tried to directly learn the parameters mentioned above is the problem of unstable gradients during training.\n",
    "In a way that is sort of analagous to how a residual block begins with an identity map and learns differences from identity, YOLO v3 uses the idea of\n",
    "anchor boxes originally introduced by the R-CNN team. Instead of predicting $(t_x, t_y, t_w, t_h)$ directly, we predict how those parameters are *different from\n",
    "the parameters of a typical bounding box, an anchor box*.\n",
    "YOLO v3 uses three bounding boxes per cell. At training time, once ground truth bounding box's center is mapped to a grid cell, we find which of the anchors for\n",
    "that cell has the highest IoU with the ground truth box.\n",
    "\n",
    "### So What Does YOLO Actually Predict?\n",
    "\n",
    "First, let's understand that all predictions are relative to the grid cell. YOLO predicts the following:\n",
    "- Offsets $(t_x, t_y)$ are specified relative to the top left corner of the grid cell, as a ratio between 0 and 1, using a sigmoid to limit the values.\n",
    "- Height, and width $(t_w, t_h)$ are specified relative to the dimensions of an anchor box.\n",
    "\n",
    "Thus, YOLO does not predict absolute coordinates -- it predicts values that can then be used to compute the box's position and size in absolute coordinates.\n",
    "This diagram gives the idea. We see that the absolute $t_x$ is the grid cell's $(c_x, c_y)$ plus $\\sigma(t_x)$ times the grid cell width. Similarly for $t_y$.\n",
    "The absolute width of the predicted bounding box is the width of the anchor box times $e^{tw}$. Similarly for the height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/yolo06.png\" title=\"GroundTruthBox\" style=\"width: 640px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you can see the difference between the YOLO v3 bounding box predictions and the Faster R-CNN bounding box predictions. The offset of the center is\n",
    "encoded relative to the grid cell containing the anchor box rather than the anchor box itself. The dimensions of the bounding box, however, similar to Faster R-CNN,\n",
    "are predicted relative to the anchor box size.\n",
    "\n",
    "### Multi-scale prediction\n",
    "\n",
    "Rather than a single grid size and grid cell size,\n",
    "YOLO v3 detects objects at multiple sizes with downsampling factors of 32, 16, and 8. The largest objects are detected at the\n",
    "first, coarsest scale, whereas mid-sized objects are detected at the intermediate scale, and small objects are detected at the finest\n",
    "scale. The example below shows the three grid sizes relative to the image and an object:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/yolo_Scales.png\" title=\"GroundTruthBox\" style=\"width: 640px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO dataset format\n",
    "\n",
    "A YOLO dataset contains two sets of files, each pair with the same name: image files (in any supported format)\n",
    "and label files (in TXT, JSON, or XML format). A label file contains data in the format\n",
    "\n",
    "$$i, C_x, C_y, L_x, L_y$$\n",
    "\n",
    "- $i$: Label index\n",
    "- $C_x$: Center position in the horizontal ($x$-axis) direction, encoded in the range 0-1, where 0 means the left edge of the image and 1 means the right edge.\n",
    "- $C_y$: Center position in the vertical ($y$-axis) direction, encoded in the range 0-1, where 0 means the top edge of the image and 1 means the bottom edge.\n",
    "- $L_x$: Object width, encoded in the range 0-1, where 1 means the width of the image.\n",
    "- $L_y$: Object height, encoded in the range 0-1, where 1 means the height of the image.\n",
    "\n",
    "To calculate these values for an object, suppose $(W,H)$ is the actual size of a particular image in pixels,\n",
    "$(O_x, O_y)$ is the actual position of an object in that image, in pixels, and $(l_x,l_y)$ is the actual size of the object, again in pixels.\n",
    "The object label elements would be calculated as\n",
    "\n",
    "$$C_x = \\frac{O_x}{W}, \\; C_y = \\frac{O_y}{H} \\\\\n",
    "L_x = \\frac{l_x}{W}, \\; L_y = \\frac{l_y}{H}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv3 Architecture\n",
    "\n",
    "<img src=\"img/YOLOv3-Arch.png\" title=\"YOLOv3\" style=\"width: 960px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation for Building YOLO in PyTorch\n",
    "\n",
    "First of all, we will need OpenCV:\n",
    "\n",
    "    pip3 install --upgrade pip\n",
    "    pip install matplotlib opencv-python\n",
    "\n",
    "Create a directory where the code for your detector will live.\n",
    "\n",
    "In that directory, download util.py and darknet.py from https://github.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch.\n",
    "\n",
    "In Jupyter you would download thusly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-06 00:55:49--  https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/darknet.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.8.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.8.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11533 (11K) [text/plain]\n",
      "Saving to: ‘darknet.py’\n",
      "\n",
      "darknet.py          100%[===================>]  11.26K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-02-06 00:55:50 (80.7 MB/s) - ‘darknet.py’ saved [11533/11533]\n",
      "\n",
      "--2021-02-06 00:55:50--  https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/util.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.8.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.8.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7432 (7.3K) [text/plain]\n",
      "Saving to: ‘util.py’\n",
      "\n",
      "util.py             100%[===================>]   7.26K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-02-06 00:55:51 (4.99 MB/s) - ‘util.py’ saved [7432/7432]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/darknet.py\n",
    "!wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/util.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're in a Docker container, just run the `wget` commands at the command line. Make sure your proxy environment variables are set correctly.\n",
    "\n",
    "### Take a Look at the YOLO Darknet Configuration File\n",
    "\n",
    "Next, let's download the `yolov3.cfg` configuration file and take a look. You could grab it from the canonical Darknet github repository\n",
    "or any other place it's stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-06 00:55:57--  https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/cfg/yolov3.cfg\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.8.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.8.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8346 (8.2K) [text/plain]\n",
      "Saving to: ‘yolov3.cfg’\n",
      "\n",
      "yolov3.cfg          100%[===================>]   8.15K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-02-06 00:55:58 (23.4 MB/s) - ‘yolov3.cfg’ saved [8346/8346]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p cfg\n",
    "!wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/cfg/yolov3.cfg\n",
    "!mv yolov3.cfg cfg/yolov3.cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration file looks like this:\n",
    "\n",
    "    [net]\n",
    "    # Testing\n",
    "    batch=1\n",
    "    subdivisions=1\n",
    "    # Training\n",
    "    # batch=64\n",
    "    # subdivisions=16\n",
    "    width= 416\n",
    "\n",
    "    height = 416\n",
    "    channels=3\n",
    "    momentum=0.9\n",
    "    decay=0.0005\n",
    "    angle=0\n",
    "    saturation = 1.5\n",
    "    exposure = 1.5\n",
    "    hue=.1\n",
    "\n",
    "    learning_rate=0.001\n",
    "    burn_in=1000\n",
    "    max_batches = 500200\n",
    "    policy=steps\n",
    "    steps=400000,450000\n",
    "    scales=.1,.1\n",
    "\n",
    "    [convolutional]\n",
    "    batch_normalize=1\n",
    "    filters=32\n",
    "    size=3\n",
    "    stride=1\n",
    "    pad=1\n",
    "    activation=leaky\n",
    "\n",
    "    ...\n",
    "\n",
    "    [shortcut]\n",
    "    from=-3\n",
    "    activation=linear\n",
    "\n",
    "    ...\n",
    "\n",
    "    [yolo]\n",
    "    mask = 6,7,8\n",
    "    anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326\n",
    "    classes=80\n",
    "    num=9\n",
    "    jitter=.3\n",
    "    ignore_thresh = .7\n",
    "    truth_thresh = 1\n",
    "    random=1\n",
    "\n",
    "    [route]\n",
    "    layers = -4\n",
    "\n",
    "    [convolutional]\n",
    "    batch_normalize=1\n",
    "    filters=256\n",
    "    size=1\n",
    "    stride=1\n",
    "    pad=1\n",
    "    activation=leaky\n",
    "\n",
    "    [upsample]\n",
    "    stride=2\n",
    "\n",
    "    [route]\n",
    "    layers = -1, 61\n",
    "\n",
    "    ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the Configuration Blocks\n",
    "\n",
    "The configuration blocks fall into a few cateogies:\n",
    "\n",
    "- Net: the global configuration at the top of the configuration file. It declares the size of input images, batch size, learning rate, and so on.\n",
    "\n",
    "      batch=64\n",
    "      subdivisions=16\n",
    "      width=608\n",
    "      height=608\n",
    "      channels=3\n",
    "      momentum=0.9\n",
    "      decay=0.0005\n",
    "      angle=0\n",
    "      saturation = 1.5\n",
    "      exposure = 1.5\n",
    "      hue=.1\n",
    "\n",
    "\n",
    "- Convolutional: convolutional layer. Note that this specfication is a little more powerful than the PyTorch way of doing things, as options\n",
    "  for batch normalization and the activation function (leaky ReLU in this case) are built in.\n",
    " \n",
    "      [convolutional]\n",
    "      batch_normalize=1\n",
    "      filters=32\n",
    "      size=3\n",
    "      stride=1\n",
    "      pad=1\n",
    "      activation=leaky\n",
    "        \n",
    "\n",
    "- Shortcut: skip connections that implement residual blocks. -3 means to add the feature maps output by the previous layer to those output by the layer three layers\n",
    "  back. Linear activation means identity (no nonlinear activation of the result).\n",
    "  \n",
    "      [shortcut]\n",
    "      from=-3           # Connect the layer three layers back to here.\n",
    "      activation=linear\n",
    "\n",
    "\n",
    "- Upsample: Bilinear upsampling of the previous layer using a particular stride\n",
    "\n",
    "      [upsample]\n",
    "      stride=2\n",
    "\n",
    "\n",
    "- Route: The route layer deserves a bit of explanation. It has an attribute `layers`, which can have either one or two values.\n",
    "  \n",
    "      [route]\n",
    "      layers = -4\n",
    "\n",
    "      [route]\n",
    "      layers = -1, 61    \n",
    "  \n",
    "  When the layers attribute has only one value, it outputs the feature maps of the layer indexed by the value. In our example, it is -4, so the layer will output\n",
    "  the feature maps from the 4th layer backwards from the route layer.\n",
    "\n",
    "  When layers has two values, it returns the concatenated feature maps of the layers indexed by its values. In our example it is -1, 61, so the layer will output\n",
    "  feature maps from the previous layer (-1) and the 61st layer, concatenated along the channels (depth) dimension.\n",
    "   \n",
    "- YOLO:\n",
    " \n",
    "      [yolo]\n",
    "      mask = 0,1,2\n",
    "      anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326\n",
    "      classes=80\n",
    "      num=9\n",
    "      jitter=.3\n",
    "      ignore_thresh = .5\n",
    "      truth_thresh = 1\n",
    "      random=1\n",
    "  \n",
    "  Here we have a few important attributes:\n",
    "  \n",
    "  - anchors: describes the anchor boxes. The model contains 9 anchors, but only those in the `mask` are used.\n",
    "\n",
    "  - mask: which anchor indices will be used in this YOLO layer\n",
    "     \n",
    "  - classes: number of object classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a network from the config file\n",
    "\n",
    "We are going to follow the general approach of some of the GitHub contributors who have developed PyTorch tools\n",
    "to deal with Darknet models. In the file `darknet.py`, there's a `parse_cfg` function. The function will read\n",
    "the Darknet configuration file and store the blocks in a dictionary.\n",
    "\n",
    "<img src=\"img/configfunc.JPG\" title=\"configfunc\" style=\"width: 640px;\" />\n",
    "\n",
    "We'll then create PyTorch NN Modules for each of the blocks in the Darknet configuration as implemented in the\n",
    "`create_modules` function. Take a look at this function for more understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional block\n",
    "\n",
    "<img src=\"img/convolutionalblock.JPG\" title=\"covolutionalblock\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortcut block\n",
    "\n",
    "<img src=\"img/shortcutblock.JPG\" title=\"shortcutblock\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsample block\n",
    "\n",
    "<img src=\"img/upsampleblock.JPG\" title=\"upsampleblock\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Route block\n",
    "\n",
    "Why does it use an empty layer? The actual work will be done in the `forward()` function.\n",
    "\n",
    "<img src=\"img/routeblock.JPG\" title=\"routeblock\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO block\n",
    "\n",
    "<img src=\"img/yoloblock.JPG\" title=\"yoloblock\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the code\n",
    "\n",
    "OK, let's try it out. Depending on what you already have installed, you may need to run\n",
    "\n",
    "    # apt install libgl1-mesa-glx\n",
    "\n",
    "for the next step to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "import darknet\n",
    "\n",
    "blocks = darknet.parse_cfg(\"cfg/yolov3.cfg\")\n",
    "print(darknet.create_modules(blocks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Darknet class\n",
    "\n",
    "Let's make our own version of the `Darknet` class in `darknet.py`.\n",
    "\n",
    "The class has two main functions:\n",
    "\n",
    "1. `forward()`: forward propagation, following the instructions in the dictionary modules\n",
    "2. `load_weights()`: load a set of pretrained weights into the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "\n",
    "class MyDarknet(nn.Module):\n",
    "    def __init__(self, cfgfile):\n",
    "        super(MyDarknet, self).__init__()\n",
    "        # load the config file and create our model\n",
    "        self.blocks = darknet.parse_cfg(cfgfile)\n",
    "        self.net_info, self.module_list = darknet.create_modules(self.blocks)\n",
    "        \n",
    "    def forward(self, x, CUDA:bool):\n",
    "        modules = self.blocks[1:]\n",
    "        outputs = {}   #We cache the outputs for the route layer\n",
    "        \n",
    "        write = 0\n",
    "        # run forward propagation. Follow the instruction from dictionary modules\n",
    "        for i, module in enumerate(modules):        \n",
    "            module_type = (module[\"type\"])\n",
    "            \n",
    "            if module_type == \"convolutional\" or module_type == \"upsample\":\n",
    "                # do convolutional network\n",
    "                x = self.module_list[i](x)\n",
    "    \n",
    "            elif module_type == \"route\":\n",
    "                # concat layers\n",
    "                layers = module[\"layers\"]\n",
    "                layers = [int(a) for a in layers]\n",
    "    \n",
    "                if (layers[0]) > 0:\n",
    "                    layers[0] = layers[0] - i\n",
    "    \n",
    "                if len(layers) == 1:\n",
    "                    x = outputs[i + (layers[0])]\n",
    "    \n",
    "                else:\n",
    "                    if (layers[1]) > 0:\n",
    "                        layers[1] = layers[1] - i\n",
    "    \n",
    "                    map1 = outputs[i + layers[0]]\n",
    "                    map2 = outputs[i + layers[1]]\n",
    "                    x = torch.cat((map1, map2), 1)\n",
    "                \n",
    "    \n",
    "            elif  module_type == \"shortcut\":\n",
    "                from_ = int(module[\"from\"])\n",
    "                # residual network\n",
    "                x = outputs[i-1] + outputs[i+from_]\n",
    "    \n",
    "            elif module_type == 'yolo':        \n",
    "                anchors = self.module_list[i][0].anchors\n",
    "                #Get the input dimensions\n",
    "                inp_dim = int (self.net_info[\"height\"])\n",
    "        \n",
    "                #Get the number of classes\n",
    "                num_classes = int (module[\"classes\"])\n",
    "        \n",
    "                #Transform \n",
    "                # predict_transform is in util.py\n",
    "                x = predict_transform(x, inp_dim, anchors, num_classes, CUDA)\n",
    "                if not write:              #if no collector has been intialised. \n",
    "                    detections = x\n",
    "                    write = 1\n",
    "        \n",
    "                else:       \n",
    "                    detections = torch.cat((detections, x), 1)\n",
    "        \n",
    "            outputs[i] = x\n",
    "        \n",
    "        return detections\n",
    "\n",
    "\n",
    "    def load_weights(self, weightfile):\n",
    "        '''\n",
    "        Load pretrained weight\n",
    "        '''\n",
    "        #Open the weights file\n",
    "        fp = open(weightfile, \"rb\")\n",
    "    \n",
    "        #The first 5 values are header information \n",
    "        # 1. Major version number\n",
    "        # 2. Minor Version Number\n",
    "        # 3. Subversion number \n",
    "        # 4,5. Images seen by the network (during training)\n",
    "        header = np.fromfile(fp, dtype = np.int32, count = 5)\n",
    "        self.header = torch.from_numpy(header)\n",
    "        self.seen = self.header[3]   \n",
    "        \n",
    "        weights = np.fromfile(fp, dtype = np.float32)\n",
    "        \n",
    "        ptr = 0\n",
    "        for i in range(len(self.module_list)):\n",
    "            module_type = self.blocks[i + 1][\"type\"]\n",
    "    \n",
    "            #If module_type is convolutional load weights\n",
    "            #Otherwise ignore.\n",
    "            \n",
    "            if module_type == \"convolutional\":\n",
    "                model = self.module_list[i]\n",
    "                try:\n",
    "                    batch_normalize = int(self.blocks[i+1][\"batch_normalize\"])\n",
    "                except:\n",
    "                    batch_normalize = 0\n",
    "            \n",
    "                conv = model[0]\n",
    "                \n",
    "                \n",
    "                if (batch_normalize):\n",
    "                    bn = model[1]\n",
    "        \n",
    "                    #Get the number of weights of Batch Norm Layer\n",
    "                    num_bn_biases = bn.bias.numel()\n",
    "        \n",
    "                    #Load the weights\n",
    "                    bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])\n",
    "                    ptr += num_bn_biases\n",
    "        \n",
    "                    bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "        \n",
    "                    bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "        \n",
    "                    bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n",
    "                    ptr  += num_bn_biases\n",
    "        \n",
    "                    #Cast the loaded weights into dims of model weights. \n",
    "                    bn_biases = bn_biases.view_as(bn.bias.data)\n",
    "                    bn_weights = bn_weights.view_as(bn.weight.data)\n",
    "                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n",
    "                    bn_running_var = bn_running_var.view_as(bn.running_var)\n",
    "        \n",
    "                    #Copy the data to model\n",
    "                    bn.bias.data.copy_(bn_biases)\n",
    "                    bn.weight.data.copy_(bn_weights)\n",
    "                    bn.running_mean.copy_(bn_running_mean)\n",
    "                    bn.running_var.copy_(bn_running_var)\n",
    "                \n",
    "                else:\n",
    "                    #Number of biases\n",
    "                    num_biases = conv.bias.numel()\n",
    "                \n",
    "                    #Load the weights\n",
    "                    conv_biases = torch.from_numpy(weights[ptr: ptr + num_biases])\n",
    "                    ptr = ptr + num_biases\n",
    "                \n",
    "                    #reshape the loaded weights according to the dims of the model weights\n",
    "                    conv_biases = conv_biases.view_as(conv.bias.data)\n",
    "                \n",
    "                    #Finally copy the data\n",
    "                    conv.bias.data.copy_(conv_biases)\n",
    "                    \n",
    "                #Let us load the weights for the Convolutional layers\n",
    "                num_weights = conv.weight.numel()\n",
    "                \n",
    "                #Do the same as above for weights\n",
    "                conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights])\n",
    "                ptr = ptr + num_weights\n",
    "                \n",
    "                conv_weights = conv_weights.view_as(conv.weight.data)\n",
    "                conv.weight.data.copy_(conv_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Forward Propagation\n",
    "\n",
    "Let's propagate a single image through the network and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-06 00:58:21--  https://github.com/ayooshkathuria/pytorch-yolo-v3/raw/master/dog-cycle-car.png\n",
      "Resolving github.com (github.com)... 13.250.177.223\n",
      "Connecting to github.com (github.com)|13.250.177.223|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/ayooshkathuria/pytorch-yolo-v3/master/dog-cycle-car.png [following]\n",
      "--2021-02-06 00:58:21--  https://raw.githubusercontent.com/ayooshkathuria/pytorch-yolo-v3/master/dog-cycle-car.png\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.8.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.8.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 347445 (339K) [image/png]\n",
      "Saving to: ‘dog-cycle-car.png’\n",
      "\n",
      "dog-cycle-car.png   100%[===================>] 339.30K  1.84MB/s    in 0.2s    \n",
      "\n",
      "2021-02-06 00:58:22 (1.84 MB/s) - ‘dog-cycle-car.png’ saved [347445/347445]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/ayooshkathuria/pytorch-yolo-v3/raw/master/dog-cycle-car.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's code to load the image into memory and push it through the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "\n",
    "def get_test_input():\n",
    "    img = cv2.imread(\"dog-cycle-car.png\")\n",
    "    img = cv2.resize(img, (416,416))          #Resize to the input dimension\n",
    "    img_ =  img[:,:,::-1].transpose((2,0,1))  # BGR -> RGB | H X W C -> C X H X W \n",
    "    img_ = img_[np.newaxis,:,:,:]/255.0       #Add a channel at 0 (for batch) | Normalise\n",
    "    img_ = torch.from_numpy(img_).float()     #Convert to float\n",
    "    img_ = Variable(img_)                     # Convert to Variable\n",
    "    return img_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go ahead and try it (noting that the model hasn't been trained so we don't expect any correct result):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.4784e+01, 1.7460e+01, 1.1424e+02,  ..., 5.5892e-01,\n",
      "          3.9877e-01, 5.6024e-01],\n",
      "         [2.0277e+01, 1.4719e+01, 1.1301e+02,  ..., 5.2226e-01,\n",
      "          5.0744e-01, 4.7786e-01],\n",
      "         [1.7056e+01, 1.5934e+01, 5.1666e+02,  ..., 5.4858e-01,\n",
      "          4.3707e-01, 4.7734e-01],\n",
      "         ...,\n",
      "         [4.1254e+02, 4.1234e+02, 9.5190e+00,  ..., 5.2298e-01,\n",
      "          5.2594e-01, 4.1804e-01],\n",
      "         [4.1249e+02, 4.1238e+02, 2.5798e+01,  ..., 5.1811e-01,\n",
      "          4.4765e-01, 4.2939e-01],\n",
      "         [4.1262e+02, 4.1208e+02, 3.8786e+01,  ..., 6.1843e-01,\n",
      "          5.2593e-01, 4.4060e-01]]])\n"
     ]
    }
   ],
   "source": [
    "from util import *\n",
    "\n",
    "model = MyDarknet(\"cfg/yolov3.cfg\")\n",
    "inp = get_test_input()\n",
    "pred = model(inp, False)\n",
    "print (pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the output result\n",
    "\n",
    "The result from prediction model will be $B(13\\cdot 13 + 26\\cdot 26 + 52 \\cdot 52)3\\cdot85$. Why? We have\n",
    "- $B$: the number of images in the batch\n",
    "- $13\\cdot 13$: number of elements (grid cells) in the coarsest feature map\n",
    "- $26\\cdot 16$: number of elements (grid cells) in the medium scale feature map\n",
    "- $52\\cdot 52$: number of elements (grid cells) in the finest cale feature map\n",
    "- $3$: the number of anchor boxes per grid cell\n",
    "- $85$: number of bounding box attributes (4 for bounding box, 1 for objectness, 80 for the COCO classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a pretrained weight file\n",
    "\n",
    "Darknet stores weights as in this diagram:\n",
    "\n",
    "<img src=\"img/weights.png\" title=\"weight\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-06 01:05:23--  https://pjreddie.com/media/files/yolov3.weights\n",
      "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
      "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 248007048 (237M) [application/octet-stream]\n",
      "Saving to: ‘yolov3.weights’\n",
      "\n",
      "yolov3.weights      100%[===================>] 236.52M  7.23MB/s    in 2m 40s  \n",
      "\n",
      "2021-02-06 01:08:05 (1.47 MB/s) - ‘yolov3.weights’ saved [248007048/248007048]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://pjreddie.com/media/files/yolov3.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"yolov3.weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with the sample image again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[8.5426e+00, 1.9015e+01, 1.1130e+02,  ..., 1.7306e-03,\n",
      "          1.3874e-03, 9.2985e-04],\n",
      "         [1.4105e+01, 1.8867e+01, 9.4014e+01,  ..., 5.9501e-04,\n",
      "          9.2471e-04, 1.3085e-03],\n",
      "         [2.1125e+01, 1.5269e+01, 3.5793e+02,  ..., 8.3609e-03,\n",
      "          5.1067e-03, 5.8561e-03],\n",
      "         ...,\n",
      "         [4.1268e+02, 4.1069e+02, 3.7157e+00,  ..., 1.7185e-06,\n",
      "          4.0955e-06, 6.5897e-07],\n",
      "         [4.1132e+02, 4.1023e+02, 8.0353e+00,  ..., 1.3927e-05,\n",
      "          3.2252e-05, 1.2076e-05],\n",
      "         [4.1076e+02, 4.1318e+02, 4.9635e+01,  ..., 4.2174e-06,\n",
      "          1.0794e-05, 1.8104e-05]]])\n"
     ]
    }
   ],
   "source": [
    "inp = get_test_input()\n",
    "pred = model(inp, False)\n",
    "print (pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From YOLO output tensor to *true* detections\n",
    "\n",
    "In the prediction result, there are many results. We need to threshold them using the objectness score\n",
    "output for each bounding box prediction. The `write_results` function in `util.py` does just that.\n",
    "\n",
    "    def write_results(prediction, confidence, num_classes, nms_conf = 0.4)\n",
    "\n",
    "- prediction: prediction result tensor returned from the YOLO model\n",
    "- confidence: objectness score threshold to apply to the set of detections\n",
    "- num_classes: number of classes to expect\n",
    "- nms_conf: NMS IoU threshold\n",
    "\n",
    "NMS stands for \"non-maxima suppression.\" The basic idea is that if you have two predicted bounding\n",
    "boxes that overlap each other significantly, you should throw away the box with the lower confidence\n",
    "score. Overlap is measured by IoU (Intersection over Union), wich is just the ratio of the the area\n",
    "of intersection of the two regions with the area of the union of the two regions:\n",
    "\n",
    "$$ IoU(R_1,R_2) = \\frac{|R_1 \\cap R_2|}{|R_1 \\cup R_2|}. $$\n",
    "\n",
    "The default of 0.4 means if the intersection is 40% or more of the union, the two bounding boxes\n",
    "are overlapping enough that only one of the detections should survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.0000,  61.5403, 100.8597, 307.2717, 303.1132,   0.9469,   0.9985,\n",
       "           1.0000],\n",
       "        [  0.0000, 253.8483,  66.1096, 378.0396, 118.0089,   0.9992,   0.8164,\n",
       "           7.0000],\n",
       "        [  0.0000,  71.0337, 163.2243, 175.7471, 382.2702,   0.9999,   0.9936,\n",
       "          16.0000]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_results(pred, 0.5, 80, nms_conf = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the resulting detections on top of an image\n",
    "\n",
    "The model was trained on the COCO dataset, so download the class label file `coco.names`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-06 01:49:43--  https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/data/coco.names\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.8.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.8.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 625 [text/plain]\n",
      "Saving to: ‘coco.names’\n",
      "\n",
      "coco.names          100%[===================>]     625  --.-KB/s    in 0s      \n",
      "\n",
      "2021-02-06 01:49:44 (44.2 MB/s) - ‘coco.names’ saved [625/625]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/data/coco.names\n",
    "!mkdir data\n",
    "!mv coco.names data/coco.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classes(namesfile):\n",
    "    fp = open(namesfile, \"r\")\n",
    "    names = fp.read().split(\"\\n\")[:-1]\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 80\n",
    "classes = load_classes(\"data/coco.names\")\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the three surviving bounding boxes above, outputting object types 1, 7, and 16, indicate a bicycle, a truck, and a dog.\n",
    "Let's draw the detections on top of the input image for better visualization.\n",
    "\n",
    "We'll use some code based on Kathuria's `detect.py`. You can download the original as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-06 01:53:13--  https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/detect.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.8.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.8.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7273 (7.1K) [text/plain]\n",
      "Saving to: ‘detect.py’\n",
      "\n",
      "detect.py           100%[===================>]   7.10K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-02-06 01:53:13 (33.2 MB/s) - ‘detect.py’ saved [7273/7273]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch/master/detect.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our version. It will process the images in subdirectory `cocoimages` so let's make it and put our sample there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p cocoimages\n",
    "!cp dog-cycle-car.png cocoimages/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network.....\n",
      "Network successfully loaded\n",
      "dog-cycle-car.png    predicted in  0.433 seconds\n",
      "Objects Detected:    bicycle truck dog\n",
      "----------------------------------------------------------\n",
      "SUMMARY\n",
      "----------------------------------------------------------\n",
      "Task                     : Time Taken (in seconds)\n",
      "\n",
      "Reading addresses        : 0.000\n",
      "Loading batch            : 0.016\n",
      "Detection (1 images)     : 1.737\n",
      "Output Processing        : 0.000\n",
      "Drawing Boxes            : 0.013\n",
      "Average time_per_img     : 1.766\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import time\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import cv2 \n",
    "from util import *\n",
    "import argparse\n",
    "import os \n",
    "import os.path as osp\n",
    "from darknet import Darknet\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "images = \"cocoimages\"\n",
    "batch_size = 4\n",
    "confidence = 0.5\n",
    "nms_thesh = 0.4\n",
    "start = 0\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "num_classes = 80\n",
    "classes = load_classes(\"data/coco.names\")\n",
    "\n",
    "#Set up the neural network\n",
    "\n",
    "print(\"Loading network.....\")\n",
    "model = MyDarknet(\"cfg/yolov3.cfg\")\n",
    "model.load_weights(\"yolov3.weights\")\n",
    "print(\"Network successfully loaded\")\n",
    "\n",
    "model.net_info[\"height\"] = 416\n",
    "inp_dim = int(model.net_info[\"height\"])\n",
    "assert inp_dim % 32 == 0 \n",
    "assert inp_dim > 32\n",
    "\n",
    "#If there's a GPU availible, put the model on GPU\n",
    "\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "\n",
    "model.eval()\n",
    "\n",
    "read_dir = time.time()\n",
    "\n",
    "# Detection phase\n",
    "\n",
    "try:\n",
    "    imlist = [osp.join(osp.realpath('.'), images, img) for img in os.listdir(images)]\n",
    "except NotADirectoryError:\n",
    "    imlist = []\n",
    "    imlist.append(osp.join(osp.realpath('.'), images))\n",
    "except FileNotFoundError:\n",
    "    print (\"No file or directory with the name {}\".format(images))\n",
    "    exit()\n",
    "    \n",
    "if not os.path.exists(\"des\"):\n",
    "    os.makedirs(\"des\")\n",
    "\n",
    "load_batch = time.time()\n",
    "loaded_ims = [cv2.imread(x) for x in imlist]\n",
    "\n",
    "im_batches = list(map(prep_image, loaded_ims, [inp_dim for x in range(len(imlist))]))\n",
    "im_dim_list = [(x.shape[1], x.shape[0]) for x in loaded_ims]\n",
    "im_dim_list = torch.FloatTensor(im_dim_list).repeat(1,2)\n",
    "\n",
    "\n",
    "leftover = 0\n",
    "if (len(im_dim_list) % batch_size):\n",
    "    leftover = 1\n",
    "\n",
    "if batch_size != 1:\n",
    "    num_batches = len(imlist) // batch_size + leftover            \n",
    "    im_batches = [torch.cat((im_batches[i*batch_size : min((i +  1)*batch_size,\n",
    "                        len(im_batches))]))  for i in range(num_batches)]  \n",
    "\n",
    "write = 0\n",
    "\n",
    "if CUDA:\n",
    "    im_dim_list = im_dim_list.cuda()\n",
    "    \n",
    "start_det_loop = time.time()\n",
    "for i, batch in enumerate(im_batches):\n",
    "    # Load the image \n",
    "    start = time.time()\n",
    "    if CUDA:\n",
    "        batch = batch.cuda()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(Variable(batch), CUDA)\n",
    "\n",
    "    prediction = write_results(prediction, confidence, num_classes, nms_conf = nms_thesh)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    if type(prediction) == int:\n",
    "\n",
    "        for im_num, image in enumerate(imlist[i*batch_size: min((i +  1)*batch_size, len(imlist))]):\n",
    "            im_id = i*batch_size + im_num\n",
    "            print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start)/batch_size))\n",
    "            print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \"\"))\n",
    "            print(\"----------------------------------------------------------\")\n",
    "        continue\n",
    "\n",
    "    prediction[:,0] += i*batch_size    #transform the atribute from index in batch to index in imlist \n",
    "\n",
    "    if not write:                      #If we have't initialised output\n",
    "        output = prediction  \n",
    "        write = 1\n",
    "    else:\n",
    "        output = torch.cat((output,prediction))\n",
    "\n",
    "    for im_num, image in enumerate(imlist[i*batch_size: min((i +  1)*batch_size, len(imlist))]):\n",
    "        im_id = i*batch_size + im_num\n",
    "        objs = [classes[int(x[-1])] for x in output if int(x[0]) == im_id]\n",
    "        print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start)/batch_size))\n",
    "        print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \" \".join(objs)))\n",
    "        print(\"----------------------------------------------------------\")\n",
    "\n",
    "    if CUDA:\n",
    "        torch.cuda.synchronize()       \n",
    "try:\n",
    "    output\n",
    "except NameError:\n",
    "    print (\"No detections were made\")\n",
    "    exit()\n",
    "\n",
    "im_dim_list = torch.index_select(im_dim_list, 0, output[:,0].long())\n",
    "\n",
    "scaling_factor = torch.min(416/im_dim_list,1)[0].view(-1,1)\n",
    "\n",
    "output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim_list[:,0].view(-1,1))/2\n",
    "output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim_list[:,1].view(-1,1))/2\n",
    "\n",
    "output[:,1:5] /= scaling_factor\n",
    "\n",
    "for i in range(output.shape[0]):\n",
    "    output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim_list[i,0])\n",
    "    output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim_list[i,1])\n",
    "    \n",
    "output_recast = time.time()\n",
    "class_load = time.time()\n",
    "colors = [[255, 0, 0], [255, 0, 0], [255, 255, 0], [0, 255, 0], [0, 255, 255], [0, 0, 255], [255, 0, 255]]\n",
    "\n",
    "draw = time.time()\n",
    "\n",
    "def write(x, results):\n",
    "    c1 = tuple(x[1:3].int())\n",
    "    c2 = tuple(x[3:5].int())\n",
    "    img = results[int(x[0])]\n",
    "    cls = int(x[-1])\n",
    "    color = random.choice(colors)\n",
    "    label = \"{0}\".format(classes[cls])\n",
    "    cv2.rectangle(img, c1, c2,color, 1)\n",
    "    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n",
    "    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n",
    "    cv2.rectangle(img, c1, c2,color, -1)\n",
    "    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n",
    "    return img\n",
    "\n",
    "\n",
    "list(map(lambda x: write(x, loaded_ims), output))\n",
    "\n",
    "det_names = pd.Series(imlist).apply(lambda x: \"{}/det_{}\".format(\"des\",x.split(\"/\")[-1]))\n",
    "\n",
    "list(map(cv2.imwrite, det_names, loaded_ims))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(\"SUMMARY\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"{:25s}: {}\".format(\"Task\", \"Time Taken (in seconds)\"))\n",
    "print()\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Reading addresses\", load_batch - read_dir))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Loading batch\", start_det_loop - load_batch))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Detection (\" + str(len(imlist)) +  \" images)\", output_recast - start_det_loop))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Output Processing\", class_load - output_recast))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Drawing Boxes\", end - draw))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Average time_per_img\", (end - load_batch)/len(imlist)))\n",
    "print(\"----------------------------------------------------------\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! You got the YOLO result\n",
    "\n",
    "<img src=\"img/dogresult.png\" title=\"weight\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv4 implementation\n",
    "\n",
    "YOLO v4 was developed based on YOLO v3 by a new group of authors, Alexey Bochkovskiy and colleagues, who took\n",
    "over the development of Darknet and YOLO after [Joseph Redmon quit computer vision research](https://twitter.com/pjreddie/status/1230524770350817280?lang=en).\n",
    "\n",
    "Take a look at the [YOLO v4 paper](https://arxiv.org/abs/2004.10934). The authors make many small and some large\n",
    "improvements to YOLOv3 to achieve a higher frame rate and higher accuracy. Source code is available at the\n",
    "[Darknet GitHub repository](https://github.com/AlexeyAB/darknet).\n",
    "\n",
    "### YOLOv4 overall architecture\n",
    "\n",
    "The YOLOv4 architecture looks like this:\n",
    "\n",
    "<img src=\"img/Block-diagram-of-YOLOv4-object-detection-The-small-modules-included-are-CBM.png\" title=\"YOLOV4\" style=\"width: 960px;\" />\n",
    "\n",
    "*Source: https://www.researchgate.net/publication/351652673_A_real-time_deep_learning_forest_fire_monitoring_algorithm_based_on_an_improved_Pruned_KD_model/figures?lo=1*\n",
    "\n",
    "As you can see, there are several modules in YOLOv4. For the backbone:\n",
    "\n",
    "- **CBM**: Convolution $\\rightarrow$ BatchNorm $\\rightarrow$ Mish Activation\n",
    "- **CBL**: Convolution $\\rightarrow$ BatchNorm $\\rightarrow$ Leaky ReLU Activation\n",
    "- **CSPResNet**: Cross-stage partial residual network block. A CSPResNet block separates the input feature map block into two parts. The first part bypasses the computations in the block and becomes part of the input to the next block. The second part is passed throuth a residual block implemented with 2 CBMs.\n",
    "\n",
    "For the neck:\n",
    "\n",
    "- **SPP**: Spatial pyramid pooling layer\n",
    "- **PAN**: Path aggregation network\n",
    "\n",
    "And finally, for the head:\n",
    "\n",
    "- **YOLO**: Same as in YOLO v3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross stage partial network backbone\n",
    "\n",
    "Let's look at CSP in the backbone first. This figure shows the idea of a CSPDenseNet block compared to an ordinary dense block:\n",
    "\n",
    "<img src=\"img/CSPdensenet.png\" title=\"CSP\" style=\"width: 800px;\" />\n",
    "\n",
    "A dense block is just a convolution, batch norm, and activation function.\n",
    "\n",
    "CSPResNet further replaces the dense block with a residual block (two CBMs as stated above).\n",
    "\n",
    "### Spatial pyramid pooling in the neck\n",
    "\n",
    "Next is the spatial pyramid pooling (SPP) layer. Take a look at the [SPP paper](https://arxiv.org/abs/1406.4729).\n",
    "This layer is used for multiscale analysis of a set of feature maps produced by convolutional layers. A SPP produces\n",
    "a fixed-size representation of the input regardless of the input feature map size.\n",
    "\n",
    "Here is an example from the SPP paper:\n",
    "\n",
    "<img src=\"img/SPP.jpeg\" title=\"SPP\" style=\"width: 800px;\" />\n",
    "\n",
    "*Source: https://arxiv.org/pdf/1406.4729.pdf, Figure 3*\n",
    "\n",
    "In the figure, the 256-channel output of a convolutional layer is being pooled at three different resolutions: 4x4, 2x2, and 1x1. The resulting\n",
    "feature maps are concatenated into a 1D vector that could then be analyzed with fully connected layers.\n",
    "\n",
    "In YOLO v4, however, the SPP module is implemented slightly differently,\n",
    "using three different maxpool layers applied to the same input. For example, the first\n",
    "maxpool of the SPP in `yolov4.cfg` looks like this:\n",
    "\n",
    "    [maxpool]\n",
    "    stride=1\n",
    "    size=13\n",
    "\n",
    "In Darknet, this means a 13x13 overlapping maximum operations with padding to obtain the same output size as the input size.\n",
    "The other two maxpool operations are 9x9 and 5x5, each with stride 1, creating three sets of feature maps that are the same size\n",
    "as the input, which are concatenated with the input and passed to the next stage of processing.\n",
    "\n",
    "### Path aggregation network in the neck\n",
    "\n",
    "The last new module in YOLO v4 is the path aggregation network or PANet in the neck. PANet is a typle of feature pyramid\n",
    "network that extracts features from various points in the backbone.\n",
    "Take a look at the [PANet paper](https://arxiv.org/abs/1803.01534). It was originally designed for instance segmentation.\n",
    "From the YOLO v4 diagram above, we see that the PANet has two flows: upward, from the lowest resolution output of the SPP, and downward,\n",
    "from the high resolution block of feature maps (76x76x256) from the middle of the CSPDarknet53 backbone. The low-to-high resolution\n",
    "path on the left upscales the low resolution feature maps and combines them with the higher resolution feature maps, while the high-to-low\n",
    "resolution path on the right uses strided convolution to downscale the high resolution information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mish activation function in the CBM modules\n",
    "\n",
    "Next, let's take a look at the newish activation function used in YOLOv4: Mish.\n",
    "Mish is a SoftPlus activation function that is non-monotonic and designed for\n",
    "neural networks that regularize themselves. It was inspired by the *swish* activation function.\n",
    "It has a range from -0.31 to $\\infty$, due to the SoftPlus function:\n",
    "\n",
    "$$\\mathrm{SoftPlus}(x)=\\ln(1+e^x) \\\\\n",
    "f(x)=x \\tanh(\\mathrm{SoftPlus}(x))=x \\tanh(\\ln(1+e^x)) $$.\n",
    "\n",
    "Here's a graph:\n",
    "\n",
    "<img src=\"img/mish_activation_function_graph.png\" title=\"weight\" style=\"width: 480px;\" />\n",
    "\n",
    "Compared to other activation functions, you can see that Mish is closest to Swish:\n",
    "\n",
    "<img src=\"img/mish_activation_function_compare_with_other_activation_functions.png\" title=\"weight\" style=\"width: 480px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mish activation function code\n",
    "\n",
    "Create a file `mish.py` and add your `Mish` class as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import tanh\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class Mish(nn.Module):\n",
    "     def __init__(self):\n",
    "         super().__init__()\n",
    "\n",
    "     def forward(self, x):\n",
    "         return x * tanh(F.softplus(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch DarkNet code modifications\n",
    "\n",
    "Because YOLOv4 extends YOLOv3 in several ways, we need to add or modify some modules in `darknet.py`,\n",
    "in both the `create_module()` and `forward()` methods:\n",
    "\n",
    "- Add the overlapping *max pooling* operation used in the SPP block\n",
    "- Add multipath feature map concatenation to the *route layer*\n",
    "- Add the *Mish* activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_module()\n",
    "# max pooling layer\n",
    "elif x[\"type\"] == \"maxpool\":\n",
    "    stride = int(x[\"stride\"])\n",
    "    size = int(x[\"size\"])\n",
    "    max_pool = nn.MaxPool2d(size, stride, padding=size//2)\n",
    "    module.add_module(\"maxpool_{}\".format(index), max_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward()\n",
    "if module_type in [\"convolutional\", \"upsample\", \"maxpool\"]:\n",
    "    x = self.modul_list[i](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Route layer\n",
    "\n",
    "We need to create it in create module too because the number of output filter has been changed at the route module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_module()\n",
    "# route layer\n",
    "elif x[\"type\"] == \"route\":\n",
    "    x[\"layers\"] = x[\"layers\"].splite(',')\n",
    "    filters = 0\n",
    "    \n",
    "    for i in range(len(x[\"layers\"])):\n",
    "        pointer = int(x[\"layers\"][i])\n",
    "        if pointer > 0:\n",
    "            filters += output_filters[pointer]\n",
    "        else:\n",
    "            filters += output_filters[index + pointer]\n",
    "            \n",
    "    route = EmptyLayer()\n",
    "    module.add_module(\"route_{0}\".format(index), route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward()\n",
    "elif module_type == \"route\":\n",
    "    layers = [int(a) for a in layers]\n",
    "    maps[]\n",
    "    for l in range(0, len(layers)):\n",
    "        if layers[l] > 0:\n",
    "            layers[l] = layers[l] - i\n",
    "        maps.append(outputs[i + layers[l]])\n",
    "    x = torch.cat((maps), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mish activation layer\n",
    "\n",
    "as from above, we introduced mish activation, now add the module in the <code>create_module()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elif activation == \"mish\":\n",
    "    activn = Mish()\n",
    "    module.add_module(\"mish_{0}\".format(index), activn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full create_module() function\n",
    "\n",
    "And now, here is full code for the `create_module()` function that should\n",
    "work for the YOLOv4 configuration file. We've noted the modifications with triple single quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not for get to add Mish activation at the top of darknet.py\n",
    "from mish import Mish\n",
    "\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# And now create_module() function\n",
    "def create_modules(blocks):\n",
    "    net_info = blocks[0]     #Captures the information about the input and pre-processing    \n",
    "    module_list = nn.ModuleList()\n",
    "    prev_filters = 3\n",
    "    output_filters = []\n",
    "    \n",
    "    for index, x in enumerate(blocks[1:]):\n",
    "        module = nn.Sequential()\n",
    "    \n",
    "        #check the type of block\n",
    "        #create a new module for the block\n",
    "        #append to module_list\n",
    "        \n",
    "        #If it's a convolutional layer\n",
    "        if (x[\"type\"] == \"convolutional\"):\n",
    "            #Get the info about the layer\n",
    "            activation = x[\"activation\"]\n",
    "            try:\n",
    "                batch_normalize = int(x[\"batch_normalize\"])\n",
    "                bias = False\n",
    "            except:\n",
    "                batch_normalize = 0\n",
    "                bias = True\n",
    "        \n",
    "            filters= int(x[\"filters\"])\n",
    "            padding = int(x[\"pad\"])\n",
    "            kernel_size = int(x[\"size\"])\n",
    "            stride = int(x[\"stride\"])\n",
    "        \n",
    "            if padding:\n",
    "                pad = (kernel_size - 1) // 2\n",
    "            else:\n",
    "                pad = 0\n",
    "        \n",
    "            #Add the convolutional layer\n",
    "            conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias = bias)\n",
    "            module.add_module(\"conv_{0}\".format(index), conv)\n",
    "        \n",
    "            #Add the Batch Norm Layer\n",
    "            if batch_normalize:\n",
    "                bn = nn.BatchNorm2d(filters)\n",
    "                module.add_module(\"batch_norm_{0}\".format(index), bn)\n",
    "        \n",
    "            #Check the activation. \n",
    "            #It is either Linear or a Leaky ReLU for YOLO\n",
    "            if activation == \"leaky\":\n",
    "                activn = nn.LeakyReLU(0.1, inplace = True)\n",
    "                module.add_module(\"leaky_{0}\".format(index), activn)\n",
    "                \n",
    "            ''' Mish activation modification here '''\n",
    "            elif activation == \"mish\":\n",
    "                activn = Mish()\n",
    "                module.add_module(\"mish_{0}\".format(index), activn)\n",
    "\n",
    "        \n",
    "        #If it's an upsampling layer\n",
    "        #We use Bilinear2dUpsampling\n",
    "        elif (x[\"type\"] == \"upsample\"):\n",
    "            stride = int(x[\"stride\"])\n",
    "            upsample = nn.Upsample(scale_factor = 2, mode = \"nearest\")\n",
    "            module.add_module(\"upsample_{}\".format(index), upsample)\n",
    "            \n",
    "        ''' route layermodification here '''\n",
    "        elif (x[\"type\"] == \"route\"):\n",
    "            x[\"layers\"] = x[\"layers\"].split(',')\n",
    "            filters = 0\n",
    "\n",
    "            for i in range(len(x[\"layers\"])):\n",
    "                pointer = int(x[\"layers\"][i])\n",
    "                if  pointer > 0:\n",
    "                    filters += output_filters[pointer]\n",
    "                else:\n",
    "                    filters += output_filters[index + pointer]\n",
    "\n",
    "            route = EmptyLayer()\n",
    "            module.add_module(\"route_{0}\".format(index), route)\n",
    "    \n",
    "        #shortcut corresponds to skip connection\n",
    "        elif x[\"type\"] == \"shortcut\":\n",
    "            shortcut = EmptyLayer()\n",
    "            module.add_module(\"shortcut_{}\".format(index), shortcut)\n",
    "            \n",
    "        #Yolo is the detection layer\n",
    "        elif x[\"type\"] == \"yolo\":\n",
    "            mask = x[\"mask\"].split(\",\")\n",
    "            mask = [int(x) for x in mask]\n",
    "    \n",
    "            anchors = x[\"anchors\"].split(\",\")\n",
    "            anchors = [int(a) for a in anchors]\n",
    "            anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]\n",
    "            anchors = [anchors[i] for i in mask]\n",
    "    \n",
    "            detection = DetectionLayer(anchors)\n",
    "            module.add_module(\"Detection_{}\".format(index), detection)\n",
    "        \n",
    "        ''' Max pooling layer modification here '''\n",
    "        elif x[\"type\"] == \"maxpool\":\n",
    "            stride = int(x[\"stride\"])\n",
    "            size = int(x[\"size\"])\n",
    "            max_pool = nn.MaxPool2d(size, stride, padding=size // 2)\n",
    "            module.add_module(\"maxpool_{}\".format(index), max_pool)\n",
    "                              \n",
    "        module_list.append(module)\n",
    "        prev_filters = filters\n",
    "        output_filters.append(filters)\n",
    "        \n",
    "    return (net_info, module_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full darknet class function\n",
    "\n",
    "Here is the correspondingly modified `MyDarkNet` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDarknet(nn.Module):\n",
    "    def __init__(self, cfgfile):\n",
    "        super(Darknet, self).__init__()\n",
    "        self.blocks = parse_cfg(cfgfile)\n",
    "        self.net_info, self.module_list = create_modules(self.blocks)\n",
    "        \n",
    "    def forward(self, x, CUDA):\n",
    "        modules = self.blocks[1:]\n",
    "        outputs = {}   #We cache the outputs for the route layer\n",
    "        \n",
    "        write = 0\n",
    "        for i, module in enumerate(modules):        \n",
    "            module_type = (module[\"type\"])\n",
    "            \n",
    "            ''' max pooling '''\n",
    "            if module_type in [\"convolutional\", \"upsample\", \"maxpool\"]:\n",
    "                x = self.module_list[i](x)\n",
    "            \n",
    "            ''' route layer '''\n",
    "            elif module_type == \"route\":\n",
    "                layers = module[\"layers\"]\n",
    "                layers = [int(a) for a in layers]\n",
    "                maps = []\n",
    "                for l in range(0, len(layers)):\n",
    "                    if layers[l] > 0:\n",
    "                        layers[l] = layers[l] - i\n",
    "                    maps.append(outputs[i + layers[l]])\n",
    "                x = torch.cat((maps), 1)\n",
    "                \n",
    "    \n",
    "            elif  module_type == \"shortcut\":\n",
    "                from_ = int(module[\"from\"])\n",
    "                x = outputs[i-1] + outputs[i+from_]\n",
    "    \n",
    "            elif module_type == 'yolo':        \n",
    "                anchors = self.module_list[i][0].anchors\n",
    "                #Get the input dimensions\n",
    "                inp_dim = int (self.net_info[\"height\"])\n",
    "        \n",
    "                #Get the number of classes\n",
    "                num_classes = int (module[\"classes\"])\n",
    "        \n",
    "                #Transform \n",
    "                x = predict_transform(x, inp_dim, anchors, num_classes, CUDA)\n",
    "                if not write:              #if no collector has been intialised. \n",
    "                    detections = x\n",
    "                    write = 1\n",
    "        \n",
    "                else:       \n",
    "                    detections = torch.cat((detections, x), 1)\n",
    "        \n",
    "            outputs[i] = x\n",
    "        \n",
    "        return detections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifications to inference code\n",
    "\n",
    "Here is code for the new inference test. Put it in `run_yolov4.py`. We have the following modifications:\n",
    "1. Resize the input image to $608\\times 608$ using the function `letterbox_image`.\n",
    "2. Change the image format from BGR to RGB\n",
    "3. Change the normalization factor for bounding boxes from 416 to 608\n",
    "4. Transform the output back to BGR in order to save to a file using OpenCV\n",
    "\n",
    "Here is sample code for `run_yolov4.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import time\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import cv2 \n",
    "from util import *\n",
    "import argparse\n",
    "import os \n",
    "import os.path as osp\n",
    "from darknet import Darknet\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "images = \"cocoimages\"\n",
    "batch_size = 1\n",
    "confidence = 0.5\n",
    "nms_thesh = 0.4\n",
    "start = 0\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "num_classes = 80\n",
    "classes = load_classes(\"data/coco.names\")\n",
    "\n",
    "#Set up the neural network\n",
    "\n",
    "print(\"Loading network.....\")\n",
    "model = Darknet(\"cfg/yolov4.cfg\")\n",
    "model.load_weights(\"yolov4.weights\")\n",
    "print(\"Network successfully loaded\")\n",
    "\n",
    "model.net_info[\"height\"] = 608\n",
    "model.net_info[\"width\"] = 608\n",
    "inp_dim = int(model.net_info[\"height\"])\n",
    "assert inp_dim % 32 == 0 \n",
    "assert inp_dim > 32\n",
    "\n",
    "#If there's a GPU availible, put the model on GPU\n",
    "\n",
    "if CUDA:\n",
    "    model.cuda()\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "\n",
    "model.eval()\n",
    "\n",
    "read_dir = time.time()\n",
    "\n",
    "# Detection phase\n",
    "\n",
    "try:\n",
    "    imlist = [osp.join(osp.realpath('.'), images, img) for img in os.listdir(images)]\n",
    "except NotADirectoryError:\n",
    "    imlist = []\n",
    "    imlist.append(osp.join(osp.realpath('.'), images))\n",
    "except FileNotFoundError:\n",
    "    print (\"No file or directory with the name {}\".format(images))\n",
    "    exit()\n",
    "    \n",
    "if not os.path.exists(\"des\"):\n",
    "    os.makedirs(\"des\")\n",
    "\n",
    "load_batch = time.time()\n",
    "# loaded_ims = [letterbox_image(cv2.imread(x), (inp_dim, inp_dim)) for x in imlist]\n",
    "\n",
    "img = cv2.imread(imlist[0])\n",
    "\n",
    "print(type(img), img.shape)\n",
    "img = letterbox_image(img, (inp_dim, inp_dim))\n",
    "cv2.imwrite('test.jpg', img)\n",
    "img = cv2.imread('test.jpg')\n",
    "print(img.shape)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# img = torch.from_numpy(img).float().div(255.0).unsqueeze(0)\n",
    "# print(img.shape)\n",
    "# img = torch.from_numpy(img.transpose(2, 0, 1)).float().div(255.0).unsqueeze(0)\n",
    "print(img.shape)\n",
    "\n",
    "loaded_ims = [img]\n",
    "\n",
    "\n",
    "im_batches = list(map(prep_image, loaded_ims, [inp_dim for x in range(len(imlist))]))\n",
    "\n",
    "\n",
    "im_dim_list = [(x.shape[1], x.shape[0]) for x in loaded_ims]\n",
    "im_dim_list = torch.FloatTensor(im_dim_list).repeat(1,2)\n",
    "print(im_dim_list)\n",
    "\n",
    "leftover = 0\n",
    "if (len(im_dim_list) % batch_size):\n",
    "    leftover = 1\n",
    "\n",
    "if batch_size != 1:\n",
    "    num_batches = len(imlist) // batch_size + leftover            \n",
    "    im_batches = [torch.cat((im_batches[i*batch_size : min((i +  1)*batch_size,\n",
    "                        len(im_batches))]))  for i in range(num_batches)]  \n",
    "\n",
    "write = 0\n",
    "\n",
    "if CUDA:\n",
    "    im_dim_list = im_dim_list.cuda()\n",
    "    \n",
    "start_det_loop = time.time()\n",
    "for i, batch in enumerate(im_batches):\n",
    "    # Load the image \n",
    "    start = time.time()\n",
    "    if CUDA:\n",
    "        batch = batch.cuda()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(Variable(batch), CUDA)\n",
    "\n",
    "    prediction = write_results(prediction, confidence, num_classes, nms_conf = nms_thesh)\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    if type(prediction) == int:\n",
    "\n",
    "        for im_num, image in enumerate(imlist[i*batch_size: min((i +  1)*batch_size, len(imlist))]):\n",
    "            im_id = i*batch_size + im_num\n",
    "            print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start)/batch_size))\n",
    "            print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \"\"))\n",
    "            print(\"----------------------------------------------------------\")\n",
    "        continue\n",
    "\n",
    "    prediction[:,0] += i*batch_size    #transform the atribute from index in batch to index in imlist \n",
    "\n",
    "    if not write:                      #If we have't initialised output\n",
    "        output = prediction  \n",
    "        write = 1\n",
    "    else:\n",
    "        output = torch.cat((output,prediction))\n",
    "\n",
    "    for im_num, image in enumerate(imlist[i*batch_size: min((i +  1)*batch_size, len(imlist))]):\n",
    "        im_id = i*batch_size + im_num\n",
    "        objs = [classes[int(x[-1])] for x in output if int(x[0]) == im_id]\n",
    "        print(\"{0:20s} predicted in {1:6.3f} seconds\".format(image.split(\"/\")[-1], (end - start)/batch_size))\n",
    "        print(\"{0:20s} {1:s}\".format(\"Objects Detected:\", \" \".join(objs)))\n",
    "        print(\"----------------------------------------------------------\")\n",
    "\n",
    "    if CUDA:\n",
    "        torch.cuda.synchronize()       \n",
    "try:\n",
    "    output\n",
    "except NameError:\n",
    "    print (\"No detections were made\")\n",
    "    exit()\n",
    "\n",
    "im_dim_list = torch.index_select(im_dim_list, 0, output[:,0].long())\n",
    "\n",
    "scaling_factor = torch.min(model.net_info[\"height\"]/im_dim_list,1)[0].view(-1,1)\n",
    "\n",
    "output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim_list[:,0].view(-1,1))/2\n",
    "output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim_list[:,1].view(-1,1))/2\n",
    "\n",
    "output[:,1:5] /= scaling_factor\n",
    "\n",
    "for i in range(output.shape[0]):\n",
    "    output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim_list[i,0])\n",
    "    output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim_list[i,1])\n",
    "    \n",
    "output_recast = time.time()\n",
    "class_load = time.time()\n",
    "colors = [[255, 0, 0], [255, 0, 0], [255, 255, 0], [0, 255, 0], [0, 255, 255], [0, 0, 255], [255, 0, 255]]\n",
    "\n",
    "draw = time.time()\n",
    "\n",
    "def write(x, results):\n",
    "    c1 = tuple(x[1:3].int())\n",
    "    c2 = tuple(x[3:5].int())\n",
    "    img = results[int(x[0])]\n",
    "    print(img.shape)\n",
    "    print(type(img))\n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # cv2.imshow('555',img)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()\n",
    "    cls = int(x[-1])\n",
    "    color = random.choice(colors)\n",
    "    label = \"{0}\".format(classes[cls])\n",
    "    cv2.rectangle(img, c1, c2,color, 1)\n",
    "    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n",
    "    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n",
    "    cv2.rectangle(img, c1, c2,color, -1)\n",
    "    \n",
    "    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n",
    "    return img\n",
    "\n",
    "\n",
    "list(map(lambda x: write(x, loaded_ims), output))\n",
    "\n",
    "det_names = pd.Series(imlist).apply(lambda x: \"{}/det_{}\".format(\"des\",x.split(\"/\")[-1]))\n",
    "\n",
    "list(map(cv2.imwrite, det_names, [cv2.cvtColor(loaded_ims[0], cv2.COLOR_BGR2RGB)]))\n",
    "end = time.time()\n",
    "\n",
    "print(\"SUMMARY\")\n",
    "print(\"----------------------------------------------------------\")\n",
    "print(\"{:25s}: {}\".format(\"Task\", \"Time Taken (in seconds)\"))\n",
    "print()\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Reading addresses\", load_batch - read_dir))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Loading batch\", start_det_loop - load_batch))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Detection (\" + str(len(imlist)) +  \" images)\", output_recast - start_det_loop))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Output Processing\", class_load - output_recast))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Drawing Boxes\", end - draw))\n",
    "print(\"{:25s}: {:2.3f}\".format(\"Average time_per_img\", (end - load_batch)/len(imlist)))\n",
    "print(\"----------------------------------------------------------\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the weights for the model `yolov4.weight` from [this link](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwiwuaTq0NP1AhVNILcAHT0JAHAQFnoECAgQAQ&url=https%3A%2F%2Fgithub.com%2FAlexeyAB%2Fdarknet%2Freleases%2Fdownload%2Fdarknet_yolo_v3_optimal%2Fyolov4.weights&usg=AOvVaw30if4joxtTaS8DAh12vYQ4).\n",
    "\n",
    "You'll also find the yolov4.cfg file in the [YOLOv4 GitHub repository](https://github.com/AlexeyAB/darknet) (in folder cfg)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv4 training\n",
    "\n",
    "After adding the missing components to our Darknet configuration transformer for YOLOv3 to support YOLOv4,\n",
    "let's work on training. However, inference and training part are quite different from each other. We need\n",
    "to add augmentation of images and labels, and also compute the new loss functions.\n",
    "\n",
    "The process of training requires these steps:\n",
    "\n",
    "1. Load the pretrained weights for ImageNet\n",
    "2. Implement training data augmentation\n",
    "3. Convert ground truth annotations to targets for the three YOLO heads\n",
    "4. Implement the appropriate loss functions (CIOU loss for bounding boxes, weighted binary cross entropy for confidence scores, and binary cross entropy for class scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll help you get started. Use the albumentation package for augmentation and the pycocotools package for\n",
    "dealing with the COCO dataset.\n",
    "Create a file named `train_yolov4.py` and the code below.\n",
    "\n",
    "#### Load pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading network.....\")\n",
    "model = Darknet(\"cfg/yolov4.cfg\")\n",
    "model.load_weights(\"csdarknet53-omega_final.weights\", backbone_only=True)\n",
    "print(\"Network successfully loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to modify your `load_weights` method to load only the backbone\n",
    "layer weights from the CSP Darknet 53 weight file.\n",
    "\n",
    "#### Image augmentation\n",
    "\n",
    "Use the \"Albumentation\" package for augmentations. Augmentataion is easy when we're doing\n",
    "classification, because the target doesn't change. However, object detection augmentation,\n",
    "requires transforming both the input image and the output labels according to the type of\n",
    "augmentation (for example, a horizontal flip requires a translation of all bounding boxes).\n",
    "Albumentation is a library that does this for you easily. Take a look at the\n",
    "[Albumentation documentation](https://albumentations.ai/docs/) for more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "...\n",
    "\n",
    "# Training dataset transformation\n",
    "train_transform = A.Compose([\n",
    "    A.SmallestMaxSize(256),\n",
    "    A.RandomCrop(width=224, height=224),\n",
    "    # A.HorizontalFlip(p=0.5),\n",
    "    # A.RandomBrightnessContrast(p=0.2),\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']),\n",
    ")\n",
    "\n",
    "# Validation dataset transformation\n",
    "eval_transform = A.Compose([\n",
    "    A.SmallestMaxSize(256),\n",
    "    A.CenterCrop(width=224, height=224),\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create CustomCoco dataset class\n",
    "\n",
    "Create a new class ``CustomCoco`` to get transformed bounding boxes with their labels.\n",
    "The bounding box transformation is in the ``__getitem__()`` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if self.transform is not None:\n",
    "    bboxes = list(obj['bbox'] for obj in target)\n",
    "    category_ids = list(obj['category_id'] for obj in target)\n",
    "    transformed = self.transform(image=img, bboxes=bboxes, category_ids=category_ids)\n",
    "    img = transformed['image'],\n",
    "    bboxes = torch.Tensor(transformed['bboxes'])\n",
    "    cat_ids = torch.Tensor(transformed['category_ids'])\n",
    "    labels, bboxes = self.__create_label(bboxes, cat_ids.type(torch.IntTensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Ground truth formatting\n",
    "\n",
    "Creating ground truth target tensors is a bit tricky.\n",
    "Refer to [argusswift's yolov4 code](https://github.com/argusswift/YOLOv4-pytorch).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = np.array(bboxes)\n",
    "class_inds = np.array(class_inds)\n",
    "anchors = ANCHORS # all the anchors\n",
    "strides = np.array(STRIDES) # list of strides\n",
    "train_output_size = IP_SIZE / strides # image with different scales\n",
    "anchors_per_scale = NUM_ANCHORS # anchor per scale\n",
    "\n",
    "# print(train_output_size)\n",
    "\n",
    "label = [\n",
    "    np.zeros(\n",
    "        (\n",
    "            int(train_output_size[i]),\n",
    "            int(train_output_size[i]),\n",
    "            anchors_per_scale,\n",
    "            5 + NUM_CLASSES,\n",
    "        )\n",
    "    )\n",
    "    for i in range(3)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also align the label and bounding box ground truth for each scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_size_s = int(train_output_size[2]) * int(train_output_size[2]) * anchors_per_scale\n",
    "flatten_size_m = int(train_output_size[1]) * int(train_output_size[1]) * anchors_per_scale\n",
    "flatten_size_l = int(train_output_size[0]) * int(train_output_size[0]) * anchors_per_scale\n",
    "\n",
    "label_s = torch.Tensor(label[2]).view(1, flatten_size_s, 5 + NUM_CLASSES).squeeze(0)\n",
    "label_m = torch.Tensor(label[1]).view(1, flatten_size_m, 5 + NUM_CLASSES).squeeze(0)\n",
    "label_l = torch.Tensor(label[0]).view(1, flatten_size_l, 5 + NUM_CLASSES).squeeze(0)\n",
    "\n",
    "bboxes_s = torch.Tensor(bboxes_xywh[2])\n",
    "        bboxes_m = torch.Tensor(bboxes_xywh[1])\n",
    "bboxes_l = torch.Tensor(bboxes_xywh[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the three sets of bounding box tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbboxes, mbboxes, lbboxes = bboxes_xywh\n",
    "\n",
    "labels = torch.cat([label_l, label_m, label_s], 0)\n",
    "bboxes = torch.cat([bboxes_l, bboxes_m, bboxes_s], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is full code you can use for your ``CustomCoco`` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../utils\")\n",
    "import torch\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from torchvision.datasets import CocoDetection\n",
    "# from . import data_augment as dataAug\n",
    "# from . import tools\n",
    "\n",
    "from typing import Any, Callable, Optional, Tuple\n",
    "import json\n",
    "\n",
    "ANCHORS = [\n",
    "            [[12, 16], [19, 36], [40, 28]],\n",
    "            [[36, 75], [76, 55], [72, 146]],\n",
    "            [[142, 110], [192, 243], [459, 401]]\n",
    "]\n",
    "\n",
    "STRIDES = [8, 16, 32]\n",
    "\n",
    "IP_SIZE = 224\n",
    "NUM_ANCHORS = 3\n",
    "NUM_CLASSES = 80\n",
    "\n",
    "with open('coco_cats.json') as js:\n",
    "    data = json.load(js)[\"categories\"]\n",
    "\n",
    "cats_dict = {}\n",
    "for i in range(0, 80):\n",
    "    cats_dict[str(data[i]['id'])] = i\n",
    "\n",
    "\n",
    "\n",
    "class CustomCoco(CocoDetection):\n",
    "    def __init__(\n",
    "            self,\n",
    "            root: str,\n",
    "            annFile: str,\n",
    "            transform: Optional[Callable] = None,\n",
    "            target_transform: Optional[Callable] = None,\n",
    "            transforms: Optional[Callable] = None,\n",
    "    ) -> None:\n",
    "\n",
    "        super(CocoDetection, self).__init__(root, transforms, transform, target_transform)\n",
    "        from pycocotools.coco import COCO\n",
    "        self.coco = COCO(annFile)\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tuple (image, target). target is the object returned by ``coco.loadAnns``.\n",
    "        \"\"\"\n",
    "        coco = self.coco\n",
    "        img_id = self.ids[index]\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        target = coco.loadAnns(ann_ids)\n",
    "        # self.target = target\n",
    "\n",
    "        path = coco.loadImgs(img_id)[0]['file_name']\n",
    "\n",
    "        img = Image.open(os.path.join(self.root, path)).convert('RGB')\n",
    "        img = np.array(img)\n",
    "\n",
    "        category_ids = list(obj['category_id'] for obj in target)\n",
    "        bboxes = list(obj['bbox'] for obj in target)\n",
    "  \n",
    "        if self.transform is not None:\n",
    "            bboxes = list(obj['bbox'] for obj in target)\n",
    "            category_ids = list(obj['category_id'] for obj in target)\n",
    "            transformed = self.transform(image=img, bboxes=bboxes, category_ids=category_ids)\n",
    "            img = transformed['image'],\n",
    "            bboxes = torch.Tensor(transformed['bboxes'])\n",
    "            cat_ids = torch.Tensor(transformed['category_ids'])\n",
    "            labels, bboxes = self.__create_label(bboxes, cat_ids.type(torch.IntTensor))\n",
    "\n",
    "        return img, labels, bboxes\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __create_label(self, bboxes, class_inds):\n",
    "        \"\"\"\n",
    "        Label assignment. For a single picture all GT box bboxes are assigned anchor.\n",
    "        1、Select a bbox in order, convert its coordinates(\"xyxy\") to \"xywh\"; and scale bbox'\n",
    "           xywh by the strides.\n",
    "        2、Calculate the iou between the each detection layer'anchors and the bbox in turn, and select the largest\n",
    "            anchor to predict the bbox.If the ious of all detection layers are smaller than 0.3, select the largest\n",
    "            of all detection layers' anchors to predict the bbox.\n",
    "        Note :\n",
    "        1、The same GT may be assigned to multiple anchors. And the anchors may be on the same or different layer.\n",
    "        2、The total number of bboxes may be more than it is, because the same GT may be assigned to multiple layers\n",
    "        of detection.\n",
    "        \"\"\"\n",
    "        # print(\"Class indices: \", class_inds)\n",
    "        bboxes = np.array(bboxes)\n",
    "        class_inds = np.array(class_inds)\n",
    "        anchors = ANCHORS # all the anchors\n",
    "        strides = np.array(STRIDES) # list of strides\n",
    "        train_output_size = IP_SIZE / strides # image with different scales\n",
    "        anchors_per_scale = NUM_ANCHORS # anchor per scale\n",
    "\n",
    "        # print(train_output_size)\n",
    "\n",
    "        label = [\n",
    "            np.zeros(\n",
    "                (\n",
    "                    int(train_output_size[i]),\n",
    "                    int(train_output_size[i]),\n",
    "                    anchors_per_scale,\n",
    "                    5 + NUM_CLASSES,\n",
    "                )\n",
    "            )\n",
    "            for i in range(3)\n",
    "        ]\n",
    "        # for i in range(3):\n",
    "            # label[i][..., 5] = 1.0\n",
    "\n",
    "        # 150 bounding box ground truths per scale\n",
    "        bboxes_xywh = [\n",
    "            np.zeros((150, 4)) for _ in range(3)\n",
    "        ]  # Darknet the max_num is 30\n",
    "        bbox_count = np.zeros((3,))\n",
    "\n",
    "        for i in range(len(bboxes)):\n",
    "            bbox_coor = bboxes[i][:4]\n",
    "            bbox_class_ind = cats_dict[str(class_inds[i])]\n",
    "            # bbox_mix = bboxes[i][5]\n",
    "\n",
    "            # onehot\n",
    "            one_hot = np.zeros(NUM_CLASSES, dtype=np.float32)\n",
    "            one_hot[bbox_class_ind] = 1.0\n",
    "            # one_hot_smooth = dataAug.LabelSmooth()(one_hot, self.num_classes)\n",
    "\n",
    "            # convert \"xyxy\" to \"xywh\"\n",
    "            bbox_xywh = np.concatenate(\n",
    "                [\n",
    "                    (0.5 * bbox_coor[2:] + bbox_coor[:2]) ,\n",
    "                    bbox_coor[2:],\n",
    "                ],\n",
    "                axis=-1,\n",
    "            )\n",
    "            # print(\"bbox_xywh: \", bbox_xywh)\n",
    "            \n",
    "            bbox_xywh_scaled = (\n",
    "                1.0 * bbox_xywh[np.newaxis, :] / strides[:, np.newaxis]\n",
    "            )\n",
    "\n",
    "            # print(\"bbox_xywhscaled: \", bbox_xywh_scaled)\n",
    "\n",
    "            iou = []\n",
    "            exist_positive = False\n",
    "            for i in range(3):\n",
    "                anchors_xywh = np.zeros((anchors_per_scale, 4))\n",
    "                anchors_xywh[:, 0:2] = (\n",
    "                    np.floor(bbox_xywh_scaled[i, 0:2]).astype(np.int32) + 0.5\n",
    "                )  # 0.5 for compensation\n",
    "\n",
    "                # assign all anchors \n",
    "                anchors_xywh[:, 2:4] = anchors[i]\n",
    "\n",
    "                iou_scale = iou_xywh_numpy(\n",
    "                    bbox_xywh_scaled[i][np.newaxis, :], anchors_xywh\n",
    "                )\n",
    "                iou.append(iou_scale)\n",
    "                iou_mask = iou_scale > 0.3\n",
    "\n",
    "                if np.any(iou_mask):\n",
    "                    xind, yind = np.floor(bbox_xywh_scaled[i, 0:2]).astype(\n",
    "                        np.int32\n",
    "                    )\n",
    "\n",
    "                    # Bug : 当多个bbox对应同一个anchor时，默认将该anchor分配给最后一个bbox\n",
    "                    label[i][yind, xind, iou_mask, 0:4] = bbox_xywh * strides[i]\n",
    "                    label[i][yind, xind, iou_mask, 4:5] = 1.0\n",
    "                    label[i][yind, xind, iou_mask, 5:] = one_hot\n",
    "\n",
    "                    bbox_ind = int(bbox_count[i] % 150)  # BUG : 150为一个先验值,内存消耗大\n",
    "                    bboxes_xywh[i][bbox_ind, :4] = bbox_xywh * strides[i]\n",
    "                    bbox_count[i] += 1\n",
    "\n",
    "                    exist_positive = True\n",
    "\n",
    "            if not exist_positive:\n",
    "                # check if a ground truth bb have the best anchor with any scale\n",
    "                best_anchor_ind = np.argmax(np.array(iou).reshape(-1), axis=-1)\n",
    "                best_detect = int(best_anchor_ind / anchors_per_scale)\n",
    "                best_anchor = int(best_anchor_ind % anchors_per_scale)\n",
    "\n",
    "                xind, yind = np.floor(\n",
    "                    bbox_xywh_scaled[best_detect, 0:2]\n",
    "                ).astype(np.int32)\n",
    "\n",
    "                label[best_detect][yind, xind, best_anchor, 0:4] = bbox_xywh * strides[best_detect]\n",
    "                label[best_detect][yind, xind, best_anchor, 4:5] = 1.0\n",
    "                # label[best_detect][yind, xind, best_anchor, 5:6] = bbox_mix\n",
    "                label[best_detect][yind, xind, best_anchor, 5:] = one_hot \n",
    "\n",
    "                bbox_ind = int(bbox_count[best_detect] % 150)\n",
    "                bboxes_xywh[best_detect][bbox_ind, :4] = bbox_xywh * strides[best_detect]\n",
    "                bbox_count[best_detect] += 1\n",
    "\n",
    "        flatten_size_s = int(train_output_size[2]) * int(train_output_size[2]) * anchors_per_scale\n",
    "        flatten_size_m = int(train_output_size[1]) * int(train_output_size[1]) * anchors_per_scale\n",
    "        flatten_size_l = int(train_output_size[0]) * int(train_output_size[0]) * anchors_per_scale\n",
    "\n",
    "        label_s = torch.Tensor(label[2]).view(1, flatten_size_s, 5 + NUM_CLASSES).squeeze(0)\n",
    "        label_m = torch.Tensor(label[1]).view(1, flatten_size_m, 5 + NUM_CLASSES).squeeze(0)\n",
    "        label_l = torch.Tensor(label[0]).view(1, flatten_size_l, 5 + NUM_CLASSES).squeeze(0)\n",
    "\n",
    "        bboxes_s = torch.Tensor(bboxes_xywh[2])\n",
    "        bboxes_m = torch.Tensor(bboxes_xywh[1])\n",
    "        bboxes_l = torch.Tensor(bboxes_xywh[0])\n",
    "        # label_sbbox, label_mbbox, label_lbbox = label\n",
    "        sbboxes, mbboxes, lbboxes = bboxes_xywh\n",
    "        # print(\"label\")\n",
    "        labels = torch.cat([label_l, label_m, label_s], 0)\n",
    "        bboxes = torch.cat([bboxes_l, bboxes_m, bboxes_s], 0)\n",
    "        return labels, bboxes\n",
    "\n",
    "    def __create_label_old(self, bboxes, class_inds):\n",
    "        # anno = annotation.strip().split(\" \")\n",
    "        label = np.zeros(\n",
    "                (\n",
    "                    IP_SIZE,\n",
    "                    IP_SIZE,\n",
    "                    NUM_ANCHORS,\n",
    "                    5 + NUM_CLASSES,\n",
    "                )\n",
    "                )\n",
    "\n",
    "        # x y w h obj cls\n",
    "        # label[..., 5] = 1.0 # objness = 1\n",
    "\n",
    "        bboxes_xywh = np.zeros((450, 4))\n",
    "        bbox_count = 0\n",
    "\n",
    "\n",
    "        for i in range(len(bboxes)):\n",
    "            bbox_coor = bboxes[i][:4]\n",
    "            # print(bbox_coor)\n",
    "\n",
    "            # onehot\n",
    "            one_hot = np.zeros(NUM_CLASSES, dtype=np.float32)\n",
    "            one_hot[class_inds[i]] = 1.0\n",
    "            bbox_xywh  = np.concatenate(\n",
    "                [\n",
    "                    (0.5 * np.array(bbox_coor[2:]) + np.array(bbox_coor[:2])),\n",
    "                    np.array(bbox_coor[2:]),\n",
    "                ],\n",
    "                axis=-1,\n",
    "            )\n",
    "\n",
    "            # print(bbox_xywh)\n",
    "\n",
    "            iou = []\n",
    "            anchors_xywh = np.zeros((NUM_ANCHORS, 4))\n",
    "            anchors_xywh[:, 0:2] = (\n",
    "                np.floor(bbox_xywh[ 0:2]).astype(np.int32) + 0.5\n",
    "            )  # 0.5 for compensation\n",
    "            anchors_xywh[:, 2:4] = ANCHORS\n",
    "\n",
    "            iou_scale = iou_xywh_numpy(\n",
    "                bbox_xywh, anchors_xywh\n",
    "            )\n",
    "            iou.append(iou_scale)\n",
    "            iou_mask = iou_scale > 0.3\n",
    "\n",
    "            exist_positive = False\n",
    "\n",
    "            if np.any(iou_mask):\n",
    "                xind, yind = np.floor(bbox_xywh[0:2]).astype(\n",
    "                    np.int32\n",
    "                )\n",
    "                label[yind, xind, iou_mask, 0:4] = bbox_xywh\n",
    "                label[yind, xind, iou_mask, 4:5] = 1.0\n",
    "                # label[yind, xind, iou_mask, 5:6] = bbox_mix\n",
    "                label[yind, xind, iou_mask, 5:] = one_hot\n",
    "\n",
    "                # bbox_ind = int(bbox_count[i] % 150)  # BUG : 150为一个先验值,内存消耗大\n",
    "                bboxes_xywh[bbox_count, :4] = bbox_xywh\n",
    "                # bbox_count[i] += 1\n",
    "                bbox_count += 1\n",
    "\n",
    "                exist_positive = True\n",
    "\n",
    "\n",
    "            if not exist_positive:\n",
    "                best_anchor_ind = np.argmax(np.array(iou).reshape(-1), axis=-1)\n",
    "                # best_detect = int(best_anchor_ind / anchors_per_scale)\n",
    "                # best_anchor = int(best_anchor_ind % anchors_per_scale)\n",
    "\n",
    "                xind, yind = np.floor(\n",
    "                    bbox_xywh[best_anchor_ind, 0:2]\n",
    "                ).astype(np.int32)\n",
    "\n",
    "                label[yind, xind, best_anchor_ind, 0:4] = bbox_xywh\n",
    "                label[yind, xind, best_anchor_ind, 4:5] = 1.0\n",
    "                # label[yind, xind, best_anchor, 5:6] = bbox_mix\n",
    "                label[yind, xind, best_anchor_ind, 5:] = one_hot\n",
    "\n",
    "                bbox_ind = bbox_count\n",
    "                bboxes_xywh[bbox_ind, :4] = bbox_xywh\n",
    "                bbox_count += 1\n",
    "\n",
    "        # print('output')\n",
    "        # print(label.shape ,bboxes_xywh.shape)\n",
    "        return label, bboxes_xywh\n",
    "    \n",
    "def iou_xywh_numpy(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    :param boxes1: boxes1和boxes2的shape可以不相同，但是需要满足广播机制\n",
    "    :param boxes2: 且需要保证最后一维为坐标维，以及坐标的存储结构为(x,y,w,h)，其中(x,y)是bbox的中心坐标\n",
    "    :return: 返回boxes1和boxes2的IOU，IOU的shape为boxes1和boxes2广播后的shape[:-1]\n",
    "    \"\"\"\n",
    "    boxes1 = np.array(boxes1)\n",
    "    boxes2 = np.array(boxes2)\n",
    "    # print(boxes1, boxes2)\n",
    "\n",
    "    boxes1_area = boxes1[..., 2] * boxes1[..., 3]\n",
    "    boxes2_area = boxes2[..., 2] * boxes2[..., 3]\n",
    "\n",
    "    # 分别计算出boxes1和boxes2的左上角坐标、右下角坐标\n",
    "    # 存储结构为(xmin, ymin, xmax, ymax)，其中(xmin,ymin)是bbox的左上角坐标，(xmax,ymax)是bbox的右下角坐标\n",
    "    boxes1 = np.concatenate([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n",
    "                                boxes1[..., :2] + boxes1[..., 2:] * 0.5], axis=-1)\n",
    "    boxes2 = np.concatenate([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n",
    "                                boxes2[..., :2] + boxes2[..., 2:] * 0.5], axis=-1)\n",
    "\n",
    "    # 计算出boxes1与boxes1相交部分的左上角坐标、右下角坐标\n",
    "    left_up = np.maximum(boxes1[..., :2], boxes2[..., :2])\n",
    "    right_down = np.minimum(boxes1[..., 2:], boxes2[..., 2:])\n",
    "\n",
    "    # 因为两个boxes没有交集时，(right_down - left_up) < 0，所以maximum可以保证当两个boxes没有交集时，它们之间的iou为0\n",
    "    inter_section = np.maximum(right_down - left_up, 0.0)\n",
    "    inter_area = inter_section[..., 0] * inter_section[..., 1]\n",
    "    union_area = boxes1_area + boxes2_area - inter_area\n",
    "    IOU = 1.0 * inter_area / union_area\n",
    "    return IOU\n",
    "\n",
    "\n",
    "def CIOU_xywh_torch(boxes1,boxes2):\n",
    "    '''\n",
    "    cal CIOU of two boxes or batch boxes\n",
    "    :param boxes1:[xmin,ymin,xmax,ymax] or\n",
    "                [[xmin,ymin,xmax,ymax],[xmin,ymin,xmax,ymax],...]\n",
    "    :param boxes2:[xmin,ymin,xmax,ymax]\n",
    "    :return:\n",
    "    '''\n",
    "    # cx cy w h->xyxy\n",
    "    boxes1 = torch.cat([boxes1[..., :2] - boxes1[..., 2:] * 0.5,\n",
    "                        boxes1[..., :2] + boxes1[..., 2:] * 0.5], dim=-1)\n",
    "    boxes2 = torch.cat([boxes2[..., :2] - boxes2[..., 2:] * 0.5,\n",
    "                        boxes2[..., :2] + boxes2[..., 2:] * 0.5], dim=-1)\n",
    "\n",
    "    boxes1 = torch.cat([torch.min(boxes1[..., :2], boxes1[..., 2:]),\n",
    "                        torch.max(boxes1[..., :2], boxes1[..., 2:])], dim=-1)\n",
    "    boxes2 = torch.cat([torch.min(boxes2[..., :2], boxes2[..., 2:]),\n",
    "                        torch.max(boxes2[..., :2], boxes2[..., 2:])], dim=-1)\n",
    "\n",
    "    # (x2 minus x1 = width)  * (y2 - y1 = height)\n",
    "    boxes1_area = (boxes1[..., 2] - boxes1[..., 0]) * (boxes1[..., 3] - boxes1[..., 1])\n",
    "    boxes2_area = (boxes2[..., 2] - boxes2[..., 0]) * (boxes2[..., 3] - boxes2[..., 1])\n",
    "\n",
    "    # upper left of the intersection region (x,y)\n",
    "    inter_left_up = torch.max(boxes1[..., :2], boxes2[..., :2])\n",
    "\n",
    "    # bottom right of the intersection region (x,y)\n",
    "    inter_right_down = torch.min(boxes1[..., 2:], boxes2[..., 2:])\n",
    "\n",
    "    # if there is overlapping we will get (w,h) else set to (0,0) because it could be negative if no overlapping\n",
    "    inter_section = torch.max(inter_right_down - inter_left_up, torch.zeros_like(inter_right_down))\n",
    "    inter_area = inter_section[..., 0] * inter_section[..., 1]\n",
    "    union_area = boxes1_area + boxes2_area - inter_area\n",
    "    ious = 1.0 * inter_area / union_area\n",
    "\n",
    "    # cal outer boxes\n",
    "    outer_left_up = torch.min(boxes1[..., :2], boxes2[..., :2])\n",
    "    outer_right_down = torch.max(boxes1[..., 2:], boxes2[..., 2:])\n",
    "    outer = torch.max(outer_right_down - outer_left_up, torch.zeros_like(inter_right_down))\n",
    "    outer_diagonal_line = torch.pow(outer[..., 0], 2) + torch.pow(outer[..., 1], 2)\n",
    "\n",
    "    # cal center distance\n",
    "    # center x center y\n",
    "    boxes1_center = (boxes1[..., :2] +  boxes1[...,2:]) * 0.5\n",
    "    boxes2_center = (boxes2[..., :2] +  boxes2[...,2:]) * 0.5\n",
    "\n",
    "    # euclidean distance\n",
    "    # x1-x2 square \n",
    "    center_dis = torch.pow(boxes1_center[...,0]-boxes2_center[...,0], 2) +\\\n",
    "                 torch.pow(boxes1_center[...,1]-boxes2_center[...,1], 2)\n",
    "\n",
    "    # cal penalty term\n",
    "    # cal width,height\n",
    "    boxes1_size = torch.max(boxes1[..., 2:] - boxes1[..., :2], torch.zeros_like(inter_right_down))\n",
    "    boxes2_size = torch.max(boxes2[..., 2:] - boxes2[..., :2], torch.zeros_like(inter_right_down))\n",
    "    v = (4 / (math.pi ** 2)) * torch.pow(\n",
    "            torch.atan((boxes1_size[...,0]/torch.clamp(boxes1_size[...,1],min = 1e-6))) -\n",
    "            torch.atan((boxes2_size[..., 0] / torch.clamp(boxes2_size[..., 1],min = 1e-6))), 2)\n",
    "\n",
    "    alpha = v / (1-ious+v)\n",
    "\n",
    "    #cal ciou\n",
    "    cious = ious - (center_dis / outer_diagonal_line + alpha*v)\n",
    "\n",
    "    return cious\n",
    "\n",
    "def calculate_APs(iou_threshold, batches, targets):\n",
    "    from pycocotools.coco import COCO\n",
    "    coco = COCO('/root/COCO/annotations/instances_val2017.json')\n",
    "    ids = list(sorted(coco.imgs.keys()))\n",
    "\n",
    "    # img_id = ids[index]\n",
    "    # ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "    # ids = list(range(0,91))\n",
    "    target = coco.anns\n",
    "    # idx = list(target.keys())\n",
    "    print(len(target))\n",
    "    print(type(target))\n",
    "    number_of_classes = 80\n",
    "    # target = target\n",
    "    for index in ids:\n",
    "        print(target[index])\n",
    "    # print(len(idx))\n",
    "    # print(ids)\n",
    "    # self.target = target\n",
    "    # for i in range(0, 500):\n",
    "    #     img_id = idx[i]\n",
    "    #     tar = target[img_id]\n",
    "    #     print(tar)\n",
    "    # path = coco.loadImgs(img_id)[0]['file_name']\n",
    "    \n",
    "    APs = {}\n",
    "    recalls = {}\n",
    "    precisions = {}\n",
    "    # 80 classes\n",
    "    # print(target)\n",
    "    # for i in range(0, 80):\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add MSE Loss\n",
    "\n",
    "Let's come back to the `train_yolov4.py` file. Add MSE loss in the code to begin with.\n",
    "In your homework, you'll want to replace MSE with CIOU loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "with torch.set_grad_enabled(True):\n",
    "    outputs = model(inputs, True)\n",
    "\n",
    "    pred_xywh = outputs[..., 0:4] / 224\n",
    "    pred_conf = outputs[..., 4:5]\n",
    "    pred_cls = outputs[..., 5:]\n",
    "\n",
    "    label_xywh = labels[..., :4] / 224\n",
    "\n",
    "    # label_xywh = torch.cat([label_xy, label_wh], dim=-1)\n",
    "    label_obj_mask = labels[..., 4:5]\n",
    "    label_noobj_mask = (1.0 - label_obj_mask)\n",
    "    \n",
    "    lambda_coord = 0.001\n",
    "    lambda_noobj = 0.05\n",
    "    label_cls = labels[..., 5:]\n",
    "    loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the full code to start with for your ``train_yolov4.py`` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import time\n",
    "from torch.utils.data import Subset\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import cv2 \n",
    "from util import *\n",
    "import argparse\n",
    "import os \n",
    "import os.path as osp\n",
    "from darknet import Darknet\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import random\n",
    "import albumentations as A\n",
    "from custom_coco import CIOU_xywh_torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "from copy import copy\n",
    "from copy import deepcopy\n",
    "import torch.nn.functional as F\n",
    "from train import train_model, evaluate_model\n",
    "from custom_coco import CustomCoco, calculate_APs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set device to GPU or CPU\n",
    "gpu = \"1\"\n",
    "device = torch.device(\"cuda:{}\".format(gpu) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "path2data_train=\"/root/COCO/train2017\"\n",
    "path2json_train=\"/root/COCO/annotations/instances_train2017.json\"\n",
    "\n",
    "path2data_val=\"/root/COCO/val2017\"\n",
    "path2json_val=\"/root/COCO/annotations/instances_val2017.json\"\n",
    "\n",
    "\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.SmallestMaxSize(256),\n",
    "    A.RandomCrop(width=224, height=224),\n",
    "    # A.HorizontalFlip(p=0.5),\n",
    "    # A.RandomBrightnessContrast(p=0.2),\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']),\n",
    ")\n",
    "\n",
    "eval_transform = A.Compose([\n",
    "    A.SmallestMaxSize(256),\n",
    "    A.CenterCrop(width=224, height=224),\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']),\n",
    ")\n",
    "\n",
    "# raw_train_dataset = torchvision.datasets.CocoDetection(root = path2data_train,\n",
    "                                # annFile = path2json_train, transform=none_train_transform)\n",
    "\n",
    "# train_dataset = torchvision.datasets.CocoDetection(root = path2data_train,\n",
    "                                # annFile = path2json_train, transform=train_transform)\n",
    "BATCH_SIZE = 10\n",
    "val_dataset = Subset(CustomCoco(root = path2data_val,\n",
    "                                annFile = path2json_val, transform=eval_transform), list(range(0,20)))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                                            shuffle=False, num_workers=1, collate_fn=collate_fn)\n",
    "\n",
    "#If there's a GPU availible, put the model on GPU\n",
    "# CUDA = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "print(\"Loading network.....\")\n",
    "model = Darknet(\"cfg/yolov4.cfg\")\n",
    "model.load_weights(\"csdarknet53-omega_final.weights\", backbone_only=True)\n",
    "print(\"Network successfully loaded\")\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params_to_update = model.parameters()\n",
    "optimizer = optim.Adam(params_to_update, lr=0.001)\n",
    "for e in range(0, 40):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels, bboxes in val_dataloader:\n",
    "        inputs = torch.from_numpy(np.array(inputs)).squeeze(1).permute(0,3,1,2).float()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = torch.stack(labels).to(device)\n",
    "        \n",
    "        running_corrects = 0\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        # it uses for update training weights\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs, True)\n",
    "            # pred_xy = outputs[..., :2] / 224\n",
    "            # pred_wh = torch.sqrt(outputs[..., 2:4] / 224)\n",
    "\n",
    "            pred_xywh = outputs[..., 0:4] / 224\n",
    "            # pred_xywh = torch.cat([pred_xy, pred_wh], dim=-1)\n",
    "            pred_conf = outputs[..., 4:5]\n",
    "            pred_cls = outputs[..., 5:]\n",
    "\n",
    "\n",
    "            # label_xy = labels[..., :2] / 224\n",
    "            # label_wh = torch.sqrt(labels[..., 2:4] / 224)\n",
    "\n",
    "            label_xywh = labels[..., :4] / 224\n",
    "\n",
    "            # label_xywh = torch.cat([label_xy, label_wh], dim=-1)\n",
    "            label_obj_mask = labels[..., 4:5]\n",
    "            label_noobj_mask = (1.0 - label_obj_mask)  # * (\n",
    "                # iou_max < self.__iou_threshold_loss\n",
    "            # ).float()\n",
    "            lambda_coord = 0.001\n",
    "            lambda_noobj = 0.05\n",
    "            label_cls = labels[..., 5:]\n",
    "            loss = nn.MSELoss()\n",
    "            loss_bce = nn.BCELoss()\n",
    "\n",
    "            # ciou = CIOU_xywh_torch(p_d_xywh, label_xywh).unsqueeze(-1)\n",
    "\n",
    "            loss_coord = lambda_coord * label_obj_mask * loss(input=pred_xywh, target=label_xywh)\n",
    "            loss_conf = (label_obj_mask * loss_bce(input=pred_conf, target=label_obj_mask)) + \\\n",
    "                        (lambda_noobj * label_noobj_mask * loss_bce(input=pred_conf, target=label_obj_mask))\n",
    "            loss_cls = label_obj_mask * loss_bce(input=pred_cls, target=label_cls)\n",
    "\n",
    "            loss_coord = torch.sum(loss_coord)\n",
    "            loss_conf = torch.sum(loss_conf)\n",
    "            loss_cls = torch.sum(loss_cls)\n",
    "\n",
    "            # print(pred_xywh.shape, label_xywh.shape)\n",
    "\n",
    "            ciou = CIOU_xywh_torch(pred_xywh, label_xywh)\n",
    "            # print(ciou.shape)\n",
    "            ciou = ciou.unsqueeze(-1)\n",
    "            # print(ciou.shape)\n",
    "            # print(label_obj_mask.shape)\n",
    "            loss_ciou = torch.sum(label_obj_mask * (1.0 - ciou))\n",
    "            # print(loss_coord)\n",
    "            loss =  loss_ciou +  loss_conf + loss_cls\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            # print('Running loss')\n",
    "            # print(loss_coord, loss_conf, loss_cls)\n",
    "    epoch_loss = running_loss / 750\n",
    "    print(epoch_loss)\n",
    "    print('Epoch')\n",
    "\n",
    "    print(calculate_APs(0.5, None, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case we missed anything, full code containing these modifications from YOLO v3 can be loaded from [here](yolov4.zip)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Follow the YOLO v3 and YOLO v4 implementations in PyTorch and get inference working for both models\n",
    "   using the pretrained Darknet weights. This will give you a good understanding of YOLO in general\n",
    "   and many of the specific improvements made in YOLO v4. For YOLO v4, pay attention to the main points:\n",
    "\n",
    "   1. Implementation of the mish activation function\n",
    "   2. Option for the maxpool layer in the `create_modules` function and in your model's `forward()` method.\n",
    "   3. Enabling a `[route]` module to concatenate more than two previous layers\n",
    "   4. Loading the pre-trained weights [provided by the authors](https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights)\n",
    "   4. Scale inputs to 608$\\times$608 and make sure you're passing input channels in RGB order, not OpenCV's BGR order.\n",
    "\n",
    "2. Train your implementation of YOLO v4. The COCO dataset is available on the AIT DS&AI GPU servers at `/home/fidji/mdailey/Datasets/COCO`.\n",
    "   Here the purpose is not to get the best possible model (that would require implementing all\n",
    "   of the \"bag of freebies\" training tricks described in the paper), but just the ones described here, to\n",
    "   get a feel for their importance.\n",
    "   This involves the following:\n",
    "   \n",
    "   1. Get a set of ImageNet pretrained weights for CSPDarknet53 [from the Darknet GitHub repository](https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/csdarknet53-omega_final.weights)\n",
    "   2. Add a method to load the pretrained weights into the backbone portion of your PyTorch YOLOv4 model.\n",
    "   3. Implement a basic `train_yolo` function similar to the `train_model` function you developed in previous\n",
    "      labs for classifiers that preprocesses the input with basic augmentation transformations, converts the\n",
    "      anchor-relative outputs to bounding box coordinates, computes MSE loss for the bounding box coordinates,\n",
    "      backpropagates the loss, and takes a step for the optimizer. Use the recommended IoU thresholds to determine\n",
    "      which predicted bounding boxes to include in the loss. You will find many examples of how to do this\n",
    "      online.\n",
    "   4. Train your model on COCO. Training on the full dataset to completion would take several days, so you can stop early after verifying\n",
    "      the model is learning in the first few epochs.\n",
    "   5. Compute mAP for your model on the COCO validation set.\n",
    "   6. Implement the CIoU loss function and observe its effect on mAP.\n",
    "   7. (Optional) Train on COCO to completion and see how close you can get to the mAP reported in the paper.\n",
    "\n",
    "   There is some useful information on working with the COCO dataset as a\n",
    "   Torchvision Dataset in [this blog](https://medium.com/howtoai/pytorch-torchvision-coco-dataset-b7f5e8cad82).\n",
    "   Use the instructor's copy of the COCO training and validation datasets from the shared network drive\n",
    "   so that we don't use resources for multiple copies of the dataset. Once you have access to the dataset you can use the dataset easily.\n",
    "\n",
    "2. The state of the art in fast single-stage object detection in 2022 appears to be YOLOR.\n",
    "   Take a look at the [paper](https://arxiv.org/abs/2105.04206) and the [PyTorch source code](https://github.com/WongKinYiu/yolor).\n",
    "   Execute the same exercise as above to modify your PyTorch Darknet tools to read the YOLOR configuration file and get the model working\n",
    "   first for inference then for training. We will execute this task collaboratively and share experience on the class discussion board as\n",
    "   we go along.\n",
    "   \n",
    "   Note that YOLOR is built on top of [Scaled YOLOv4](https://arxiv.org/abs/2011.08036), in which several different versions of the\n",
    "   CSP Darknet backbone are created for different speed/accuracy tradeoffs. We probably want to start with YOLOR P6, which is based on\n",
    "   YOLOv4-P6, which obtains 54.3% COCO mAP at 30 fps with resolution 1280$\\times$1280.\n",
    "\n",
    "   We want you to modify the YOLOv4 code you already developed to work with the YOLOR model. Don't just replace your code with the YOLOR\n",
    "   repository code. Instead, try to trace the execution of the image preparation, `create_modules()` function, `forward()` method, and\n",
    "   so on.\n",
    "   \n",
    "   You'll need to make at least the following modifications to make your code run the same as YOLOR:\n",
    "   1. Allow flexible input image size for the letterboxing, with input size 1280 in the longest dimension and a multiple of 64 pixels in the\n",
    "      other dimension.\n",
    "   2. Since the YOLOR weight files are PyTorch pickle files, you'll need to name the convolutional and implicit latent parameter layers\n",
    "      the same as in the YOLOR weight file so that you can load the state dictionary as with other PyTorch models.\n",
    "   3. Utilize the YOLOR non maxima suppression function, as it is much faster than the one we gave you in this manual for the YOLOR output\n",
    "      tensors.\n",
    "   4. Utilize the YOLOR conversion of model output tensors to bounding box coordinates, confidence scores, and class scores. The organization\n",
    "      and post processing is different from our YOLOv4 configuration, so your `predict_transform()` function won't work as is.\n",
    "      \n",
    "   Good luck!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
