{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTML Lab 4: YOLO (v3 and v7)\n",
    "\n",
    "NAME = \"Todsavad Tangtortan\"\n",
    "ID = \"123012\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Homework (Will release on 3 Feb 2023)\n",
    "\n",
    "## Independent exercise: YOLOR\n",
    "### Part I: Inference (due next week)\n",
    "\n",
    "In the lab, we saw how the Darknet configuration file for YOLOv3 could be read in Python and mapped\n",
    "to PyTorch modules.\n",
    "\n",
    "For your independent work do the same thing for YOLOv4. Download the `yolov4.cfg` file\n",
    "from the [YOLOv4 GitHub repository](https://github.com/AlexeyAB/darknet) and modify your\n",
    "`MyDarknet` class and utility code (`darknet.py`, `util.py`) as\n",
    "necessary to map the structures to PyTorch.\n",
    "\n",
    "The changes you'll have to make:\n",
    "\n",
    "1. Implement the mish activation function\n",
    "2. Add an option for a maxpool layer in the `create_modules` function and in your model's `forward()` method.\n",
    "3. Enable a `[route]` module to concatenate more than two previous layers\n",
    "4. Load the pre-trained weights [provided by the authors](https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights)\n",
    "4. Scale inputs to 608$\\times$608 and make sure you're passing input channels in RGB order, not OpenCV's BGR order."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify `darknet.py`\n",
    "- Implement the mish activation function.\n",
    "\n",
    "        class Mish(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "\n",
    "            def forward(self, x):\n",
    "                return x * torch.tanh(F.softplus(x))\n",
    "                \n",
    "        if activation == \"mish\":\n",
    "            activn = Mish()\n",
    "            module.add_module(\"mish_{0}\".format(index), activn)\n",
    "            \n",
    "- Add an option for a maxpool layer in the `create_modules` function.\n",
    "\n",
    "        \n",
    "        elif x[\"type\"] == \"maxpool\":\n",
    "            stride = int(x[\"stride\"])\n",
    "            size = int(x[\"size\"])\n",
    "            assert size % 2\n",
    "            maxpool = nn.MaxPool2d(kernel_size=size, stride=stride, padding=size // 2)\n",
    "            module.add_module(\"maxpool_{0}\".format(index), maxpool)\n",
    "\n",
    "- Add condition in your model's `forward()` method.\n",
    "\n",
    "        if module_type == \"convolutional\" or module_type == \"upsample\" or module_type == \"maxpool\":\n",
    "            x = self.module_list[i](x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Enable a `[route]` module to concatenate more than two previous layers\n",
    "\n",
    "\n",
    "        elif module_type == \"route\":\n",
    "                \n",
    "            # concat layers\n",
    "            layers = module[\"layers\"]\n",
    "            layers = [int(a) for a in layers]\n",
    "            \n",
    "            if (layers[0]) > 0:\n",
    "                layers[0] = layers[0] - i\n",
    "\n",
    "            if len(layers) == 1:    # 1 item in layer                 \n",
    "                x = outputs[i + (layers[0])]\n",
    "\n",
    "            else:   # more than 1 item in layer \n",
    "                if len(layers) == 4:       # 4 items in layer                                      \n",
    "                    if (layers[1]) > 0:\n",
    "                        layers[1] = layers[1] - i\n",
    "\n",
    "                    if (layers[2]) > 0:\n",
    "                        layers[2] = layers[2] - i\n",
    "\n",
    "                    if (layers[3]) > 0:\n",
    "                        layers[3] = layers[3] - i\n",
    "\n",
    "                    map1 = outputs[i + layers[0]]\n",
    "                    map2 = outputs[i + layers[1]]\n",
    "                    map3 = outputs[i + layers[2]]\n",
    "                    map4 = outputs[i + layers[3]]\n",
    "                    x = torch.cat((map1, map2, map3, map4), 1)\n",
    "\n",
    "                else:           # 2 items in layer                \n",
    "                    if (layers[1]) > 0:\n",
    "                        layers[1] = layers[1] - i\n",
    "    \n",
    "                    map1 = outputs[i + layers[0]]\n",
    "                    map2 = outputs[i + layers[1]]\n",
    "                    x = torch.cat((map1, map2), 1)\n",
    "\n",
    "- Load the pre-trained weights [provided by the authors](https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights)\n",
    "\n",
    "        model = Darknet(\"cfg/yolov4.cfg\")\n",
    "        model.load_weights(\"yolov4.weights\")\n",
    "\n",
    "- Scale inputs to 608$\\times$608 and make sure you're passing input channels in RGB order, not OpenCV's BGR order.\n",
    "\n",
    "## Modify `util.py`\n",
    "\n",
    "        def prep_image(img, inp_dim):\n",
    "            \"\"\"\n",
    "            Prepare image for inputting to the neural network. \n",
    "            \n",
    "            Returns a Variable \n",
    "            \"\"\"\n",
    "            # pylint: disable=no-member\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = letterbox_image(img, (inp_dim, inp_dim))\n",
    "            img = img[:,:,::-1].transpose((2,0,1)).copy()\n",
    "            img =  torch.from_numpy(img).float().div(255.0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 608, 608])\n",
      "torch.Size([1, 22743, 85])\n",
      "torch.Size([1, 22743, 85])\n"
     ]
    }
   ],
   "source": [
    "from darknet import *\n",
    "import torch\n",
    "import cv2\n",
    "from util import *\n",
    "\n",
    "def load_classes(namesfile):\n",
    "    fp = open(namesfile, \"r\")\n",
    "    names = fp.read().split(\"\\n\")[:-1]\n",
    "    return names\n",
    "\n",
    "model = Darknet(\"cfg/yolov4.cfg\")\n",
    "model.module_list[114].conv_114 = nn.Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "input = get_test_input()\n",
    "print(input.shape)\n",
    "prediction = model(input, False)\n",
    "# print(prediction)\n",
    "print(prediction.shape)\n",
    "\n",
    "model.load_weights(\"yolov4.weights\")\n",
    "input = get_test_input()\n",
    "prediction = model(input, False)\n",
    "write_results(prediction.detach(), 0.5, 80, nms_conf = 0.4)\n",
    "# print(prediction)\n",
    "print(prediction.shape)\n",
    "# num_classes = 80\n",
    "# classes = load_classes(\"../data/coco.names\")\n",
    "\n",
    "# print(classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Training (due in two weeks)\n",
    "\n",
    "Train the YOLOv4 model on the COCO dataset (or another dataset if you have one available).\n",
    "Here the purpose is not to get the best possible model (that would require implementing all\n",
    "of the \"bag of freebies\" training tricks described in the paper), but just some of them, to\n",
    "get a feel for their importance.\n",
    "\n",
    "1. Get a set of ImageNet pretrained weights for CSPDarknet53 [from the Darknet GitHub repository](https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/csdarknet53-omega_final.weights)\n",
    "2. Add a method to load the pretrained weights into the backbone portion of your PyTorch YOLOv4 model.\n",
    "3. Implement a basic `train_yolo` function similar to the `train_model` function you developed in previous\n",
    "   labs for classifiers that preprocesses the input with basic augmentation transformations, converts the\n",
    "   anchor-relative outputs to bounding box coordinates, computes MSE loss for the bounding box coordinates,\n",
    "   backpropagates the loss, and takes a step for the optimizer. Use the recommended IoU thresholds to determine\n",
    "   which predicted bounding boxes to include in the loss. You will find many examples of how to do this\n",
    "   online.\n",
    "4. Train your model on COCO. Training on the full dataset to completion would take several days, so you can stop early after verifying\n",
    "   the model is learning in the first few epochs.\n",
    "5. Compute mAP for your model on the COCO validation set.\n",
    "6. Implement the CIoU loss function and observe its effect on mAP.\n",
    "7. (Optional) Train on COCO to completion and see how close you can get to the mAP reported in the paper.\n",
    "\n",
    "There is some useful information on working with the COCO dataset as a\n",
    "Torchvision Dataset in [this blog](https://medium.com/howtoai/pytorch-torchvision-coco-dataset-b7f5e8cad82).\n",
    "For your work on this lab, the instructor will place the entire COCO training and validation datasets on a shared network drive for you to access\n",
    "so that we don't use resources for multiple copies of the dataset. Once you have access to the dataset you can use the dataset easily:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
