{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 08: Cycle GANs and Pix2Pix\n",
    "\n",
    "This lab will introduce CycleGANs and Pix2Pix. We will see how to use the code from the GitHub repositories\n",
    "and also take a close look at the code. It is very useful when you are using these algorithms for your project.\n",
    "\n",
    "## CycleGANs\n",
    "\n",
    "THe CycleGAN is a very popular GAN architecture. It is used to learn transformations between images of different styles.\n",
    "\n",
    "Example Cycle GANs:\n",
    "\n",
    " - Mapping between artistic and realistic images\n",
    " - Transformation between images of horses and zebras\n",
    " - Transformation between winter images and summer images\n",
    " - FaceApp or DeepFake\n",
    " - Super-resolution reconstruction of images\n",
    "\n",
    "Assume that ${\\cal X}$ is the set of images of horses and ${\\cal Y}$ is the set of images of zebras.\n",
    "\n",
    "The goal of CycleGAN is to learn a mapping function $G: \\mathcal{X} \\rightarrow \\mathcal{Y}$,\n",
    "such that an image generated by $G(X)$ for some $X\\in\\mathcal{X}$ are indistinguishable from\n",
    "samples $Y$ from a training set over $\\mathcal{Y}$.\n",
    "This objective is achieved using an adversarial loss function. We not only learn $G(\\cdot)$, but we also learn an inverse mapping function $F: \\mathcal{Y} \\rightarrow \\mathcal{X}$\n",
    "with the help of a cycle-consistency loss to encourage $F(G(X)) \\approx X$.\n",
    "\n",
    "While training, two kinds of training observations may be given as input:\n",
    "\n",
    " - *Paired* images $\\{(X^{(i)}, Y^{(i)})\\}_{i\\in 1..N}$.\n",
    " - *Unpaired* image sets $\\{X^{(i)}\\}_{i\\in 1..N_{X}}$ and $\\{Y^{(i)}\\}_{i\\in 1..N_{Y}}$ without any special relationship between $X^{(i)}$ and $Y^{(i)}$.\n",
    "\n",
    "The adversarial formulation of the Cycle GAN includes a discriminator $D_X$ that attempts to classify observations\n",
    "$G(X^{(i)})$ and $Y^{(i)}$ as fake or real.\n",
    "Similarly, we also have a discriminator $D_Y$ that attempts to distinguish $F(Y^{(i)})$ from $X^{(i)}$.\n",
    "\n",
    "<img src=\"img/CycleGANmodel.jpg\" title=\"CycleGAN\" style=\"width: 640px;\" />\n",
    "\n",
    "The generator may look like this:\n",
    "\n",
    "<img src=\"img/CycleGANGenerator.jpg\" title=\"CycleGAN Generator\" style=\"width: 640px;\" />\n",
    "\n",
    "And the discriminator amy look like this:\n",
    "\n",
    "<img src=\"img/CycleGANdiscriminator.jpg\" title=\"CycleGAN Discriminator\" style=\"width: 640px;\" />\n",
    "\n",
    "Besides the adversarial Loss, the Cycle GAN uses two cycle-consistency losses; this enables training without paired images.\n",
    "We minimize reconstruction losses $\\| F(G(X)) - X \\|$ and $\\|G(F(Y)) - Y\\|$.\n",
    "In summary, the Cycle GAN comprises the three loss functions:\n",
    "\n",
    "<img src=\"img/CycleGAN-formulation.png\" title=\"CycleGAN formulation\" style=\"width: 560px;\" />\n",
    "\n",
    "The optimization is similar to that of the ordinary GAN, except we have two generators and two discriminators:\n",
    "\n",
    "<img src=\"img/Optimized-loss-function-CycleGan.png\" title=\"CycleGAN optimization\" style=\"width: 320px;\" />\n",
    "\n",
    "## Results\n",
    "\n",
    "Here are some results from a Cycle GAN trained on horses and zebras:\n",
    "\n",
    "<img src=\"img/CycleGANResultsA2B.jpg\" title=\"CycleGAN Results\" style=\"width: 640px;\" />\n",
    "\n",
    "<img src=\"img/CycleGANdistortionB2A.jpg\" title=\"CycleGAN Distort\" style=\"width: 640px;\" />\n",
    "\n",
    "You can take a look at [the diegoalejogm GitHub repository](https://github.com/diegoalejogm/gans/blob/master/CycleGans.ipynb) for some\n",
    "examples of Cycle GANs constructed from scratch.\n",
    "\n",
    "## Get and prepare Cycle GAN implementation\n",
    "\n",
    "Today, we'll use the authors' implementation of Cycle GANs. Go ahead and download the Cycle GAN implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'pytorch-CycleGAN-and-pix2pix' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation requires dominate and visdom for visualization. They enable monitoring the result of training via a Web server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting dominate\n",
      "  Downloading dominate-2.6.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting visdom\n",
      "  Downloading visdom-0.1.8.9.tar.gz (676 kB)\n",
      "\u001b[K     |████████████████████████████████| 676 kB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.8 in /opt/conda/lib/python3.8/site-packages (from visdom) (1.19.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from visdom) (1.4.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from visdom) (2.24.0)\n",
      "Requirement already satisfied: tornado in /opt/conda/lib/python3.8/site-packages (from visdom) (6.1)\n",
      "Requirement already satisfied: pyzmq in /opt/conda/lib/python3.8/site-packages (from visdom) (20.0.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from visdom) (1.15.0)\n",
      "Collecting jsonpatch\n",
      "  Downloading jsonpatch-1.31-py2.py3-none-any.whl (12 kB)\n",
      "Collecting torchfile\n",
      "  Downloading torchfile-0.1.0.tar.gz (5.2 kB)\n",
      "Collecting websocket-client\n",
      "  Downloading websocket_client-0.58.0-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 14.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow-simd in /opt/conda/lib/python3.8/site-packages (from visdom) (7.0.0.post3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests->visdom) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->visdom) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->visdom) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->visdom) (2.10)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-2.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Building wheels for collected packages: visdom, torchfile\n",
      "  Building wheel for visdom (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for visdom: filename=visdom-0.1.8.9-py3-none-any.whl size=655255 sha256=9f6b6b4e2e5e1524abd5d4f6095c2080a86e8ab2fbfcb0d603f899cca0fcf536\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-s2siovju/wheels/6e/f5/76/7906cebf407d8dbf3b26cb28800b7d3b486ae7e7c8859a5748\n",
      "  Building wheel for torchfile (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torchfile: filename=torchfile-0.1.0-py3-none-any.whl size=5711 sha256=2b765a7f7bd41ad17ddb746af245cc9d896051a73d375a129a4a0ed650f0b9d7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-s2siovju/wheels/b9/99/d1/9f3f4411a958a22ccf782e33c7238a07f04a9597f2f3b38792\n",
      "Successfully built visdom torchfile\n",
      "Installing collected packages: dominate, jsonpointer, jsonpatch, torchfile, websocket-client, visdom\n",
      "Successfully installed dominate-2.6.0 jsonpatch-1.31 jsonpointer-2.0 torchfile-0.1.0 visdom-0.1.8.9 websocket-client-0.58.0\n"
     ]
    }
   ],
   "source": [
    "!pip install dominate visdom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, download a data set. You can try a different data set if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd pytorch-CycleGAN-and-pix2pix ; ./datasets/download_cyclegan_dataset.sh horse2zebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a training run\n",
    "\n",
    "We won't be able to finish a training run in class -- 200 epochs of the horse2zebra dataset with batch size 1 takes 10-20 hours on our GPUs. However, we can start a run and see how it goes.\n",
    "\n",
    "We'll also see an example of how to use visdom, which is probably better than matplotlib for visualization when we are running on the server.\n",
    "\n",
    "### In terminal 1:\n",
    "\n",
    "    python -m visdom.server\n",
    "   \n",
    "### In terminal 2 (ssh with parameter -L 8097:localhost:8097 or let VSCode forward the port for you):\n",
    "\n",
    "    python train.py --dataroot ./datasets/horse2zebra --name horse2zebra_cyclegan --model cycle_gan\n",
    "\n",
    "## Tips\n",
    "\n",
    "Take a look at `CycleGAN.ipynb` in the repository to understand how to train/test on your dataset.\n",
    "\n",
    "Configuration options are listed in the classes in the `options` subdirectory.\n",
    "\n",
    "Get your GANs off and running. After every 100 iterations, you should see updated samples for a real pair $(X, Y)$, including $G(X), F(Y), F(G(X)), G(F(Y)), F(X),$ and $G(Y)$. These\n",
    "are labeled as fakeB, fakeA, recA, recB, idtB, and idtA. If we can all run concurrently you should be getting horsey zebras and somewhat striped horses by the end of lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking into the code\n",
    "\n",
    "First, let visit the <code>train.py</code> code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from options.train_options import TrainOptions\n",
    "from data import create_dataset\n",
    "from models import create_model\n",
    "from util.visualizer import Visualizer\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    opt = TrainOptions().parse()   # get training options\n",
    "    dataset = create_dataset(opt)  # create a dataset given opt.dataset_mode and other options\n",
    "    dataset_size = len(dataset)    # get the number of images in the dataset.\n",
    "    print('The number of training images = %d' % dataset_size)\n",
    "\n",
    "    model = create_model(opt)      # create a model given opt.model and other options\n",
    "    model.setup(opt)               # regular setup: load and print networks; create schedulers\n",
    "    visualizer = Visualizer(opt)   # create a visualizer that display/save images and plots\n",
    "    total_iters = 0                # the total number of training iterations\n",
    "\n",
    "    for epoch in range(opt.epoch_count, opt.n_epochs + opt.n_epochs_decay + 1):    # outer loop for different epochs; we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>\n",
    "        epoch_start_time = time.time()  # timer for entire epoch\n",
    "        iter_data_time = time.time()    # timer for data loading per iteration\n",
    "        epoch_iter = 0                  # the number of training iterations in current epoch, reset to 0 every epoch\n",
    "        visualizer.reset()              # reset the visualizer: make sure it saves the results to HTML at least once every epoch\n",
    "        model.update_learning_rate()    # update learning rates in the beginning of every epoch.\n",
    "        for i, data in enumerate(dataset):  # inner loop within one epoch\n",
    "            iter_start_time = time.time()  # timer for computation per iteration\n",
    "            if total_iters % opt.print_freq == 0:\n",
    "                t_data = iter_start_time - iter_data_time\n",
    "\n",
    "            total_iters += opt.batch_size\n",
    "            epoch_iter += opt.batch_size\n",
    "            model.set_input(data)         # unpack data from dataset and apply preprocessing\n",
    "            model.optimize_parameters()   # calculate loss functions, get gradients, update network weights\n",
    "\n",
    "            if total_iters % opt.display_freq == 0:   # display images on visdom and save images to a HTML file\n",
    "                save_result = total_iters % opt.update_html_freq == 0\n",
    "                model.compute_visuals()\n",
    "                visualizer.display_current_results(model.get_current_visuals(), epoch, save_result)\n",
    "\n",
    "            if total_iters % opt.print_freq == 0:    # print training losses and save logging information to the disk\n",
    "                losses = model.get_current_losses()\n",
    "                t_comp = (time.time() - iter_start_time) / opt.batch_size\n",
    "                visualizer.print_current_losses(epoch, epoch_iter, losses, t_comp, t_data)\n",
    "                if opt.display_id > 0:\n",
    "                    visualizer.plot_current_losses(epoch, float(epoch_iter) / dataset_size, losses)\n",
    "\n",
    "            if total_iters % opt.save_latest_freq == 0:   # cache our latest model every <save_latest_freq> iterations\n",
    "                print('saving the latest model (epoch %d, total_iters %d)' % (epoch, total_iters))\n",
    "                save_suffix = 'iter_%d' % total_iters if opt.save_by_iter else 'latest'\n",
    "                model.save_networks(save_suffix)\n",
    "\n",
    "            iter_data_time = time.time()\n",
    "        if epoch % opt.save_epoch_freq == 0:              # cache our model every <save_epoch_freq> epochs\n",
    "            print('saving the model at the end of epoch %d, iters %d' % (epoch, total_iters))\n",
    "            model.save_networks('latest')\n",
    "            model.save_networks(epoch)\n",
    "\n",
    "        print('End of epoch %d / %d \\t Time Taken: %d sec' % (epoch, opt.n_epochs + opt.n_epochs_decay, time.time() - epoch_start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code contains 3 parts:\n",
    "\n",
    "1. Load the dataset\n",
    "2. Load the model\n",
    "3. Training\n",
    "\n",
    "### Command line arguments (`argparse`)\n",
    "\n",
    "To select the model, dataset, and other settings, we use the <code>argparse</code> library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = TrainOptions().parse()   # get training options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>TrainOptions()</code> function is in the <code>/options/train_options.py</code> which inherited from <code>/options/base_options.py</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .base_options import BaseOptions\n",
    "\n",
    "\n",
    "class TrainOptions(BaseOptions):\n",
    "    \"\"\"This class includes training options.\n",
    "\n",
    "    It also includes shared options defined in BaseOptions.\n",
    "    \"\"\"\n",
    "\n",
    "    def initialize(self, parser):\n",
    "        parser = BaseOptions.initialize(self, parser)\n",
    "        # visdom and HTML visualization parameters\n",
    "        parser.add_argument('--display_freq', type=int, default=400, help='frequency of showing training results on screen')\n",
    "        parser.add_argument('--display_ncols', type=int, default=4, help='if positive, display all images in a single visdom web panel with certain number of images per row.')\n",
    "        parser.add_argument('--display_id', type=int, default=1, help='window id of the web display')\n",
    "        parser.add_argument('--display_server', type=str, default=\"http://localhost\", help='visdom server of the web display')\n",
    "        parser.add_argument('--display_env', type=str, default='main', help='visdom display environment name (default is \"main\")')\n",
    "        parser.add_argument('--display_port', type=int, default=8097, help='visdom port of the web display')\n",
    "        parser.add_argument('--update_html_freq', type=int, default=1000, help='frequency of saving training results to html')\n",
    "        parser.add_argument('--print_freq', type=int, default=100, help='frequency of showing training results on console')\n",
    "        parser.add_argument('--no_html', action='store_true', help='do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/')\n",
    "        # network saving and loading parameters\n",
    "        parser.add_argument('--save_latest_freq', type=int, default=5000, help='frequency of saving the latest results')\n",
    "        parser.add_argument('--save_epoch_freq', type=int, default=5, help='frequency of saving checkpoints at the end of epochs')\n",
    "        parser.add_argument('--save_by_iter', action='store_true', help='whether saves model by iteration')\n",
    "        parser.add_argument('--continue_train', action='store_true', help='continue training: load the latest model')\n",
    "        parser.add_argument('--epoch_count', type=int, default=1, help='the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...')\n",
    "        parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n",
    "        # training parameters\n",
    "        parser.add_argument('--n_epochs', type=int, default=100, help='number of epochs with the initial learning rate')\n",
    "        parser.add_argument('--n_epochs_decay', type=int, default=100, help='number of epochs to linearly decay learning rate to zero')\n",
    "        parser.add_argument('--beta1', type=float, default=0.5, help='momentum term of adam')\n",
    "        parser.add_argument('--lr', type=float, default=0.0002, help='initial learning rate for adam')\n",
    "        parser.add_argument('--gan_mode', type=str, default='lsgan', help='the type of GAN objective. [vanilla| lsgan | wgangp]. vanilla GAN loss is the cross-entropy objective used in the original GAN paper.')\n",
    "        parser.add_argument('--pool_size', type=int, default=50, help='the size of image buffer that stores previously generated images')\n",
    "        parser.add_argument('--lr_policy', type=str, default='linear', help='learning rate policy. [linear | step | plateau | cosine]')\n",
    "        parser.add_argument('--lr_decay_iters', type=int, default=50, help='multiply by a gamma every lr_decay_iters iterations')\n",
    "\n",
    "        self.isTrain = True\n",
    "        return parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class and function\n",
    "\n",
    "Next, let's see the loading dataset. The function in <code>train.py</code> is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset(opt)  # create a dataset given opt.dataset_mode and other options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>create_dataset()</code> links to <code>/data/__init__.py</code> which is linked to <code>CustomDatasetDataLoader</code> class.\n",
    "\n",
    "Don't be worry about the linking and linking again, the datasetloaders have 2 types. You can take a look into the specific file.\n",
    "\n",
    "1. Pair data (<code>aligned_dataset.py</code> or special <code>colorization.py</code>) - the data must have pair set each other. This is used for Pix2Pix. The folder of dataset must have **trainA** and **trainB** with the **same image name** for training phase.\n",
    "2. Unpair data (<code>unaligned_dataset.py</code>) - the data have 2 data folders which do not have paired each other. The folder must have **trainA** and **trainB** for training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(opt):\n",
    "    \"\"\"Create a dataset given the option.\n",
    "\n",
    "    This function wraps the class CustomDatasetDataLoader.\n",
    "        This is the main interface between this package and 'train.py'/'test.py'\n",
    "\n",
    "    Example:\n",
    "        >>> from data import create_dataset\n",
    "        >>> dataset = create_dataset(opt)\n",
    "    \"\"\"\n",
    "    data_loader = CustomDatasetDataLoader(opt)\n",
    "    dataset = data_loader.load_data()\n",
    "    return dataset\n",
    "\n",
    "class CustomDatasetDataLoader():\n",
    "    \"\"\"Wrapper class of Dataset class that performs multi-threaded data loading\"\"\"\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        \"\"\"Initialize this class\n",
    "\n",
    "        Step 1: create a dataset instance given the name [dataset_mode]\n",
    "        Step 2: create a multi-threaded data loader.\n",
    "        \"\"\"\n",
    "        self.opt = opt\n",
    "        dataset_class = find_dataset_using_name(opt.dataset_mode)\n",
    "        self.dataset = dataset_class(opt)\n",
    "        print(\"dataset [%s] was created\" % type(self.dataset).__name__)\n",
    "        self.dataloader = torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            batch_size=opt.batch_size,\n",
    "            shuffle=not opt.serial_batches,\n",
    "            num_workers=int(opt.num_threads))\n",
    "\n",
    "    def load_data(self):\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of data in the dataset\"\"\"\n",
    "        return min(len(self.dataset), self.opt.max_dataset_size)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Return a batch of data\"\"\"\n",
    "        for i, data in enumerate(self.dataloader):\n",
    "            if i * self.opt.batch_size >= self.opt.max_dataset_size:\n",
    "                break\n",
    "            yield data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the code of unaligned and aligned dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unaligned_dataset.py\n",
    "# this is default dataset loader and use for cyclegan\n",
    "\n",
    "import os\n",
    "from data.base_dataset import BaseDataset, get_transform\n",
    "from data.image_folder import make_dataset\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "\n",
    "class UnalignedDataset(BaseDataset):\n",
    "    \"\"\"\n",
    "    This dataset class can load unaligned/unpaired datasets.\n",
    "\n",
    "    It requires two directories to host training images from domain A '/path/to/data/trainA'\n",
    "    and from domain B '/path/to/data/trainB' respectively.\n",
    "    You can train the model with the dataset flag '--dataroot /path/to/data'.\n",
    "    Similarly, you need to prepare two directories:\n",
    "    '/path/to/data/testA' and '/path/to/data/testB' during test time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        \"\"\"Initialize this dataset class.\n",
    "\n",
    "        Parameters:\n",
    "            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\n",
    "        \"\"\"\n",
    "        BaseDataset.__init__(self, opt)\n",
    "        self.dir_A = os.path.join(opt.dataroot, opt.phase + 'A')  # create a path '/path/to/data/trainA'\n",
    "        self.dir_B = os.path.join(opt.dataroot, opt.phase + 'B')  # create a path '/path/to/data/trainB'\n",
    "\n",
    "        self.A_paths = sorted(make_dataset(self.dir_A, opt.max_dataset_size))   # load images from '/path/to/data/trainA'\n",
    "        self.B_paths = sorted(make_dataset(self.dir_B, opt.max_dataset_size))    # load images from '/path/to/data/trainB'\n",
    "        self.A_size = len(self.A_paths)  # get the size of dataset A\n",
    "        self.B_size = len(self.B_paths)  # get the size of dataset B\n",
    "        btoA = self.opt.direction == 'BtoA'\n",
    "        input_nc = self.opt.output_nc if btoA else self.opt.input_nc       # get the number of channels of input image\n",
    "        output_nc = self.opt.input_nc if btoA else self.opt.output_nc      # get the number of channels of output image\n",
    "        self.transform_A = get_transform(self.opt, grayscale=(input_nc == 1))\n",
    "        self.transform_B = get_transform(self.opt, grayscale=(output_nc == 1))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return a data point and its metadata information.\n",
    "\n",
    "        Parameters:\n",
    "            index (int)      -- a random integer for data indexing\n",
    "\n",
    "        Returns a dictionary that contains A, B, A_paths and B_paths\n",
    "            A (tensor)       -- an image in the input domain\n",
    "            B (tensor)       -- its corresponding image in the target domain\n",
    "            A_paths (str)    -- image paths\n",
    "            B_paths (str)    -- image paths\n",
    "        \"\"\"\n",
    "        A_path = self.A_paths[index % self.A_size]  # make sure index is within then range\n",
    "        if self.opt.serial_batches:   # make sure index is within then range\n",
    "            index_B = index % self.B_size\n",
    "        else:   # randomize the index for domain B to avoid fixed pairs.\n",
    "            index_B = random.randint(0, self.B_size - 1)\n",
    "        B_path = self.B_paths[index_B]\n",
    "        A_img = Image.open(A_path).convert('RGB')\n",
    "        B_img = Image.open(B_path).convert('RGB')\n",
    "        # apply image transformation\n",
    "        A = self.transform_A(A_img)\n",
    "        B = self.transform_B(B_img)\n",
    "\n",
    "        return {'A': A, 'B': B, 'A_paths': A_path, 'B_paths': B_path}\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of images in the dataset.\n",
    "\n",
    "        As we have two datasets with potentially different number of images,\n",
    "        we take a maximum of\n",
    "        \"\"\"\n",
    "        return max(self.A_size, self.B_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aligned_dataset.py\n",
    "# this dataset is used for pix2pix\n",
    "\n",
    "import os\n",
    "from data.base_dataset import BaseDataset, get_params, get_transform\n",
    "from data.image_folder import make_dataset\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class AlignedDataset(BaseDataset):\n",
    "    \"\"\"A dataset class for paired image dataset.\n",
    "\n",
    "    It assumes that the directory '/path/to/data/train' contains image pairs in the form of {A,B}.\n",
    "    During test time, you need to prepare a directory '/path/to/data/test'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        \"\"\"Initialize this dataset class.\n",
    "\n",
    "        Parameters:\n",
    "            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\n",
    "        \"\"\"\n",
    "        BaseDataset.__init__(self, opt)\n",
    "        self.dir_AB = os.path.join(opt.dataroot, opt.phase)  # get the image directory\n",
    "        self.AB_paths = sorted(make_dataset(self.dir_AB, opt.max_dataset_size))  # get image paths\n",
    "        assert(self.opt.load_size >= self.opt.crop_size)   # crop_size should be smaller than the size of loaded image\n",
    "        self.input_nc = self.opt.output_nc if self.opt.direction == 'BtoA' else self.opt.input_nc\n",
    "        self.output_nc = self.opt.input_nc if self.opt.direction == 'BtoA' else self.opt.output_nc\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Return a data point and its metadata information.\n",
    "\n",
    "        Parameters:\n",
    "            index - - a random integer for data indexing\n",
    "\n",
    "        Returns a dictionary that contains A, B, A_paths and B_paths\n",
    "            A (tensor) - - an image in the input domain\n",
    "            B (tensor) - - its corresponding image in the target domain\n",
    "            A_paths (str) - - image paths\n",
    "            B_paths (str) - - image paths (same as A_paths)\n",
    "        \"\"\"\n",
    "        # read a image given a random integer index\n",
    "        AB_path = self.AB_paths[index]\n",
    "        AB = Image.open(AB_path).convert('RGB')\n",
    "        # split AB image into A and B\n",
    "        w, h = AB.size\n",
    "        w2 = int(w / 2)\n",
    "        A = AB.crop((0, 0, w2, h))\n",
    "        B = AB.crop((w2, 0, w, h))\n",
    "\n",
    "        # apply the same transform to both A and B\n",
    "        transform_params = get_params(self.opt, A.size)\n",
    "        A_transform = get_transform(self.opt, transform_params, grayscale=(self.input_nc == 1))\n",
    "        B_transform = get_transform(self.opt, transform_params, grayscale=(self.output_nc == 1))\n",
    "\n",
    "        A = A_transform(A)\n",
    "        B = B_transform(B)\n",
    "\n",
    "        return {'A': A, 'B': B, 'A_paths': AB_path, 'B_paths': AB_path}\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of images in the dataset.\"\"\"\n",
    "        return len(self.AB_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n",
    "\n",
    "Then, take a look in the loading model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(opt)      # create a model given opt.model and other options\n",
    "model.setup(opt)               # regular setup: load and print networks; create schedulers\n",
    "visualizer = Visualizer(opt)   # create a visualizer that display/save images and plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>create_model()</code> function create the model from user setting. If you select 'cycle_gan' (default), the code will go to <code>/model/cycle_gan_model.py</code>, and if you select 'pix2pix', the code will go to <code>/model/pix2pix_model.py</code>.\n",
    "\n",
    "Before seeing the code, let's looking the training zone. You will see that every function which related to the model works inside the model class. Thus, we can see inside model class and can understand how code work.\n",
    "\n",
    "#### Important function\n",
    "The functions which use in training process are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.update_learning_rate()    # update learning rates in the beginning of every epoch.\n",
    "model.set_input(data)         # unpack data from dataset and apply preprocessing\n",
    "model.optimize_parameters()   # calculate loss functions, get gradients, update network weights\n",
    "model.compute_visuals()\n",
    "model.save_networks(save_suffix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CycleGAN model\n",
    "\n",
    "Let's see <code>cycle_gan_model.py</code>, initialize function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, opt):\n",
    "    \"\"\"Initialize the CycleGAN class.\n",
    "\n",
    "    Parameters:\n",
    "        opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n",
    "    \"\"\"\n",
    "    BaseModel.__init__(self, opt)\n",
    "    # specify the training losses you want to print out. The training/test scripts will call <BaseModel.get_current_losses>\n",
    "    self.loss_names = ['D_A', 'G_A', 'cycle_A', 'idt_A', 'D_B', 'G_B', 'cycle_B', 'idt_B']\n",
    "    # specify the images you want to save/display. The training/test scripts will call <BaseModel.get_current_visuals>\n",
    "    visual_names_A = ['real_A', 'fake_B', 'rec_A']\n",
    "    visual_names_B = ['real_B', 'fake_A', 'rec_B']\n",
    "    if self.isTrain and self.opt.lambda_identity > 0.0:  # if identity loss is used, we also visualize idt_B=G_A(B) ad idt_A=G_A(B)\n",
    "        visual_names_A.append('idt_B')\n",
    "        visual_names_B.append('idt_A')\n",
    "\n",
    "    self.visual_names = visual_names_A + visual_names_B  # combine visualizations for A and B\n",
    "    # specify the models you want to save to the disk. The training/test scripts will call <BaseModel.save_networks> and <BaseModel.load_networks>.\n",
    "    if self.isTrain:\n",
    "        self.model_names = ['G_A', 'G_B', 'D_A', 'D_B']\n",
    "    else:  # during test time, only load Gs\n",
    "        self.model_names = ['G_A', 'G_B']\n",
    "\n",
    "    # define networks (both Generators and discriminators)\n",
    "    # The naming is different from those used in the paper.\n",
    "    # Code (vs. paper): G_A (G), G_B (F), D_A (D_Y), D_B (D_X)\n",
    "    self.netG_A = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, opt.norm,\n",
    "                                    not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\n",
    "    self.netG_B = networks.define_G(opt.output_nc, opt.input_nc, opt.ngf, opt.netG, opt.norm,\n",
    "                                    not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\n",
    "\n",
    "    if self.isTrain:  # define discriminators\n",
    "        self.netD_A = networks.define_D(opt.output_nc, opt.ndf, opt.netD,\n",
    "                                        opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids)\n",
    "        self.netD_B = networks.define_D(opt.input_nc, opt.ndf, opt.netD,\n",
    "                                        opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids)\n",
    "\n",
    "    if self.isTrain:\n",
    "        if opt.lambda_identity > 0.0:  # only works when input and output images have the same number of channels\n",
    "            assert(opt.input_nc == opt.output_nc)\n",
    "        self.fake_A_pool = ImagePool(opt.pool_size)  # create image buffer to store previously generated images\n",
    "        self.fake_B_pool = ImagePool(opt.pool_size)  # create image buffer to store previously generated images\n",
    "        # define loss functions\n",
    "        self.criterionGAN = networks.GANLoss(opt.gan_mode).to(self.device)  # define GAN loss.\n",
    "        self.criterionCycle = torch.nn.L1Loss()\n",
    "        self.criterionIdt = torch.nn.L1Loss()\n",
    "        # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n",
    "        self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "        self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "        self.optimizers.append(self.optimizer_G)\n",
    "        self.optimizers.append(self.optimizer_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator and Discriminator\n",
    "\n",
    "CycleGANs contain two generators and two discriminators, often with the same structure:\n",
    "- G_A = G\n",
    "- G_B = F\n",
    "- D_A = D_Y\n",
    "- D_B = D_X\n",
    "\n",
    "<img src=\"img/CycleGANArc.png\" title=\"CycleGAN\" style=\"width: 300px;\" />\n",
    "\n",
    "In the default mode, the generators use 9 ResNet blocks and the discriminators use 3 basic CNN blocks.\n",
    "\n",
    "#### The generator\n",
    "\n",
    "The generator code is in <code>/model/networks.py</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_G(input_nc, output_nc, ngf, netG, norm='batch', use_dropout=False, init_type='normal', init_gain=0.02, gpu_ids=[]):\n",
    "    \"\"\"Create a generator\n",
    "\n",
    "    Parameters:\n",
    "        input_nc (int) -- the number of channels in input images\n",
    "        output_nc (int) -- the number of channels in output images\n",
    "        ngf (int) -- the number of filters in the last conv layer\n",
    "        netG (str) -- the architecture's name: resnet_9blocks | resnet_6blocks | unet_256 | unet_128\n",
    "        norm (str) -- the name of normalization layers used in the network: batch | instance | none\n",
    "        use_dropout (bool) -- if use dropout layers.\n",
    "        init_type (str)    -- the name of our initialization method.\n",
    "        init_gain (float)  -- scaling factor for normal, xavier and orthogonal.\n",
    "        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2\n",
    "\n",
    "    Returns a generator\n",
    "\n",
    "    Our current implementation provides two types of generators:\n",
    "        U-Net: [unet_128] (for 128x128 input images) and [unet_256] (for 256x256 input images)\n",
    "        The original U-Net paper: https://arxiv.org/abs/1505.04597\n",
    "\n",
    "        Resnet-based generator: [resnet_6blocks] (with 6 Resnet blocks) and [resnet_9blocks] (with 9 Resnet blocks)\n",
    "        Resnet-based generator consists of several Resnet blocks between a few downsampling/upsampling operations.\n",
    "        We adapt Torch code from Justin Johnson's neural style transfer project (https://github.com/jcjohnson/fast-neural-style).\n",
    "\n",
    "\n",
    "    The generator has been initialized by <init_net>. It uses RELU for non-linearity.\n",
    "    \"\"\"\n",
    "    net = None\n",
    "    norm_layer = get_norm_layer(norm_type=norm)\n",
    "\n",
    "    if netG == 'resnet_9blocks':\n",
    "        net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=9)\n",
    "    elif netG == 'resnet_6blocks':\n",
    "        net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=6)\n",
    "    elif netG == 'unet_128':\n",
    "        net = UnetGenerator(input_nc, output_nc, 7, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "    elif netG == 'unet_256':\n",
    "        net = UnetGenerator(input_nc, output_nc, 8, ngf, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "    else:\n",
    "        raise NotImplementedError('Generator model name [%s] is not recognized' % netG)\n",
    "    return init_net(net, init_type, init_gain, gpu_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the resnet generator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetGenerator(nn.Module):\n",
    "    \"\"\"Resnet-based generator that consists of Resnet blocks between a few downsampling/upsampling operations.\n",
    "\n",
    "    We adapt Torch code and idea from Justin Johnson's neural style transfer project(https://github.com/jcjohnson/fast-neural-style)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, padding_type='reflect'):\n",
    "        \"\"\"Construct a Resnet-based generator\n",
    "\n",
    "        Parameters:\n",
    "            input_nc (int)      -- the number of channels in input images\n",
    "            output_nc (int)     -- the number of channels in output images\n",
    "            ngf (int)           -- the number of filters in the last conv layer\n",
    "            norm_layer          -- normalization layer\n",
    "            use_dropout (bool)  -- if use dropout layers\n",
    "            n_blocks (int)      -- the number of ResNet blocks\n",
    "            padding_type (str)  -- the name of padding layer in conv layers: reflect | replicate | zero\n",
    "        \"\"\"\n",
    "        assert(n_blocks >= 0)\n",
    "        super(ResnetGenerator, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        model = [nn.ReflectionPad2d(3),\n",
    "                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n",
    "                 norm_layer(ngf),\n",
    "                 nn.ReLU(True)]\n",
    "\n",
    "        n_downsampling = 2\n",
    "        for i in range(n_downsampling):  # add downsampling layers\n",
    "            mult = 2 ** i\n",
    "            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n",
    "                      norm_layer(ngf * mult * 2),\n",
    "                      nn.ReLU(True)]\n",
    "\n",
    "        mult = 2 ** n_downsampling\n",
    "        for i in range(n_blocks):       # add ResNet blocks\n",
    "\n",
    "            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n",
    "\n",
    "        for i in range(n_downsampling):  # add upsampling layers\n",
    "            mult = 2 ** (n_downsampling - i)\n",
    "            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n",
    "                                         kernel_size=3, stride=2,\n",
    "                                         padding=1, output_padding=1,\n",
    "                                         bias=use_bias),\n",
    "                      norm_layer(int(ngf * mult / 2)),\n",
    "                      nn.ReLU(True)]\n",
    "        model += [nn.ReflectionPad2d(3)]\n",
    "        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n",
    "        model += [nn.Tanh()]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward\"\"\"\n",
    "        return self.model(input)\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    \"\"\"Define a Resnet block\"\"\"\n",
    "\n",
    "    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "        \"\"\"Initialize the Resnet block\n",
    "\n",
    "        A resnet block is a conv block with skip connections\n",
    "        We construct a conv block with build_conv_block function,\n",
    "        and implement skip connections in <forward> function.\n",
    "        Original Resnet paper: https://arxiv.org/pdf/1512.03385.pdf\n",
    "        \"\"\"\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
    "\n",
    "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "        \"\"\"Construct a convolutional block.\n",
    "\n",
    "        Parameters:\n",
    "            dim (int)           -- the number of channels in the conv layer.\n",
    "            padding_type (str)  -- the name of padding layer: reflect | replicate | zero\n",
    "            norm_layer          -- normalization layer\n",
    "            use_dropout (bool)  -- if use dropout layers.\n",
    "            use_bias (bool)     -- if the conv layer uses bias or not\n",
    "\n",
    "        Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU))\n",
    "        \"\"\"\n",
    "        conv_block = []\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim), nn.ReLU(True)]\n",
    "        if use_dropout:\n",
    "            conv_block += [nn.Dropout(0.5)]\n",
    "\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim)]\n",
    "\n",
    "        return nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function (with skip connections)\"\"\"\n",
    "        out = x + self.conv_block(x)  # add skip connections\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The discriminator\n",
    "\n",
    "For discriminator, the function <code>define_D</code> is shown as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_D(input_nc, ndf, netD, n_layers_D=3, norm='batch', init_type='normal', init_gain=0.02, gpu_ids=[]):\n",
    "    \"\"\"Create a discriminator\n",
    "\n",
    "    Parameters:\n",
    "        input_nc (int)     -- the number of channels in input images\n",
    "        ndf (int)          -- the number of filters in the first conv layer\n",
    "        netD (str)         -- the architecture's name: basic | n_layers | pixel\n",
    "        n_layers_D (int)   -- the number of conv layers in the discriminator; effective when netD=='n_layers'\n",
    "        norm (str)         -- the type of normalization layers used in the network.\n",
    "        init_type (str)    -- the name of the initialization method.\n",
    "        init_gain (float)  -- scaling factor for normal, xavier and orthogonal.\n",
    "        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2\n",
    "\n",
    "    Returns a discriminator\n",
    "\n",
    "    Our current implementation provides three types of discriminators:\n",
    "        [basic]: 'PatchGAN' classifier described in the original pix2pix paper.\n",
    "        It can classify whether 70×70 overlapping patches are real or fake.\n",
    "        Such a patch-level discriminator architecture has fewer parameters\n",
    "        than a full-image discriminator and can work on arbitrarily-sized images\n",
    "        in a fully convolutional fashion.\n",
    "\n",
    "        [n_layers]: With this mode, you can specify the number of conv layers in the discriminator\n",
    "        with the parameter <n_layers_D> (default=3 as used in [basic] (PatchGAN).)\n",
    "\n",
    "        [pixel]: 1x1 PixelGAN discriminator can classify whether a pixel is real or not.\n",
    "        It encourages greater color diversity but has no effect on spatial statistics.\n",
    "\n",
    "    The discriminator has been initialized by <init_net>. It uses Leakly RELU for non-linearity.\n",
    "    \"\"\"\n",
    "    net = None\n",
    "    norm_layer = get_norm_layer(norm_type=norm)\n",
    "\n",
    "    if netD == 'basic':  # default PatchGAN classifier\n",
    "        net = NLayerDiscriminator(input_nc, ndf, n_layers=3, norm_layer=norm_layer)\n",
    "    elif netD == 'n_layers':  # more options\n",
    "        net = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer)\n",
    "    elif netD == 'pixel':     # classify if each pixel is real or fake\n",
    "        net = PixelDiscriminator(input_nc, ndf, norm_layer=norm_layer)\n",
    "    else:\n",
    "        raise NotImplementedError('Discriminator model name [%s] is not recognized' % netD)\n",
    "    return init_net(net, init_type, init_gain, gpu_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is <code>NLayerDiscriminator</code> function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLayerDiscriminator(nn.Module):\n",
    "    \"\"\"Defines a PatchGAN discriminator\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n",
    "        \"\"\"Construct a PatchGAN discriminator\n",
    "\n",
    "        Parameters:\n",
    "            input_nc (int)  -- the number of channels in input images\n",
    "            ndf (int)       -- the number of filters in the last conv layer\n",
    "            n_layers (int)  -- the number of conv layers in the discriminator\n",
    "            norm_layer      -- normalization layer\n",
    "        \"\"\"\n",
    "        super(NLayerDiscriminator, self).__init__()\n",
    "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "\n",
    "        kw = 4\n",
    "        padw = 1\n",
    "        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "        for n in range(1, n_layers):  # gradually increase the number of filters\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2 ** n, 8)\n",
    "            sequence += [\n",
    "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n",
    "                norm_layer(ndf * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2 ** n_layers, 8)\n",
    "        sequence += [\n",
    "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n",
    "            norm_layer(ndf * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n",
    "        self.model = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward.\"\"\"\n",
    "        return self.model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "For loss GAN function (default mode), they use **lsgan** or **MSELoss**. The function is in <code>networks.py</code>.\n",
    "\n",
    "The other losses, i.e., Cycle loss and Identity loss, use <code>L1Loss()</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the line self.criterionGAN = networks.GANLoss(opt.gan_mode).to(self.device)  # define GAN loss.\n",
    "\n",
    "class GANLoss(nn.Module):\n",
    "    \"\"\"Define different GAN objectives.\n",
    "\n",
    "    The GANLoss class abstracts away the need to create the target label tensor\n",
    "    that has the same size as the input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0):\n",
    "        \"\"\" Initialize the GANLoss class.\n",
    "\n",
    "        Parameters:\n",
    "            gan_mode (str) - - the type of GAN objective. It currently supports vanilla, lsgan, and wgangp.\n",
    "            target_real_label (bool) - - label for a real image\n",
    "            target_fake_label (bool) - - label of a fake image\n",
    "\n",
    "        Note: Do not use sigmoid as the last layer of Discriminator.\n",
    "        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.\n",
    "        \"\"\"\n",
    "        super(GANLoss, self).__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(target_real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(target_fake_label))\n",
    "        self.gan_mode = gan_mode\n",
    "        if gan_mode == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "        elif gan_mode == 'vanilla':\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        elif gan_mode in ['wgangp']:\n",
    "            self.loss = None\n",
    "        else:\n",
    "            raise NotImplementedError('gan mode %s not implemented' % gan_mode)\n",
    "\n",
    "    def get_target_tensor(self, prediction, target_is_real):\n",
    "        \"\"\"Create label tensors with the same size as the input.\n",
    "\n",
    "        Parameters:\n",
    "            prediction (tensor) - - tpyically the prediction from a discriminator\n",
    "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
    "\n",
    "        Returns:\n",
    "            A label tensor filled with ground truth label, and with the size of the input\n",
    "        \"\"\"\n",
    "\n",
    "        if target_is_real:\n",
    "            target_tensor = self.real_label\n",
    "        else:\n",
    "            target_tensor = self.fake_label\n",
    "        return target_tensor.expand_as(prediction)\n",
    "\n",
    "    def __call__(self, prediction, target_is_real):\n",
    "        \"\"\"Calculate loss given Discriminator's output and grount truth labels.\n",
    "\n",
    "        Parameters:\n",
    "            prediction (tensor) - - tpyically the prediction output from a discriminator\n",
    "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
    "\n",
    "        Returns:\n",
    "            the calculated loss.\n",
    "        \"\"\"\n",
    "        if self.gan_mode in ['lsgan', 'vanilla']:\n",
    "            target_tensor = self.get_target_tensor(prediction, target_is_real)\n",
    "            loss = self.loss(prediction, target_tensor)\n",
    "        elif self.gan_mode == 'wgangp':\n",
    "            if target_is_real:\n",
    "                loss = -prediction.mean()\n",
    "            else:\n",
    "                loss = prediction.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "From the code, both generator and discriminator use **ADAM optimization**. There are 4 optimizers but they store in chain for generator and chain for discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()), lr=opt.lr, betas=(opt.beta1, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set_input function\n",
    "\n",
    "The <code>set_input()</code> function is used for setup input of image A and image B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_input(self, input):\n",
    "    \"\"\"Unpack input data from the dataloader and perform necessary pre-processing steps.\n",
    "\n",
    "    Parameters:\n",
    "        input (dict): include the data itself and its metadata information.\n",
    "\n",
    "    The option 'direction' can be used to swap domain A and domain B.\n",
    "    \"\"\"\n",
    "    AtoB = self.opt.direction == 'AtoB'\n",
    "    self.real_A = input['A' if AtoB else 'B'].to(self.device)\n",
    "    self.real_B = input['B' if AtoB else 'A'].to(self.device)\n",
    "    self.image_paths = input['A_paths' if AtoB else 'B_paths']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step function\n",
    "\n",
    "The training step is in <code>optimize_parameters</code> function. The function explain the step by step of cycle GAN training as:\n",
    "\n",
    "**Note**: Given $fake$ as fake image, $real$ as real image, and rec as rectified image. $A$ and $B$ is the source and destination of 2 kinds of dataset. $G$ is a generator model, and $D$ is a discriminator model.\n",
    "\n",
    "1. Do forward propagation (function <code>forward()</code>) - This step is the step of cycle gan generating. The process is:\n",
    "    1. Create <code>fake_B</code> from generator A with input <code>real_A</code>\n",
    "        - $fake_B = G_A(real_A)$\n",
    "    2. Create <code>rec_A</code> from generator B with input <code>fake_B</code>\n",
    "        - $rec_A = G_B(fake_A)$\n",
    "    3. Create <code>fake_A</code> from generator B with input <code>real_B</code>\n",
    "        - $fake_A = G_B(real_B)$\n",
    "    4. Create <code>rec_B</code> from generator A with input <code>fake_A</code>\n",
    "        - $rec_B = G_A(fake_A)$\n",
    "2. Do back propagation of G (function <code>backward_G()</code>) - This step is to calculate gradients for both generators and update their weight. The process is:\n",
    "    1. Calculate identity loss:\n",
    "        - $\\mathcal{L}_{idt_A} = \\|G_A(real_B) - real_B\\|$\n",
    "        - $\\mathcal{L}_{idt_B} = \\|G_B(real_A) - real_A\\|$\n",
    "    2. Calculate GAN loss (Use discriminator here):\n",
    "        - $\\mathcal{L}_{G_A} = \\text{MSE}(D_A(fake_B))$\n",
    "        - $\\mathcal{L}_{G_B} = \\text{MSE}(D_B(fake_A))$\n",
    "    3. Calculate Cycle loss:\n",
    "        - $\\mathcal{L}_{cyc_A} = \\| rec_A - real_A\\|$\n",
    "        - $\\mathcal{L}_{cyc_B} = \\| rec_A - real_B\\|$\n",
    "    4. Sum all of losses to be $loss_G$ and back propation from the loss\n",
    "3. Do back propagation of D (function <code>backward_D_A</code> and <code>backward_D_B</code> - This step is to calculate loss of discriminator and update their weight.\n",
    "    - $\\mathcal{L}_{D_A} = \\text{MSE}(D_A(real_B)) + \\text{MSE}(1 - D_A(fake_B))$\n",
    "    - $\\mathcal{L}_{D_B} = \\text{MSE}(D_B(real_A)) + \\text{MSE}(1 - D_B(fake_A))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optimize_parameters function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_parameters(self):\n",
    "    \"\"\"Calculate losses, gradients, and update network weights; called in every training iteration\"\"\"\n",
    "    # forward\n",
    "    self.forward()      # compute fake images and reconstruction images.\n",
    "    # G_A and G_B\n",
    "    self.set_requires_grad([self.netD_A, self.netD_B], False)  # Ds require no gradients when optimizing Gs\n",
    "    self.optimizer_G.zero_grad()  # set G_A and G_B's gradients to zero\n",
    "    self.backward_G()             # calculate gradients for G_A and G_B\n",
    "    self.optimizer_G.step()       # update G_A and G_B's weights\n",
    "    # D_A and D_B\n",
    "    self.set_requires_grad([self.netD_A, self.netD_B], True)\n",
    "    self.optimizer_D.zero_grad()   # set D_A and D_B's gradients to zero\n",
    "    self.backward_D_A()      # calculate gradients for D_A\n",
    "    self.backward_D_B()      # calculate graidents for D_B\n",
    "    self.optimizer_D.step()  # update D_A and D_B's weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self):\n",
    "    \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n",
    "    self.fake_B = self.netG_A(self.real_A)  # G_A(A)\n",
    "    self.rec_A = self.netG_B(self.fake_B)   # G_B(G_A(A))\n",
    "    self.fake_A = self.netG_B(self.real_B)  # G_B(B)\n",
    "    self.rec_B = self.netG_A(self.fake_A)   # G_A(G_B(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### backward_G function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_G(self):\n",
    "    \"\"\"Calculate the loss for generators G_A and G_B\"\"\"\n",
    "    lambda_idt = self.opt.lambda_identity\n",
    "    lambda_A = self.opt.lambda_A\n",
    "    lambda_B = self.opt.lambda_B\n",
    "    # Identity loss\n",
    "    if lambda_idt > 0:\n",
    "        # G_A should be identity if real_B is fed: ||G_A(B) - B||\n",
    "        self.idt_A = self.netG_A(self.real_B)\n",
    "        self.loss_idt_A = self.criterionIdt(self.idt_A, self.real_B) * lambda_B * lambda_idt\n",
    "        # G_B should be identity if real_A is fed: ||G_B(A) - A||\n",
    "        self.idt_B = self.netG_B(self.real_A)\n",
    "        self.loss_idt_B = self.criterionIdt(self.idt_B, self.real_A) * lambda_A * lambda_idt\n",
    "    else:\n",
    "        self.loss_idt_A = 0\n",
    "        self.loss_idt_B = 0\n",
    "\n",
    "    # GAN loss D_A(G_A(A))\n",
    "    self.loss_G_A = self.criterionGAN(self.netD_A(self.fake_B), True)\n",
    "    # GAN loss D_B(G_B(B))\n",
    "    self.loss_G_B = self.criterionGAN(self.netD_B(self.fake_A), True)\n",
    "    # Forward cycle loss || G_B(G_A(A)) - A||\n",
    "    self.loss_cycle_A = self.criterionCycle(self.rec_A, self.real_A) * lambda_A\n",
    "    # Backward cycle loss || G_A(G_B(B)) - B||\n",
    "    self.loss_cycle_B = self.criterionCycle(self.rec_B, self.real_B) * lambda_B\n",
    "    # combined loss and calculate gradients\n",
    "    self.loss_G = self.loss_G_A + self.loss_G_B + self.loss_cycle_A + self.loss_cycle_B + self.loss_idt_A + self.loss_idt_B\n",
    "    self.loss_G.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### backward_D function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_D_A(self):\n",
    "    \"\"\"Calculate GAN loss for discriminator D_A\"\"\"\n",
    "    fake_B = self.fake_B_pool.query(self.fake_B)\n",
    "    self.loss_D_A = self.backward_D_basic(self.netD_A, self.real_B, fake_B)\n",
    "\n",
    "def backward_D_B(self):\n",
    "    \"\"\"Calculate GAN loss for discriminator D_B\"\"\"\n",
    "    fake_A = self.fake_A_pool.query(self.fake_A)\n",
    "    self.loss_D_B = self.backward_D_basic(self.netD_B, self.real_A, fake_A)\n",
    "\n",
    "def backward_D_basic(self, netD, real, fake):\n",
    "    \"\"\"Calculate GAN loss for the discriminator\n",
    "\n",
    "    Parameters:\n",
    "        netD (network)      -- the discriminator D\n",
    "        real (tensor array) -- real images\n",
    "        fake (tensor array) -- images generated by a generator\n",
    "\n",
    "    Return the discriminator loss.\n",
    "    We also call loss_D.backward() to calculate the gradients.\n",
    "    \"\"\"\n",
    "    # Real\n",
    "    pred_real = netD(real)\n",
    "    loss_D_real = self.criterionGAN(pred_real, True)\n",
    "    # Fake\n",
    "    pred_fake = netD(fake.detach())\n",
    "    loss_D_fake = self.criterionGAN(pred_fake, False)\n",
    "    # Combined loss and calculate gradients\n",
    "    loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "    loss_D.backward()\n",
    "    return loss_D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pix2Pix or Image-to-Image translation\n",
    "\n",
    "Reference: https://towardsdatascience.com/pix2pix-869c17900998\n",
    "\n",
    "Image-to-Image translation is one of the GANs tasks.\n",
    "It is used for image synthesis, which translates from a source image as a random vector z to be the target image.\n",
    "Pix2Pix can convert an image such as edges of an object, sematic segmentation, and normal image to the photo image, or edges, segmented image.\n",
    "\n",
    "The application of Image-to-Image translation are:\n",
    "- Colorization\n",
    "- Super-resolution\n",
    "- Image to drawing or drawing to image\n",
    "- Sematic segmentation\n",
    "\n",
    "Most of all applications are image synthesis.\n",
    "\n",
    "<img src=\"img/pix2pix1.png\" title=\"Pix2Pix\" style=\"width: 400px;\" />\n",
    "\n",
    "<img src=\"img/pix2pix2.png\" title=\"Pix2Pix\" style=\"width: 400px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The process of Pix2Pix\n",
    "\n",
    "The steps of the Pix2Pix are very simple. In training process, the steps are:\n",
    "1. Use a source image as input to the generator to let the generator create a target fake image.\n",
    "2. Overlay the target real/fake image with the source image and feed them into the discriminator to let the discriminator to predict that it is real or fake.\n",
    "3. Calculate loss and update their weight like normal GANs\n",
    "\n",
    "<img src=\"img/pix2pix3.png\" title=\"Pix2Pix\" style=\"width: 640px;\" />\n",
    "\n",
    "When the users want to use it in testing mode, they just bring the generator and discard the discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "The loss function of Discriminator is:\n",
    "\n",
    "$$\\mathcal{L}_D = \\mathbb{E}_{x,y}[\\log D(x,y)] + \\mathbb{E}_{x,z}[1 - \\log D(x,G(x,z))].$$\n",
    "\n",
    "**Note**: Sometime, the input has noise $z$. If there is no noise vector, leave it blank.\n",
    "\n",
    "The loss function of Generator contains 2 parts, GAN loss and L1 Loss. The L1 loss is:\n",
    "\n",
    "$$\\mathcal{L}_{L1} = \\mathbb{E}_{x,y,z}[\\|y-G(x,z)\\|],$$\n",
    "\n",
    "and the overall of generator loss is:\n",
    "\n",
    "$$\\mathcal{L}_G = \\mathbb{E}_{x,z}[\\log D(x,G(x,z))] + \\lambda \\mathcal{L}_{L1}.$$\n",
    "\n",
    "When $\\lambda$ is the hyper-parameters, and the author reported the most success with the lambda parameter equal to 100. (From TA: Oh my gosh, I used 0.1 for my experiment with other generator. 0_o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator networks\n",
    "\n",
    "The Generator in Pix2Pix use an auto-Encoder Network. \n",
    "The Generator takes in the Image to be translated and compresses it into a low-dimensional, “Bottleneck”, vector representation. The Generator then learns how to upsample this into the output image.\n",
    "In the paper, the author used U-Net256 as the generator.\n",
    "\n",
    "U-Net is similar to ResNets in the way that information from earlier layers are integrated into later layers. The U-Net skip connections are also interesting because they do not require any resizing, projections etc. since the spatial resolution of the layers being connected already match each other.\n",
    "\n",
    "<img src=\"img/u-net-architecture.png\" title=\"U-Net\" style=\"width: 640px;\" />\n",
    "\n",
    "Source: https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator networks (PatchGAN)\n",
    "\n",
    "The PatchGAN discriminator used in pix2pix is another unique component to this design. The PatchGAN / Markovian discriminator works by classifying individual (N x N) patches in the image as “real vs. fake”, opposed to classifying the entire image as “real vs. fake”. The authors reason that this enforces more constraints that encourage sharp high-frequency detail. Additionally, the PatchGAN has fewer parameters and runs faster than classifying the entire image. The image below depicts results experimenting with the size of N for the N x N patches to be classified:\n",
    "\n",
    "<img src=\"img/patchgan.png\" title=\"PatchGAN\" style=\"width: 640px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the code\n",
    "\n",
    "#### Datasets\n",
    "\n",
    "Download one of the official datasets with:\n",
    "\n",
    "    bash ./datasets/download_pix2pix_dataset.sh [cityscapes, night2day, edges2handbags, edges2shoes, facades, maps]\n",
    "\n",
    "Or use your own dataset by creating the appropriate folders and adding in the images. Follow the instructions [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/datasets.md#pix2pix-datasets).\n",
    "\n",
    "### Training\n",
    "\n",
    "The command in **Terminal 2** is:\n",
    "\n",
    "    python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA\n",
    "    \n",
    "and you can use visdom.server too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking into the code (Pix2Pix)\n",
    "\n",
    "Because we have explained the code about dataset and the main. Let's take a look only <code>model/pix2pix_model.py</code>.\n",
    "\n",
    "First, take a look into initialize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, opt):\n",
    "        \"\"\"Initialize the pix2pix class.\n",
    "\n",
    "        Parameters:\n",
    "            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n",
    "        \"\"\"\n",
    "        BaseModel.__init__(self, opt)\n",
    "        # specify the training losses you want to print out. The training/test scripts will call <BaseModel.get_current_losses>\n",
    "        self.loss_names = ['G_GAN', 'G_L1', 'D_real', 'D_fake']\n",
    "        # specify the images you want to save/display. The training/test scripts will call <BaseModel.get_current_visuals>\n",
    "        self.visual_names = ['real_A', 'fake_B', 'real_B']\n",
    "        # specify the models you want to save to the disk. The training/test scripts will call <BaseModel.save_networks> and <BaseModel.load_networks>\n",
    "        if self.isTrain:\n",
    "            self.model_names = ['G', 'D']\n",
    "        else:  # during test time, only load G\n",
    "            self.model_names = ['G']\n",
    "        # define networks (both generator and discriminator)\n",
    "        self.netG = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, opt.norm,\n",
    "                                      not opt.no_dropout, opt.init_type, opt.init_gain, self.gpu_ids)\n",
    "\n",
    "        if self.isTrain:  # define a discriminator; conditional GANs need to take both input and output images; Therefore, #channels for D is input_nc + output_nc\n",
    "            self.netD = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf, opt.netD,\n",
    "                                          opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids)\n",
    "\n",
    "        if self.isTrain:\n",
    "            # define loss functions\n",
    "            self.criterionGAN = networks.GANLoss(opt.gan_mode).to(self.device)\n",
    "            self.criterionL1 = torch.nn.L1Loss()\n",
    "            # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n",
    "            self.optimizer_G = torch.optim.Adam(self.netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "            self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n",
    "            self.optimizers.append(self.optimizer_G)\n",
    "            self.optimizers.append(self.optimizer_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of step process are the same as cycle gan, but for the discriminator, the input layers size is the combination between image input channels and output channels.\n",
    "\n",
    "    opt.input_nc + opt.output_nc\n",
    "    \n",
    "And for the generator, the pix2pix class has change the default of generator model to be **UNET-256** and the loss GAN is vanilla GAN which means it use **log sigmoid** for calculation in loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_commandline_options(parser, is_train=True):\n",
    "    \"\"\"Add new dataset-specific options, and rewrite default values for existing options.\n",
    "\n",
    "    Parameters:\n",
    "        parser          -- original option parser\n",
    "        is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n",
    "\n",
    "    Returns:\n",
    "        the modified parser.\n",
    "\n",
    "    For pix2pix, we do not use image buffer\n",
    "    The training objective is: GAN Loss + lambda_L1 * ||G(A)-B||_1\n",
    "    By default, we use vanilla GAN loss, UNet with batchnorm, and aligned datasets.\n",
    "    \"\"\"\n",
    "    # changing the default values to match the pix2pix paper (https://phillipi.github.io/pix2pix/)\n",
    "    parser.set_defaults(norm='batch', netG='unet_256', dataset_mode='aligned')\n",
    "    if is_train:\n",
    "        parser.set_defaults(pool_size=0, gan_mode='vanilla')\n",
    "        parser.add_argument('--lambda_L1', type=float, default=100.0, help='weight for L1 loss')\n",
    "\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input images\n",
    "\n",
    "For pix2pix, it can convert from A to B and B to A too. Thus, we swap the input in here.\n",
    "- If 'AtoB', $real_A = real_A$ and $real_B = real_B$ (no change)\n",
    "- If 'BtoA', $real_A = real_B$ and $real_A = real_B$ (swap them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_input(self, input):\n",
    "    \"\"\"Unpack input data from the dataloader and perform necessary pre-processing steps.\n",
    "\n",
    "    Parameters:\n",
    "        input (dict): include the data itself and its metadata information.\n",
    "\n",
    "    The option 'direction' can be used to swap images in domain A and domain B.\n",
    "    \"\"\"\n",
    "    AtoB = self.opt.direction == 'AtoB'\n",
    "    self.real_A = input['A' if AtoB else 'B'].to(self.device)\n",
    "    self.real_B = input['B' if AtoB else 'A'].to(self.device)\n",
    "    self.image_paths = input['A_paths' if AtoB else 'B_paths']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network (U-Net class)\n",
    "\n",
    "The u-net class is in <code>networks.py</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetGenerator(nn.Module):\n",
    "    \"\"\"Create a Unet-based generator\"\"\"\n",
    "\n",
    "    def __init__(self, input_nc, output_nc, num_downs, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        \"\"\"Construct a Unet generator\n",
    "        Parameters:\n",
    "            input_nc (int)  -- the number of channels in input images\n",
    "            output_nc (int) -- the number of channels in output images\n",
    "            num_downs (int) -- the number of downsamplings in UNet. For example, # if |num_downs| == 7,\n",
    "                                image of size 128x128 will become of size 1x1 # at the bottleneck\n",
    "            ngf (int)       -- the number of filters in the last conv layer\n",
    "            norm_layer      -- normalization layer\n",
    "\n",
    "        We construct the U-Net from the innermost layer to the outermost layer.\n",
    "        It is a recursive process.\n",
    "        \"\"\"\n",
    "        super(UnetGenerator, self).__init__()\n",
    "        # construct unet structure\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)  # add the innermost layer\n",
    "        for i in range(num_downs - 5):          # add intermediate layers with ngf * 8 filters\n",
    "            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
    "        # gradually reduce the number of filters from ngf * 8 to ngf\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
    "        self.model = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer)  # add the outermost layer\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"Standard forward\"\"\"\n",
    "        return self.model(input)\n",
    "\n",
    "\n",
    "class UnetSkipConnectionBlock(nn.Module):\n",
    "    \"\"\"Defines the Unet submodule with skip connection.\n",
    "        X -------------------identity----------------------\n",
    "        |-- downsampling -- |submodule| -- upsampling --|\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, outer_nc, inner_nc, input_nc=None,\n",
    "                 submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False):\n",
    "        \"\"\"Construct a Unet submodule with skip connections.\n",
    "\n",
    "        Parameters:\n",
    "            outer_nc (int) -- the number of filters in the outer conv layer\n",
    "            inner_nc (int) -- the number of filters in the inner conv layer\n",
    "            input_nc (int) -- the number of channels in input images/features\n",
    "            submodule (UnetSkipConnectionBlock) -- previously defined submodules\n",
    "            outermost (bool)    -- if this module is the outermost module\n",
    "            innermost (bool)    -- if this module is the innermost module\n",
    "            norm_layer          -- normalization layer\n",
    "            use_dropout (bool)  -- if use dropout layers.\n",
    "        \"\"\"\n",
    "        super(UnetSkipConnectionBlock, self).__init__()\n",
    "        self.outermost = outermost\n",
    "        if type(norm_layer) == functools.partial:\n",
    "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
    "        else:\n",
    "            use_bias = norm_layer == nn.InstanceNorm2d\n",
    "        if input_nc is None:\n",
    "            input_nc = outer_nc\n",
    "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n",
    "                             stride=2, padding=1, bias=use_bias)\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm = norm_layer(inner_nc)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = norm_layer(outer_nc)\n",
    "\n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1)\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
    "                                        kernel_size=4, stride=2,\n",
    "                                        padding=1, bias=use_bias)\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "\n",
    "            if use_dropout:\n",
    "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
    "            else:\n",
    "                model = down + [submodule] + up\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:   # add skip connections\n",
    "            return torch.cat([x, self.model(x)], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator network\n",
    "\n",
    "Use the same as CycleGANs which has 3 CNNs layers as default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training step function\n",
    "\n",
    "The training step is in <code>optimize_parameters</code> function. If you look inside and compare between CycleGAN and Pix2Pix, you can see that Pix2Pix is too simpler than CycleGAN a lot. The function explain the step by step of Pix2Pix training as:\n",
    "\n",
    "**Note**: Given $fake$ as fake image, $real$ as real image, and rec as rectified image. $A$ and $B$ is the source and destination of 2 kinds of dataset. $G$ is a generator model, and $D$ is a discriminator model.\n",
    "\n",
    "1. Do forward propagation (function <code>forward()</code>) - Create <code>fake_B</code> from generator A with input <code>real_A</code>\n",
    "    - $fake_B = G_A(real_A)$\n",
    "2. Do back propagation of G (function <code>backward_G()</code>) - This step is to calculate gradients for both generators and update their weight. The process is:\n",
    "    1. Concatenate A and B togeter:\n",
    "        - $real_{AB} = (real_A|real_B)$\n",
    "        - $fake_{AB} = (real_A|fake_B)$\n",
    "    2. Calculate GAN loss (Use discriminator here):\n",
    "        - $\\mathcal{L}_{G} = \\text{sigmoid} (D(fake_{AB}))$\n",
    "    3. Calculate L1 loss:\n",
    "        - $\\mathcal{L}_{L1} = \\|real_B-fake_B\\|$\n",
    "    4. Sum all of losses to be $loss_G$ and back propation from the loss\n",
    "        - $\\mathcal{L} = \\mathcal{L}_{G} + \\lambda \\mathcal{L}_{L1}$\n",
    "        - Note that $\\lambda = 100$\n",
    "3. Do back propagation of D (function <code>backward_D</code> - This step is to calculate loss of discriminator and update their weight.\n",
    "    - $\\mathcal{L}_{D} = \\text{sigmoid}(D(real_AB)) + \\text{sigmoid}(1 - D(fake_{AB}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### optimize_parameter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_parameters(self):\n",
    "    self.forward()                   # compute fake images: G(A)\n",
    "    # update D\n",
    "    self.set_requires_grad(self.netD, True)  # enable backprop for D\n",
    "    self.optimizer_D.zero_grad()     # set D's gradients to zero\n",
    "    self.backward_D()                # calculate gradients for D\n",
    "    self.optimizer_D.step()          # update D's weights\n",
    "    # update G\n",
    "    self.set_requires_grad(self.netD, False)  # D requires no gradients when optimizing G\n",
    "    self.optimizer_G.zero_grad()        # set G's gradients to zero\n",
    "    self.backward_G()                   # calculate graidents for G\n",
    "    self.optimizer_G.step()             # udpate G's weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self):\n",
    "    \"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"\n",
    "    self.fake_B = self.netG(self.real_A)  # G(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### backward_G function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_G(self):\n",
    "    \"\"\"Calculate GAN and L1 loss for the generator\"\"\"\n",
    "    # First, G(A) should fake the discriminator\n",
    "    fake_AB = torch.cat((self.real_A, self.fake_B), 1)\n",
    "    pred_fake = self.netD(fake_AB)\n",
    "    self.loss_G_GAN = self.criterionGAN(pred_fake, True)\n",
    "    # Second, G(A) = B\n",
    "    self.loss_G_L1 = self.criterionL1(self.fake_B, self.real_B) * self.opt.lambda_L1\n",
    "    # combine loss and calculate gradients\n",
    "    self.loss_G = self.loss_G_GAN + self.loss_G_L1\n",
    "    self.loss_G.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### backward_D function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_D(self):\n",
    "    \"\"\"Calculate GAN loss for the discriminator\"\"\"\n",
    "    # Fake; stop backprop to the generator by detaching fake_B\n",
    "    fake_AB = torch.cat((self.real_A, self.fake_B), 1)  # we use conditional GANs; we need to feed both input and output to the discriminator\n",
    "    pred_fake = self.netD(fake_AB.detach())\n",
    "    self.loss_D_fake = self.criterionGAN(pred_fake, False)\n",
    "    # Real\n",
    "    real_AB = torch.cat((self.real_A, self.real_B), 1)\n",
    "    pred_real = self.netD(real_AB)\n",
    "    self.loss_D_real = self.criterionGAN(pred_real, True)\n",
    "    # combine loss and calculate gradients\n",
    "    self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n",
    "    self.loss_D.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent experiments\n",
    "\n",
    "Do the following:\n",
    "1. Train a Cycle GAN on the `horses2zebras` dataset provided. Document your results in your report.\n",
    "2. Create a new data set `aitict2celeba` consisting of AIT ICT faces and CelebA faces. Use the URLs for the datasets provided in class. Document your results in your report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
