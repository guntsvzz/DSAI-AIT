{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 12: Gym installation and Actor Critic\n",
    "\n",
    "Today we'll explore deep reinforcement learning with policy gradient\n",
    "and actor-critic approaches.\n",
    "Here are some of the references used in today's lab:\n",
    "\n",
    "- https://gym.openai.com\n",
    "- Deep Reinforcement Learning Hands-On (Packtpub)\n",
    "- https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py\n",
    "- https://towardsdatascience.com/breaking-down-richard-suttons-policy-gradient-9768602cb63b\n",
    "- https://towardsdatascience.com/learning-reinforcement-learning-reinforce-with-pytorch-5e8ad7fc7da0\n",
    "- https://github.com/woithook/A2C-Pytorch-implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement learning\n",
    "\n",
    "Reinforcement Learning (RL) is a branch of machine learning in which we set up an agent to learn in an interactive environment by trial and error using feedback on its actions and experiences. RL uses a reward (or punishment) function to signal \"good\" and \"bad\" behavior.\n",
    "\n",
    "The RL setting has two main components:\n",
    "1. Environment\n",
    "2. Agent\n",
    "\n",
    "Generally, at each step, \n",
    "1. The **agent** outputs an ***action***, which is input to the **environment**.\n",
    "2. The **environment** evolves according to its dynamics, changing to a ***new state***.\n",
    "3. The **agent** observes the ***new state*** of the **environment** and (optionally) obtains a ***reward***\n",
    "\n",
    "The process continues until hopefully the agent learns what behavior maximizes its reward.\n",
    "\n",
    "<img src=\"img/RL.jpg\" title=\"Introduction\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to create and design the agent to be as smart as possible. However, an agent cannot learn without an environment. So first we will install OpenAI's Gym library for creating benchmark RL agent environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym\n",
    "\n",
    "Many of the popular simulation environments for RL are build on OpenAI's Gym library.\n",
    "\n",
    "[OpenAI](https://openai.com) is a research company trying to develop systems exhibiting *artificial general intelligence* (AGI).\n",
    "They developed Gym to support the development of RL algorithms. Gym ships with many reinforcement learning simulations and tasks. Visit [the Gym website](https://gym.openai.com) for a full list of environments.\n",
    "\n",
    "<img src=\"img/RL_gym.PNG\" title=\"Gym example\" style=\"width: 600px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Environment in your PC\n",
    "\n",
    "***Note***: you can use *google colab*, if your computer does not support.\n",
    "\n",
    "***Note2***: No need to follow this instruction, if you can do it, show your result to me is fine.\n",
    "\n",
    "**System requirement**: Ubuntu (Linux), Nvidia GPU (optional, really, but you may cry when try to run more complex agent)\n",
    "    \n",
    "Things to install for desktop version:\n",
    "1. mujoco_py library\n",
    "2. OpenAI library: gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windows\n",
    "\n",
    "We haven't tested on Windows, unfortunately, but the following links should help:\n",
    "1. Install [OpenAI Gym](https://towardsdatascience.com/how-to-install-openai-gym-in-a-windows-environment-338969e24d30)\n",
    "2. Install [OpenAI Gym with Box2D and Mujoco](https://medium.com/@sayanmndl21/install-openai-gym-with-box2d-and-mujoco-in-windows-10-e25ee9b5c1d5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linux (your local PC)\n",
    "\n",
    "The steps to install Gym seem to be constantly changing. The following steps currently work on Ubuntu Linux 20.04:\n",
    "\n",
    "1. `sudo apt-get update ; sudo apt-get upgrade ; sudo apt-get install libglew-dev patchelf libosmesa6-dev libgl1-mesa-glx libglfw3 wget swig`\n",
    "2. `wget https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz`\n",
    "3. `tar xvzf mujoco210-linux-x86_64.tar.gz`\n",
    "4. `mkdir -p ~/.mujoco ; mv mujoco210 ~/.mujoco/ ; rm mujoco210-linux-x86_64.tar.gz`\n",
    "5. `pip3 install gym`\n",
    "6. `pip3 install mujoco`\n",
    "7. Edit ~/.bashrc and add line `export LD_LIBRARY_PATH=$HOME/.mujoco/mujoco210/bin:/usr/lib/nvidia`\n",
    "8. `source ~/.bashrc`\n",
    "9. `python3`\n",
    "       >>> import mujoco\n",
    "       >>>\n",
    "10. `pip3 install gym pyvirtualdisplay`\n",
    "11. `pip3 install gym[atari,accept-rom-license]`\n",
    "12. `pip3 install gym[Robotics,classic_control]`\n",
    "13. `pip3 install gym-robotics`\n",
    "14. `pip3 install box2d-py`\n",
    "15. `pip3 install gym[Box_2D]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative (if mujoco-py doesn't work)\n",
    "\n",
    "If the above method does not work, the `free-mujoco-py` library may work instead:\n",
    "1. `pip3 uninstall mujoco-py`\n",
    "2. `pip3 install free-mujoco-py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Test it!***: After installing Gym as above, try this code.\n",
    "If there is any error, you should solve the problem or use the free Mujoco alternative above instead.\n",
    "This code will take a long time to compile libraries the first time you run it but should be fast after the first try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "\n",
    "env = gym.make(\"FetchPickAndPlace-v1\")\n",
    "env.reset()\n",
    "env.render()\n",
    "time.sleep(5)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colab\n",
    "\n",
    "If you cannot setup on your own computer, you can use Google Colab to and train in this class. Unfortunately, you have to re-run the\n",
    "installation steps every time your environment shuts down (12 hours for the free version and 24 hours for the Pro version).\n",
    "\n",
    "You can access Google colab from this [colab](https://colab.research.google.com/). Sign in as your e-mail and it will ready to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a Google colab file and copy the code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Install xvfb & other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install x11-utils > /dev/null 2>&1 \n",
    "!pip install pyglet > /dev/null 2>&1 \n",
    "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Install mujoco\n",
    "This step used for 3D simulators. Ex. Robots, and some control plants\n",
    "\n",
    "In some simulators, it does not require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include this at the top of your colab code\n",
    "\n",
    "import os\n",
    "\n",
    "if not os.path.exists('.mujoco_setup_complete'):\n",
    "    # Get the prereqs\n",
    "    !apt-get -qq update\n",
    "    !apt-get -qq install -y libosmesa6-dev libgl1-mesa-glx libglfw3 libgl1-mesa-dev libglew-dev patchelf\n",
    "    # Get Mujoco\n",
    "    !mkdir ~/.mujoco\n",
    "    !wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O mujoco.tar.gz\n",
    "    !tar -zxf mujoco.tar.gz -C \"$HOME/.mujoco\"\n",
    "    !rm mujoco.tar.gz\n",
    "    # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
    "    !echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/.mujoco/mujoco210/bin' >> ~/.bashrc \n",
    "    !echo 'export LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libGLEW.so' >> ~/.bashrc \n",
    "    # THE ANNOYING ONE, FORCE IT INTO LDCONFIG SO WE ACTUALLY GET ACCESS TO IT THIS SESSION\n",
    "    !echo \"/root/.mujoco/mujoco210/bin\" > /etc/ld.so.conf.d/mujoco_ld_lib_path.conf\n",
    "    !ldconfig\n",
    "    # Install Mujoco-py\n",
    "    !pip3 install -U 'mujoco-py<2.2,>=2.1'\n",
    "    # run once\n",
    "    !touch .mujoco_setup_complete\n",
    "\n",
    "try:\n",
    "    if _mujoco_run_once:\n",
    "        pass\n",
    "except NameError:\n",
    "    _mujoco_run_once = False\n",
    "    \n",
    "if not _mujoco_run_once:\n",
    "    # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
    "    try:\n",
    "        os.environ['LD_LIBRARY_PATH']=os.environ['LD_LIBRARY_PATH'] + ':/root/.mujoco/mujoco210/bin'\n",
    "    except KeyError:\n",
    "        os.environ['LD_LIBRARY_PATH']='/root/.mujoco/mujoco210/bin'\n",
    "    try:\n",
    "        os.environ['LD_PRELOAD']=os.environ['LD_PRELOAD'] + ':/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
    "    except KeyError:\n",
    "        os.environ['LD_PRELOAD']='/usr/lib/x86_64-linux-gnu/libGLEW.so'\n",
    "    # presetup so we don't see output on first env initialization\n",
    "    import mujoco_py\n",
    "    _mujoco_run_once = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Install pyvirtual display\n",
    "\n",
    "This allows you to show the simulation in Google Colab (and Jupyter notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
    "!pip install -U gym>=0.21.0\n",
    "!pip install -U gym[atari,accept-rom-license]\n",
    "!pip install -U gym[Robotics,classic_control]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From begin of March 2022, the installation has been changed (again!)\n",
    "\n",
    "You need to install gym-robotics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym-robotics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also, there are somethings change (Ex: LunarLander), you may need to install box2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install box2d-py\n",
    "!pip3 install gym[Box_2D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the example and test in colab from the [link](https://colab.research.google.com/drive/1WonMpHUG_0MO8jedG7ePmoGOo-JVPS7r?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your environment\n",
    "\n",
    "After you finish installation, use the code below to check that your installation is correct.\n",
    "\n",
    "For a local installation, it's simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# env = gym.make(\"CartPole-v0\")\n",
    "# env = gym.make(\"DoubleDunk-v0\")\n",
    "# env = gym.make(\"SpaceInvaders-v0\")\n",
    "# env = gym.make(\"Acrobot-v1\") # double invert pendulum\n",
    "# env = gym.make(\"Ant-v2\")\n",
    "\n",
    "env = gym.make(\"FetchPickAndPlace-v1\")\n",
    "env.reset()\n",
    "\n",
    "for i in range(200):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    screen = env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Jupyter / Colab, we need matplotlib and ipythondisplay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Jupyter notebooks or running on a remote server, you need to create a virtual display and initializing your screen size. This example uses a 400x300 virtual display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Jupyter, we can \n",
    "use gym's `rgb_array` rendering functionality, render to a \"Screen\" variable, then plot the screen variable using Matplotlib,\n",
    "indirectly using Ipython display. If you have a local Python installation, you can skip the `rgb_array` and Matplotlib code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make(\"CartPole-v0\")\n",
    "# env = gym.make(\"DoubleDunk-v0\")\n",
    "# env = gym.make(\"SpaceInvaders-v0\")\n",
    "# env = gym.make(\"Acrobot-v1\") # double invert pendulum\n",
    "# env = gym.make(\"Ant-v2\")\n",
    "env = gym.make(\"FetchPickAndPlace-v1\")\n",
    "env.reset()\n",
    "prev_screen = env.render(mode='rgb_array')\n",
    "plt.imshow(prev_screen)\n",
    "\n",
    "for i in range(20):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    screen = env.render(mode='rgb_array')\n",
    "\n",
    "    plt.imshow(screen)\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "ipythondisplay.clear_output(wait=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save simulator video\n",
    "\n",
    "The virtual display is very slow. For a long run, we should save a video and see the result later.\n",
    "In versions 0.22 and lower, Gym has a `Monitor` class that can be used to record videos. In version 0.23\n",
    "and above, the wrapper is called `RecordVideo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "vdo_path = 'video_rl/'\n",
    "if not os.path.exists(vdo_path):\n",
    "    os.mkdir(vdo_path)\n",
    "\n",
    "# env = gym.make(\"CartPole-v0\")\n",
    "# env = gym.make(\"DoubleDunk-v0\")\n",
    "# env = gym.make(\"SpaceInvaders-v0\")\n",
    "# env = gym.make(\"Acrobot-v1\") # double invert pendulum\n",
    "# env = gym.make(\"Ant-v2\")\n",
    "\n",
    "env = RecordVideo(gym.make('Ant-v3'), vdo_path)\n",
    "env.reset()\n",
    "\n",
    "for i in range(500):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    screen = env.render()\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get a video in the `video_rl` directory.\n",
    "\n",
    "<video controls src=\"img/openai_test2.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient and Actor-Critic methods\n",
    "\n",
    "We've already looked at Q-learning and other TD methods for reinforcement learning. These methods estimate a\n",
    "value function, then their policy is just to select the action maximizing the value in the current state.\n",
    "\n",
    "As an alternative, today we'll look at a class of RL algorithms called Policy Gradient (PG) methods.\n",
    "Instead of learning the value function then finding the value maximizing the value,\n",
    "PG methods learn the policy directly by increasing or decreasing the probability of a particular selected action in proportion to\n",
    "some measure of that action's value.\n",
    "\n",
    "The REINFORCE algorithm is a simple PG method that obtains state-action value function measurements $Q$\n",
    "alongside policy decisions and utilizes\n",
    "the $Q$ value of a state action pair to decide how much to increase or decrease the probability of a particular action in that state.\n",
    "\n",
    "Since REINFORCE has one model for the policy $\\pi$ and only uses actual rewards to calculate $Q$ values,\n",
    "it's not quite an Actor-Critic method. However, since it does directly\n",
    "optimizes the policy function using its gradient with respect to its parameters, it's a PG method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCE\n",
    "\n",
    "The policy gradient loss function in REINFORCE is \n",
    "$$\\mathcal{L}=-Q(s,a)\\log \\pi(a|s)$$\n",
    "\n",
    "Without $Q$, minimizing this loss would be equivalent to maximizing the probability of taking action $a$ in state $s$.\n",
    "With $Q$, we're increasing the probability of action $a$ in proportion to $Q(s,a)$, the critic's estimate of how valuable\n",
    "action $a$ is in state $s$. If $Q(s,a)$ is small, we'll only increase the probability of taking action $a$ slightly, and if it's\n",
    "negative, we'll *decrease* the probability of taking action $a$.\n",
    "\n",
    "The steps of REINFORCE are as follows.\n",
    "\n",
    "1. Initialize the network with random weights\n",
    "2. Play $N$ full episodes, saving their $(s, a, r, s’)$ transitions\n",
    "3. For every step $t$ of every episode $k$, calculate the discounted total reward for subsequent steps $Q_{k,t}=\\sum_{i=0}\\gamma^i r_i$\n",
    "4. Calculate the loss function for all transitions $\\mathcal{L}=-\\sum_{k,t}Q_{k,t}\\log (\\pi(s_{k,t},a_{k,t}))$\n",
    "5. Perform SGD update of weights of $\\pi$ to minimize $\\mathcal{L}$\n",
    "6. Repeat from step 2 until converged\n",
    "\n",
    "REINFORCE is different from Q-learning in several important aspects:\n",
    "\n",
    "1. No explicit exploration is needed. In Q-learning, we use an $\\epsilon$-greedy strategy to explore the environment and prevent our agent from getting stuck with a non-optimal policy. Now, with action probabilities returned by the network, the exploration is performed automatically so long as there is a non-zero probability for every action. Since the network is initialized with random weights, in the beginning, the action will be approximately uniform, corresponding to random agent behavior.\n",
    "2. No replay buffer is used. PG methods are *on-policy* meaning we don’t train on data obtained from an old version of the policy. On policy methods are usually faster than off-policy methods in terms of the number of updates required, but they usually require much more interaction with the environment than off-policy methods such as DQN.\n",
    "3. No target network is needed. Here we use Q-values, but they’re obtained from our experience in the environment. In DQN, we used a target network to break correlations in Q-value approximation, but we’re not approximating it anymore. However, other PG methods do use a target network to estimate $Q$. These are called Actor-Critic methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_rl/ existed, do nothing\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()\n",
    "\n",
    "import os\n",
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "vdo_path = 'video_rl/'\n",
    "if not os.path.exists(vdo_path):\n",
    "    os.mkdir(vdo_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95\n",
    "seed = 0\n",
    "render = False\n",
    "log_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe886abacf0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.reset(seed=seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policies vs. Action Values\n",
    "\n",
    "The output of a DQN is going to be a vector of value estimates while the output of the policy gradient is going to be a probability distribution over actions.\n",
    "\n",
    "For example, a policy network and a DQN network that have learned the CartPole task with two actions (left and right). If we pass a state s to each, we might get the following from the DQN:\n",
    "\n",
    "$$\\pi_{DQN}(s) = [12.3, 23.9]$$\n",
    "\n",
    "and the policy gradient:\n",
    "\n",
    "$$\\pi_{PG}(s) = [0.28, 0.72]$$\n",
    "\n",
    "The DQN gives us estimates of the discounted future rewards of the state and we make our selection based on these values (typically taking the maximum value according to some epsilon-greedy rule). The policy gradient, on the other hand, gives us probabilities of our actions. The way we make our selection, in this case, is by choosing action 0 28% of the time and action 1 72% of the time. These probabilities will change as the network gains more experience.\n",
    "\n",
    "### Values to Probabilities\n",
    "\n",
    "To get these probabilities in PG method, we use a simple function called softmax at the output layer. The function is given below:\n",
    "\n",
    "\n",
    "$$\\sigma(x)_i= \\frac{e^{x_i}}{\\sum_{j=1}^k e^{x_j}}$$\n",
    "\n",
    "This squashes all of our values to be between 0 and 1, and ensures that all of the outputs sum to 1 ($\\sum \\sigma(x) = 1$). Because we’re using the exp(x) function to scale our values, the largest ones tend to dominate and get more of the probability assigned to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy class\n",
    "\n",
    "We’re going to set up a simple class called <code>policy</code> that will contain our neural network. It’s going to have two hidden layers with a ReLU activation function and softmax output. We’ll also give it a method called predict that enables us to do a forward pass through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(Policy, self).__init__()\n",
    "        self.n_inputs = env.observation_space.shape[0]\n",
    "        self.n_outputs = env.action_space.n\n",
    "        \n",
    "        self.affine1 = nn.Linear(self.n_inputs, 128)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(128, self.n_outputs)\n",
    "\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        probs = self.forward(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        self.saved_log_probs.append(m.log_prob(action))\n",
    "        return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = Policy(env)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other thing we need is our discounting function to discount future rewards based on the discount factor $\\gamma$ we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode():\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    returns = []\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing I’ve done here that’s a bit non-standard is subtract the mean of the rewards at the end. This helps to stabilize the learning, particularly in cases such as this one where all the rewards are positive because the gradients change more with negative or below-average rewards than they would if the rewards weren’t normalized like this.\n",
    "\n",
    "Now for the REINFORCE algorithm itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "def reinforce():\n",
    "    running_reward = 10\n",
    "    for i_episode in count(1):\n",
    "        state, ep_reward = env.reset(), 0\n",
    "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "            action = policy.select_action(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            if render:\n",
    "                env.render()\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # calculate reward\n",
    "        # It accepts a list of rewards for the whole episode and needs to calculate \n",
    "        # the discounted total reward for every step. To do this efficiently,\n",
    "        # we calculate the reward from the end of the local reward list.\n",
    "        # The last step of the episode will have the total reward equal to its local reward.\n",
    "        # The step before the last will have the total reward of ep_reward + gamma * running_reward\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tLast reward: 22.00\tAverage reward: 17.46\n",
      "Episode 20\tLast reward: 43.00\tAverage reward: 29.58\n",
      "Episode 30\tLast reward: 44.00\tAverage reward: 37.32\n",
      "Episode 40\tLast reward: 76.00\tAverage reward: 51.04\n",
      "Episode 50\tLast reward: 131.00\tAverage reward: 78.77\n",
      "Episode 60\tLast reward: 174.00\tAverage reward: 125.78\n",
      "Episode 70\tLast reward: 37.00\tAverage reward: 116.83\n",
      "Episode 80\tLast reward: 133.00\tAverage reward: 116.19\n",
      "Episode 90\tLast reward: 236.00\tAverage reward: 181.02\n",
      "Episode 100\tLast reward: 179.00\tAverage reward: 235.53\n",
      "Episode 110\tLast reward: 78.00\tAverage reward: 184.59\n",
      "Episode 120\tLast reward: 74.00\tAverage reward: 138.73\n",
      "Episode 130\tLast reward: 387.00\tAverage reward: 163.54\n",
      "Episode 140\tLast reward: 178.00\tAverage reward: 257.02\n",
      "Episode 150\tLast reward: 500.00\tAverage reward: 335.32\n",
      "Episode 160\tLast reward: 306.00\tAverage reward: 349.12\n",
      "Episode 170\tLast reward: 500.00\tAverage reward: 345.62\n",
      "Episode 180\tLast reward: 500.00\tAverage reward: 374.02\n",
      "Episode 190\tLast reward: 239.00\tAverage reward: 357.80\n",
      "Episode 200\tLast reward: 158.00\tAverage reward: 296.96\n",
      "Episode 210\tLast reward: 230.00\tAverage reward: 248.40\n",
      "Episode 220\tLast reward: 292.00\tAverage reward: 222.24\n",
      "Episode 230\tLast reward: 216.00\tAverage reward: 220.33\n",
      "Episode 240\tLast reward: 181.00\tAverage reward: 216.99\n",
      "Episode 250\tLast reward: 173.00\tAverage reward: 198.41\n",
      "Episode 260\tLast reward: 147.00\tAverage reward: 190.08\n",
      "Episode 270\tLast reward: 187.00\tAverage reward: 178.19\n",
      "Episode 280\tLast reward: 500.00\tAverage reward: 229.17\n",
      "Episode 290\tLast reward: 500.00\tAverage reward: 333.00\n",
      "Episode 300\tLast reward: 500.00\tAverage reward: 349.00\n",
      "Episode 310\tLast reward: 225.00\tAverage reward: 330.39\n",
      "Episode 320\tLast reward: 207.00\tAverage reward: 301.73\n",
      "Episode 330\tLast reward: 326.00\tAverage reward: 312.91\n",
      "Episode 340\tLast reward: 355.00\tAverage reward: 362.05\n",
      "Episode 350\tLast reward: 322.00\tAverage reward: 346.84\n",
      "Episode 360\tLast reward: 500.00\tAverage reward: 363.66\n",
      "Episode 370\tLast reward: 500.00\tAverage reward: 414.74\n",
      "Episode 380\tLast reward: 500.00\tAverage reward: 443.37\n",
      "Episode 390\tLast reward: 500.00\tAverage reward: 440.09\n",
      "Episode 400\tLast reward: 141.00\tAverage reward: 446.15\n",
      "Episode 410\tLast reward: 500.00\tAverage reward: 467.76\n",
      "Solved! Running reward is now 475.0503553554182 and the last episode runs to 500 time steps!\n"
     ]
    }
   ],
   "source": [
    "reinforce()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = RecordVideo(gym.make(\"CartPole-v1\"), vdo_path)\n",
    "is_done = False\n",
    "state = env.reset()\n",
    "while not is_done:\n",
    "    action = policy.select_action(state)\n",
    "    \n",
    "    state, reward, is_done, info = env.step(action)\n",
    "    # print(reward, is_done)\n",
    "    screen = env.render(mode='rgb_array')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Despite the fact our network returns probabilities, we’re not applying softmax nonlinearity to the output. The reason behind this is that we’ll use the PyTorch log_softmax function to calculate the logarithm of the softmax output at once. This way of calculation is much more numerically stable, but we need to remember that output from the network is not probability, but raw scores (usually called logits)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy-based versus value-based methods\n",
    "\n",
    "Let’s now step back from the code we’ve just seen and talk about the differences that both the families of methods have:\n",
    "\n",
    "- Policy methods are directly optimizing what we care about: our behavior. The value methods such as DQN are doing the same indirectly, learning the value first and providing to us policy based on this value.\n",
    "- Policy methods are on-policy and require fresh samples from the environment. The value methods can benefit from old data, obtained from the old policy, human demonstration, and other sources.\n",
    "- Policy methods are usually less sample-efficient, which means they require more interaction with the environment. The value methods can benefit from the large replay buffers. However, sample efficiency doesn’t mean that value methods are more computationally efficient and very often it’s the opposite. In the above example, during the training, we need to access our NN only once, to get the probabilities of actions. In DQN, we need to process two batch of states: one for the current state and another for the next state in the Bellman update.\n",
    "\n",
    "### REINFORCE issues\n",
    "\n",
    "1. Full episodes are required\n",
    "\n",
    "The origin of the complete episodes requirement is to get as accurate a Q estimation as possible. When we talked about DQN, we saw that in practice, it’s fine to replace the exact value for a discounted reward with our estimation using the one-step Bellman equation $Q(s,a)=r_a+\\gamma V(s')$. To estimate V(s), we’ve used our own Q-estimation, but in the case of PG, we don’t have $V(s)$ or $Q(s, a)$ anymore.\n",
    "\n",
    "$$Q(s,a)=r_a+\\gamma V(s')$$\n",
    "\n",
    "To overcome this, two approaches exist. On the one hand, we can ask our network to estimate $V(s)$ and use this estimation to obtain $Q$.\n",
    "This approach is called the **Actor-Critic method**, which is the most popular method from the PG family.\n",
    "\n",
    "2. High gradients variance\n",
    "\n",
    "In the PG formula $\\nabla J \\approx \\mathbb{E}[Q(s,a)\\nabla \\log \\pi(a|s)]$, we have a gradient proportional to the discounted reward from the given state. However, the range of this reward is heavily environment-dependent. Such a large difference can seriously affect our training dynamics, as one lucky episode will dominate in the final gradient.\n",
    "\n",
    "In mathematical terms, our PGs have high variance and we need to do something about this in complex environments, otherwise, the training process can become unstable. The usual approach to handle this is subtracting a value called baseline from the Q. The possible choices of the baseline are as follows:\n",
    "\n",
    "2.1. Some constant value, which normally is the mean of the discounted rewards\n",
    "\n",
    "2.2. The moving average of the discounted rewards\n",
    "\n",
    "2.3. Value of the state $V(s)$\n",
    "\n",
    "3. Exploration\n",
    "\n",
    "Even with the policy represented as probability distribution, there is a high chance that the agent will converge to some locally-optimal policy and stop exploring the environment. In DQN, we solved this using epsilon-greedy action selection: with probability epsilon, the agent took some random action instead of the action dictated by the current policy. We can use the same approach, of course, but PG allows us to follow a better path, called the entropy bonus.\n",
    "\n",
    "4. Correlation between samples\n",
    "\n",
    "In Deep Q-Networks, training samples in one single episode are usually heavily correlated, which is bad for SGD training. In the case of DQN, we solved this issue by having a large replay buffer with 100k-1M observations that we sampled our training batch from. This solution is not applicable to the PG family anymore, due to the fact that those methods belong to the on-policy class. The implication is simple: using old samples generated by the old policy, we’ll get PG for that old policy, not for our current one.\n",
    "\n",
    "The obvious, but, unfortunately wrong solution would be to reduce the replay buffer size. It might work in some simple cases, but in general, we need fresh training data generated by our current policy. To solve this, parallel environments are normally used. The idea is simple: instead of communicating with one environment, we use several and use their transitions as training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient (PG)\n",
    "\n",
    "Before we can implement the policy gradient algorithm, we should go over specific math involved with the algorithm.\n",
    "\n",
    "1. Define **tau** to be a trajectory or a sequence of actions and the corresponding rewards obtained by executing these actions.\n",
    "\n",
    "$$\\tau = (a_0,r_0,a_1,r_1,\\cdots)$$\n",
    "\n",
    "Then define a function of the rewards to be a discounted or undiscounted (for episodic tasks) sum of trajectory’s rewards. In practice, we will find that even for episodic tasks, it is more beneficial to use discounted sum of rewards, defined as follows:\n",
    "\n",
    "$$R(\\tau)=\\sum_{t=0}^T \\gamma^t r_t$$\n",
    "\n",
    "2. Define performance measure J as an expected value of some function of the rewards that came from the most recent batch of trajectories (obtained under the current policy execution).\n",
    "\n",
    "$$J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}[R(\\tau)]$$\n",
    "\n",
    "Let’s investigate the performance measure a little closer. By the definition of expectation, we obtain:\n",
    "\n",
    "$$\\mathbb{E}_{\\tau \\sim \\pi}[R(\\tau)] = \\int_T R(\\tau)P(\\tau|\\pi)d\\tau$$\n",
    "\n",
    "It is essential to understand that we would like to use the gradient of the performance measure to optimize our policy (agent). Hence, we obtain the following:\n",
    "\n",
    "$$\\nabla_\\Theta J(\\pi) = \\nabla_\\Theta \\mathbb{E}_{\\tau \\sim \\pi}[R(\\tau)] =  \\int_T R(\\tau)\\nabla_\\Theta P(\\tau|\\pi)d\\tau$$\n",
    "\n",
    "Now, there is an excellent trick or the log-derivative trick that comes from the following identity:\n",
    "\n",
    "$$\\frac{\\partial \\log (x)}{\\partial x} = \\frac{1}{x} => \\partial x = x\\partial \\log(x)$$\n",
    "\n",
    "We can use this trick to replace the gradient of the probability of the trajectory with the product of the probability and gradient of the log-probability of the trajectory, or:\n",
    "\n",
    "$$\\int_T R(\\tau)\\nabla_\\Theta P(\\tau|\\pi)d\\tau = \\int_T R(\\tau)\\nabla_\\Theta \\log P(\\tau|\\pi)d\\tau$$\n",
    "\n",
    "If we look closely at the right-hand side, we can notice that it is an expectation itself:\n",
    "\n",
    "$$\\int_T R(\\tau)\\nabla_\\Theta \\log P(\\tau|\\pi)d\\tau = \\mathbb{E}_{\\tau \\sim \\pi}[\\nabla_\\Theta \\log P(\\tau|\\pi)R(\\tau)]$$\n",
    "\n",
    "But what is the log-probability of the trajectory? It turns out that we can use the chain rule to define the probability over the trajectory:\n",
    "\n",
    "$$P(\\tau|\\pi) = \\rho(s_0) \\prod_{t=0}^{T-1}P(s_{t+1}|s_t,a_t)\\pi(a_t|s_t) $$\n",
    "\n",
    "Note that Chain rule can be used to define the probability distribution over the trajectory space.\n",
    "\n",
    "Intuitively this chain rule makes a lot of sense. We sample the initial state from some initial state distribution. Then, as our actions are stochastic, we choose an action with some probability over the action space, which is our policy. Finally, the transition model is stochastic too; hence, we multiply by the probability of transitioning from the previous state to the next state. We continue this process until we reach the end of the episode.\n",
    "\n",
    "Now, let us look into the log-probability distribution over the trajectory space:\n",
    "\n",
    "$$\\log P(\\tau|\\pi) = \\log \\rho(s_0)+\\sum_{t=0}^{T-1}\\log P(s_{t+1}|s_t,a_t)+ \\log \\pi(a_t|s_t)$$\n",
    "\n",
    "Let us calculate the gradient of the log-probability over the trajectory space with respect to the parameters of the policy:\n",
    "\n",
    "$$\\nabla_\\Theta \\log P(\\tau|\\pi) = \\nabla_\\Theta \\log \\rho(s_0) + \\sum_{t=0}^{T-1} \\nabla_\\Theta \\log P(s_{t+1}|s_t,a_t) + \\nabla_\\Theta \\log \\pi(a_t|s_t) = \\nabla \\log \\pi(a_t|s_t)$$\n",
    "\n",
    "We see that only the policy probability distribution depends on the policy parameters. Hence, the rest of the terms evaluate to zero. Finally, we can put all of the math together to obtain:\n",
    "\n",
    "$$\\nabla_\\Theta J(\\pi)= \\mathbb{E}_{\\tau \\sim \\pi}[\\nabla \\log \\pi(a_t|s_t) R(\\tau)]$$\n",
    "\n",
    "Now, since the gradient of the performance measure is an expectation, we can estimate it with sampling, which is extremely easy. We will generate several trajectories under the current policy (as the policy gradient is an on-policy algorithm) and then will calculate the mean of the weighted (by R(tau)) log-probabilities that we obtain from the agent (policy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewards\n",
    "\n",
    "Instead of the sum of the discounted rewards, we are going to use the sum of the discounted rewards from the time t to the end of the episode. These are called rewards-to-go and are used more frequently in policy gradient methods as the actions that are committed after time t should not have any effect on the rewards that were obtained before these actions happened.\n",
    "\n",
    "$$R_(\\tau) = \\sum_{t'=t}^{T-1}\\gamma^t r_t$$\n",
    "\n",
    "### Entropy Bonus\n",
    "\n",
    "In the code, we will also use an entropy bonus to discourage strict certainty. The idea is relatively simple: we subtract the entropy of the policy from the “loss” during the policy optimization. If the agent is overly confident in its actions, then the entropy of the policy becomes small, and the bonus vanishes. The entropy of the policy is a recurrent theme in reinforcement learning and is used in other algorithms such as Soft Actor-Critic.\n",
    "\n",
    "### Baseline\n",
    "We also are going to use a baseline. A baseline is a quantity that gets subtracted from R(tau) without affecting the expectation because, typically, the baseline is a state-specific quantity. We will use a state-specific mean of the trajectory’s rewards as a baseline.\n",
    "\n",
    "The baseline reduces variance in the policy gradient estimation. Intuitively, it makes a lot of sense, especially in the case of the CartPole problem. Suppose that our agent can balance the pole for 2 seconds. Is that good or bad? If, on average, the agent balanced the pole for 1 second before this episode, then yes, it is much better performance. The policy gradient will be estimated to be positive in this case, and the agent will take a step in the direction of further improvement.\n",
    "\n",
    "However, if the agent on average balanced the pole for 3 seconds before the episode, the policy gradient will be estimated to be negative, and we will still make the step in the right direction, away from the parameters that made the agent balance the pole for 2 seconds. If we don’t use the baseline, both 1 second and 3 seconds and 10 seconds episode will result in similar gradient directions; hence, training might take much longer.\n",
    "It is important to note that for more complicated problems such as LunarLander, the baseline is less intuitive as we have both negative and positive rewards, and the scale of the rewards is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PG on CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layer_size=64):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_layer_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_layer_size, output_size)\n",
    "        self.softmax = torch.nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.from_numpy(x).float()\n",
    "        return self.softmax(self.fc2(torch.nn.functional.relu(self.fc1(x))))\n",
    "\n",
    "    def get_action_and_logp(self, x):\n",
    "        action_prob = self.forward(x)\n",
    "        m = torch.distributions.Categorical(action_prob)\n",
    "        action = m.sample()\n",
    "        logp = m.log_prob(action)\n",
    "        return action.item(), logp\n",
    "\n",
    "    def act(self, x):\n",
    "        action, _ = self.get_action_and_logp(x)\n",
    "        return action\n",
    "\n",
    "\n",
    "class ValueNet(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size=64):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden_layer_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden_layer_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.from_numpy(x).float()\n",
    "        return self.fc2(torch.nn.functional.relu(self.fc1(x)))\n",
    "\n",
    "\n",
    "def vpg(env, num_iter=200, num_traj=10, max_num_steps=1000, gamma=0.98,\n",
    "        policy_learning_rate=0.01, value_learning_rate=0.01,\n",
    "        policy_saved_path='vpg_policy.pt', value_saved_path='vpg_value.pt'):\n",
    "    input_size = env.observation_space.shape[0]\n",
    "    output_size = env.action_space.n\n",
    "    Trajectory = namedtuple('Trajectory', 'states actions rewards dones logp')\n",
    "\n",
    "    def collect_trajectory():\n",
    "        state_list = []\n",
    "        action_list = []\n",
    "        reward_list = []\n",
    "        dones_list = []\n",
    "        logp_list = []\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "        while not done and steps <= max_num_steps:\n",
    "            action, logp = policy.get_action_and_logp(state)\n",
    "            newstate, reward, done, _ = env.step(action)\n",
    "            #reward = reward + float(state[0])\n",
    "            state_list.append(state)\n",
    "            action_list.append(action)\n",
    "            reward_list.append(reward)\n",
    "            dones_list.append(done)\n",
    "            logp_list.append(logp)\n",
    "            steps += 1\n",
    "            state = newstate\n",
    "\n",
    "        traj = Trajectory(states=state_list, actions=action_list,\n",
    "                          rewards=reward_list, logp=logp_list, dones=dones_list)\n",
    "        return traj\n",
    "\n",
    "    def calc_returns(rewards):\n",
    "        dis_rewards = [gamma**i * r for i, r in enumerate(rewards)]\n",
    "        return [sum(dis_rewards[i:]) for i in range(len(dis_rewards))]\n",
    "\n",
    "    policy = PolicyNet(input_size, output_size)\n",
    "    value = ValueNet(input_size)\n",
    "    policy_optimizer = torch.optim.Adam(\n",
    "        policy.parameters(), lr=policy_learning_rate)\n",
    "    value_optimizer = torch.optim.Adam(\n",
    "        value.parameters(), lr=value_learning_rate)\n",
    "\n",
    "    mean_return_list = []\n",
    "    for it in range(num_iter):\n",
    "        traj_list = [collect_trajectory() for _ in range(num_traj)]\n",
    "        returns = [calc_returns(traj.rewards) for traj in traj_list]\n",
    "\n",
    "        policy_loss_terms = [-1. * traj.logp[j] * (returns[i][j] - value(traj.states[j]))\n",
    "                             for i, traj in enumerate(traj_list) for j in range(len(traj.actions))]\n",
    "\n",
    "        policy_loss = 1. / num_traj * torch.cat(policy_loss_terms).sum()\n",
    "        policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        policy_optimizer.step()\n",
    "\n",
    "        value_loss_terms = [1. / len(traj.actions) * (value(traj.states[j]) - returns[i][j])**2.\n",
    "                            for i, traj in enumerate(traj_list) for j in range(len(traj.actions))]\n",
    "        value_loss = 1. / num_traj * torch.cat(value_loss_terms).sum()\n",
    "        value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        value_optimizer.step()\n",
    "\n",
    "        mean_return = 1. / num_traj * \\\n",
    "            sum([traj_returns[0] for traj_returns in returns])\n",
    "        mean_return_list.append(mean_return)\n",
    "        if it % 10 == 0:\n",
    "            print('Iteration {}: Mean Return = {}'.format(it, mean_return))\n",
    "            torch.save(policy.state_dict(), policy_saved_path)\n",
    "            torch.save(value.state_dict(), value_saved_path)\n",
    "    return policy, mean_return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_rl2/ existed, do nothing\n",
      "Iteration 0: Mean Return = 27.8\n",
      "Iteration 10: Mean Return = 42.2\n",
      "Iteration 20: Mean Return = 73.4\n",
      "Iteration 30: Mean Return = 87.60000000000001\n",
      "Iteration 40: Mean Return = 162.4\n",
      "Iteration 50: Mean Return = 142.8\n",
      "Iteration 60: Mean Return = 119.0\n",
      "Iteration 70: Mean Return = 179.4\n",
      "Iteration 80: Mean Return = 200.0\n",
      "Iteration 90: Mean Return = 200.0\n",
      "Iteration 100: Mean Return = 180.4\n",
      "Iteration 110: Mean Return = 189.60000000000002\n",
      "Iteration 120: Mean Return = 200.0\n",
      "Iteration 130: Mean Return = 200.0\n",
      "Iteration 140: Mean Return = 200.0\n",
      "Iteration 150: Mean Return = 200.0\n",
      "Iteration 160: Mean Return = 200.0\n",
      "Iteration 170: Mean Return = 200.0\n",
      "Iteration 180: Mean Return = 200.0\n",
      "Iteration 190: Mean Return = 200.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vdo_path = 'video_rl2/'\n",
    "if not os.path.exists(vdo_path):\n",
    "    print(\"No folder \", vdo_path, 'exist. Create the folder')\n",
    "    os.mkdir(vdo_path)\n",
    "    print(\"Create directory finished\")\n",
    "else:\n",
    "    print(vdo_path, 'existed, do nothing')\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "agent, mean_return_list = vpg(env, num_iter=200, max_num_steps=500, gamma=1.0,\n",
    "                              num_traj=5)\n",
    "\n",
    "env = RecordVideo(gym.make(\"CartPole-v1\"), vdo_path, force=True)\n",
    "\n",
    "plt.plot(mean_return_list)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Mean Return')\n",
    "plt.savefig('vpg_returns.png', format='png', dpi=300)\n",
    "\n",
    "state = env.reset()\n",
    "for t in range(1000):\n",
    "    action = agent.act(state)\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic\n",
    "\n",
    "The next step in reducing the variance is making our baseline state-dependent (which, intuitively, is a good idea, as different states could have very different baselines). Indeed, to decide about the suitability of a particular action in some state, we're using the discounted total reward of the action. However, the total reward itself could be represented as a value of the state plus advantage of the action: $Q(s, a) = V(s) + A(s, a)$.\n",
    "\n",
    "So, why can't we use $V(s)$ as a baseline? In that case, the scale of our gradient will be just advantage $A(s, a)$, showing how this taken action is better in respect to the average state's value. In fact, we can do this, and it is a very good idea for improving the PG method. The only problem here is: we don't know the value of the $V(s)$ state to subtract it from the discounted total reward $Q(s, a)$. To solve this, let's use another neural network, which will approximate V(s) for every observation. To train it, we can exploit the same training procedure we used in DQN methods: we'll carry out the Bellman step and then minimize the mean square error to improve $V(s)$ approximation.\n",
    "\n",
    "When we know the value for any state (or, at least, have some approximation of it), we can use it to calculate the PG and update our policy network to increase probabilities for actions with good advantage values and decrease the chance of actions with bad advantage. The policy network (which returns probability distribution of actions) is called the actor, as it tells us what to do. Another network is called critic, as it allows us to understand how good our actions were. Below is an illustration of the architecture.\n",
    "\n",
    "<img src=\"img/ActorCritic2.png\" title=\"The A2C architecture\" style=\"width: 400px;\" />\n",
    "\n",
    "In practice, policy and value networks partially overlap, mostly due to the efficiency and convergence considerations. In this case, policy and value are implemented as different heads of the network, taking the output from the common body and transforming it into the probability distribution and a single number representing the value of the state. This helps both networks to share low-level features (such as convolution filters in the Atari agent), but combine them in a different way. This architecture is shown below.\n",
    "\n",
    "<img src=\"img/ActorCritic3.png\" title=\"A2C architecture with a shared network body\" style=\"width: 400px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic steps\n",
    "\n",
    "From a training point of view, we complete these steps:\n",
    "\n",
    "1. Initialize network parameters $\\theta$ with random values\n",
    "2. Play N steps in the environment using the current policy $\\pi_\\theta$, saving state $s_t$, action $a_t$, reward $r_t$\n",
    "3. R = 0 if the end of the episode is reached or \n",
    "4. For $i=t-1,\\cdots,t_{start}$ (note that steps are processed backwards):\n",
    "    - $R\\leftarrow r_i + \\gamma R$\n",
    "    - Accumulate the PG $\\partial \\theta_\\pi \\leftarrow \\partial \\theta_\\pi + \\nabla_\\theta \\log \\pi_\\theta(a_i|s_i)(R-V\\theta(s_i))$\n",
    "    - Accxumulate the value gradients $\\partial \\theta_v \\leftarrow \\partial_v + \\frac{\\partial(R-V_\\theta (s_i))^2}{\\partial \\theta_v}$\n",
    "5. Update network parameters using the accumulated gradients, moving in the direction of PG $\\theta_\\pi$ and in the opposite direction of the value gradients $\\partial \\theta_v$\n",
    "6. Repeat from step 2 until convergence is reached\n",
    "\n",
    "The preceding algorithm is an outline, similar to those which are usually printed in research papers. In practice, some considerations need to be taken:\n",
    "\n",
    " - Entropy bonus is usually added to improve exploration. It's typically written as an entropy value added to the loss function:$\\mathcal{L}_H = \\beta \\sum_i \\pi_\\theta (s_i) \\log \\pi_\\theta(s_i)$. This function has a minimum when probability distribution is uniform, so by adding it to the loss function, we're pushing our agent away from being too certain about its actions.\n",
    " - Gradients accumulation is usually implemented as a loss function combining all three components: policy loss, value loss, and entropy loss. You should be careful with signs of these losses, as PGs? are showing you the direction of policy improvement, but both value and entropy losses should be minimized.\n",
    " - To improve stability, it's worth using several environments, providing you with observations concurrently (when we have multiple environments and our training batch will be created from their observations). We'll look at several ways of doing this in the next chapter.\n",
    " \n",
    "The preceding method is called Actor-Critic, or sometimes Advantage Actor-Critic, which is abbreviated as A2C for short. The version with several environments running in parallel is called Advantage Asynchronous Actor-Critic, which is also known as A3C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic CartPole A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "STATE_DIM = 4\n",
    "ACTION_DIM = 2\n",
    "STEP = 2000\n",
    "SAMPLE_NUMS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size,hidden_size,action_size):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size,hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size,action_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = F.log_softmax(self.fc3(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size,hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size,output_size)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll_out(actor_network,task,sample_nums,value_network,init_state):\n",
    "    #task.reset()\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    is_done = False\n",
    "    final_r = 0\n",
    "    state = init_state\n",
    "\n",
    "    for j in range(sample_nums):\n",
    "        states.append(state)\n",
    "        log_softmax_action = actor_network(torch.Tensor([state]))\n",
    "        softmax_action = torch.exp(log_softmax_action)\n",
    "        action = np.random.choice(ACTION_DIM,p=softmax_action.cpu().data.numpy()[0])\n",
    "        one_hot_action = [int(k == action) for k in range(ACTION_DIM)]\n",
    "        next_state,reward,done,_ = task.step(action)\n",
    "        #fix_reward = -10 if done else 1\n",
    "        actions.append(one_hot_action)\n",
    "        rewards.append(reward)\n",
    "        final_state = next_state\n",
    "        state = next_state\n",
    "        if done:\n",
    "            is_done = True\n",
    "            state = task.reset()\n",
    "            break\n",
    "    if not is_done:\n",
    "        final_r = value_network(torch.Tensor([final_state])).cpu().data.numpy()\n",
    "\n",
    "    return states,actions,rewards,final_r,state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_reward(r, gamma,final_r):\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = final_r\n",
    "    for t in reversed(range(0, len(r))):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A2C():\n",
    "    # init a task generator for data fetching\n",
    "    task = gym.make(\"CartPole-v1\")\n",
    "    init_state = task.reset()\n",
    "\n",
    "    # init value network\n",
    "    value_network = ValueNetwork(input_size = STATE_DIM,hidden_size = 40,output_size = 1)\n",
    "    value_network_optim = torch.optim.Adam(value_network.parameters(),lr=0.01)\n",
    "\n",
    "    # init actor network\n",
    "    actor_network = ActorNetwork(STATE_DIM,40,ACTION_DIM)\n",
    "    actor_network_optim = torch.optim.Adam(actor_network.parameters(),lr = 0.01)\n",
    "\n",
    "    steps =[]\n",
    "    task_episodes =[]\n",
    "    test_results =[]\n",
    "\n",
    "    for step in range(STEP):\n",
    "        states,actions,rewards,final_r,current_state = roll_out(actor_network,task,SAMPLE_NUMS,value_network,init_state)\n",
    "        init_state = current_state\n",
    "        actions_var = torch.Tensor(actions).view(-1,ACTION_DIM)\n",
    "        states_var = torch.Tensor(states).view(-1,STATE_DIM)\n",
    "\n",
    "        # train actor network\n",
    "        actor_network_optim.zero_grad()\n",
    "        log_softmax_actions = actor_network(states_var)\n",
    "        vs = value_network(states_var).detach()\n",
    "        # calculate qs\n",
    "        qs = torch.Tensor(discount_reward(rewards,0.99,final_r))\n",
    "\n",
    "        advantages = qs - vs\n",
    "        actor_network_loss = - torch.mean(torch.sum(log_softmax_actions*actions_var,1)* advantages)\n",
    "        actor_network_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(actor_network.parameters(),0.5)\n",
    "        actor_network_optim.step()\n",
    "\n",
    "        # train value network\n",
    "        value_network_optim.zero_grad()\n",
    "        target_values = qs.unsqueeze(1)\n",
    "        values = value_network(states_var)\n",
    "        criterion = nn.MSELoss()\n",
    "        value_network_loss = criterion(values,target_values)\n",
    "        value_network_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(value_network.parameters(),0.5)\n",
    "        value_network_optim.step()\n",
    "\n",
    "        # Testing\n",
    "        if (step + 1) % 50== 0:\n",
    "                result = 0\n",
    "                test_task = gym.make(\"CartPole-v1\")\n",
    "                for test_epi in range(10):\n",
    "                    state = test_task.reset()\n",
    "                    for test_step in range(200):\n",
    "                        softmax_action = torch.exp(actor_network(torch.Tensor([state])))\n",
    "                        #print(softmax_action.data)\n",
    "                        action = np.argmax(softmax_action.data.numpy()[0])\n",
    "                        next_state,reward,done,_ = test_task.step(action)\n",
    "                        result += reward\n",
    "                        state = next_state\n",
    "                        if done:\n",
    "                            break\n",
    "                print(\"step:\",step+1,\"test result:\",result/10.0)\n",
    "                steps.append(step+1)\n",
    "                test_results.append(result/10)\n",
    "                \n",
    "    return actor_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_network = A2C()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdo_path = 'video_rl3/'\n",
    "if not os.path.exists(vdo_path):\n",
    "    os.mkdir(vdo_path)\n",
    "\n",
    "env = RecordVideo(gym.make(\"CartPole-v1\"), vdo_path)\n",
    "\n",
    "state = env.reset()\n",
    "for t in range(1000):\n",
    "    softmax_action = torch.exp(actor_network(torch.Tensor([state])))\n",
    "    #print(softmax_action.data)\n",
    "    action = np.argmax(softmax_action.data.numpy()[0])\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take-home exercise\n",
    "\n",
    "Implement REINFORCE and A2C for one of the Atari games such as Space Invaders using a CNN for the policy\n",
    "network and (for A2C) the value network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
