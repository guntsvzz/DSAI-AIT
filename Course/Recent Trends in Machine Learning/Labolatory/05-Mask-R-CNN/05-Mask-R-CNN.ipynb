{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 06: Mask R-CNN\n",
    "\n",
    "[Mask R-CNN](https://arxiv.org/abs/1703.06870) was originally released under the Caffe2 framework by the FAIR team as part of a project called \"Detectron.\"\n",
    "\n",
    "After Caffe2 and PyTorch merged, eventually, FAIR released a ground-up reimplementation of Detectron with new models and\n",
    "features. This framework is [Detectron2](https://github.com/facebookresearch/detectron2). If you're looking for a complete\n",
    "approach to bounding box regression, mask regression, and skeleton point (pose) estimation, take a look.\n",
    "\n",
    "In the meantime, however, other groups quickly implemented Mask R-CNN directly in TensorFlow and Keras.\n",
    "Today we'll work with a [PyTorch implementation of Mask R-CNN](https://github.com/multimodallearning/pytorch-mask-rcnn) derived from those efforts.\n",
    "\n",
    "In case the original code is inaccessible, we've archived the version of the project used to create this lab manual\n",
    "[in the GitHub repository for this lab](https://github.com/dsai-asia/RTML/tree/main/Labs/06-Mask-R-CNN/torch_mask_rcnn_code).\n",
    "\n",
    "The full Mask R-CNN structure with a ResNet-50 FPN backbone (later we will be using a ResNet-101 backbone) looks like this:\n",
    "\n",
    "<img src=\"img/MaskRCNNArchitecture.png\" title=\"FullMaskR-CNN\" style=\"width: 860px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each intance of an object in an image, Mask R-CNN attempts to generate\n",
    " - A bounding box\n",
    " - Class scores\n",
    " - A segmentation mask\n",
    "\n",
    "The backbone and neck of Mask R-CNN are based on\n",
    " - A feature pyramid network (FPN)\n",
    " - ResNet\n",
    "\n",
    "## Feature Pyramid Networks\n",
    "\n",
    "We've seen the idea of the feature pyramid network (FPN) in YOLOv3 and YOLOv4. It is a feature extractor using a pyramid concept.\n",
    "We begin with ordinary progressive downsampling of the input to get a multiscale representation of the input, but rather\n",
    "than using that \"low-level\" multiscale representation directly, we progressively upsample the coarse representation of\n",
    "the input using input from the low-level feature maps. The idea is shown in the left-hand panel of the diagram above.\n",
    "By incorporating information from both the low-level \"bottom-up\" fine grained feature map and the upsampled coarser grained feature\n",
    "map in the pyramid, the fine grained representation at the bottom of the pyramid contains much more useful or more \"semantic\"\n",
    "information about the input.\n",
    "\n",
    "Feature pyramids can, in principle, come in many different forms. Refer to the figure below, extracted\n",
    "from [the original FPN paper](https://arxiv.org/abs/1612.03144) and discussed in a\n",
    "[nice blog by Sik-Ho Tsang](https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610):\n",
    "\n",
    "<img src=\"img/Pyramid.png\" title=\"PyramidNets\" style=\"width: 800px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classical image pyramid, used by many techniques aiming at multiscale detection, looks like (a).\n",
    "Classifiers we built early on in the class, such as AlexNet, look like (b). (c) and (d) utilize multiple\n",
    "feature maps derived from the input through progressive downscaling. The difference in the FPN (d) is\n",
    "the inclusion of both bottom up and top down pathways:\n",
    "\n",
    "<img src=\"img/bottomup.jpeg\" title=\"Bottom-up\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that as we process the input in multiple progressively downsampled layers, we are increasingly analyzing higher-level features\n",
    "with larger receptive fields with some invariance to imaging conditions (translation, scale, lighting, etc.).\n",
    "\n",
    "The top-down representations use progressively *upsampled* layers in which we are increasingly analyzing the input at high resolution but\n",
    "with all the benefits of the downsampled representation.\n",
    "\n",
    "The main risk of the top-down upsampling is that we would lose information about the details of the original input in constructing the\n",
    "fine-grained feature maps. For that reason, we add lateral connections to the bottom-up feature maps of the same size:\n",
    "\n",
    "<img src=\"img/lateralconnection.png\" title=\"lateralconnection\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet backbone\n",
    "\n",
    "The bottom-up part of the FPN used in Mask-RCNN is ResNet. It is used similar to how\n",
    "Darknet-53 is used in YOLO. We take the classifier structure as the bottom-up half (left side) of\n",
    "the pyramid, then we add the top-down part (right side) to obtain the FPN.\n",
    "\n",
    "Mask R-CNN taps into ResNet in 4 or 5 places according to the implementation, at the ouptut of various residual blocks.\n",
    "\n",
    "Here's a figure to explain this, this time from [Jonathan Hui's Medium site](https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c):\n",
    "\n",
    "<img src=\"img/upanddown.png\" title=\"upanddown\" style=\"width: 500px;\" />\n",
    "\n",
    "(There would be a P6 there as well if we're extracting a 5-scale pyramid)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region Proposal Network (RPN)\n",
    "\n",
    "The Faster R-CNN RPN connects to the top of the FPN pyramid. It performs classification and bounding box regression for each possible proposal.\n",
    "\n",
    "<img src=\"img/RPN.png\" title=\"RPN\" style=\"width: 600px;\" />\n",
    "\n",
    "## Detection network\n",
    "\n",
    "The detection network uses the results of the RPN as well as the output of the FPN. With the RPN bounding box as input, we assign the box to one of the levels of the pyramid.\n",
    "Specifically, we use\n",
    "\n",
    "$$ k = \\left\\lfloor k_0 +\\log_2\\left( \\frac{\\sqrt{wh}}{224} \\right) \\right\\rfloor $$\n",
    "\n",
    "Then the ROIAlign block interpolates the appropriate set of features from the best level (level $k$) of the pyramid. The region is aligned and scaled to a size of\n",
    "56$\\times$56, and the resulting representation is forwarded to the mask prediction head.\n",
    "\n",
    "## Mask Head\n",
    "\n",
    "The mask head is a FCN that up-samples from the detection result, and the patch size is finally re-scaled back to the input size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding\n",
    "\n",
    "So let's start investigating how Mask R-CNN works in detail.\n",
    "First, clone the Github respository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/multimodallearning/pytorch-mask-rcnn.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaskRCNN class (model.py)\n",
    "\n",
    "First, let's visit the MaskRCNN class and its <code>init()</code> function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, config, model_dir):\n",
    "        \"\"\"\n",
    "        config: A Sub-class of the Config class\n",
    "        model_dir: Directory to save training logs and trained weights\n",
    "        \"\"\"\n",
    "        super(MaskRCNN, self).__init__()\n",
    "        self.config = config\n",
    "        self.model_dir = model_dir\n",
    "        self.set_log_dir()\n",
    "        self.build(config=config)\n",
    "        self.initialize_weights()\n",
    "        self.loss_history = []\n",
    "        self.val_loss_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important action here is calling `build()`, which creates the network based on a\n",
    "configuration object. `set_log_dir()` just sets up saving to a log file, and `initialize_weights()` loads\n",
    "weights. Let's take a look at the `build()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(self, config):\n",
    "    \"\"\"Build Mask R-CNN architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    # Image size must be dividable by 2 multiple times\n",
    "    h, w = config.IMAGE_SHAPE[:2]\n",
    "    if h / 2**6 != int(h / 2**6) or w / 2**6 != int(w / 2**6):\n",
    "        raise Exception(\"Image size must be dividable by 2 at least 6 times \"\n",
    "                        \"to avoid fractions when downscaling and upscaling.\"\n",
    "                        \"For example, use 256, 320, 384, 448, 512, ... etc. \")\n",
    "\n",
    "    # Build the shared convolutional layers.\n",
    "    # Bottom-up Layers\n",
    "    # Returns a list of the last layers of each stage, 5 in total.\n",
    "    # Don't create the thead (stage 5), so we pick the 4th item in the list.\n",
    "    resnet = ResNet(\"resnet101\", stage5=True)\n",
    "    C1, C2, C3, C4, C5 = resnet.stages()\n",
    "\n",
    "    # Top-down Layers\n",
    "    # TODO: add assert to varify feature map sizes match what's in config\n",
    "    self.fpn = FPN(C1, C2, C3, C4, C5, out_channels=256)\n",
    "\n",
    "    # Generate Anchors\n",
    "    self.anchors = Variable(torch.from_numpy(utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES,\n",
    "                                                                            config.RPN_ANCHOR_RATIOS,\n",
    "                                                                            config.BACKBONE_SHAPES,\n",
    "                                                                            config.BACKBONE_STRIDES,\n",
    "                                                                            config.RPN_ANCHOR_STRIDE)).float(), requires_grad=False)\n",
    "    if self.config.GPU_COUNT:\n",
    "        self.anchors = self.anchors.cuda()\n",
    "\n",
    "    # RPN\n",
    "    self.rpn = RPN(len(config.RPN_ANCHOR_RATIOS), config.RPN_ANCHOR_STRIDE, 256)\n",
    "\n",
    "    # FPN Classifier\n",
    "    self.classifier = Classifier(256, config.POOL_SIZE, config.IMAGE_SHAPE, config.NUM_CLASSES)\n",
    "\n",
    "    # FPN Mask\n",
    "    self.mask = Mask(256, config.MASK_POOL_SIZE, config.IMAGE_SHAPE, config.NUM_CLASSES)\n",
    "\n",
    "    # Fix batch norm layers\n",
    "    def set_bn_fix(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('BatchNorm') != -1:\n",
    "            for p in m.parameters(): p.requires_grad = False\n",
    "\n",
    "    self.apply(set_bn_fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone\n",
    "\n",
    "The backbone is initialized then tapped into in the lines\n",
    "\n",
    "    resnet = ResNet(\"resnet101\", stage5=True)\n",
    "    C1, C2, C3, C4, C5 = resnet.stages()\n",
    "\n",
    "We see that this version is using ResNet101 and extracting a 5-stage pyramid. How to find out what stages are being used? Take\n",
    "a look at the ResNet class itself and take a look at its `__init__` and forward methods. We see that C1-C5 are the major\n",
    "blocks of the network. The first (C1) is a single 7$\\times$7 convolution with batch norm, ReLU, and a MaxPool operation. The others\n",
    "(C2-C5) are ResNet residual blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, architecture, stage5=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        assert architecture in [\"resnet50\", \"resnet101\"]\n",
    "        self.inplanes = 64\n",
    "        self.layers = [3, 4, {\"resnet50\": 6, \"resnet101\": 23}[architecture], 3]\n",
    "        self.block = Bottleneck\n",
    "        self.stage5 = stage5\n",
    "\n",
    "        self.C1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64, eps=0.001, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SamePad2d(kernel_size=3, stride=2),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.C2 = self.make_layer(self.block, 64, self.layers[0])\n",
    "        self.C3 = self.make_layer(self.block, 128, self.layers[1], stride=2)\n",
    "        self.C4 = self.make_layer(self.block, 256, self.layers[2], stride=2)\n",
    "        if self.stage5:\n",
    "            self.C5 = self.make_layer(self.block, 512, self.layers[3], stride=2)\n",
    "        else:\n",
    "            self.C5 = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.C1(x)\n",
    "        x = self.C2(x)\n",
    "        x = self.C3(x)\n",
    "        x = self.C4(x)\n",
    "        x = self.C5(x)\n",
    "        return x\n",
    "\n",
    "    def stages(self):\n",
    "        return [self.C1, self.C2, self.C3, self.C4, self.C5]\n",
    "\n",
    "    def make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(planes * block.expansion, eps=0.001, momentum=0.01),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FPN\n",
    "\n",
    "Let's go back to the MaskRCNN class. The next stage is the top-down layers of the FPN:\n",
    "\n",
    "    self.fpn = FPN(C1, C2, C3, C4, C5, out_channels=256)\n",
    "\n",
    "The main idea of the FPN is in its `forward()` method. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPN(nn.Module):\n",
    "    def __init__(self, C1, C2, C3, C4, C5, out_channels):\n",
    "        super(FPN, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.C1 = C1\n",
    "        self.C2 = C2\n",
    "        self.C3 = C3\n",
    "        self.C4 = C4\n",
    "        self.C5 = C5\n",
    "        self.P6 = nn.MaxPool2d(kernel_size=1, stride=2)\n",
    "        self.P5_conv1 = nn.Conv2d(2048, self.out_channels, kernel_size=1, stride=1)\n",
    "        self.P5_conv2 = nn.Sequential(\n",
    "            SamePad2d(kernel_size=3, stride=1),\n",
    "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1),\n",
    "        )\n",
    "        self.P4_conv1 =  nn.Conv2d(1024, self.out_channels, kernel_size=1, stride=1)\n",
    "        self.P4_conv2 = nn.Sequential(\n",
    "            SamePad2d(kernel_size=3, stride=1),\n",
    "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1),\n",
    "        )\n",
    "        self.P3_conv1 = nn.Conv2d(512, self.out_channels, kernel_size=1, stride=1)\n",
    "        self.P3_conv2 = nn.Sequential(\n",
    "            SamePad2d(kernel_size=3, stride=1),\n",
    "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1),\n",
    "        )\n",
    "        self.P2_conv1 = nn.Conv2d(256, self.out_channels, kernel_size=1, stride=1)\n",
    "        self.P2_conv2 = nn.Sequential(\n",
    "            SamePad2d(kernel_size=3, stride=1),\n",
    "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.C1(x)\n",
    "        x = self.C2(x)\n",
    "        c2_out = x          # keep C2 output\n",
    "        x = self.C3(x)\n",
    "        c3_out = x          # keep C3 output\n",
    "        x = self.C4(x)\n",
    "        c4_out = x          # keep C4 output\n",
    "        x = self.C5(x)\n",
    "        p5_out = self.P5_conv1(x)       # top-most of pyramid\n",
    "        p4_out = self.P4_conv1(c4_out) + F.upsample(p5_out, scale_factor=2)         # lateral connections, 2nd top output\n",
    "        p3_out = self.P3_conv1(c3_out) + F.upsample(p4_out, scale_factor=2)         # lateral connections, 3rd top output\n",
    "        p2_out = self.P2_conv1(c2_out) + F.upsample(p3_out, scale_factor=2)         # lateral connections, 4th top output\n",
    "\n",
    "        p5_out = self.P5_conv2(p5_out)\n",
    "        p4_out = self.P4_conv2(p4_out)\n",
    "        p3_out = self.P3_conv2(p3_out)\n",
    "        p2_out = self.P2_conv2(p2_out)\n",
    "\n",
    "        # P6 is used for the 5th anchor scale in RPN. Generated by\n",
    "        # subsampling from P5 with stride of 2.\n",
    "        p6_out = self.P6(p5_out)        # max pooling for RPN\n",
    "\n",
    "        return [p2_out, p3_out, p4_out, p5_out, p6_out]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchors\n",
    "\n",
    "Anchor box sizes are set in the configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RPN\n",
    "\n",
    "Next, let's see how the RPN is produced.\n",
    "\n",
    "    self.rpn = RPN(len(config.RPN_ANCHOR_RATIOS), config.RPN_ANCHOR_STRIDE, 256)\n",
    "\n",
    "The RPN class is reproduced below. But also take a look at its output: class scores, softmaxed class scores, and bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN(nn.Module):\n",
    "    \"\"\"Builds the model of Region Proposal Network.\n",
    "\n",
    "    anchors_per_location: number of anchors per pixel in the feature map\n",
    "    anchor_stride: Controls the density of anchors. Typically 1 (anchors for\n",
    "                   every pixel in the feature map), or 2 (every other pixel).\n",
    "\n",
    "    Returns:\n",
    "        rpn_logits: [batch, H, W, 2] Anchor classifier logits (before softmax)\n",
    "        rpn_probs: [batch, W, W, 2] Anchor classifier probabilities.\n",
    "        rpn_bbox: [batch, H, W, (dy, dx, log(dh), log(dw))] Deltas to be\n",
    "                  applied to anchors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, anchors_per_location, anchor_stride, depth):\n",
    "        super(RPN, self).__init__()\n",
    "        self.anchors_per_location = anchors_per_location\n",
    "        self.anchor_stride = anchor_stride\n",
    "        self.depth = depth\n",
    "\n",
    "        self.padding = SamePad2d(kernel_size=3, stride=self.anchor_stride)\n",
    "        self.conv_shared = nn.Conv2d(self.depth, 512, kernel_size=3, stride=self.anchor_stride)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv_class = nn.Conv2d(512, 2 * anchors_per_location, kernel_size=1, stride=1)     # class, score\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.conv_bbox = nn.Conv2d(512, 4 * anchors_per_location, kernel_size=1, stride=1)      # x,y,w,h\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shared convolutional base of the RPN\n",
    "        x = self.relu(self.conv_shared(self.padding(x)))\n",
    "\n",
    "        # Anchor Score. [batch, anchors per location * 2, height, width].\n",
    "        rpn_class_logits = self.conv_class(x)\n",
    "\n",
    "        # Reshape to [batch, 2, anchors]\n",
    "        rpn_class_logits = rpn_class_logits.permute(0,2,3,1)\n",
    "        rpn_class_logits = rpn_class_logits.contiguous()\n",
    "        rpn_class_logits = rpn_class_logits.view(x.size()[0], -1, 2)\n",
    "\n",
    "        # Softmax on last dimension of BG/FG.\n",
    "        rpn_probs = self.softmax(rpn_class_logits)              # output class\n",
    "\n",
    "        # Bounding box refinement. [batch, H, W, anchors per location, depth]\n",
    "        # where depth is [x, y, log(w), log(h)]\n",
    "        rpn_bbox = self.conv_bbox(x)\n",
    "\n",
    "        # Reshape to [batch, 4, anchors]\n",
    "        rpn_bbox = rpn_bbox.permute(0,2,3,1)\n",
    "        rpn_bbox = rpn_bbox.contiguous()\n",
    "        rpn_bbox = rpn_bbox.view(x.size()[0], -1, 4)\n",
    "\n",
    "        return [rpn_class_logits, rpn_probs, rpn_bbox]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposal classifier\n",
    "\n",
    "The Faster R-CNN head contains the region proposal classifier.\n",
    "\n",
    "    self.classifier = Classifier(256, config.POOL_SIZE, config.IMAGE_SHAPE, config.NUM_CLASSES)\n",
    "\n",
    "The classifier is mainly composed of convolutional layers. The most\n",
    "interesting process is the pyramidal ROI alignment at multiple scales.\n",
    "In the Classifier class, the ROIs are assigned pyramid levels, then\n",
    "ROIAlign is performed. Note that the `pyramid_roi_align()` method uses a\n",
    "\"crop and resize\" method that is not quite identical to ROIAlign but does\n",
    "do bilinear interpolation of the feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyramid_roi_align(inputs, pool_size, image_shape):\n",
    "    \"\"\"Implements ROI Pooling on multiple levels of the feature pyramid.\n",
    "\n",
    "    Params:\n",
    "    - pool_size: [height, width] of the output pooled regions. Usually [7, 7]\n",
    "    - image_shape: [height, width, channels]. Shape of input image in pixels\n",
    "\n",
    "    Inputs:\n",
    "    - boxes: [batch, num_boxes, (y1, x1, y2, x2)] in normalized\n",
    "             coordinates.\n",
    "    - Feature maps: List of feature maps from different levels of the pyramid.\n",
    "                    Each is [batch, channels, height, width]\n",
    "\n",
    "    Output:\n",
    "    Pooled regions in the shape: [num_boxes, height, width, channels].\n",
    "    The width and height are those specific in the pool_shape in the layer\n",
    "    constructor.\n",
    "    \"\"\"\n",
    "\n",
    "    # Currently only supports batchsize 1\n",
    "    for i in range(len(inputs)):\n",
    "        inputs[i] = inputs[i].squeeze(0)\n",
    "\n",
    "    # Crop boxes [batch, num_boxes, (y1, x1, y2, x2)] in normalized coords\n",
    "    boxes = inputs[0]\n",
    "\n",
    "    # Feature Maps. List of feature maps from different level of the\n",
    "    # feature pyramid. Each is [batch, height, width, channels]\n",
    "    feature_maps = inputs[1:]\n",
    "\n",
    "    # Assign each ROI to a level in the pyramid based on the ROI area.\n",
    "    y1, x1, y2, x2 = boxes.chunk(4, dim=1)\n",
    "    h = y2 - y1\n",
    "    w = x2 - x1\n",
    "\n",
    "    # Equation 1 in the Feature Pyramid Networks paper. Account for\n",
    "    # the fact that our coordinates are normalized here.\n",
    "    # e.g. a 224x224 ROI (in pixels) maps to P4\n",
    "    image_area = Variable(torch.FloatTensor([float(image_shape[0]*image_shape[1])]), requires_grad=False)\n",
    "    if boxes.is_cuda:\n",
    "        image_area = image_area.cuda()\n",
    "    roi_level = 4 + log2(torch.sqrt(h*w)/(224.0/torch.sqrt(image_area)))\n",
    "    roi_level = roi_level.round().int()\n",
    "    roi_level = roi_level.clamp(2,5)\n",
    "\n",
    "\n",
    "    # Loop through levels and apply ROI pooling to each. P2 to P5.\n",
    "    pooled = []\n",
    "    box_to_level = []\n",
    "    for i, level in enumerate(range(2, 6)):\n",
    "        ix  = roi_level==level\n",
    "        if not ix.any():\n",
    "            continue\n",
    "        ix = torch.nonzero(ix)[:,0]\n",
    "        level_boxes = boxes[ix.data, :]\n",
    "\n",
    "        # Keep track of which box is mapped to which level\n",
    "        box_to_level.append(ix.data)\n",
    "\n",
    "        # Stop gradient propogation to ROI proposals\n",
    "        level_boxes = level_boxes.detach()\n",
    "\n",
    "        # Crop and Resize\n",
    "        # From Mask R-CNN paper: \"We sample four regular locations, so\n",
    "        # that we can evaluate either max or average pooling. In fact,\n",
    "        # interpolating only a single value at each bin center (without\n",
    "        # pooling) is nearly as effective.\"\n",
    "        #\n",
    "        # Here we use the simplified approach of a single value per bin,\n",
    "        # which is how it's done in tf.crop_and_resize()\n",
    "        # Result: [batch * num_boxes, pool_height, pool_width, channels]\n",
    "        ind = Variable(torch.zeros(level_boxes.size()[0]),requires_grad=False).int()\n",
    "        if level_boxes.is_cuda:\n",
    "            ind = ind.cuda()\n",
    "        feature_maps[i] = feature_maps[i].unsqueeze(0)  #CropAndResizeFunction needs batch dimension\n",
    "        pooled_features = CropAndResizeFunction(pool_size, pool_size, 0)(feature_maps[i], level_boxes, ind)\n",
    "        pooled.append(pooled_features)\n",
    "\n",
    "    # Pack pooled features into one tensor\n",
    "    pooled = torch.cat(pooled, dim=0)\n",
    "\n",
    "    # Pack box_to_level mapping into one array and add another\n",
    "    # column representing the order of pooled boxes\n",
    "    box_to_level = torch.cat(box_to_level, dim=0)\n",
    "\n",
    "    # Rearrange pooled features to match the order of the original boxes\n",
    "    _, box_to_level = torch.sort(box_to_level)\n",
    "    pooled = pooled[box_to_level, :, :]\n",
    "\n",
    "    return pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask Head\n",
    "\n",
    "The last step creates the mask head. We have convolutions and upsampling to the original image size.\n",
    "The difference between the classifier head and the mask head is that the classifier head takes the proposal bounding boxes\n",
    "and outputs refined final bounding boxes, whereas the mask head uses those final refined bounding boxes to create masks.\n",
    "\n",
    "The mask creation step in the MaskRCNN class looks like this:\n",
    "\n",
    "    self.mask = Mask(256, config.MASK_POOL_SIZE, config.IMAGE_SHAPE, config.NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction function\n",
    "\n",
    "Now let's look at the overall\n",
    "`predict()` method. There are two modes there: inference (evaluation/validation/test) mode and training mode.\n",
    "Both modes have the same steps, but during training, we have to calculate ROI sizes for comparisons between the predicted output and the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict(self, input, mode):\n",
    "        molded_images = input[0]\n",
    "        image_metas = input[1]\n",
    "\n",
    "        if mode == 'inference':\n",
    "            self.eval()\n",
    "        elif mode == 'training':\n",
    "            self.train()\n",
    "\n",
    "        # Set batchnorm always in eval mode during training\n",
    "        def set_bn_eval(m):\n",
    "            classname = m.__class__.__name__\n",
    "            if classname.find('BatchNorm') != -1:\n",
    "                m.eval()\n",
    "                \n",
    "        self.apply(set_bn_eval)\n",
    "\n",
    "        # Feature extraction\n",
    "        [p2_out, p3_out, p4_out, p5_out, p6_out] = self.fpn(molded_images)\n",
    "\n",
    "        # Note that P6 is used in RPN, but not in the classifier heads.\n",
    "        rpn_feature_maps = [p2_out, p3_out, p4_out, p5_out, p6_out]\n",
    "        mrcnn_feature_maps = [p2_out, p3_out, p4_out, p5_out]\n",
    "\n",
    "        # Loop through pyramid layers\n",
    "        layer_outputs = []  # list of lists\n",
    "        for p in rpn_feature_maps:\n",
    "            layer_outputs.append(self.rpn(p))\n",
    "\n",
    "        # Concatenate layer outputs\n",
    "        # Convert from list of lists of level outputs to list of lists\n",
    "        # of outputs across levels.\n",
    "        # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]]\n",
    "        outputs = list(zip(*layer_outputs))\n",
    "        outputs = [torch.cat(list(o), dim=1) for o in outputs]\n",
    "        rpn_class_logits, rpn_class, rpn_bbox = outputs\n",
    "\n",
    "        # Generate proposals\n",
    "        # Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates\n",
    "        # and zero padded.\n",
    "        proposal_count = self.config.POST_NMS_ROIS_TRAINING if mode == \"training\" \\\n",
    "            else self.config.POST_NMS_ROIS_INFERENCE\n",
    "        rpn_rois = proposal_layer([rpn_class, rpn_bbox],\n",
    "                                  proposal_count=proposal_count,\n",
    "                                  nms_threshold=self.config.RPN_NMS_THRESHOLD,\n",
    "                                  anchors=self.anchors,\n",
    "                                  config=self.config)\n",
    "\n",
    "        if mode == 'inference':\n",
    "            # Network Heads\n",
    "            # Proposal classifier and BBox regressor heads\n",
    "            mrcnn_class_logits, mrcnn_class, mrcnn_bbox = self.classifier(mrcnn_feature_maps, rpn_rois)\n",
    "\n",
    "            # Detections\n",
    "            # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in image coordinates\n",
    "            detections = detection_layer(self.config, rpn_rois, mrcnn_class, mrcnn_bbox, image_metas)\n",
    "\n",
    "            # Convert boxes to normalized coordinates\n",
    "            # TODO: let DetectionLayer return normalized coordinates to avoid\n",
    "            #       unnecessary conversions\n",
    "            h, w = self.config.IMAGE_SHAPE[:2]\n",
    "            scale = Variable(torch.from_numpy(np.array([h, w, h, w])).float(), requires_grad=False)\n",
    "            if self.config.GPU_COUNT:\n",
    "                scale = scale.cuda()\n",
    "            detection_boxes = detections[:, :4] / scale\n",
    "\n",
    "            # Add back batch dimension\n",
    "            detection_boxes = detection_boxes.unsqueeze(0)\n",
    "\n",
    "            # Create masks for detections\n",
    "            mrcnn_mask = self.mask(mrcnn_feature_maps, detection_boxes)\n",
    "\n",
    "            # Add back batch dimension\n",
    "            detections = detections.unsqueeze(0)\n",
    "            mrcnn_mask = mrcnn_mask.unsqueeze(0)\n",
    "\n",
    "            return [detections, mrcnn_mask]\n",
    "\n",
    "        elif mode == 'training':\n",
    "\n",
    "            gt_class_ids = input[2]\n",
    "            gt_boxes = input[3]\n",
    "            gt_masks = input[4]\n",
    "\n",
    "            # Normalize coordinates\n",
    "            h, w = self.config.IMAGE_SHAPE[:2]\n",
    "            scale = Variable(torch.from_numpy(np.array([h, w, h, w])).float(), requires_grad=False)\n",
    "            if self.config.GPU_COUNT:\n",
    "                scale = scale.cuda()\n",
    "            gt_boxes = gt_boxes / scale\n",
    "\n",
    "            # Generate detection targets\n",
    "            # Subsamples proposals and generates target outputs for training\n",
    "            # Note that proposal class IDs, gt_boxes, and gt_masks are zero\n",
    "            # padded. Equally, returned rois and targets are zero padded.\n",
    "            rois, target_class_ids, target_deltas, target_mask = \\\n",
    "                detection_target_layer(rpn_rois, gt_class_ids, gt_boxes, gt_masks, self.config)\n",
    "\n",
    "            if not rois.size():\n",
    "                mrcnn_class_logits = Variable(torch.FloatTensor())\n",
    "                mrcnn_class = Variable(torch.IntTensor())\n",
    "                mrcnn_bbox = Variable(torch.FloatTensor())\n",
    "                mrcnn_mask = Variable(torch.FloatTensor())\n",
    "                if self.config.GPU_COUNT:\n",
    "                    mrcnn_class_logits = mrcnn_class_logits.cuda()\n",
    "                    mrcnn_class = mrcnn_class.cuda()\n",
    "                    mrcnn_bbox = mrcnn_bbox.cuda()\n",
    "                    mrcnn_mask = mrcnn_mask.cuda()\n",
    "            else:\n",
    "                # Network Heads\n",
    "                # Proposal classifier and BBox regressor heads\n",
    "                mrcnn_class_logits, mrcnn_class, mrcnn_bbox = self.classifier(mrcnn_feature_maps, rois)\n",
    "\n",
    "                # Create masks for detections\n",
    "                mrcnn_mask = self.mask(mrcnn_feature_maps, rois)\n",
    "\n",
    "            return [rpn_class_logits, rpn_bbox, target_class_ids, mrcnn_class_logits, target_deltas, mrcnn_bbox, target_mask, mrcnn_mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Check `config.py` to see the possible\n",
    "configuration templates to change configuration information such as image size and number of classes.\n",
    "Sample configuration and dataset setup can be seen in coco.py.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask R-CNN in TorchVision\n",
    "\n",
    "OK, let's get it all working.\n",
    "\n",
    "You can use the Mask R-CNN implementation from [the multimodallearning Github repository](https://github.com/multimodallearning/pytorch-mask-rcnn)\n",
    "or [the Pytorch torchvision R-CNN implementation](https://pytorch.org/vision/stable/models.html#mask-r-cnn).\n",
    "\n",
    "This is a quickstart on the torchvision version of Mask R-CNN.\n",
    "\n",
    "For help with fine tuning, see [the PyTorch instance segmentation fine tuning tutorial](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html).\n",
    "\n",
    "1. Clone this repository.\n",
    "\n",
    "    git clone https://github.com/multimodallearning/pytorch-mask-rcnn.git\n",
    "    \n",
    "2. We use functions from two more repositories that need to be build with the right --arch option for cuda support. The two functions are Non-Maximum Suppression from ruotianluo's pytorch-faster-rcnn repository and longcw's RoiAlign.\n",
    "\n",
    "|GPU\t|arch|\n",
    "|-----|-----|\n",
    "|TitanX|\tsm_52|\n",
    "|GTX 960M|\tsm_50|\n",
    "|GTX 1070|\tsm_61|\n",
    "|GTX 1080 (Ti)|\tsm_61|\n",
    "    cd nms/src/cuda/\n",
    "    nvcc -c -o nms_kernel.cu.o nms_kernel.cu -x cu -Xcompiler -fPIC -arch=[arch]\n",
    "    cd ../../\n",
    "    python build.py\n",
    "    cd ../\n",
    "\n",
    "    cd roialign/roi_align/src/cuda/\n",
    "    nvcc -c -o crop_and_resize_kernel.cu.o crop_and_resize_kernel.cu -x cu -Xcompiler -fPIC -arch=[arch]\n",
    "    cd ../../\n",
    "    python build.py\n",
    "    cd ../../\n",
    "\n",
    "3. As we use the COCO dataset (https://cocodataset.org/#home) install the Python COCO API(https://github.com/cocodataset/cocoapi) and create a symlink.\n",
    "\n",
    "    ln -s /path/to/coco/cocoapi/PythonAPI/pycocotools/ pycocotools\n",
    "    \n",
    "Download the pretrained models on COCO and ImageNet from https://drive.google.com/drive/folders/1LXUgC2IZUYNEoXr05tdqyKFZY0pZyPDc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo\n",
    "\n",
    "If this doesn't work with your version of torchvision, you may need to re-install torchvison.\n",
    "\n",
    "To test your installation simply run the demo with\n",
    "\n",
    "    python demo.py\n",
    "\n",
    "## Training on COCO\n",
    "\n",
    "Training and evaluation code is in coco.py. You can run it from the command line as such:\n",
    "\n",
    "    # Train a new model starting from pre-trained COCO weights\n",
    "    python coco.py train --dataset=/path/to/coco/ --model=coco\n",
    "\n",
    "    # Train a new model starting from ImageNet weights\n",
    "    python coco.py train --dataset=/path/to/coco/ --model=imagenet\n",
    "\n",
    "    # Continue training a model that you had trained earlier\n",
    "    python coco.py train --dataset=/path/to/coco/ --model=/path/to/weights.h5\n",
    "\n",
    "    # Continue training the last model you trained. This will find\n",
    "    # the last trained weights in the model directory.\n",
    "    python coco.py train --dataset=/path/to/coco/ --model=last\n",
    "\n",
    "If you have not yet downloaded the COCO dataset you should run the command with the download option set, e.g.:\n",
    "\n",
    "    # Train a new model starting from pre-trained COCO weights\n",
    "    python coco.py train --dataset=/path/to/coco/ --model=coco --download=true\n",
    "\n",
    "You can also run the COCO evaluation code with:\n",
    "\n",
    "    # Run COCO evaluation on the last trained model\n",
    "    python coco.py evaluate --dataset=/path/to/coco/ --model=last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a pre-trained Mask R-CNN model on test images\n",
    "\n",
    "First, let's copy some utility code from the torchvision library, load a pre-trained Mask R-CNN model,\n",
    "and create a dataloader for the COCO validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /opt/pytorch/vision/references/detection/utils.py /home/jovyan/work/RTML/Mask\\ R-CNN/\n",
    "!cp /opt/pytorch/vision/references/detection/coco_utils.py /home/jovyan/work/RTML/Mask\\ R-CNN/\n",
    "!cp /opt/pytorch/vision/references/detection/transforms.py /home/jovyan/work/RTML/Mask\\ R-CNN/\n",
    "!cp /opt/pytorch/vision/references/detection/engine.py /home/jovyan/work/RTML/Mask\\ R-CNN/\n",
    "!cp /opt/pytorch/vision/references/detection/coco_eval.py /home/jovyan/work/RTML/Mask\\ R-CNN/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision.datasets import CocoDetection\n",
    "\n",
    "import utils\n",
    "from coco_utils import get_coco\n",
    "import transforms\n",
    "\n",
    "# Load a model pre-trained on COCO and put it in inference mode\n",
    "\n",
    "print('Loading pretrained model...')\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True).cuda()\n",
    "model.eval()\n",
    "\n",
    "# Load the COCO 2017 train and val sets. We use the CocoDetection class definition\n",
    "# from ./coco_utils.py, not the original torchvision.CocoDetection class. Also, we\n",
    "# use transforms from ./transforms, not torchvision.transforms, because they need\n",
    "# to transform the bboxes and masks along with the image.\n",
    "\n",
    "coco_path = \"/home/jovyan/work/COCO\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "print('Loading COCO train, val datasets...')\n",
    "coco_train_dataset = get_coco(coco_path, 'train', transform)\n",
    "coco_val_dataset = get_coco(coco_path, 'val', transform)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(coco_val_dataset, batch_size=8, shuffle=False, num_workers=4, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(val_dataloader))\n",
    "images = [ img.cuda() for img in images ]\n",
    "predictions = model(images)\n",
    "\n",
    "print('Prediction keys:', list(dict(predictions[0])))\n",
    "print('Boxes shape:', predictions[0]['boxes'].shape)\n",
    "print('Labels shape:', predictions[0]['labels'].shape)\n",
    "print('Scores shape:', predictions[0]['scores'].shape)\n",
    "print('Masks shape:', predictions[0]['masks'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `predictions` list has one entry for each element of the batch. Each entry has the following keys:\n",
    "1. `boxes`: A tensor containing $[x1,y1,x2,y2]$ coordinates for the 100 top-scoring bounding boxes.\n",
    "2. `labels`: A tensor containing integer IDs of the labels corresponding to the 100 top bounding boxes.\n",
    "3. `scores`: A tensor containing the scores of the top 100 bounding boxes, sorted from highest score to lowest.\n",
    "4. `masks`: The mask corresponding to the most likely class for each of the top 100 bounding boxes. Each mask is the same size as the input image.\n",
    "\n",
    "With that information, let's write some code to visualize a result. The `draw_segmentation_map()` function is\n",
    "adapted from [Debugger Cafe's tutorial on Mask R-CNN](https://debuggercafe.com/instance-segmentation-with-pytorch-and-mask-r-cnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "# Array of labels for COCO dataset (91 elements)\n",
    "\n",
    "coco_names = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "# Random colors to use for labeling objects\n",
    "\n",
    "COLORS = np.random.uniform(0, 255, size=(len(coco_names), 3)).astype(np.uint8)\n",
    "\n",
    "# Overlay masks, bounding boxes, and labels on input numpy image\n",
    "\n",
    "def draw_segmentation_map(image, masks, boxes, labels):\n",
    "    alpha = 1\n",
    "    beta = 0.5 # transparency for the segmentation map\n",
    "    gamma = 0 # scalar added to each sum\n",
    "    # convert from RGB to OpenCV BGR format\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    for i in range(len(masks)):\n",
    "        mask = masks[i,:,:]\n",
    "        red_map = np.zeros_like(mask).astype(np.uint8)\n",
    "        green_map = np.zeros_like(mask).astype(np.uint8)\n",
    "        blue_map = np.zeros_like(mask).astype(np.uint8)\n",
    "        # apply a randon color mask to each object\n",
    "        color = COLORS[random.randrange(0, len(COLORS))]\n",
    "        red_map[mask > 0.5] = color[0]\n",
    "        green_map[mask > 0.5] = color[1]\n",
    "        blue_map[mask > 0.5] = color[2]\n",
    "        # combine all the masks into a single image\n",
    "        segmentation_map = np.stack([red_map, green_map, blue_map], axis=2)\n",
    "        # apply colored mask to the image\n",
    "        image = cv2.addWeighted(image, alpha, segmentation_map, beta, gamma)\n",
    "        # draw the bounding box around each object\n",
    "        p1 = (int(boxes[i][0]), int(boxes[i][1]))\n",
    "        p2 = (int(boxes[i][2]), int(boxes[i][3]))\n",
    "        color = (int(color[0]), int(color[1]), int(color[2]))\n",
    "        cv2.rectangle(image, p1, p2, color, 2)\n",
    "        # put the label text above the objects\n",
    "        p = (int(boxes[i][0]), int(boxes[i][1]-10))\n",
    "        cv2.putText(image, labels[i], p, cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2, cv2.LINE_AA)\n",
    "    \n",
    "    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Overlay masks, bounding boxes, and labels of objects with scores greater than\n",
    "# threshold on one of the images in the input tensor using the predictions output by Mask R-CNN.\n",
    "\n",
    "def prediction_to_mask_image(images, predictions, img_index, threshold):\n",
    "    scores = predictions[img_index]['scores']\n",
    "    boxes_to_use = scores >= threshold\n",
    "    img = (images[img_index].cpu().permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n",
    "    masks = predictions[img_index]['masks'][boxes_to_use, :, :].cpu().detach().squeeze(1).numpy()\n",
    "    boxes = predictions[img_index]['boxes'][boxes_to_use, :].cpu().detach().numpy()\n",
    "    labels = predictions[img_index]['labels'][boxes_to_use].cpu().numpy()\n",
    "    labels = [ coco_names[l] for l in labels ]\n",
    "\n",
    "    return draw_segmentation_map(img, masks, boxes, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the code above to visualize the predictions for the first image\n",
    "in the validation set (index 0), using a threshold of 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "masked_img = prediction_to_mask_image(images, predictions, 0, 0.5)\n",
    "plt.figure(1, figsize=(12, 9), dpi=100)\n",
    "plt.imshow(masked_img)\n",
    "plt.title('Validation image result')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on the COCO validation set\n",
    "\n",
    "Let's get predictions in a loop for the full COCO 2017 validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import evaluate\n",
    "\n",
    "results = evaluate(model, val_dataloader, 'cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent Work\n",
    "\n",
    "Do the following:\n",
    "\n",
    " 1. Get the demo up and running.\n",
    " 2. Evaluate the pretrained COCO model on the COCO validation set we used last week.\n",
    " 3. Run Mask R-CNN on the Cityscapes dataset in inference mode. Report your results and see what errors you find. [Here is the Cityscapes link](https://www.cityscapes-dataset.com/), and we'll also provide a copy of the dataset on the lab server.\n",
    " 4. Fine tune the COCO Mask R-CNN on Cityscapes and report the results. Are they close to what's reported in the Mask R-CNN paper?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional information you might want to know\n",
    "\n",
    "The huge problem on Cityscapes is about the dataset. It is difficult to augment the dataset. We can address the problem by modifying the dataset class.\n",
    "\n",
    "In <code>coco_utils.py</code>, add a function <code>get_cityscapes</code> for getting the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cityscapes(root, ann_file, transforms):\n",
    "    t = [ConvertCocoPolysToMask()]\n",
    "\n",
    "    if transforms is not None:\n",
    "        t.append(transforms)\n",
    "    transforms = T.Compose(t)\n",
    "    dataset = CocoDetection(root, ann_file, transforms=transforms)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to use it, import the annotation file and images path into your program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/root/Cityscapes/\"                        # last year cityscapes path\n",
    "train_annotation_file = \"[cityscapes_train].json\" # cityscapes annotation json train file\n",
    "val_annotation_file = \"[cityscapes_val].json\"     # cityscapes annotation json eval file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_cityscapes(path,train_annotation_file, transform)                                \n",
    "\n",
    "val_dataset = get_cityscapes(path,val_annotation_file, transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For augmentation, I copied from my thesis-Thyroid nodule dataset, if you like, you can modify the code for supporting the augmentation.\n",
    "\n",
    "**_Using the dataset, you need to convert json mask file to mask images in different directory name but the mask image files need to be the same name as source images._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations.augmentations.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from os import listdir\n",
    "import logging\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.transforms import RandomCrop\n",
    "\n",
    "class ThyroidNoduleDataset(Dataset):\n",
    "    def __init__(self, imgs_dir, masks_dir, transform=None):\n",
    "        # source images direction\n",
    "        self.imgs_dir = imgs_dir\n",
    "        # masks images direction\n",
    "        self.masks_dir = masks_dir\n",
    "        self.transform = transform\n",
    "        self.transform = A.Compose([t for t in transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n",
    "\n",
    "        list_files = listdir(imgs_dir)\n",
    "        list_masks = listdir(masks_dir)\n",
    "\n",
    "        # remove unmasked images\n",
    "        for file in list_files:\n",
    "            if file not in list_masks:\n",
    "                list_files.remove(file)\n",
    "\n",
    "        self.ids = [file for file in list_files if not file.startswith('.')]\n",
    "\n",
    "        logging.info(f'Creating dataset with {len(self.ids)} examples')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def preprocess_mask(self, mask):\n",
    "        if len(mask.shape) == 2:\n",
    "            mask = np.expand_dims(mask, axis=2)\n",
    "        #mask = mask.astype(np.float32)\n",
    "        mask[mask <= 100] = 0\n",
    "        mask[mask > 100] = 1\n",
    "        return mask\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.ids[i]\n",
    "        mask_file = self.masks_dir + idx\n",
    "        img_file = self.imgs_dir + idx\n",
    "\n",
    "        image = cv2.imread(img_file)\n",
    "        if len(image.shape) == 2:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif len(image.shape) == 3 and image.shape[2] == 4:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)\n",
    "        else:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(mask_file, cv2.IMREAD_GRAYSCALE)\n",
    "        mask = self.preprocess_mask(mask)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"]\n",
    "            if not isinstance(mask, np.ndarray):\n",
    "                mask = torch.reshape(mask, [1,mask.shape[0], mask.shape[1]])\n",
    "            else:\n",
    "                image = numpy_to_torch(image) / 255.0\n",
    "                mask = numpy_to_torch(mask)\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'mask': mask,\n",
    "            'img_file': img_file,\n",
    "            'mask_file': mask_file,\n",
    "            'file_name' : idx\n",
    "        }\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(image_size, image_size),\n",
    "        A.ShiftScaleRotate(shift_limit=0.2, scale_limit=0.15, rotate_limit=30, p=0.6, border_mode=cv2.BORDER_CONSTANT, value=0, mask_value=0),\n",
    "        #A.RandomCrop(image_size, image_size, always_apply=False, p=1.0),\n",
    "        A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_dir = 'Train/'\n",
    "imgs_dir_name = 'images'\n",
    "masks_dir_name = 'masks'\n",
    "transform = create_transform(256, True)\n",
    "dataset = ThyroidNoduleDataset(dir_img, dir_mask, transform)\n",
    "\n",
    "# get a batch data\n",
    "batch = dataset[15]\n",
    "image, mask, img_file, mask_file, img_name = batch['image'], batch['mask'], batch['img_file'], batch['mask_file'], batch['file_name']\n",
    "\n",
    "# loop random shuffle\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "for batch in train_loader:\n",
    "    # train code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting json coco file to images (Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% \n",
    "# import library\n",
    "import sys\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from utils import FileTreeMaker\n",
    "import os\n",
    "from os import walk\n",
    "import cv2\n",
    "#matplotlib.use('TkAgg')\n",
    "plt.figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "#%% Import coco files\n",
    "# annotation path with json file\n",
    "# the folder must contain\n",
    "# annotaion_dir ---- annotation_file.json\n",
    "#                |\n",
    "#                --- annotation_file ----        (image folder with same name as annotation file)\n",
    "#                                     |\n",
    "#                                     --- image1.jpg\n",
    "#                                     --- image2.png\n",
    "\n",
    "annotation_dir = 'annotation_dir'\n",
    "file_tree = FileTreeMaker()\n",
    "print(file_tree.make(root=annotation_dir, level=1, \n",
    "            exclude_names=['json', 'Label', 'txt', 'Crop'],\n",
    "            include_names=[], output_name=\"\"))\n",
    "print(file_tree.dir_full_names)\n",
    "# %%\n",
    "# get source images files list\n",
    "fs = []\n",
    "for (dirpath, dirnames, filenames) in walk(annotation_dir):\n",
    "    fs.extend(filenames)\n",
    "    break\n",
    "#cocos = []\n",
    "show = False\n",
    "for f in fs:\n",
    "    print('At folder: ', f[:-10])\n",
    "    annFile = annotation_dir + '/' + f\n",
    "    img_dir = annotation_dir + '/' + f[:-4]\n",
    "    mask_dir = annotation_dir + '/' + f[:-4] + '_Mask'\n",
    "\n",
    "    if not os.path.exists(mask_dir):\n",
    "        print(\"No folder \", mask_dir, 'exist. Create the folder')\n",
    "        os.mkdir(mask_dir)\n",
    "        print(\"Create directory finished\")\n",
    "\n",
    "    coco=COCO(annFile)\n",
    "\n",
    "    # display COCO categories and supercategories\n",
    "    cats = coco.loadCats(coco.getCatIds())\n",
    "    nms=[cat['name'] for cat in cats]\n",
    "    #print('COCO categories: {}\\n'.format(' '.join(nms)))\n",
    "    catIds = coco.getCatIds(catNms=nms);\n",
    "    #print('catIds: ', catIds)\n",
    "    imgIds = coco.getImgIds(catIds=catIds );\n",
    "    #print('imgIds: ', imgIds)\n",
    "    for id in imgIds:\n",
    "        img = coco.loadImgs(id)[0]\n",
    "        #print(img)\n",
    "\n",
    "        # load and display image\n",
    "        # use url to load image\n",
    "        I = cv2.imread(img_dir + '/' + img['file_name'])\n",
    "        if show:\n",
    "            plt.figure(num=None, figsize=(10, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "            plt.axis('off')\n",
    "            plt.imshow(I)\n",
    "            plt.show()\n",
    "\n",
    "        annIds = coco.getAnnIds(imgIds=img['id'], catIds=catIds, iscrowd=None)\n",
    "        anns = coco.loadAnns(annIds)\n",
    "        for j in range(len(anns)):\n",
    "            if j == 0:\n",
    "                mask = coco.annToMask(anns[j])\n",
    "            else:\n",
    "                mask = (mask | coco.annToMask(anns[j]))\n",
    "\n",
    "        # load and display instance annotations\n",
    "        if show:\n",
    "            plt.figure(num=None, figsize=(10, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "            plt.axis('off')\n",
    "            plt.imshow(I)\n",
    "            coco.showAnns(anns)\n",
    "            plt.show()\n",
    "\n",
    "        im = np.array(mask) * 255\n",
    "        if show:\n",
    "            print(im.shape, im.max())\n",
    "            plt.figure(num=None, figsize=(10, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "            plt.imshow(im); plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "        cv2.imwrite(mask_dir + '/' + img['file_name'], im)\n",
    "# %%\n",
    "print('finish')\n",
    "# %%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
