{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 05: Mask R-CNN and U-NET\n",
    "\n",
    "Mask R-CNN was originally released under the Caffe2 framework by the FAIR team as part of a project called \"Detectron.\"\n",
    "\n",
    "After Caffe2 and PyTorch merged, eventually, FAIR released a ground-up reimplementation of Detectron with new models and\n",
    "features. This framework is [Detectron2](https://github.com/facebookresearch/detectron2). If you're looking for a complete\n",
    "approach to bounding box regression, mask regression, and skeleton point (pose) estimation, take a look.\n",
    "\n",
    "In the meantime, however, other groups quickly implemented Mask R-CNN directly in TensorFlow and Keras.\n",
    "Today we'll analyze the network with a [PyTorch implementation of Mask R-CNN](https://github.com/multimodallearning/pytorch-mask-rcnn), because the code is easier to understand coding architecture. However, the link is old, so we will implement the code running into [PyTorch Simple Mask R-CNN](https://github.com/Okery/PyTorch-Simple-MaskRCNN).\n",
    "\n",
    "The full of Mask R-CNN structure is below:\n",
    "\n",
    "<img src=\"img/MaskRCNNArchitecture.png\" title=\"FullMaskR-CNN\" style=\"width: 860px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each intance of an object in an image, Mask R-CNN attempts to generate\n",
    " - A bounding box\n",
    " - A segmentation mask\n",
    "\n",
    "The backbone and neck of Mask R-CNN are based on\n",
    " - A feature pyramid networks (FPN)\n",
    " - ResNet 101\n",
    "\n",
    "## Feature Pyramid Networks\n",
    "\n",
    "We've seen the feature pyramid network (FPN) in YOLOv3 and YOLOv4. It is a feature extractor using a pyramid concept.\n",
    "We begin with ordinary progressive downsampling of the input to get a multiscale representation of the input, but rather\n",
    "than using that \"low-level\" multiscale representation directly, we progressively upsample the coarse representation of\n",
    "the input using input from the low-level feature maps. The idea is shown in the left-hand panel of the diagram above.\n",
    "By incorporating information from both the low-level \"bottom-up\" fine grained feature map and the upsampled coarser grained feature\n",
    "map in the pyramid, the fine grained representation at the bottom of the pyramid contain much more useful or more \"semantic\"\n",
    "information about the input.\n",
    "\n",
    "Feature pyramids can, in principle, come in many different forms. Refer to the figure below, which is\n",
    "from [Towards Data Science](https://towardsdatascience.com/review-fpn-feature-pyramid-network-object-detection-262fc7482610):\n",
    "\n",
    "<img src=\"img/Pyramid.png\" title=\"PyramidNets\" style=\"width: 800px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classical image pyramid, used by many techniques aiming at multiscale detection, looks like (a).\n",
    "Classifiers we built early on in the class, such as AlexNet, look like (b). (c) and (d) utilize multiple\n",
    "feature maps derived from the input through progressive downscaling. The difference in the FPN (d) is\n",
    "the inclusion of both bottom up and top down pathways:\n",
    "\n",
    "<img src=\"img/bottomup.jpeg\" title=\"Bottom-up\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that as we process the input in multiple progressively downsampled layers, we are increasingly analyzing higher-level features\n",
    "with larger receptive fields with some invariance to imaging conditions (translation, scale, lighting, etc.).\n",
    "\n",
    "The top-down representations use progressively *upsampled* layers in which we are increasingly analyzing the input at high resolution but\n",
    "with all the benefits of the downsampled representation.\n",
    "\n",
    "The main risk of the top-down upsampling is that we would lose information about the details of the original input in constructing the\n",
    "fine-grained feature maps. For that reason, we add lateral connections to the bottom-up feature maps of the same size:\n",
    "\n",
    "<img src=\"img/lateralconnection.png\" title=\"lateralconnection\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet backbone\n",
    "\n",
    "The bottom-up part of the FPN used in Mask-RCNN is ResNet. It is used similar to how\n",
    "Darknet-53 is used in YOLO. We take the classifier structure as the bottom-up half of\n",
    "the pyramid, then we add the top down part to obtain the FPN.\n",
    "\n",
    "Mask R-CNN taps into ResNet in 4 or 5 places according to the implementation, at the ouptut of various residual blocks.\n",
    "\n",
    "Here's a figure to explain this, this time from [Jonathan Hui's Medium site](https://jonathan-hui.medium.com/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c):\n",
    "\n",
    "<img src=\"img/upanddown.png\" title=\"upanddown\" style=\"width: 500px;\" />\n",
    "\n",
    "(There would be a P6 there as well if we're extracting a 5-scale pyramid)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region Proposal Network (RPN)\n",
    "\n",
    "A Region Proposal Network (RPN) is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position.\n",
    "\n",
    "RPNs are designed to predict region proposals with a wide range of scales and aspect ratios. RPNs use anchor boxes that serve as references at multiple scales and aspect ratios. The scheme can be thought of as a pyramid of regression references, which avoids enumerating images or filters of multiple scales or aspect ratios.\n",
    "\n",
    "The Faster R-CNN RPN connects to the top of the pyramid. It performs classification and bounding box regression for each possible proposal.\n",
    "\n",
    "<img src=\"img/RPN.png\" title=\"RPN\" style=\"width: 600px;\" />\n",
    "\n",
    "## Detection network\n",
    "\n",
    "The detection network uses the results of the RPN as well as the output of the FPN. With the RPN bounding box as input, we assign the box to one of the levels of the pyramid.\n",
    "Specifically, we use\n",
    "\n",
    "$$ k = \\left\\lfloor k_0 +\\log_2\\left( \\frac{\\sqrt{wh}}{224} \\right) \\right\\rfloor $$\n",
    "\n",
    "Then the ROIAlign block interpolates the appropriate set of features from the best level (level $k$) of the pyramid. The region is aligned and scaled to a size of\n",
    "56$\\times$56, and the resulting representation is forwarded to the mask prediction head.\n",
    "\n",
    "## Mask Head\n",
    "\n",
    "The mask head is a FCN that up-samples from the detection result, and the patch size is finally re-scaled back to the input size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding\n",
    "\n",
    "So let's start investigating how Mask R-CNN works in detail.\n",
    "First, open the [Github respository](https://github.com/multimodallearning/pytorch-mask-rcnn.git):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaskRCNN class (model.py)\n",
    "\n",
    "Let's visit the MaskRCNN class and its <code>init()</code> function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, config, model_dir):\n",
    "        \"\"\"\n",
    "        config: A Sub-class of the Config class\n",
    "        model_dir: Directory to save training logs and trained weights\n",
    "        \"\"\"\n",
    "        super(MaskRCNN, self).__init__()\n",
    "        self.config = config\n",
    "        self.model_dir = model_dir\n",
    "        self.set_log_dir()\n",
    "        self.build(config=config)\n",
    "        self.initialize_weights()\n",
    "        self.loss_history = []\n",
    "        self.val_loss_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important action here is calling `build()`, which creates the network based on a\n",
    "configuration object. `set_log_dir()` just sets up saving to a log file, and `initialize_weights()` loads\n",
    "weights. Let's take a look at the `build()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build(self, config):\n",
    "    \"\"\"Build Mask R-CNN architecture.\n",
    "    \"\"\"\n",
    "\n",
    "    # Image size must be dividable by 2 multiple times\n",
    "    h, w = config.IMAGE_SHAPE[:2]\n",
    "    if h / 2**6 != int(h / 2**6) or w / 2**6 != int(w / 2**6):\n",
    "        raise Exception(\"Image size must be dividable by 2 at least 6 times \"\n",
    "                        \"to avoid fractions when downscaling and upscaling.\"\n",
    "                        \"For example, use 256, 320, 384, 448, 512, ... etc. \")\n",
    "\n",
    "    # Build the shared convolutional layers.\n",
    "    # Bottom-up Layers\n",
    "    # Returns a list of the last layers of each stage, 5 in total.\n",
    "    # Don't create the thead (stage 5), so we pick the 4th item in the list.\n",
    "    resnet = ResNet(\"resnet101\", stage5=True)\n",
    "    C1, C2, C3, C4, C5 = resnet.stages()\n",
    "\n",
    "    # Top-down Layers\n",
    "    # TODO: add assert to varify feature map sizes match what's in config\n",
    "    self.fpn = FPN(C1, C2, C3, C4, C5, out_channels=256)\n",
    "\n",
    "    # Generate Anchors\n",
    "    self.anchors = Variable(torch.from_numpy(utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES,\n",
    "                                                                            config.RPN_ANCHOR_RATIOS,\n",
    "                                                                            config.BACKBONE_SHAPES,\n",
    "                                                                            config.BACKBONE_STRIDES,\n",
    "                                                                            config.RPN_ANCHOR_STRIDE)).float(), requires_grad=False)\n",
    "    if self.config.GPU_COUNT:\n",
    "        self.anchors = self.anchors.cuda()\n",
    "\n",
    "    # RPN\n",
    "    self.rpn = RPN(len(config.RPN_ANCHOR_RATIOS), config.RPN_ANCHOR_STRIDE, 256)\n",
    "\n",
    "    # FPN Classifier\n",
    "    self.classifier = Classifier(256, config.POOL_SIZE, config.IMAGE_SHAPE, config.NUM_CLASSES)\n",
    "\n",
    "    # FPN Mask\n",
    "    self.mask = Mask(256, config.MASK_POOL_SIZE, config.IMAGE_SHAPE, config.NUM_CLASSES)\n",
    "\n",
    "    # Fix batch norm layers\n",
    "    def set_bn_fix(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('BatchNorm') != -1:\n",
    "            for p in m.parameters(): p.requires_grad = False\n",
    "\n",
    "    self.apply(set_bn_fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backbone\n",
    "\n",
    "The backbone is initialized then tapped into in the lines\n",
    "\n",
    "    resnet = ResNet(\"resnet101\", stage5=True)\n",
    "    C1, C2, C3, C4, C5 = resnet.stages()\n",
    "\n",
    "We see that this version is using ResNet101 and extracting a 5-stages pyramid. How to find out what stages are being used? Take\n",
    "a look at the ResNet class itself and take a look at its `__init__` and forward methods. We see that the C1-C5 are the main\n",
    "blocks of the network. The first (C1) is a single 7$\\times$7 convolution with batch norm, ReLU, and a MaxPool operation. The others\n",
    "(C2-C5) are ResNet residual blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, architecture, stage5=False):\n",
    "        super(ResNet, self).__init__()\n",
    "        assert architecture in [\"resnet50\", \"resnet101\"]\n",
    "        self.inplanes = 64\n",
    "        self.layers = [3, 4, {\"resnet50\": 6, \"resnet101\": 23}[architecture], 3]\n",
    "        self.block = Bottleneck\n",
    "        self.stage5 = stage5\n",
    "\n",
    "        self.C1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(64, eps=0.001, momentum=0.01),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SamePad2d(kernel_size=3, stride=2),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.C2 = self.make_layer(self.block, 64, self.layers[0])\n",
    "        self.C3 = self.make_layer(self.block, 128, self.layers[1], stride=2)\n",
    "        self.C4 = self.make_layer(self.block, 256, self.layers[2], stride=2)\n",
    "        if self.stage5:\n",
    "            self.C5 = self.make_layer(self.block, 512, self.layers[3], stride=2)\n",
    "        else:\n",
    "            self.C5 = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.C1(x)\n",
    "        x = self.C2(x)\n",
    "        x = self.C3(x)\n",
    "        x = self.C4(x)\n",
    "        x = self.C5(x)\n",
    "        return x\n",
    "\n",
    "    def stages(self):\n",
    "        return [self.C1, self.C2, self.C3, self.C4, self.C5]\n",
    "\n",
    "    def make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(planes * block.expansion, eps=0.001, momentum=0.01),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FPN\n",
    "\n",
    "Let's go back to the MaskRCNN class. The next stage is FPN (Top-down layers)\n",
    "\n",
    "    self.fpn = FPN(C1, C2, C3, C4, C5, out_channels=256)\n",
    "\n",
    "The main idea of the FPN is in its `forward()` method. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPN(nn.Module):\n",
    "    def __init__(self, C1, C2, C3, C4, C5, out_channels):\n",
    "        super(FPN, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.C1 = C1\n",
    "        self.C2 = C2\n",
    "        self.C3 = C3\n",
    "        self.C4 = C4\n",
    "        self.C5 = C5\n",
    "        self.P6 = nn.MaxPool2d(kernel_size=1, stride=2)\n",
    "        self.P5_conv1 = nn.Conv2d(2048, self.out_channels, kernel_size=1, stride=1)\n",
    "        self.P5_conv2 = nn.Sequential(\n",
    "            SamePad2d(kernel_size=3, stride=1),\n",
    "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1),\n",
    "        )\n",
    "        self.P4_conv1 =  nn.Conv2d(1024, self.out_channels, kernel_size=1, stride=1)\n",
    "        self.P4_conv2 = nn.Sequential(\n",
    "            SamePad2d(kernel_size=3, stride=1),\n",
    "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1),\n",
    "        )\n",
    "        self.P3_conv1 = nn.Conv2d(512, self.out_channels, kernel_size=1, stride=1)\n",
    "        self.P3_conv2 = nn.Sequential(\n",
    "            SamePad2d(kernel_size=3, stride=1),\n",
    "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1),\n",
    "        )\n",
    "        self.P2_conv1 = nn.Conv2d(256, self.out_channels, kernel_size=1, stride=1)\n",
    "        self.P2_conv2 = nn.Sequential(\n",
    "            SamePad2d(kernel_size=3, stride=1),\n",
    "            nn.Conv2d(self.out_channels, self.out_channels, kernel_size=3, stride=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.C1(x)\n",
    "        x = self.C2(x)\n",
    "        c2_out = x          # keep C2 output\n",
    "        x = self.C3(x)\n",
    "        c3_out = x          # keep C3 output\n",
    "        x = self.C4(x)\n",
    "        c4_out = x          # keep C4 output\n",
    "        x = self.C5(x)\n",
    "        p5_out = self.P5_conv1(x)       # top-most of pyramid\n",
    "        p4_out = self.P4_conv1(c4_out) + F.upsample(p5_out, scale_factor=2)         # lateral connections, 2nd top output\n",
    "        p3_out = self.P3_conv1(c3_out) + F.upsample(p4_out, scale_factor=2)         # lateral connections, 3rd top output\n",
    "        p2_out = self.P2_conv1(c2_out) + F.upsample(p3_out, scale_factor=2)         # lateral connections, 4th top output\n",
    "\n",
    "        p5_out = self.P5_conv2(p5_out)\n",
    "        p4_out = self.P4_conv2(p4_out)\n",
    "        p3_out = self.P3_conv2(p3_out)\n",
    "        p2_out = self.P2_conv2(p2_out)\n",
    "\n",
    "        # P6 is used for the 5th anchor scale in RPN. Generated by\n",
    "        # subsampling from P5 with stride of 2.\n",
    "        p6_out = self.P6(p5_out)        # max pooling for RPN\n",
    "\n",
    "        return [p2_out, p3_out, p4_out, p5_out, p6_out]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchors\n",
    "\n",
    "Anchor box sizes are set in the configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RPN\n",
    "\n",
    "Next, let's see how the RPN is produced.\n",
    "\n",
    "    self.rpn = RPN(len(config.RPN_ANCHOR_RATIOS), config.RPN_ANCHOR_STRIDE, 256)\n",
    "\n",
    "The RPN class is reproduced below. But also take a look at its output. It releases classes (score, and softmax), and bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN(nn.Module):\n",
    "    \"\"\"Builds the model of Region Proposal Network.\n",
    "\n",
    "    anchors_per_location: number of anchors per pixel in the feature map\n",
    "    anchor_stride: Controls the density of anchors. Typically 1 (anchors for\n",
    "                   every pixel in the feature map), or 2 (every other pixel).\n",
    "\n",
    "    Returns:\n",
    "        rpn_logits: [batch, H, W, 2] Anchor classifier logits (before softmax)\n",
    "        rpn_probs: [batch, W, W, 2] Anchor classifier probabilities.\n",
    "        rpn_bbox: [batch, H, W, (dy, dx, log(dh), log(dw))] Deltas to be\n",
    "                  applied to anchors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, anchors_per_location, anchor_stride, depth):\n",
    "        super(RPN, self).__init__()\n",
    "        self.anchors_per_location = anchors_per_location\n",
    "        self.anchor_stride = anchor_stride\n",
    "        self.depth = depth\n",
    "\n",
    "        self.padding = SamePad2d(kernel_size=3, stride=self.anchor_stride)\n",
    "        self.conv_shared = nn.Conv2d(self.depth, 512, kernel_size=3, stride=self.anchor_stride)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv_class = nn.Conv2d(512, 2 * anchors_per_location, kernel_size=1, stride=1)     # class, score\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.conv_bbox = nn.Conv2d(512, 4 * anchors_per_location, kernel_size=1, stride=1)      # x,y,w,h\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shared convolutional base of the RPN\n",
    "        x = self.relu(self.conv_shared(self.padding(x)))\n",
    "\n",
    "        # Anchor Score. [batch, anchors per location * 2, height, width].\n",
    "        rpn_class_logits = self.conv_class(x)\n",
    "\n",
    "        # Reshape to [batch, 2, anchors]\n",
    "        rpn_class_logits = rpn_class_logits.permute(0,2,3,1)\n",
    "        rpn_class_logits = rpn_class_logits.contiguous()\n",
    "        rpn_class_logits = rpn_class_logits.view(x.size()[0], -1, 2)\n",
    "\n",
    "        # Softmax on last dimension of BG/FG.\n",
    "        rpn_probs = self.softmax(rpn_class_logits)              # output class\n",
    "\n",
    "        # Bounding box refinement. [batch, H, W, anchors per location, depth]\n",
    "        # where depth is [x, y, log(w), log(h)]\n",
    "        rpn_bbox = self.conv_bbox(x)\n",
    "\n",
    "        # Reshape to [batch, 4, anchors]\n",
    "        rpn_bbox = rpn_bbox.permute(0,2,3,1)\n",
    "        rpn_bbox = rpn_bbox.contiguous()\n",
    "        rpn_bbox = rpn_bbox.view(x.size()[0], -1, 4)\n",
    "\n",
    "        return [rpn_class_logits, rpn_probs, rpn_bbox]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposal classifier\n",
    "\n",
    "The Faster R-CNN head contains the region proposal classifier.\n",
    "\n",
    "    self.classifier = Classifier(256, config.POOL_SIZE, config.IMAGE_SHAPE, config.NUM_CLASSES)\n",
    "\n",
    "The classifier is mainly composed of convolutional layers. The most\n",
    "interesting process is the pyramidal ROI alignment at multiple scales.\n",
    "At the Classifier class, the combine ROIs aligned is in function pyramid_roi_align.\n",
    "\n",
    "Let's take a look the `pyramid_roi_align()` method, which contains the cropping and resizing of incoming feature maps according to region proposals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyramid_roi_align(inputs, pool_size, image_shape):\n",
    "    \"\"\"Implements ROI Pooling on multiple levels of the feature pyramid.\n",
    "\n",
    "    Params:\n",
    "    - pool_size: [height, width] of the output pooled regions. Usually [7, 7]\n",
    "    - image_shape: [height, width, channels]. Shape of input image in pixels\n",
    "\n",
    "    Inputs:\n",
    "    - boxes: [batch, num_boxes, (y1, x1, y2, x2)] in normalized\n",
    "             coordinates.\n",
    "    - Feature maps: List of feature maps from different levels of the pyramid.\n",
    "                    Each is [batch, channels, height, width]\n",
    "\n",
    "    Output:\n",
    "    Pooled regions in the shape: [num_boxes, height, width, channels].\n",
    "    The width and height are those specific in the pool_shape in the layer\n",
    "    constructor.\n",
    "    \"\"\"\n",
    "\n",
    "    # Currently only supports batchsize 1\n",
    "    for i in range(len(inputs)):\n",
    "        inputs[i] = inputs[i].squeeze(0)\n",
    "\n",
    "    # Crop boxes [batch, num_boxes, (y1, x1, y2, x2)] in normalized coords\n",
    "    boxes = inputs[0]\n",
    "\n",
    "    # Feature Maps. List of feature maps from different level of the\n",
    "    # feature pyramid. Each is [batch, height, width, channels]\n",
    "    feature_maps = inputs[1:]\n",
    "\n",
    "    # Assign each ROI to a level in the pyramid based on the ROI area.\n",
    "    y1, x1, y2, x2 = boxes.chunk(4, dim=1)\n",
    "    h = y2 - y1\n",
    "    w = x2 - x1\n",
    "\n",
    "    # Equation 1 in the Feature Pyramid Networks paper. Account for\n",
    "    # the fact that our coordinates are normalized here.\n",
    "    # e.g. a 224x224 ROI (in pixels) maps to P4\n",
    "    image_area = Variable(torch.FloatTensor([float(image_shape[0]*image_shape[1])]), requires_grad=False)\n",
    "    if boxes.is_cuda:\n",
    "        image_area = image_area.cuda()\n",
    "    roi_level = 4 + log2(torch.sqrt(h*w)/(224.0/torch.sqrt(image_area)))\n",
    "    roi_level = roi_level.round().int()\n",
    "    roi_level = roi_level.clamp(2,5)\n",
    "\n",
    "\n",
    "    # Loop through levels and apply ROI pooling to each. P2 to P5.\n",
    "    pooled = []\n",
    "    box_to_level = []\n",
    "    for i, level in enumerate(range(2, 6)):\n",
    "        ix  = roi_level==level\n",
    "        if not ix.any():\n",
    "            continue\n",
    "        ix = torch.nonzero(ix)[:,0]\n",
    "        level_boxes = boxes[ix.data, :]\n",
    "\n",
    "        # Keep track of which box is mapped to which level\n",
    "        box_to_level.append(ix.data)\n",
    "\n",
    "        # Stop gradient propogation to ROI proposals\n",
    "        level_boxes = level_boxes.detach()\n",
    "\n",
    "        # Crop and Resize\n",
    "        # From Mask R-CNN paper: \"We sample four regular locations, so\n",
    "        # that we can evaluate either max or average pooling. In fact,\n",
    "        # interpolating only a single value at each bin center (without\n",
    "        # pooling) is nearly as effective.\"\n",
    "        #\n",
    "        # Here we use the simplified approach of a single value per bin,\n",
    "        # which is how it's done in tf.crop_and_resize()\n",
    "        # Result: [batch * num_boxes, pool_height, pool_width, channels]\n",
    "        ind = Variable(torch.zeros(level_boxes.size()[0]),requires_grad=False).int()\n",
    "        if level_boxes.is_cuda:\n",
    "            ind = ind.cuda()\n",
    "        feature_maps[i] = feature_maps[i].unsqueeze(0)  #CropAndResizeFunction needs batch dimension\n",
    "        pooled_features = CropAndResizeFunction(pool_size, pool_size, 0)(feature_maps[i], level_boxes, ind)\n",
    "        pooled.append(pooled_features)\n",
    "\n",
    "    # Pack pooled features into one tensor\n",
    "    pooled = torch.cat(pooled, dim=0)\n",
    "\n",
    "    # Pack box_to_level mapping into one array and add another\n",
    "    # column representing the order of pooled boxes\n",
    "    box_to_level = torch.cat(box_to_level, dim=0)\n",
    "\n",
    "    # Rearrange pooled features to match the order of the original boxes\n",
    "    _, box_to_level = torch.sort(box_to_level)\n",
    "    pooled = pooled[box_to_level, :, :]\n",
    "\n",
    "    return pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mask Head\n",
    "\n",
    "The last step creates the mask head. We have convolutions and upsampling to the original image size.\n",
    "The difference between the classifier head and the mask head is that the classifier head takes the proposal bounding boxes\n",
    "and outputs final bounding boxes whereas the mask head uses the final bounding boxes to create masks.\n",
    "\n",
    "The mask creation in the MaskRCNN class is here:\n",
    "\n",
    "    self.mask = Mask(256, config.MASK_POOL_SIZE, config.IMAGE_SHAPE, config.NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction function\n",
    "\n",
    "Now let's look at the overall\n",
    "`predict()` method. There are two modes there: inference (evaluate) mode and training mode.\n",
    "Both modes have the same steps, but during training, we have to calculate ROI sizes for comparisons between the predicted output and the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def predict(self, input, mode):\n",
    "        molded_images = input[0]\n",
    "        image_metas = input[1]\n",
    "\n",
    "        if mode == 'inference':\n",
    "            self.eval()\n",
    "        elif mode == 'training':\n",
    "            self.train()\n",
    "\n",
    "            # Set batchnorm always in eval mode during training\n",
    "            def set_bn_eval(m):\n",
    "                classname = m.__class__.__name__\n",
    "                if classname.find('BatchNorm') != -1:\n",
    "                    m.eval()\n",
    "\n",
    "            self.apply(set_bn_eval)\n",
    "\n",
    "        # Feature extraction\n",
    "        [p2_out, p3_out, p4_out, p5_out, p6_out] = self.fpn(molded_images)\n",
    "\n",
    "        # Note that P6 is used in RPN, but not in the classifier heads.\n",
    "        rpn_feature_maps = [p2_out, p3_out, p4_out, p5_out, p6_out]\n",
    "        mrcnn_feature_maps = [p2_out, p3_out, p4_out, p5_out]\n",
    "\n",
    "        # Loop through pyramid layers\n",
    "        layer_outputs = []  # list of lists\n",
    "        for p in rpn_feature_maps:\n",
    "            layer_outputs.append(self.rpn(p))\n",
    "\n",
    "        # Concatenate layer outputs\n",
    "        # Convert from list of lists of level outputs to list of lists\n",
    "        # of outputs across levels.\n",
    "        # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]]\n",
    "        outputs = list(zip(*layer_outputs))\n",
    "        outputs = [torch.cat(list(o), dim=1) for o in outputs]\n",
    "        rpn_class_logits, rpn_class, rpn_bbox = outputs\n",
    "\n",
    "        # Generate proposals\n",
    "        # Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates\n",
    "        # and zero padded.\n",
    "        proposal_count = self.config.POST_NMS_ROIS_TRAINING if mode == \"training\" \\\n",
    "            else self.config.POST_NMS_ROIS_INFERENCE\n",
    "        rpn_rois = proposal_layer([rpn_class, rpn_bbox],\n",
    "                                 proposal_count=proposal_count,\n",
    "                                 nms_threshold=self.config.RPN_NMS_THRESHOLD,\n",
    "                                 anchors=self.anchors,\n",
    "                                 config=self.config)\n",
    "\n",
    "        if mode == 'inference':\n",
    "            # Network Heads\n",
    "            # Proposal classifier and BBox regressor heads\n",
    "            mrcnn_class_logits, mrcnn_class, mrcnn_bbox = self.classifier(mrcnn_feature_maps, rpn_rois)\n",
    "\n",
    "            # Detections\n",
    "            # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in image coordinates\n",
    "            detections = detection_layer(self.config, rpn_rois, mrcnn_class, mrcnn_bbox, image_metas)\n",
    "\n",
    "            # Convert boxes to normalized coordinates\n",
    "            # TODO: let DetectionLayer return normalized coordinates to avoid\n",
    "            #       unnecessary conversions\n",
    "            h, w = self.config.IMAGE_SHAPE[:2]\n",
    "            scale = Variable(torch.from_numpy(np.array([h, w, h, w])).float(), requires_grad=False)\n",
    "            if self.config.GPU_COUNT:\n",
    "                scale = scale.cuda()\n",
    "            detection_boxes = detections[:, :4] / scale\n",
    "\n",
    "            # Add back batch dimension\n",
    "            detection_boxes = detection_boxes.unsqueeze(0)\n",
    "\n",
    "            # Create masks for detections\n",
    "            mrcnn_mask = self.mask(mrcnn_feature_maps, detection_boxes)\n",
    "\n",
    "            # Add back batch dimension\n",
    "            detections = detections.unsqueeze(0)\n",
    "            mrcnn_mask = mrcnn_mask.unsqueeze(0)\n",
    "\n",
    "            return [detections, mrcnn_mask]\n",
    "\n",
    "        elif mode == 'training':\n",
    "\n",
    "            gt_class_ids = input[2]\n",
    "            gt_boxes = input[3]\n",
    "            gt_masks = input[4]\n",
    "\n",
    "            # Normalize coordinates\n",
    "            h, w = self.config.IMAGE_SHAPE[:2]\n",
    "            scale = Variable(torch.from_numpy(np.array([h, w, h, w])).float(), requires_grad=False)\n",
    "            if self.config.GPU_COUNT:\n",
    "                scale = scale.cuda()\n",
    "            gt_boxes = gt_boxes / scale\n",
    "\n",
    "            # Generate detection targets\n",
    "            # Subsamples proposals and generates target outputs for training\n",
    "            # Note that proposal class IDs, gt_boxes, and gt_masks are zero\n",
    "            # padded. Equally, returned rois and targets are zero padded.\n",
    "            rois, target_class_ids, target_deltas, target_mask = \\\n",
    "                detection_target_layer(rpn_rois, gt_class_ids, gt_boxes, gt_masks, self.config)\n",
    "\n",
    "            if not rois.size():\n",
    "                mrcnn_class_logits = Variable(torch.FloatTensor())\n",
    "                mrcnn_class = Variable(torch.IntTensor())\n",
    "                mrcnn_bbox = Variable(torch.FloatTensor())\n",
    "                mrcnn_mask = Variable(torch.FloatTensor())\n",
    "                if self.config.GPU_COUNT:\n",
    "                    mrcnn_class_logits = mrcnn_class_logits.cuda()\n",
    "                    mrcnn_class = mrcnn_class.cuda()\n",
    "                    mrcnn_bbox = mrcnn_bbox.cuda()\n",
    "                    mrcnn_mask = mrcnn_mask.cuda()\n",
    "            else:\n",
    "                # Network Heads\n",
    "                # Proposal classifier and BBox regressor heads\n",
    "                mrcnn_class_logits, mrcnn_class, mrcnn_bbox = self.classifier(mrcnn_feature_maps, rois)\n",
    "\n",
    "                # Create masks for detections\n",
    "                mrcnn_mask = self.mask(mrcnn_feature_maps, rois)\n",
    "\n",
    "            return [rpn_class_logits, rpn_bbox, target_class_ids, mrcnn_class_logits, target_deltas, mrcnn_bbox, target_mask, mrcnn_mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Check `config.py` to see the possible\n",
    "configuration templates to change configuration information such as image size and number of classes.\n",
    "Sample configuration and dataset setup can be seen in coco.py.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do Mask R-CNN training\n",
    "\n",
    "OK, let's get it all working.\n",
    "\n",
    "Clone the github respository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Okery/PyTorch-Simple-MaskRCNN.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use \"train.py\" to train and the code.\n",
    "\n",
    "- dataset - available now: coco, voc. I have prepared the cityscapes class for students to modify (but do nothing)\n",
    "- data-dir - I am using my path, so please change the path\n",
    "- epoch - number of looping dataset\n",
    "- iters - max iteration per epoch, should write -1 for train all dataset.\n",
    "- use-cuda - check at the line 13 to change cuda slot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional: Download coco dataset\n",
    "\n",
    "COCO dataset directory should be like this:\n",
    "\n",
    "<code>\n",
    "coco2017/\n",
    "    annotations/\n",
    "        instances_train2017.json\n",
    "        instances_val2017.json\n",
    "        ...\n",
    "    train2017/\n",
    "        000000000009.jpg\n",
    "        ...\n",
    "    val2017/\n",
    "        000000000139.jpg\n",
    "        ...\n",
    "</code>\n",
    "\n",
    "You can create fifty-one.py to download the dataset\n",
    "- change the dataset name correctly is better\n",
    "- when I download the dataset, the data path may not ok for the code, you just change the dataset folder name and everything will be fine.\n",
    "- City scapes need to download manually and extract by the code.\n",
    "\n",
    "https://voxel51.com/docs/fiftyone/user_guide/dataset_zoo/datasets.html#dataset-zoo-cityscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fifty-one.py\n",
    "# !pip install fiftyone\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "# List available zoo datasets\n",
    "print(foz.list_zoo_datasets())\n",
    "\n",
    "#\n",
    "# Load the COCO-2017 validation split into a FiftyOne dataset\n",
    "#\n",
    "# This will download the dataset from the web, if necessary\n",
    "#\n",
    "dataset_name = \"coco-2017\"\n",
    "# dataset_name = \"cityscapes\"\n",
    "dataset = foz.load_zoo_dataset(dataset_name)\n",
    "\n",
    "# Give the dataset a new name, and make it persistent so that you can\n",
    "# work with it in future sessions\n",
    "dataset.name = dataset_name\n",
    "dataset.persistent = True\n",
    "\n",
    "# Visualize the in the App\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-NET\n",
    "\n",
    "U-Net is a convolutional neural network that was developed for biomedical image segmentation. The network is based on a fully convolutional network whose architecture was modified and extended to work with fewer training images and yield more precise segmentation.\n",
    "\n",
    "U-Net was first proposed in a research paper published in 2015.\n",
    "\n",
    "U-net architecture is symmetric and consists of two major parts: * The left part is called the contracting path, constituted by the general convolutional process.\n",
    "\n",
    "The right part is an expansive path, constituted by transposed 2D convolutional layers.\n",
    "\n",
    "The architecture of U-NET is\n",
    "\n",
    "<img src=\"img/u-net-architecture.png\" title=\"U-NET\" style=\"width: 500px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea is to supplement a usual contracting network by successive layers, where upsampling operators replace pooling operations. Hence these layers increase the resolution of the output. What’s more, a successive convolutional layer can then learn to assemble a precise output based on this information.\n",
    "\n",
    "#### Key features\n",
    "- U-Net learns segmentation in an end-to-end setting: You input a raw image and get a segmentation map as the output.\n",
    "\n",
    "- U-Net is able to precisely localize and distinguish borders: Performs classification on every pixel so that the input and output share the same size.\n",
    "\n",
    "- U-Net uses very few annotated images: Data augmentation with elastic deformations reduces the number of annotated images required for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The U-NET class\n",
    "\n",
    "Let's see the code.\n",
    "\n",
    "The example code get from [https://amaarora.github.io/2020/09/13/unet.html](https://amaarora.github.io/2020/09/13/unet.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv2(self.relu(self.conv1(x)))\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, chs=(3,64,128,256,512,1024)):\n",
    "        super().__init__()\n",
    "        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
    "        self.pool       = nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ftrs = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x)\n",
    "            ftrs.append(x)\n",
    "            x = self.pool(x)\n",
    "        return ftrs\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, chs=(1024, 512, 256, 128, 64)):\n",
    "        super().__init__()\n",
    "        self.chs         = chs\n",
    "        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n",
    "        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)]) \n",
    "        \n",
    "    def forward(self, x, encoder_features):\n",
    "        for i in range(len(self.chs)-1):\n",
    "            x        = self.upconvs[i](x)\n",
    "            enc_ftrs = self.crop(encoder_features[i], x)\n",
    "            x        = torch.cat([x, enc_ftrs], dim=1)\n",
    "            x        = self.dec_blocks[i](x)\n",
    "        return x\n",
    "    \n",
    "    def crop(self, enc_ftrs, x):\n",
    "        _, _, H, W = x.shape\n",
    "        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n",
    "        return enc_ftrs\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, enc_chs=(3,64,128,256,512,1024), dec_chs=(1024, 512, 256, 128, 64), num_class=1, retain_dim=False, out_sz=(572,572)):\n",
    "        super().__init__()\n",
    "        self.encoder     = Encoder(enc_chs)\n",
    "        self.decoder     = Decoder(dec_chs)\n",
    "        self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)\n",
    "        self.retain_dim  = retain_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_ftrs = self.encoder(x)\n",
    "        out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
    "        out      = self.head(out)\n",
    "        if self.retain_dim:\n",
    "            out = F.interpolate(out, out_sz)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent Work\n",
    "\n",
    "Do the following:\n",
    "\n",
    " 1. Get the demo up and running.\n",
    " 2. Evaluate the pretrained COCO model on the COCO validation set we used last week.\n",
    " 3. Download the Cityscapes dataset and run Mask R-CNN on it in inference model. Report your results and see what errors you find. [Link](https://www.cityscapes-dataset.com/)\n",
    " 4. Fine tune the COCO Mask R-CNN on Cityscapes and report the results. Are they close to what's reported in the Mask R-CNN paper?\n",
    " 5. Try U-NET and compare with Mask R-CNN\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
