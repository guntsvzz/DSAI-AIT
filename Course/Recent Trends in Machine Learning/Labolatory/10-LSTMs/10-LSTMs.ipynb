{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 10: Long Short Term Memory (LSTM) Models\n",
    "\n",
    "Today we will build a chatbot using the PyTorch LSTM cell.\n",
    "\n",
    "The material in this lab comes from several sources:\n",
    "- Hands-On Natural Language Processing with Pytorch 1.x, PacktPub\n",
    "- https://d2l.ai/chapter_recurrent-modern/beam-search.html\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html\n",
    "- https://towardsdatascience.com/building-a-lstm-by-hand-on-pytorch-59c02a4ec091\n",
    "- https://github.com/usmanali414/Chatbot-using-Pytorch-LSTM (source code)\n",
    "\n",
    "## What is the LSTM?\n",
    "\n",
    "Simple RNNs (SRNs) like we used in Lab 09\n",
    "have difficulty retaining information over a long period of time. LSTM addresses some of the issues arising\n",
    "in SRNs. LSTM adds two gates to the typical SRN: an *update gate* and a *forget gate*. These two gates help a network\n",
    "learn long-term dependencies, for example, so that it can remember the relevant words within a sentence while ignoring\n",
    "all the irrelevant information.\n",
    "\n",
    "At a high level,\n",
    "LSTMs are similar in structure to SRNs. Like SRNs, LSTMs propagate a hidden state from step to step.\n",
    "However, the inner details of an LSTM cell are quite different from the SRN.\n",
    "\n",
    "<img src=\"img/LSTM2.png\" title=\"LSTM\" width=200/>\n",
    "\n",
    "## LSTM cell\n",
    "\n",
    "The inner workings of an LSTM cell are significantly more complicated. At time step\n",
    "$t$, the processing looks like this:\n",
    "\n",
    "<img src=\"img/LSTMcell2.jpg\" title=\"LSTM cell\" style=\"width: 800px;\" />\n",
    "\n",
    "The hidden state $h_t$ is the actual output of the layer that is passed\n",
    "on to any later layers. It is obtained from\n",
    "the internal cell state $c_t$ after it is modulated by the output gate.\n",
    "The cell state $c_t$ is cell's internal state. It may be partially or\n",
    "completely cleared by the forget gate, then the new input may be written\n",
    "to it, under the control of the input gate.\n",
    "\n",
    "### Forget gate\n",
    "\n",
    "The forget gate can preserve or attenutate the previous cell state.\n",
    "It is highlighted in the bold rectangle:\n",
    "\n",
    "<img src=\"img/lstm_forgetgate2.png\" title=\"Forget gate\" style=\"width: 480px;\" />\n",
    "\n",
    "Forget gate processing:\n",
    "\n",
    "$f_t = \\sigma(W_{if}x_t + b_{if} + W_{hf}h_{t-1} + b_{hf})$\n",
    "\n",
    "$c'_t = f_t \\odot c_{t-1}$\n",
    "\n",
    "Note that $\\odot$ here means pointwise multiplication of two equal-length vectors and\n",
    "that the attenuation of the previous cell state is dependent on both the current input\n",
    "and the previous hidden state.\n",
    "\n",
    "### Input gate\n",
    "\n",
    "The input gate allows a value depending on the current input and previous hidden state\n",
    "to be added to the partly forgotten previous cell state $c'_t$ to obtain the new cell state\n",
    "$c_t$. However, the addition of the new information to the cell state is controlled by\n",
    "a multiplicative gate just like the forget gate that also depends on the input and previous\n",
    "hidden state. The input gate is highlighted in the bold rectangle here:\n",
    "\n",
    "<img src=\"img/lstm_inputgate2.jpg\" title=\"Input gate\" style=\"width: 480px;\" />\n",
    "\n",
    "Input gate processing:\n",
    "\n",
    "$i_t = \\sigma(W_{ii}x_t + b_{ii} + W_{hi}h_{t-1} + b_{hi})$\n",
    "\n",
    "$c^+_t = \\tanh(W_{ic}x_t + b_{ic} + W_{hc}h_{t-1} + b_{hc})$\n",
    "\n",
    "$c_t = c'_t + i_t \\odot c^+_t$\n",
    "\n",
    "### Output gate\n",
    "\n",
    "The hidden state $h_t$, already explained above, is a value output by the\n",
    "cell that is used by any later layers at the current\n",
    "time step. The output gate allows some or all of the current\n",
    "cell state to be propagated to the next step as the hidden state.\n",
    "We can think of the output $h_t$ as a (possibly attentuated) copy of\n",
    "the current cell state. The output gate controls this\n",
    "attentuation. It is highlighted in the bold rectangle:\n",
    "\n",
    "<img src=\"img/lstm_outputgate2.png\" title=\"Output gate\" style=\"width: 480px;\" />\n",
    "\n",
    "Output gate processing:\n",
    "\n",
    "$o_t=\\sigma(W_{io}x_t + b_{io} + W_{ho}h_{t-1} + b_{ho})$\n",
    "\n",
    "$h_t=o_t \\cdot \\tanh(c_t)$\n",
    "\n",
    "### Summary\n",
    "\n",
    "In summary, The LSTM cell performs the following computations:\n",
    "\n",
    "$i_t=\\sigma(W_{ii}x_t + b_{ii} + W_{hi}h_{t-1} + b_{hi})$\n",
    "\n",
    "$f_t=\\sigma(W_{if}x_t + b_{if} + W_{hf}h_{t-1} + b_{hf})$\n",
    "\n",
    "$c^+_t = \\tanh(W_{ic}x_t + b_{ic} + W_{hc}h_{t-1} + b_{hc})$\n",
    "\n",
    "$o_t=\\sigma(W_{io}x_t + b_{io} + W_{ho}h_{t-1} + b_{ho})$\n",
    "\n",
    "$c_t=f_t \\odot c_{t-1} + i_t \\odot c^+_t$\n",
    "\n",
    "$h_t=o_t \\odot \\tanh(c_t)$\n",
    "\n",
    "## LSTM code example\n",
    "\n",
    "OK, now let's look at an example of how we might make our own LSTM\n",
    "cell in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class NaiveCustomLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_sz: int, hidden_sz: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        \n",
    "        # Parameters for computing i_t\n",
    "        self.U_i = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_i = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        # Parameters for computing f_t\n",
    "        self.U_f = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_f = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        # Parameters for computing c_t\n",
    "        self.U_c = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_c = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_c = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        # Parameters for computing o_t\n",
    "        self.U_o = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_o = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        self.init_weights()\n",
    "                \n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.normal_(mean=0, std=stdv)\n",
    "         \n",
    "    def forward(self, x, init_states=None):\n",
    "        \"\"\"\n",
    "        forward: Run input x through the cell. Assumes x.shape is (batch_size, sequence_length, input_size)\n",
    "        \"\"\"\n",
    "        bs, seq_sz, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        \n",
    "        if init_states is None:\n",
    "            h_t, c_t = (\n",
    "                torch.zeros(bs, self.hidden_size).to(x.device),\n",
    "                torch.zeros(bs, self.hidden_size).to(x.device),\n",
    "            )\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "            \n",
    "        for t in range(seq_sz):\n",
    "            x_t = x[:, t, :]\n",
    "            \n",
    "            i_t = torch.sigmoid(x_t @ self.U_i + h_t @ self.V_i + self.b_i)\n",
    "            f_t = torch.sigmoid(x_t @ self.U_f + h_t @ self.V_f + self.b_f)\n",
    "            g_t = torch.tanh(x_t @ self.U_c + h_t @ self.V_c + self.b_c)\n",
    "            o_t = torch.sigmoid(x_t @ self.U_o + h_t @ self.V_o + self.b_o)\n",
    "            c_t = f_t * c_t + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            \n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "        \n",
    "        # Reshape hidden_seq tensor to (batch size, sequence length, hidden_size)\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "\n",
    "        return hidden_seq, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our chatbot\n",
    "\n",
    "A chatbot, given input text, should output an appropriate sequence of tokens. One reasonable strategy would\n",
    "be to output, given input sequence $x$, the sequence maximizing $P(y \\mid x)$ using a generative model with\n",
    "parameters learned from a dataset using maximum likelihood.\n",
    "\n",
    "We know that $$P(y \\mid x) = \\prod_{t = 1}^T P(y_t | y_1, \\ldots, y_{t-1}, x)$$.\n",
    "\n",
    "We'll use an LSTM cell as our model of $P(y_t | y_1, \\ldots, y_{t-1}, x)$.\n",
    "One (bad) strategy to find the optimal output for $x$ would be to randomly sample ouput token sequences $y$, calculate\n",
    "$P(y \\mid x)$, and repeat, finally returning the sequence with the highest probability.\n",
    "\n",
    "A better strategy, however, would be a smarter search procedure. \n",
    "We'll go through two search methods: *greedy search* and *beam search*.\n",
    "\n",
    "### Greedy search\n",
    "\n",
    "Greedy search is a simple strategy. At time step $t'$ of the output sequence, given the previous tokens and a representation\n",
    "$c$ of the input, the token with the highest predicted probability from the set of possible tokens $\\gamma$ is selected:\n",
    "\n",
    "$$y_{t'}=\\underset{y\\in\\gamma}{\\textrm{argmax}} P(y \\mid y_1,...,y_{t'-1},c).$$\n",
    "\n",
    "We'll allow two strategies to end the sequence: if the token\n",
    "`<eos>` is output, we stop immediately; otherwise, we keep generating outputs until a maximum output sentence length $T'$ is reached.\n",
    "\n",
    "The disadvantage of this greedy search method is that the actual optimal sequence $y^*$ will contain some locally suboptimal steps.\n",
    "To give some intuition, consider a game like chess. We have to sacrifice some of our own pieces (short term suboptimality) in order to obtain a long\n",
    "term advantage and win the game. Similarly, when selecting an output sentence for our chatbot, some of the tokens in the optimal sequence may have a\n",
    "relatively low probability given the previous sequence.\n",
    "\n",
    "Here's how greedy search works:\n",
    "\n",
    "<img src=\"img/lstm_greedysearch.PNG\" title=\"At each time step, greedy search selects the token with the highest conditional probability\" style=\"width: 240px;\" />\n",
    "\n",
    "We have four tokens `A`, `B`, `C`, and `<eos>` in the output dictionary. The four numbers under each time step represent conditional probabilities of generating\n",
    "`A`, `B`, `C`, and `<eos>` at that time step, given the blue highlighted token was selected for the previous time step, respectively.\n",
    "\n",
    "At each time step, greedy search selects the token with the highest conditional probability. Therefore, the sequence\n",
    "`A`, `B`, `C`, and `<eos>` will be ouptut. The conditional probability of this output sequence is $0.5 \\times 0.4 \\times 0.4 \\times 0.6 = 0.048$.\n",
    "\n",
    "In another example, suppose that at time step 2, we select the token `C`, which has the second highest conditional probability:\n",
    "When `C` is selected at step 2, the conditional probabilities at step 3 will change. Suppose we obtain the following probabilities\n",
    "and then continue selecting the greedy optimal tokens on step 3 and 4:\n",
    "\n",
    "<img src=\"img/lstm_greedysearch2.PNG\" title=\"At time step 2, the token “C”, which has the second highest conditional probability, is selected\" style=\"width: 240px;\" />\n",
    "\n",
    "In this case, we obtain a conditional probability $0.5 \\times 0.3 \\times 0.6 \\times 0.6 = 0.054$, which is larger than what we got from the greedy search. In this example, greedy search is clearly suboptimal.\n",
    "\n",
    "OK, we'll begin with greedy search then introduce a more sophisticated search procedure, beam\n",
    "search. For now, let's start implementing our chatbot. First, we pull in some imports and find the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the data\n",
    "\n",
    "The [Cornell Movie-Dialogs\n",
    "Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html)\n",
    "is a rich dataset of movie character dialogs with the following statistics:\n",
    "-  220,579 conversational exchanges between 10,292 pairs of movie\n",
    "   characters\n",
    "-  9035 characters from 617 movies\n",
    "-  304,713 total utterances\n",
    "\n",
    "The dataset is large and diverse, with great variation in\n",
    "language formality, time periods, sentiment, and so on. This\n",
    "diversity will hopefully let us build a model that can handle\n",
    "many different contexts.\n",
    "\n",
    "The [PyTorch chatbot tutorial](https://pytorch.org/tutorials/beginner/chatbot_tutorial.html)\n",
    "shows how to preprocess the raw Movie Dialogs dataset. We've done this for you already.\n",
    "\n",
    "Next, note that we are dealing with sequences of words, which cannot be directly\n",
    "mapped to a continuous vector space as we need for LSTMs. We will therefore create a mapping\n",
    "from each unique word in the dataset to an index value. (Note that in the readings in class we've seen\n",
    "better ways to handle natural language vocabularies!)\n",
    "To perform our simple version of the mapping, we define a class `Voc`. It\n",
    "creates both the forward mapping from words to indices and the reverse mapping from\n",
    "indices back to words, as well as a count of each word and a total word count.\n",
    "The class's behavior includes a method to add a word to the vocabulary (`addWord`), a\n",
    "method to add all words in a sentence at once\n",
    "(`addSentence`), and a method to trim infrequently seen words (`trim`). We'll discuss\n",
    "trimming more later. Here it is:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserved word tokens\n",
    "\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "class Voc:\n",
    "\n",
    "    def __init__(self):        \n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run through the dataset and assemble our vocabulary,\n",
    "as well as the training query/response sentence pairs.\n",
    "Before we can use the data, we have to perform some\n",
    "preprocessing. First, we convert Unicode strings to ASCII using\n",
    "`unicodeToAscii`. Then we convert every letter to lowercase and\n",
    "trim all non-letter characters except for basic punctuation\n",
    "(function `normalizeString`). Finally, to improve training convergence,\n",
    "we filter out sentences with length greater than the `MAX_LENGTH`\n",
    "threshold (function `filterPairs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10  # Maximum sentence length to consider\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Read query/response pairs and return a Voc object\n",
    "\n",
    "def readVocs(datafile):\n",
    "    print(\"Reading lines...\")    \n",
    "    # Read the file and split into lines\n",
    "    lines = open(datafile, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    voc = Voc()\n",
    "    return voc, pairs\n",
    "\n",
    "# Boolean function returning True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
    "\n",
    "def filterPair(p):\n",
    "    # Input sequences need to preserve the last word for EOS token\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "# Filter pairs using the filterPair predicate\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "# Using the functions defined above, return a populated voc object and pairs list\n",
    "\n",
    "def loadPrepareData(datafile):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(datafile)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With those routines specficied, now we read chat dataset file, preprocess it, and mold it into pairs of question-answer sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wget https://github.com/dsai-asia/RTML/raw/main/Labs/11-LSTMs/chatDataset.txt\n",
    "\n",
    "# Load/Assemble Voc and pairs\n",
    "\n",
    "datafile = 'chatDataset.txt'\n",
    "voc, pairs = loadPrepareData(datafile)\n",
    "\n",
    "# Print some pairs to validate\n",
    "\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already mentioned, we will trim out rarely-used\n",
    "words from the vocabulary. This will help improve convergence during\n",
    "training, because with a lower-dimensional input feature space, it will be easier\n",
    "to estimate the probability model $P(y \\mid x)$. We trim as a two-step\n",
    "process:\n",
    "\n",
    "1. Trim words appearing fewer than `MIN_COUNT` times with the previously-given `Voc.trim`\n",
    "   method.\n",
    "\n",
    "2. Filter out all sentence pairs containing trimmed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MIN_COUNT = 3    # Minimum word count threshold for trimming\n",
    "\n",
    "def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "    # Trim words used under the MIN_COUNT from the voc\n",
    "    voc.trim(MIN_COUNT)\n",
    "    # Filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # Check input sentence\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # Check output sentence\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "# Trim vocabulary and pairs\n",
    "\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into testing and training pair sets\n",
    "\n",
    "Let's split the dataset into the first 45,000 pairs for training and the rest for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpairs = pairs[45000:]\n",
    "pairs  = pairs[:45000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert pairs to tensors\n",
    "\n",
    "First, let's make tensors representing sentences in which we encode\n",
    "each sequence as a sequence of indices. The sequences should all be\n",
    "padded to to a length of `MAX_LENGTH` so that they are all the same\n",
    "size. The transformation will look like this: \n",
    "\n",
    "![title](img/seq2seq_batches.png)\n",
    "\n",
    "`zeroPadding` does the padding.\n",
    "\n",
    "The ``inputVar`` function handles the process of converting sentences to\n",
    "tensor, ultimately creating a correctly shaped zero-padded tensor. It\n",
    "also returns a tensor of ``lengths`` for each of the sequences in the\n",
    "batch which will be passed to our decoder later.\n",
    "\n",
    "The ``outputVar`` function performs a similar function to ``inputVar``,\n",
    "but instead of returning a ``lengths`` tensor, it returns a binary mask\n",
    "tensor and a maximum target sentence length. The binary mask tensor has\n",
    "the same shape as the output target tensor, but every element that is a\n",
    "*PAD_token* is 0 and all others are 1.\n",
    "\n",
    "``batch2TrainData`` simply takes a bunch of pairs and returns the input\n",
    "and target tensors using the aforementioned functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# Return a padded input sequence tensor and the lengths of each original sequence\n",
    "\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "# Return a padded target sequence tensor, a padding mask, and the max target length\n",
    "\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.BoolTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "# Return all items for a given batch of pairs\n",
    "\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "# Example for validation\n",
    "\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As LSTM takes time series data so we need to convert our pairs of sentences into time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_batch = pairs[:5]\n",
    "print(pair_batch)\n",
    "pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "print(pair_batch)\n",
    "print(target_variable)\n",
    "print(mask)\n",
    "print(max_target_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define models\n",
    "\n",
    "Seq2Seq Model\n",
    "\n",
    "The brains of our chatbot is a sequence-to-sequence (seq2seq) model. The\n",
    "goal of a seq2seq model is to take a variable-length sequence as an\n",
    "input, and return a variable-length sequence as an output using a\n",
    "fixed-sized model.\n",
    "\n",
    "`Sutskever et al. <https://arxiv.org/abs/1409.3215>`__ discovered that\n",
    "by using two separate recurrent neural nets together, we can accomplish\n",
    "this task. One RNN acts as an **encoder**, which encodes a variable\n",
    "length input sequence to a fixed-length context vector. In theory, this\n",
    "context vector (the final hidden layer of the RNN) will contain semantic\n",
    "information about the query sentence that is input to the bot. The\n",
    "second RNN is a **decoder**, which takes an input word and the context\n",
    "vector, and returns a guess for the next word in the sequence and a\n",
    "hidden state to use in the next iteration.\n",
    "\n",
    "![title](img/seq2seq_ts2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "\n",
    "The encoder RNN iterates through the input sentence one token\n",
    "(e.g. word) at a time, at each time step outputting an “output” vector\n",
    "and a “hidden state” vector. The hidden state vector is then passed to\n",
    "the next time step, while the output vector is recorded. The encoder\n",
    "transforms the context it saw at each point in the sequence into a set\n",
    "of points in a high-dimensional space, which the decoder will use to\n",
    "generate a meaningful output for the given task.\n",
    "\n",
    "At the heart of our encoder is a multi-layered LSTM.\n",
    "We will use a bidirectional variant of the LSTM, meaning that there\n",
    "are essentially two independent RNNs: one that is fed the input sequence\n",
    "in normal sequential order, and one that is fed the input sequence in\n",
    "reverse order. The outputs of each network are summed at each time step.\n",
    "Using a bidirectional LSTM will give us the advantage of encoding both\n",
    "past and future context.\n",
    "\n",
    "![title](img/RNN-bidirectional2.png)\n",
    "\n",
    "\n",
    "Note that an ``embedding`` layer is used to encode our word indices in\n",
    "an arbitrarily sized feature space. For our models, this layer will map\n",
    "each word to a feature space of size *hidden_size*. When trained, these\n",
    "values should encode semantic similarity between similar meaning words.\n",
    "\n",
    "Finally, if passing a padded batch of sequences to an RNN module, we\n",
    "must pack and unpack padding around the RNN pass using\n",
    "``nn.utils.rnn.pack_padded_sequence`` and\n",
    "``nn.utils.rnn.pad_packed_sequence`` respectively.\n",
    "\n",
    "**Computation Graph:**\n",
    "\n",
    "   1) Convert word indexes to embeddings.\n",
    "\n",
    "   2) Pack padded batch of sequences for RNN module.\n",
    "\n",
    "   3) Forward pass through LSTM.\n",
    "\n",
    "   4) Unpack padding.\n",
    "\n",
    "   5) Sum bidirectional LSTM outputs.\n",
    "   \n",
    "   6) Return output and final hidden state.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "-  ``input_seq``: batch of input sentences; shape=\\ *(max_length,\n",
    "   batch_size)*\n",
    "-  ``input_lengths``: list of sentence lengths corresponding to each\n",
    "   sentence in the batch; shape=\\ *(batch_size)*\n",
    "-  ``hidden``: hidden state; shape=\\ *(n_layers x num_directions,\n",
    "   batch_size, hidden_size)*\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "-  ``outputs``: output features from the last hidden layer of the LSTM\n",
    "   (sum of bidirectional outputs); shape=\\ *(max_length, batch_size,\n",
    "   hidden_size)*\n",
    "-  ``hidden``: updated hidden state from LSTM; shape=\\ *(n_layers x\n",
    "   num_directions, batch_size, hidden_size)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "\n",
    "Now we declare our encoder which is consist of bidirectional LSTM units.It is vital to declare bidirectional LSTM as their results are better than unidirectional LSTM in some NLP problems.What it done is instead of learning embeddings of previous words it also considers the embeddings or features of next word suitable to predict target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        \n",
    "        embedded = self.embedding(input_seq)\n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths.cpu())\n",
    "        \n",
    "        outputs, hidden = self.lstm(packed, hidden)\n",
    "        \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        \n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        \n",
    "        return outputs, hidden\n",
    "    def init_hidden(self):\n",
    "        \n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder\n",
    "\n",
    "The decoder RNN generates the response sentence in a token-by-token\n",
    "fashion. It uses the encoder’s context vectors, and internal hidden\n",
    "states to generate the next word in the sequence. It continues\n",
    "generating words until it outputs an *EOS_token*, representing the end\n",
    "of the sentence. A common problem with a vanilla seq2seq decoder is that\n",
    "if we rely soley on the context vector to encode the entire input\n",
    "sequence’s meaning, it is likely that we will have information loss.\n",
    "This is especially the case when dealing with long input sequences,\n",
    "greatly limiting the capability of our decoder.\n",
    "\n",
    "To combat this, `Bahdanau et al. <https://arxiv.org/abs/1409.0473>`__\n",
    "created an “attention mechanism” that allows the decoder to pay\n",
    "attention to certain parts of the input sequence, rather than using the\n",
    "entire fixed context at every step.\n",
    "\n",
    "At a high level, attention is calculated using the decoder’s current\n",
    "hidden state and the encoder’s outputs. The output attention weights\n",
    "have the same shape as the input sequence, allowing us to multiply them\n",
    "by the encoder outputs, giving us a weighted sum which indicates the\n",
    "parts of encoder output to pay attention to. `Sean\n",
    "Robertson’s <https://github.com/spro>`__ figure describes this very\n",
    "well:\n",
    "![title](img/attn2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "        attn_energies = attn_energies.t()\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our attention submodule, we can implement the\n",
    "actual decoder model. For the decoder, we will manually feed our batch\n",
    "one time step at a time. This means that our embedded word tensor and\n",
    "LSTM output will both have shape *(1, batch_size, hidden_size)*.\n",
    "\n",
    "**Computation Graph:**\n",
    "\n",
    "   1) Get embedding of current input word.\n",
    "\n",
    "   2) Forward through unidirectional LSTM.\n",
    "\n",
    "   3) Calculate attention weights from the current LSTM output from (2).\n",
    "\n",
    "   4) Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector.\n",
    "\n",
    "   5) Concatenate weighted context vector and LSTM output using Luong eq. 5.\n",
    "\n",
    "   6) Predict next word using Luong eq. 6 (without softmax).\n",
    "   \n",
    "   7) Return output and final hidden state.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "-  ``input_step``: one time step (one word) of input sequence batch;\n",
    "   shape=\\ *(1, batch_size)*\n",
    "-  ``last_hidden``: final hidden layer of LSTM; shape=\\ *(n_layers x\n",
    "   num_directions, batch_size, hidden_size)*\n",
    "-  ``encoder_outputs``: encoder model’s output; shape=\\ *(max_length,\n",
    "   batch_size, hidden_size)*\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "-  ``output``: softmax normalized tensor giving probabilities of each\n",
    "   word being the correct next word in the decoded sequence;\n",
    "   shape=\\ *(batch_size, voc.num_words)*\n",
    "-  ``hidden``: final hidden state of LSTM; shape=\\ *(n_layers x\n",
    "   num_directions, batch_size, hidden_size)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        \n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "       \n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))#, bidirectional=True)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "    \n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        rnn_output, hidden = self.lstm(embedded, last_hidden)\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure\n",
    "\n",
    "Masked loss\n",
    "\n",
    "Since we are dealing with batches of padded sequences, we cannot simply\n",
    "consider all elements of the tensor when calculating loss. We define\n",
    "``maskNLLLoss`` to calculate our loss based on our decoder’s output\n",
    "tensor, the target tensor, and a binary mask tensor describing the\n",
    "padding of the target tensor. This loss function calculates the average\n",
    "negative log likelihood of the elements that correspond to a *1* in the\n",
    "mask tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single training iteration\n",
    "\n",
    "The ``train`` function contains the algorithm for a single training\n",
    "iteration (a single batch of inputs).\n",
    "\n",
    "We will use a couple of clever tricks to aid in convergence:\n",
    "\n",
    "-  The first trick is using **teacher forcing**. This means that at some\n",
    "   probability, set by ``teacher_forcing_ratio``, we use the current\n",
    "   target word as the decoder’s next input rather than using the\n",
    "   decoder’s current guess. This technique acts as training wheels for\n",
    "   the decoder, aiding in more efficient training. However, teacher\n",
    "   forcing can lead to model instability during inference, as the\n",
    "   decoder may not have a sufficient chance to truly craft its own\n",
    "   output sequences during training. Thus, we must be mindful of how we\n",
    "   are setting the ``teacher_forcing_ratio``, and not be fooled by fast\n",
    "   convergence.\n",
    "\n",
    "-  The second trick that we implement is **gradient clipping**. This is\n",
    "   a commonly used technique for countering the “exploding gradient”\n",
    "   problem. In essence, by clipping or thresholding gradients to a\n",
    "   maximum value, we prevent the gradients from growing exponentially\n",
    "   and either overflow (NaN), or overshoot steep cliffs in the cost\n",
    "   function.\n",
    "\n",
    "\n",
    "\n",
    "**Sequence of Operations:**\n",
    "\n",
    "   1) Forward pass entire input batch through encoder.\n",
    "\n",
    "   2) Initialize decoder inputs as SOS_token, and hidden state as the encoder's final hidden state.\n",
    "   \n",
    "   3) Forward input batch sequence through decoder one time step at a time.\n",
    "\n",
    "   4) If teacher forcing: set next decoder input as the current target; else: set next decoder input as current decoder output.\n",
    "\n",
    "   5) Calculate and accumulate loss.\n",
    "\n",
    "   6) Perform backpropagation.\n",
    "\n",
    "   7) Clip gradients.\n",
    "   \n",
    "   8) Update encoder and decoder model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "  \n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    \n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    \n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    \n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    \n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            \n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            \n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            \n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            \n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    \n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full training procedure\n",
    "\n",
    "It is finally time to tie the full training procedure together with the\n",
    "data. The ``trainIters`` function is responsible for running\n",
    "``n_iterations`` of training given the passed models, optimizers, data,\n",
    "etc. This function is quite self explanatory, as we have done the heavy\n",
    "lifting with the ``train`` function.\n",
    "\n",
    "One thing to note is that when we save our model, we save a tarball\n",
    "containing the encoder and decoder state_dicts (parameters), the\n",
    "optimizers’ state_dicts, the loss, the iteration, etc. Saving the model\n",
    "in this way will give us the ultimate flexibility with the checkpoint.\n",
    "After loading a checkpoint, we will be able to use the model parameters\n",
    "to run inference, or we can continue training right where we left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " max_target_len "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    losslist = []\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        \n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        \n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        \n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "            losslist.append(print_loss_avg)\n",
    "        \n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            print(directory)\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n",
    "    return losslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation function\n",
    "\n",
    "After training a model, we want to be able to talk to the bot ourselves.\n",
    "First, we must define how we want the model to decode the encoded input.\n",
    "\n",
    "#### Greedy decoding\n",
    "\n",
    "Greedy decoding is the decoding method that we use during training when\n",
    "we are **NOT** using teacher forcing. In other words, for each time\n",
    "step, we simply choose the word from ``decoder_output`` with the highest\n",
    "softmax value. This decoding method is optimal on a single time-step\n",
    "level.\n",
    "\n",
    "To facilite the greedy decoding operation, we define a\n",
    "``GreedySearchDecoder`` class. When run, an object of this class takes\n",
    "an input sequence (``input_seq``) of shape *(input_seq length, 1)*, a\n",
    "scalar input length (``input_length``) tensor, and a ``max_length`` to\n",
    "bound the response sentence length. The input sentence is evaluated\n",
    "using the following computational graph:\n",
    "\n",
    "**Computation Graph:**\n",
    "\n",
    "   1) Forward input through encoder model.\n",
    "\n",
    "   2) Prepare encoder's final hidden layer to be first hidden input to the decoder.\n",
    "\n",
    "   3) Initialize decoder's first input as SOS_token.\n",
    "\n",
    "   4) Initialize tensors to append decoded words to.\n",
    "\n",
    "   5) Iteratively decode one word token at a time:\n",
    "   \n",
    "       a) Forward pass through decoder.\n",
    "\n",
    "       b) Obtain most likely word token and its softmax score.\n",
    "\n",
    "       c) Record token and score.\n",
    "\n",
    "       d) Prepare current token to be next decoder input.\n",
    "\n",
    "   6) Return collections of word tokens and scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        \n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        \n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        \n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "       \n",
    "        for _ in range(max_length):\n",
    "            \n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            \n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        \n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating some text\n",
    "\n",
    "Now that we have our decoding method defined, we can write functions for\n",
    "evaluating a string input sentence. The ``evaluate`` function manages\n",
    "the low-level process of handling the input sentence. We first format\n",
    "the sentence as an input batch of word indexes with *batch_size==1*. We\n",
    "do this by converting the words of the sentence to their corresponding\n",
    "indexes, and transposing the dimensions to prepare the tensor for our\n",
    "models. We also create a ``lengths`` tensor which contains the length of\n",
    "our input sentence. In this case, ``lengths`` is scalar because we are\n",
    "only evaluating one sentence at a time (batch_size==1). Next, we obtain\n",
    "the decoded response sentence tensor using our ``GreedySearchDecoder``\n",
    "object (``searcher``). Finally, we convert the response’s indexes to\n",
    "words and return the list of decoded words.\n",
    "\n",
    "``evaluateInput`` acts as the user interface for our chatbot. When\n",
    "called, an input text field will spawn in which we can enter our query\n",
    "sentence. After typing our input sentence and pressing *Enter*, our text\n",
    "is normalized in the same way as our training data, and is ultimately\n",
    "fed to the ``evaluate`` function to obtain a decoded output sentence. We\n",
    "loop this process, so we can keep chatting with our bot until we enter\n",
    "either “q” or “quit”.\n",
    "\n",
    "Finally, if a sentence is entered that contains a word that is not in\n",
    "the vocabulary, we handle this gracefully by printing an error message\n",
    "and prompting the user to enter another sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "  \n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    \n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    \n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    \n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    \n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    \n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            \n",
    "            input_sentence = input('> ')\n",
    "            \n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            \n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            \n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            \n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model\n",
    "\n",
    "\n",
    "Finally, it is time to run our model!\n",
    "\n",
    "Regardless of whether we want to train or test the chatbot model, we\n",
    "must initialize the individual encoder and decoder models. In the\n",
    "following block, we set our desired configurations, choose to start from\n",
    "scratch or set a checkpoint to load from, and build and initialize the\n",
    "models. Feel free to play with different model configurations to\n",
    "optimize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "\n",
    "hidden_size = 512\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.5\n",
    "batch_size = 256 \n",
    "loadFilename = None\n",
    "\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "save_dir = 'content/'\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 12000\n",
    "print_every = 100\n",
    "save_every = 2000\n",
    "loadFilename = None\n",
    "corpus_name=\"Chat\"\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "print(\"Starting Training!\")\n",
    "lossvalues = trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(lossvalues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU score calculation\n",
    "\n",
    "The BLEU score (the Bilingual Evaluation Understudy Score) is an evaluation metric used to calculate the capacity of our model to make correct predictions. It finds maximal n-gram matches between predicted sentences and reference sentences You can learn more about BLEU from [this tutorial](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu,corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "# Set dropout layers to eval mode\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "gram1_bleu_score = []\n",
    "gram2_bleu_score = []\n",
    "for i in range(0,len(testpairs),1):\n",
    "    input_sentence = testpairs[i][0]\n",
    "    reference = testpairs[i][1:]\n",
    "    templist = []\n",
    "    for k in range(len(reference)):\n",
    "        if(reference[k]!=''):\n",
    "            temp = reference[k].split(' ')\n",
    "            templist.append(temp)\n",
    "\n",
    "    input_sentence = normalizeString(input_sentence)\n",
    "    output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "    output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "    chencherry = SmoothingFunction()\n",
    "    #   print(output_words)\n",
    "    #   print(templist)\n",
    "    score1 = sentence_bleu(templist, output_words, weights=(1, 0, 0, 0), smoothing_function=chencherry.method1)\n",
    "    score2 = sentence_bleu(templist, output_words, weights=(0.5, 0.5, 0, 0), smoothing_function=chencherry.method1) \n",
    "    gram1_bleu_score.append(score1)\n",
    "    gram2_bleu_score.append(score2)\n",
    "    if i%1000 == 0:\n",
    "        print(i,sum(gram1_bleu_score)/len(gram1_bleu_score),sum(gram2_bleu_score)/len(gram2_bleu_score))\n",
    "\n",
    "print(\"Total Bleu Score for 1 grams on testing pairs: \", sum(gram1_bleu_score)/len(gram1_bleu_score) )  \n",
    "print(\"Total Bleu Score for 2 grams on testing pairs: \", sum(gram2_bleu_score)/len(gram2_bleu_score) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluateInput(encoder, decoder, searcher, voc)\n",
    "# input\n",
    "# Hi, how are you?\n",
    "# What\n",
    "# I don't understand you\n",
    "# hmm, good bye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search\n",
    "\n",
    "Beam search is an improved version of greedy search. It has a hyperparameter named beam size,  $k$. At time step 1, selecting $k$ tokens with the highest conditional probabilities. Each of them will be the first token of $k$ candidate output sequences, respectively. At each subsequent time step, based on the $k$ candidate output sequences at the previous time step, $k$ candidate output sequences has been selected with the highest conditional probabilities from $k|\\gamma|$ possible choices.\n",
    "\n",
    "<img src=\"img/lstm_beamsearch.PNG\" title=\"Beam search\" style=\"width: 640px;\" />\n",
    "\n",
    "the sequence with the highest of the following score as the output sequence has been chosen from the equation:\n",
    "\n",
    "$\\frac{1}{L^{\\alpha}}\\log{P(y_1,...,y_L)} = \\frac{1}{L^{\\alpha}} \\sum_{t'=1}^{L} \\log{P(y_{t'}|t_1,...,y_{t'-1}, c)}$\n",
    "\n",
    "Where $L$ is the length of the final candidate sequence and $\\alpha$ is usually set to 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Beam Decoder**\n",
    "\n",
    "The difference between greedy search and beam search is decoder function. Thus, greedy search function name is greedy_decode, and beam search function name is beam_decode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    def __init__(self, decoder_hidden, last_idx=SOS_token, sentence_idxes=[], sentence_scores=[]):\n",
    "        if(len(sentence_idxes) != len(sentence_scores)):\n",
    "            raise ValueError(\"length of indexes and scores should be the same\")\n",
    "        self.decoder_hidden = decoder_hidden\n",
    "        self.last_idx = last_idx\n",
    "        self.sentence_idxes =  sentence_idxes\n",
    "        self.sentence_scores = sentence_scores\n",
    "\n",
    "    def avgScore(self):\n",
    "        if len(self.sentence_scores) == 0:\n",
    "            raise ValueError(\"Calculate average score of sentence, but got no word\")\n",
    "        # return mean of sentence_score\n",
    "        return sum(self.sentence_scores) / len(self.sentence_scores)\n",
    "\n",
    "    def addTopk(self, topi, topv, decoder_hidden, beam_size, voc):\n",
    "        topv = torch.log(topv)\n",
    "        terminates, sentences = [], []\n",
    "        for i in range(beam_size):\n",
    "            if topi[0][i] == EOS_token:\n",
    "                terminates.append(([voc.index2word[idx.item()] for idx in self.sentence_idxes] + ['<EOS>'],\n",
    "                                   self.avgScore())) \n",
    "                continue\n",
    "            idxes = self.sentence_idxes[:] \n",
    "            scores = self.sentence_scores[:] \n",
    "            idxes.append(topi[0][i])\n",
    "            scores.append(topv[0][i])\n",
    "            sentences.append(Sentence(decoder_hidden, topi[0][i], idxes, scores))\n",
    "        return terminates, sentences\n",
    "\n",
    "    def toWordScore(self, voc):\n",
    "        \n",
    "        words = []\n",
    "        for i in range(len(self.sentence_idxes)):\n",
    "            if self.sentence_idxes[i] == EOS_token:\n",
    "                words.append('<EOS>')\n",
    "            else:\n",
    "                words.append(voc.index2word[self.sentence_idxes[i].item()])\n",
    "       \n",
    "        if self.sentence_idxes[-1] != EOS_token:\n",
    "            words.append('<EOS>')\n",
    "        return (words, self.avgScore())\n",
    "\n",
    "    def __repr__(self):\n",
    "        res = f\"Sentence with indices {self.sentence_idxes} \"\n",
    "        res += f\"and scores {self.sentence_scores}\"\n",
    "        return res\n",
    "\n",
    "\n",
    "def beam_decode(decoder, decoder_hidden, encoder_outputs, voc, beam_size, max_length=MAX_LENGTH):\n",
    "    terminal_sentences, prev_top_sentences, next_top_sentences = [], [], []\n",
    "    prev_top_sentences.append(Sentence(decoder_hidden))\n",
    "    for i in range(max_length):\n",
    "        \n",
    "        for sentence in prev_top_sentences:\n",
    "            decoder_input = torch.LongTensor([[sentence.last_idx]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "\n",
    "            decoder_hidden = sentence.decoder_hidden\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(beam_size)\n",
    "            term, top = sentence.addTopk(topi, topv, decoder_hidden, beam_size, voc)\n",
    "            terminal_sentences.extend(term)\n",
    "            next_top_sentences.extend(top)\n",
    "           \n",
    "        \n",
    "        next_top_sentences.sort(key=lambda s: s.avgScore(), reverse=True)\n",
    "        prev_top_sentences = next_top_sentences[:beam_size]\n",
    "        next_top_sentences = []\n",
    "        \n",
    "\n",
    "    terminal_sentences += [sentence.toWordScore(voc) for sentence in prev_top_sentences]\n",
    "    terminal_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    n = min(len(terminal_sentences), 15)\n",
    "    return terminal_sentences[:n]\n",
    "\n",
    "\n",
    "class BeamSearchDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, voc, beam_size=10):\n",
    "        super(BeamSearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.voc = voc\n",
    "        self.beam_size = beam_size\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden[:self.decoder.n_layers]\n",
    "        \n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        sentences = beam_decode(self.decoder, decoder_hidden, encoder_outputs, self.voc, self.beam_size, max_length)\n",
    "        \n",
    "        \n",
    "        all_tokens = [torch.tensor(self.voc.word2index.get(w, 0)) for w in sentences[0][0]]\n",
    "        return all_tokens, None\n",
    "\n",
    "    def __str__(self):\n",
    "        res = f\"BeamSearchDecoder with beam size {self.beam_size}\"\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "\n",
    "hidden_size = 512\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.5\n",
    "batch_size = 256 \n",
    "loadFilename = None\n",
    "\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "save_dir = 'content/'\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 12000\n",
    "print_every = 100\n",
    "save_every = 2000\n",
    "loadFilename = None\n",
    "corpus_name=\"Chat\"\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "print(\"Starting Training!\")\n",
    "lossvalues = trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(lossvalues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU score calculation\n",
    "\n",
    "Let's calculate the BLEU score with the beam search decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu,corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "# Set dropout layers to eval mode\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "\n",
    "############################################################################\n",
    "# Difference between greedy search and beam search is here\n",
    "\n",
    "# greedy search\n",
    "# searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# beam search\n",
    "searcher = BeamSearchDecoder(encoder, decoder, voc, 10)\n",
    "############################################################################\n",
    "gram1_bleu_score = []\n",
    "gram2_bleu_score = []\n",
    "\n",
    "for i in range(0,len(testpairs),1):\n",
    "    input_sentence = testpairs[i][0]\n",
    "  \n",
    "    reference = testpairs[i][1:]\n",
    "    templist = []\n",
    "    for k in range(len(reference)):\n",
    "        if(reference[k]!=''):\n",
    "            temp = reference[k].split(' ')\n",
    "            templist.append(temp)\n",
    "  \n",
    "  input_sentence = normalizeString(input_sentence)\n",
    "  output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "  output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "  chencherry = SmoothingFunction()\n",
    "  score1 = sentence_bleu(templist,output_words,weights=(1, 0, 0, 0) ,smoothing_function=chencherry.method1)\n",
    "  score2 = sentence_bleu(templist,output_words,weights=(0.5, 0.5, 0, 0),smoothing_function=chencherry.method1) \n",
    "  gram1_bleu_score.append(score1)\n",
    "  gram2_bleu_score.append(score2)\n",
    "  if i%1000 == 0:\n",
    "    print(i,sum(gram1_bleu_score)/len(gram1_bleu_score),sum(gram2_bleu_score)/len(gram2_bleu_score))\n",
    "print(\"Total Bleu Score for 1 grams on testing pairs: \", sum(gram1_bleu_score)/len(gram1_bleu_score))  \n",
    "print(\"Total Bleu Score for 2 grams on testing pairs: \", sum(gram2_bleu_score)/len(gram2_bleu_score))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat with the beam search bot\n",
    "\n",
    "Let's chat with the new chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "searcher = BeamSearchDecoder(encoder, decoder, voc, 10)\n",
    "evaluateInput(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent work\n",
    "\n",
    "To be done on your own:\n",
    "\n",
    "1. Find another dataset of sentence pairs in a different domain and see\n",
    "   if you can preprocess the data and train a chatbot model on it using the same code we developed today. Report your results.\n",
    "\n",
    "2. Replace the LSTM encoder/decoder with a Transformer. Check out the [PyTorch Transformer module documentation](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
