{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Todsavad Tangtortan'\n",
    "id   = 123012  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Find another dataset of sentence pairs in a different domain and see\n",
    "if you can preprocess the data and train a chatbot model on it using the same code we developed today. Report your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random, math, time\n",
    "\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: multi_woz_v22/v2.2_active_only\n",
      "Found cached dataset multi_woz_v22 (C:/Users/Guntsv/.cache/huggingface/datasets/multi_woz_v22/v2.2_active_only/2.2.0/6719c8b21478299411a0c6fdb7137c3ebab2e6425129af831687fb7851c69eb5)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7665ebc83e44c5a0872bcbed9bde57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['dialogue_id', 'services', 'turns'],\n",
       "        num_rows: 8437\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['dialogue_id', 'services', 'turns'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['dialogue_id', 'services', 'turns'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('multi_woz_v22')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the train data and the validation data\n",
    "train_data = dataset['train']\n",
    "valid_data = dataset['validation']\n",
    "test_data  = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 3), (1000, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid = pd.DataFrame(valid_data)\n",
    "df_test = pd.DataFrame(test_data)\n",
    "df_valid.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 1), (1000, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.drop(columns=['dialogue_id','services'],inplace=True)\n",
    "df_valid.drop(columns=['dialogue_id','services'],inplace=True)\n",
    "df_valid.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_question,all_answer = list(), list()\n",
    "for input_ids in range(len(df_test['turns'])):\n",
    "    for index, sentence in enumerate(df_test['turns'][input_ids]['utterance']):\n",
    "        if index % 2 == 0:\n",
    "            all_question.append(sentence)\n",
    "        else:\n",
    "            all_answer.append(sentence)\n",
    "df_dialogue = pd.DataFrame([all_question,all_answer]).T\n",
    "df_dialogue.rename(columns={0:'question', 1:'answer'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7372, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dialogue.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save DataFrame inorder to not reload again\n",
    "import pickle\n",
    "# with open('df_dialogue.pickle', 'wb') as f:\n",
    "#     pickle.dump(df_dialogue, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('df_dialogue.pickle', 'rb') as f:\n",
    "    df_dialogue = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.savetxt('dialogueDataset.txt', df_dialogue.values, fmt='%s', delimiter='\\t')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class NaiveCustomLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_sz: int, hidden_sz: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        \n",
    "        # Parameters for computing i_t\n",
    "        self.U_i = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_i = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        # Parameters for computing f_t\n",
    "        self.U_f = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_f = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        # Parameters for computing c_t\n",
    "        self.U_c = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_c = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_c = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        # Parameters for computing o_t\n",
    "        self.U_o = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_o = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        self.init_weights()\n",
    "                \n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.normal_(mean=0, std=stdv)\n",
    "         \n",
    "    def forward(self, x, init_states=None):\n",
    "        \"\"\"\n",
    "        forward: Run input x through the cell. Assumes x.shape is (batch_size, sequence_length, input_size)\n",
    "        \"\"\"\n",
    "        bs, seq_sz, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        \n",
    "        if init_states is None:\n",
    "            h_t, c_t = (\n",
    "                torch.zeros(bs, self.hidden_size).to(x.device),\n",
    "                torch.zeros(bs, self.hidden_size).to(x.device),\n",
    "            )\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "            \n",
    "        for t in range(seq_sz):\n",
    "            x_t = x[:, t, :]\n",
    "            \n",
    "            i_t = torch.sigmoid(x_t @ self.U_i + h_t @ self.V_i + self.b_i)\n",
    "            f_t = torch.sigmoid(x_t @ self.U_f + h_t @ self.V_f + self.b_f)\n",
    "            g_t = torch.tanh(x_t @ self.U_c + h_t @ self.V_c + self.b_c)\n",
    "            o_t = torch.sigmoid(x_t @ self.U_o + h_t @ self.V_o + self.b_o)\n",
    "            c_t = f_t * c_t + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            \n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "        \n",
    "        # Reshape hidden_seq tensor to (batch size, sequence length, hidden_size)\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "\n",
    "        return hidden_seq, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:2\" if CUDA else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserved word tokens\n",
    "\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "class Voc:\n",
    "\n",
    "    def __init__(self):        \n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run through the dataset and assemble our vocabulary,\n",
    "as well as the training query/response sentence pairs.\n",
    "Before we can use the data, we have to perform some\n",
    "preprocessing. First, we convert Unicode strings to ASCII using\n",
    "`unicodeToAscii`. Then we convert every letter to lowercase and\n",
    "trim all non-letter characters except for basic punctuation\n",
    "(function `normalizeString`). Finally, to improve training convergence,\n",
    "we filter out sentences with length greater than the `MAX_LENGTH`\n",
    "threshold (function `filterPairs`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10  # Maximum sentence length to consider\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Read query/response pairs and return a Voc object\n",
    "\n",
    "def readVocs(datafile):\n",
    "    print(\"Reading lines...\")    \n",
    "    # Read the file and split into lines\n",
    "    lines = open(datafile, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    voc = Voc()\n",
    "    return voc, pairs\n",
    "\n",
    "# Boolean function returning True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
    "\n",
    "def filterPair(p):\n",
    "    # Input sequences need to preserve the last word for EOS token\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "# Filter pairs using the filterPair predicate\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "# Using the functions defined above, return a populated voc object and pairs list\n",
    "\n",
    "def loadPrepareData(datafile):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(datafile)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 7372 sentence pairs\n",
      "Trimmed to 596 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 573\n",
      "\n",
      "pairs:\n",
      "['thanks for the service good day .', 'you re welcome ! have a great day !']\n",
      "['does it have free wifi ?', 'how about the avalon guesthouse ?']\n",
      "['what is the price for that train ?', 'it is . pounds per person .']\n",
      "['yes please book seats for me .', 'i will book the reservation now .']\n",
      "['no thanks again for all of your help', 'okay thank you and goodbye !']\n",
      "['i am planning a trip in cambridge', 'what can i help you with ?']\n",
      "['no that will be all . thanks .', 'your welcome enjoy your trip']\n",
      "['i will be travelling on wednesday .', 'alright where will you be departing ?']\n",
      "['how long will that trip take ?', 'the travel time for that train is minutes']\n",
      "['no that is it . thank you !', 'you re welcome and have a great day !']\n"
     ]
    }
   ],
   "source": [
    "# wget https://github.com/dsai-asia/RTML/raw/main/Labs/11-LSTMs/chatDataset.txt\n",
    "\n",
    "# Load/Assemble Voc and pairs\n",
    "\n",
    "datafile = 'dialogueDataset.txt'\n",
    "voc, pairs = loadPrepareData(datafile)\n",
    "\n",
    "# Print some pairs to validate\n",
    "\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already mentioned, we will trim out rarely-used\n",
    "words from the vocabulary. This will help improve convergence during\n",
    "training, because with a lower-dimensional input feature space, it will be easier\n",
    "to estimate the probability model $P(y \\mid x)$. We trim as a two-step\n",
    "process:\n",
    "\n",
    "1. Trim words appearing fewer than `MIN_COUNT` times with the previously-given `Voc.trim`\n",
    "   method.\n",
    "\n",
    "2. Filter out all sentence pairs containing trimmed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 236 / 570 = 0.4140\n",
      "Trimmed from 596 pairs to 362, 0.6074 of total\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 3    # Minimum word count threshold for trimming\n",
    "\n",
    "def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "    # Trim words used under the MIN_COUNT from the voc\n",
    "    voc.trim(MIN_COUNT)\n",
    "    # Filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # Check input sentence\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # Check output sentence\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "# Trim vocabulary and pairs\n",
    "\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into testing and training pair sets\n",
    "\n",
    "Let's split the dataset into the first 45,000 pairs for training and the rest for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "testpairs = pairs[300:]\n",
    "pairs  = pairs[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert pairs to tensors\n",
    "\n",
    "First, let's make tensors representing sentences in which we encode\n",
    "each sequence as a sequence of indices. The sequences should all be\n",
    "padded to to a length of `MAX_LENGTH` so that they are all the same\n",
    "size. The transformation will look like this: \n",
    "\n",
    "![title](img/seq2seq_batches.png)\n",
    "\n",
    "`zeroPadding` does the padding.\n",
    "\n",
    "The ``inputVar`` function handles the process of converting sentences to\n",
    "tensor, ultimately creating a correctly shaped zero-padded tensor. It\n",
    "also returns a tensor of ``lengths`` for each of the sequences in the\n",
    "batch which will be passed to our decoder later.\n",
    "\n",
    "The ``outputVar`` function performs a similar function to ``inputVar``,\n",
    "but instead of returning a ``lengths`` tensor, it returns a binary mask\n",
    "tensor and a maximum target sentence length. The binary mask tensor has\n",
    "the same shape as the output target tensor, but every element that is a\n",
    "*PAD_token* is 0 and all others are 1.\n",
    "\n",
    "``batch2TrainData`` simply takes a bunch of pairs and returns the input\n",
    "and target tensors using the aforementioned functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# Return a padded input sequence tensor and the lengths of each original sequence\n",
    "\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "# Return a padded target sequence tensor, a padding mask, and the max target length\n",
    "\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.BoolTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "# Return all items for a given batch of pairs\n",
    "\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "# Example for validation\n",
    "\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As LSTM takes time series data so we need to convert our pairs of sentences into time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['thanks for the service good day .', 'you re welcome ! have a great day !'], ['no thanks again for all of your help', 'okay thank you and goodbye !'], ['i am planning a trip in cambridge', 'what can i help you with ?'], ['no that will be all . thanks .', 'your welcome enjoy your trip'], ['no that is it . thank you !', 'you re welcome and have a great day !']]\n",
      "[['no thanks again for all of your help', 'okay thank you and goodbye !'], ['no that will be all . thanks .', 'your welcome enjoy your trip'], ['no that is it . thank you !', 'you re welcome and have a great day !'], ['thanks for the service good day .', 'you re welcome ! have a great day !'], ['i am planning a trip in cambridge', 'what can i help you with ?']]\n",
      "tensor([[ 10,  45,  14,  10,  28],\n",
      "        [ 11,  46,  15,  11, 157],\n",
      "        [ 12,  10, 109,  12, 128],\n",
      "        [  9,  47,  51,  14,  53],\n",
      "        [ 14,  48,   9,  15, 106],\n",
      "        [ 15,  13,  48,  16,   9],\n",
      "        [ 16,   2,   9,   8,   2],\n",
      "        [ 64,   0,   2,  13,   0],\n",
      "        [ 13,   0,   0,   2,   0],\n",
      "        [  2,   0,   0,   0,   0]])\n",
      "tensor([[ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True, False,  True,  True, False],\n",
      "        [ True, False, False,  True, False],\n",
      "        [ True, False, False, False, False]])\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "pair_batch = pairs[:5]\n",
    "print(pair_batch)\n",
    "pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "print(pair_batch)\n",
    "print(target_variable)\n",
    "print(mask)\n",
    "print(max_target_len)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Define models\n",
    "\n",
    "Seq2Seq Model\n",
    "\n",
    "The brains of our chatbot is a sequence-to-sequence (seq2seq) model. The\n",
    "goal of a seq2seq model is to take a variable-length sequence as an\n",
    "input, and return a variable-length sequence as an output using a\n",
    "fixed-sized model.\n",
    "\n",
    "`Sutskever et al. <https://arxiv.org/abs/1409.3215>`__ discovered that\n",
    "by using two separate recurrent neural nets together, we can accomplish\n",
    "this task. One RNN acts as an **encoder**, which encodes a variable\n",
    "length input sequence to a fixed-length context vector. In theory, this\n",
    "context vector (the final hidden layer of the RNN) will contain semantic\n",
    "information about the query sentence that is input to the bot. The\n",
    "second RNN is a **decoder**, which takes an input word and the context\n",
    "vector, and returns a guess for the next word in the sequence and a\n",
    "hidden state to use in the next iteration.\n",
    "\n",
    "![title](img/seq2seq_ts2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "\n",
    "The encoder RNN iterates through the input sentence one token\n",
    "(e.g. word) at a time, at each time step outputting an “output” vector\n",
    "and a “hidden state” vector. The hidden state vector is then passed to\n",
    "the next time step, while the output vector is recorded. The encoder\n",
    "transforms the context it saw at each point in the sequence into a set\n",
    "of points in a high-dimensional space, which the decoder will use to\n",
    "generate a meaningful output for the given task.\n",
    "\n",
    "At the heart of our encoder is a multi-layered LSTM.\n",
    "We will use a bidirectional variant of the LSTM, meaning that there\n",
    "are essentially two independent RNNs: one that is fed the input sequence\n",
    "in normal sequential order, and one that is fed the input sequence in\n",
    "reverse order. The outputs of each network are summed at each time step.\n",
    "Using a bidirectional LSTM will give us the advantage of encoding both\n",
    "past and future context.\n",
    "\n",
    "![title](img/RNN-bidirectional2.png)\n",
    "\n",
    "\n",
    "Note that an ``embedding`` layer is used to encode our word indices in\n",
    "an arbitrarily sized feature space. For our models, this layer will map\n",
    "each word to a feature space of size *hidden_size*. When trained, these\n",
    "values should encode semantic similarity between similar meaning words.\n",
    "\n",
    "Finally, if passing a padded batch of sequences to an RNN module, we\n",
    "must pack and unpack padding around the RNN pass using\n",
    "``nn.utils.rnn.pack_padded_sequence`` and\n",
    "``nn.utils.rnn.pad_packed_sequence`` respectively.\n",
    "\n",
    "**Computation Graph:**\n",
    "\n",
    "   1) Convert word indexes to embeddings.\n",
    "\n",
    "   2) Pack padded batch of sequences for RNN module.\n",
    "\n",
    "   3) Forward pass through LSTM.\n",
    "\n",
    "   4) Unpack padding.\n",
    "\n",
    "   5) Sum bidirectional LSTM outputs.\n",
    "   \n",
    "   6) Return output and final hidden state.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "-  ``input_seq``: batch of input sentences; shape=\\ *(max_length,\n",
    "   batch_size)*\n",
    "-  ``input_lengths``: list of sentence lengths corresponding to each\n",
    "   sentence in the batch; shape=\\ *(batch_size)*\n",
    "-  ``hidden``: hidden state; shape=\\ *(n_layers x num_directions,\n",
    "   batch_size, hidden_size)*\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "-  ``outputs``: output features from the last hidden layer of the LSTM\n",
    "   (sum of bidirectional outputs); shape=\\ *(max_length, batch_size,\n",
    "   hidden_size)*\n",
    "-  ``hidden``: updated hidden state from LSTM; shape=\\ *(n_layers x\n",
    "   num_directions, batch_size, hidden_size)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "\n",
    "Now we declare our encoder which is consist of bidirectional LSTM units.It is vital to declare bidirectional LSTM as their results are better than unidirectional LSTM in some NLP problems.What it done is instead of learning embeddings of previous words it also considers the embeddings or features of next word suitable to predict target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        \n",
    "        embedded = self.embedding(input_seq)\n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths.cpu())\n",
    "        \n",
    "        outputs, hidden = self.lstm(packed, hidden)\n",
    "        \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        \n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        \n",
    "        return outputs, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        \n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder\n",
    "\n",
    "The decoder RNN generates the response sentence in a token-by-token\n",
    "fashion. It uses the encoder’s context vectors, and internal hidden\n",
    "states to generate the next word in the sequence. It continues\n",
    "generating words until it outputs an *EOS_token*, representing the end\n",
    "of the sentence. A common problem with a vanilla seq2seq decoder is that\n",
    "if we rely soley on the context vector to encode the entire input\n",
    "sequence’s meaning, it is likely that we will have information loss.\n",
    "This is especially the case when dealing with long input sequences,\n",
    "greatly limiting the capability of our decoder.\n",
    "\n",
    "To combat this, `Bahdanau et al. <https://arxiv.org/abs/1409.0473>`__\n",
    "created an “attention mechanism” that allows the decoder to pay\n",
    "attention to certain parts of the input sequence, rather than using the\n",
    "entire fixed context at every step.\n",
    "\n",
    "At a high level, attention is calculated using the decoder’s current\n",
    "hidden state and the encoder’s outputs. The output attention weights\n",
    "have the same shape as the input sequence, allowing us to multiply them\n",
    "by the encoder outputs, giving us a weighted sum which indicates the\n",
    "parts of encoder output to pay attention to. `Sean\n",
    "Robertson’s <https://github.com/spro>`__ figure describes this very\n",
    "well:\n",
    "![title](img/attn2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "        attn_energies = attn_energies.t()\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our attention submodule, we can implement the\n",
    "actual decoder model. For the decoder, we will manually feed our batch\n",
    "one time step at a time. This means that our embedded word tensor and\n",
    "LSTM output will both have shape *(1, batch_size, hidden_size)*.\n",
    "\n",
    "**Computation Graph:**\n",
    "\n",
    "   1) Get embedding of current input word.\n",
    "\n",
    "   2) Forward through unidirectional LSTM.\n",
    "\n",
    "   3) Calculate attention weights from the current LSTM output from (2).\n",
    "\n",
    "   4) Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector.\n",
    "\n",
    "   5) Concatenate weighted context vector and LSTM output using Luong eq. 5.\n",
    "\n",
    "   6) Predict next word using Luong eq. 6 (without softmax).\n",
    "   \n",
    "   7) Return output and final hidden state.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "-  ``input_step``: one time step (one word) of input sequence batch;\n",
    "   shape=\\ *(1, batch_size)*\n",
    "-  ``last_hidden``: final hidden layer of LSTM; shape=\\ *(n_layers x\n",
    "   num_directions, batch_size, hidden_size)*\n",
    "-  ``encoder_outputs``: encoder model’s output; shape=\\ *(max_length,\n",
    "   batch_size, hidden_size)*\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "-  ``output``: softmax normalized tensor giving probabilities of each\n",
    "   word being the correct next word in the decoded sequence;\n",
    "   shape=\\ *(batch_size, voc.num_words)*\n",
    "-  ``hidden``: final hidden state of LSTM; shape=\\ *(n_layers x\n",
    "   num_directions, batch_size, hidden_size)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "       \n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))#, bidirectional=True)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "    \n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        rnn_output, hidden = self.lstm(embedded, last_hidden)\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return output, hidden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Training procedure\n",
    "\n",
    "Masked loss\n",
    "\n",
    "Since we are dealing with batches of padded sequences, we cannot simply\n",
    "consider all elements of the tensor when calculating loss. We define\n",
    "``maskNLLLoss`` to calculate our loss based on our decoder’s output\n",
    "tensor, the target tensor, and a binary mask tensor describing the\n",
    "padding of the target tensor. This loss function calculates the average\n",
    "negative log likelihood of the elements that correspond to a *1* in the\n",
    "mask tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single training iteration\n",
    "\n",
    "The ``train`` function contains the algorithm for a single training\n",
    "iteration (a single batch of inputs).\n",
    "\n",
    "We will use a couple of clever tricks to aid in convergence:\n",
    "\n",
    "-  The first trick is using **teacher forcing**. This means that at some\n",
    "   probability, set by ``teacher_forcing_ratio``, we use the current\n",
    "   target word as the decoder’s next input rather than using the\n",
    "   decoder’s current guess. This technique acts as training wheels for\n",
    "   the decoder, aiding in more efficient training. However, teacher\n",
    "   forcing can lead to model instability during inference, as the\n",
    "   decoder may not have a sufficient chance to truly craft its own\n",
    "   output sequences during training. Thus, we must be mindful of how we\n",
    "   are setting the ``teacher_forcing_ratio``, and not be fooled by fast\n",
    "   convergence.\n",
    "\n",
    "-  The second trick that we implement is **gradient clipping**. This is\n",
    "   a commonly used technique for countering the “exploding gradient”\n",
    "   problem. In essence, by clipping or thresholding gradients to a\n",
    "   maximum value, we prevent the gradients from growing exponentially\n",
    "   and either overflow (NaN), or overshoot steep cliffs in the cost\n",
    "   function.\n",
    "\n",
    "\n",
    "\n",
    "**Sequence of Operations:**\n",
    "\n",
    "   1) Forward pass entire input batch through encoder.\n",
    "\n",
    "   2) Initialize decoder inputs as SOS_token, and hidden state as the encoder's final hidden state.\n",
    "   \n",
    "   3) Forward input batch sequence through decoder one time step at a time.\n",
    "\n",
    "   4) If teacher forcing: set next decoder input as the current target; else: set next decoder input as current decoder output.\n",
    "\n",
    "   5) Calculate and accumulate loss.\n",
    "\n",
    "   6) Perform backpropagation.\n",
    "\n",
    "   7) Clip gradients.\n",
    "   \n",
    "   8) Update encoder and decoder model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    \n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    \n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    \n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            \n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            \n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            \n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            \n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    \n",
    "    loss.backward()\n",
    "\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full training procedure\n",
    "\n",
    "It is finally time to tie the full training procedure together with the\n",
    "data. The ``trainIters`` function is responsible for running\n",
    "``n_iterations`` of training given the passed models, optimizers, data,\n",
    "etc. This function is quite self explanatory, as we have done the heavy\n",
    "lifting with the ``train`` function.\n",
    "\n",
    "One thing to note is that when we save our model, we save a tarball\n",
    "containing the encoder and decoder state_dicts (parameters), the\n",
    "optimizers’ state_dicts, the loss, the iteration, etc. Saving the model\n",
    "in this way will give us the ultimate flexibility with the checkpoint.\n",
    "After loading a checkpoint, we will be able to use the model parameters\n",
    "to run inference, or we can continue training right where we left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_target_len "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    losslist = []\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        \n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        \n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        \n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "            losslist.append(print_loss_avg)\n",
    "        \n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            print(directory)\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n",
    "    return losslist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Evaluation function\n",
    "\n",
    "After training a model, we want to be able to talk to the bot ourselves.\n",
    "First, we must define how we want the model to decode the encoded input.\n",
    "\n",
    "### 1.5.1 Greedy decoding\n",
    "\n",
    "Greedy decoding is the decoding method that we use during training when\n",
    "we are **NOT** using teacher forcing. In other words, for each time\n",
    "step, we simply choose the word from ``decoder_output`` with the highest\n",
    "softmax value. This decoding method is optimal on a single time-step\n",
    "level.\n",
    "\n",
    "To facilite the greedy decoding operation, we define a\n",
    "``GreedySearchDecoder`` class. When run, an object of this class takes\n",
    "an input sequence (``input_seq``) of shape *(input_seq length, 1)*, a\n",
    "scalar input length (``input_length``) tensor, and a ``max_length`` to\n",
    "bound the response sentence length. The input sentence is evaluated\n",
    "using the following computational graph:\n",
    "\n",
    "**Computation Graph:**\n",
    "\n",
    "   1) Forward input through encoder model.\n",
    "\n",
    "   2) Prepare encoder's final hidden layer to be first hidden input to the decoder.\n",
    "\n",
    "   3) Initialize decoder's first input as SOS_token.\n",
    "\n",
    "   4) Initialize tensors to append decoded words to.\n",
    "\n",
    "   5) Iteratively decode one word token at a time:\n",
    "   \n",
    "       a) Forward pass through decoder.\n",
    "\n",
    "       b) Obtain most likely word token and its softmax score.\n",
    "\n",
    "       c) Record token and score.\n",
    "\n",
    "       d) Prepare current token to be next decoder input.\n",
    "\n",
    "   6) Return collections of word tokens and scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        \n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        \n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        \n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "       \n",
    "        for _ in range(max_length):\n",
    "            \n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            \n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        \n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating some text\n",
    "\n",
    "Now that we have our decoding method defined, we can write functions for\n",
    "evaluating a string input sentence. The ``evaluate`` function manages\n",
    "the low-level process of handling the input sentence. We first format\n",
    "the sentence as an input batch of word indexes with *batch_size==1*. We\n",
    "do this by converting the words of the sentence to their corresponding\n",
    "indexes, and transposing the dimensions to prepare the tensor for our\n",
    "models. We also create a ``lengths`` tensor which contains the length of\n",
    "our input sentence. In this case, ``lengths`` is scalar because we are\n",
    "only evaluating one sentence at a time (batch_size==1). Next, we obtain\n",
    "the decoded response sentence tensor using our ``GreedySearchDecoder``\n",
    "object (``searcher``). Finally, we convert the response’s indexes to\n",
    "words and return the list of decoded words.\n",
    "\n",
    "``evaluateInput`` acts as the user interface for our chatbot. When\n",
    "called, an input text field will spawn in which we can enter our query\n",
    "sentence. After typing our input sentence and pressing *Enter*, our text\n",
    "is normalized in the same way as our training data, and is ultimately\n",
    "fed to the ``evaluate`` function to obtain a decoded output sentence. We\n",
    "loop this process, so we can keep chatting with our bot until we enter\n",
    "either “q” or “quit”.\n",
    "\n",
    "Finally, if a sentence is entered that contains a word that is not in\n",
    "the vocabulary, we handle this gracefully by printing an error message\n",
    "and prompting the user to enter another sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "  \n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    \n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    \n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    \n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    \n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    \n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            \n",
    "            input_sentence = input('> ')\n",
    "            \n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            \n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            \n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            \n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.1.1 Run Model Greedy\n",
    "\n",
    "Finally, it is time to run our model!\n",
    "\n",
    "Regardless of whether we want to train or test the chatbot model, we\n",
    "must initialize the individual encoder and decoder models. In the\n",
    "following block, we set our desired configurations, choose to start from\n",
    "scratch or set a checkpoint to load from, and build and initialize the\n",
    "models. Feel free to play with different model configurations to\n",
    "optimize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "\n",
    "hidden_size = 512\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 4\n",
    "dropout_p = 0.5\n",
    "batch_size = 256 \n",
    "loadFilename = None\n",
    "\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout_p)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout_p)\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n",
      "Iteration: 100; Percent complete: 1.7%; Average loss: 3.4569\n",
      "Iteration: 200; Percent complete: 3.3%; Average loss: 1.9393\n",
      "Iteration: 300; Percent complete: 5.0%; Average loss: 0.8445\n",
      "Iteration: 400; Percent complete: 6.7%; Average loss: 0.2886\n",
      "Iteration: 500; Percent complete: 8.3%; Average loss: 0.1038\n",
      "Iteration: 600; Percent complete: 10.0%; Average loss: 0.0568\n",
      "Iteration: 700; Percent complete: 11.7%; Average loss: 0.0385\n",
      "Iteration: 800; Percent complete: 13.3%; Average loss: 0.0367\n",
      "Iteration: 900; Percent complete: 15.0%; Average loss: 0.0305\n",
      "Iteration: 1000; Percent complete: 16.7%; Average loss: 0.0189\n",
      "Iteration: 1100; Percent complete: 18.3%; Average loss: 0.0180\n",
      "Iteration: 1200; Percent complete: 20.0%; Average loss: 0.0176\n",
      "Iteration: 1300; Percent complete: 21.7%; Average loss: 0.0150\n",
      "Iteration: 1400; Percent complete: 23.3%; Average loss: 0.0137\n",
      "Iteration: 1500; Percent complete: 25.0%; Average loss: 0.0123\n",
      "Iteration: 1600; Percent complete: 26.7%; Average loss: 0.0131\n",
      "Iteration: 1700; Percent complete: 28.3%; Average loss: 0.0121\n",
      "Iteration: 1800; Percent complete: 30.0%; Average loss: 0.0111\n",
      "Iteration: 1900; Percent complete: 31.7%; Average loss: 0.0116\n",
      "Iteration: 2000; Percent complete: 33.3%; Average loss: 0.0195\n",
      "content/cb_model/Chat/2-4_512\n",
      "Iteration: 2100; Percent complete: 35.0%; Average loss: 0.0255\n",
      "Iteration: 2200; Percent complete: 36.7%; Average loss: 0.0147\n",
      "Iteration: 2300; Percent complete: 38.3%; Average loss: 0.0112\n",
      "Iteration: 2400; Percent complete: 40.0%; Average loss: 0.0109\n",
      "Iteration: 2500; Percent complete: 41.7%; Average loss: 0.0102\n",
      "Iteration: 2600; Percent complete: 43.3%; Average loss: 0.0104\n",
      "Iteration: 2700; Percent complete: 45.0%; Average loss: 0.0107\n",
      "Iteration: 2800; Percent complete: 46.7%; Average loss: 0.0100\n",
      "Iteration: 2900; Percent complete: 48.3%; Average loss: 0.0100\n",
      "Iteration: 3000; Percent complete: 50.0%; Average loss: 0.0117\n",
      "Iteration: 3100; Percent complete: 51.7%; Average loss: 0.0097\n",
      "Iteration: 3200; Percent complete: 53.3%; Average loss: 0.0098\n",
      "Iteration: 3300; Percent complete: 55.0%; Average loss: 0.0125\n",
      "Iteration: 3400; Percent complete: 56.7%; Average loss: 0.0155\n",
      "Iteration: 3500; Percent complete: 58.3%; Average loss: 0.0100\n",
      "Iteration: 3600; Percent complete: 60.0%; Average loss: 0.0102\n",
      "Iteration: 3700; Percent complete: 61.7%; Average loss: 0.0101\n",
      "Iteration: 3800; Percent complete: 63.3%; Average loss: 0.0110\n",
      "Iteration: 3900; Percent complete: 65.0%; Average loss: 0.0100\n",
      "Iteration: 4000; Percent complete: 66.7%; Average loss: 0.0098\n",
      "content/cb_model/Chat/2-4_512\n",
      "Iteration: 4100; Percent complete: 68.3%; Average loss: 0.0099\n",
      "Iteration: 4200; Percent complete: 70.0%; Average loss: 0.0109\n",
      "Iteration: 4300; Percent complete: 71.7%; Average loss: 0.0099\n",
      "Iteration: 4400; Percent complete: 73.3%; Average loss: 0.0097\n",
      "Iteration: 4500; Percent complete: 75.0%; Average loss: 0.0093\n",
      "Iteration: 4600; Percent complete: 76.7%; Average loss: 0.0098\n",
      "Iteration: 4700; Percent complete: 78.3%; Average loss: 0.0097\n",
      "Iteration: 4800; Percent complete: 80.0%; Average loss: 0.0099\n",
      "Iteration: 4900; Percent complete: 81.7%; Average loss: 0.0127\n",
      "Iteration: 5000; Percent complete: 83.3%; Average loss: 0.0225\n",
      "Iteration: 5100; Percent complete: 85.0%; Average loss: 0.0118\n",
      "Iteration: 5200; Percent complete: 86.7%; Average loss: 0.0142\n",
      "Iteration: 5300; Percent complete: 88.3%; Average loss: 0.0101\n",
      "Iteration: 5400; Percent complete: 90.0%; Average loss: 0.0099\n",
      "Iteration: 5500; Percent complete: 91.7%; Average loss: 0.0098\n",
      "Iteration: 5600; Percent complete: 93.3%; Average loss: 0.0098\n",
      "Iteration: 5700; Percent complete: 95.0%; Average loss: 0.0095\n",
      "Iteration: 5800; Percent complete: 96.7%; Average loss: 0.0098\n",
      "Iteration: 5900; Percent complete: 98.3%; Average loss: 0.0095\n",
      "Iteration: 6000; Percent complete: 100.0%; Average loss: 0.0099\n",
      "content/cb_model/Chat/2-4_512\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'content/'\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 6000\n",
    "print_every = 100\n",
    "save_every = 2000\n",
    "loadFilename = None\n",
    "corpus_name= \"Chat\"\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "print(\"Starting Training!\")\n",
    "lossvalues = trainIters(model, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(lossvalues)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU score calculation\n",
    "\n",
    "The BLEU score (the Bilingual Evaluation Understudy Score) is an evaluation metric used to calculate the capacity of our model to make correct predictions. It finds maximal n-gram matches between predicted sentences and reference sentences You can learn more about BLEU from [this tutorial](https://machinelearningmastery.com/calculate-bleu-score-for-text-python/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2021768865708778 0.04952302098832033\n",
      "Total Bleu Score for 1 grams on testing pairs:  0.31362233984315974\n",
      "Total Bleu Score for 2 grams on testing pairs:  0.18756575738694167\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu,corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "# Set dropout layers to eval mode\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "gram1_bleu_score = []\n",
    "gram2_bleu_score = []\n",
    "for i in range(0,len(testpairs),1):\n",
    "    input_sentence = testpairs[i][0]\n",
    "    reference = testpairs[i][1:]\n",
    "    templist = []\n",
    "    for k in range(len(reference)):\n",
    "        if(reference[k]!=''):\n",
    "            temp = reference[k].split(' ')\n",
    "            templist.append(temp)\n",
    "\n",
    "    input_sentence = normalizeString(input_sentence)\n",
    "    output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "    output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "    chencherry = SmoothingFunction()\n",
    "    #   print(output_words)\n",
    "    #   print(templist)\n",
    "    score1 = sentence_bleu(templist, output_words, weights=(1, 0, 0, 0), smoothing_function=chencherry.method1)\n",
    "    score2 = sentence_bleu(templist, output_words, weights=(0.5, 0.5, 0, 0), smoothing_function=chencherry.method1) \n",
    "    gram1_bleu_score.append(score1)\n",
    "    gram2_bleu_score.append(score2)\n",
    "    if i%1000 == 0:\n",
    "        print(i,sum(gram1_bleu_score)/len(gram1_bleu_score),sum(gram2_bleu_score)/len(gram2_bleu_score))\n",
    "\n",
    "print(\"Total Bleu Score for 1 grams on testing pairs: \", sum(gram1_bleu_score)/len(gram1_bleu_score) )  \n",
    "print(\"Total Bleu Score for 2 grams on testing pairs: \", sum(gram2_bleu_score)/len(gram2_bleu_score) )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat with the greedy search bot\n",
    "\n",
    "Let's chat with the new chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluateInput(encoder, decoder, searcher, voc)\n",
    "# input\n",
    "# Hi, how are you?\n",
    "# What\n",
    "# I don't understand you\n",
    "# hmm, good bye"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 Beam search\n",
    "\n",
    "Beam search is an improved version of greedy search. It has a hyperparameter named beam size,  $k$. At time step 1, selecting $k$ tokens with the highest conditional probabilities. Each of them will be the first token of $k$ candidate output sequences, respectively. At each subsequent time step, based on the $k$ candidate output sequences at the previous time step, $k$ candidate output sequences has been selected with the highest conditional probabilities from $k|\\gamma|$ possible choices.\n",
    "\n",
    "<img src=\"img/lstm_beamsearch.PNG\" title=\"Beam search\" style=\"width: 640px;\" />\n",
    "\n",
    "the sequence with the highest of the following score as the output sequence has been chosen from the equation:\n",
    "\n",
    "$\\frac{1}{L^{\\alpha}}\\log{P(y_1,...,y_L)} = \\frac{1}{L^{\\alpha}} \\sum_{t'=1}^{L} \\log{P(y_{t'}|t_1,...,y_{t'-1}, c)}$\n",
    "\n",
    "Where $L$ is the length of the final candidate sequence and $\\alpha$ is usually set to 0.75."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Beam Decoder**\n",
    "\n",
    "The difference between greedy search and beam search is decoder function. Thus, greedy search function name is greedy_decode, and beam search function name is beam_decode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    def __init__(self, decoder_hidden, last_idx=SOS_token, sentence_idxes=[], sentence_scores=[]):\n",
    "        if(len(sentence_idxes) != len(sentence_scores)):\n",
    "            raise ValueError(\"length of indexes and scores should be the same\")\n",
    "        self.decoder_hidden = decoder_hidden\n",
    "        self.last_idx = last_idx\n",
    "        self.sentence_idxes =  sentence_idxes\n",
    "        self.sentence_scores = sentence_scores\n",
    "\n",
    "    def avgScore(self):\n",
    "        if len(self.sentence_scores) == 0:\n",
    "            raise ValueError(\"Calculate average score of sentence, but got no word\")\n",
    "        # return mean of sentence_score\n",
    "        return sum(self.sentence_scores) / len(self.sentence_scores)\n",
    "\n",
    "    def addTopk(self, topi, topv, decoder_hidden, beam_size, voc):\n",
    "        topv = torch.log(topv)\n",
    "        terminates, sentences = [], []\n",
    "        for i in range(beam_size):\n",
    "            if topi[0][i] == EOS_token:\n",
    "                terminates.append(([voc.index2word[idx.item()] for idx in self.sentence_idxes] + ['<EOS>'],\n",
    "                                   self.avgScore())) \n",
    "                continue\n",
    "            idxes = self.sentence_idxes[:] \n",
    "            scores = self.sentence_scores[:] \n",
    "            idxes.append(topi[0][i])\n",
    "            scores.append(topv[0][i])\n",
    "            sentences.append(Sentence(decoder_hidden, topi[0][i], idxes, scores))\n",
    "        return terminates, sentences\n",
    "\n",
    "    def toWordScore(self, voc):\n",
    "        \n",
    "        words = []\n",
    "        for i in range(len(self.sentence_idxes)):\n",
    "            if self.sentence_idxes[i] == EOS_token:\n",
    "                words.append('<EOS>')\n",
    "            else:\n",
    "                words.append(voc.index2word[self.sentence_idxes[i].item()])\n",
    "       \n",
    "        if self.sentence_idxes[-1] != EOS_token:\n",
    "            words.append('<EOS>')\n",
    "        return (words, self.avgScore())\n",
    "\n",
    "    def __repr__(self):\n",
    "        res = f\"Sentence with indices {self.sentence_idxes} \"\n",
    "        res += f\"and scores {self.sentence_scores}\"\n",
    "        return res\n",
    "\n",
    "\n",
    "def beam_decode(decoder, decoder_hidden, encoder_outputs, voc, beam_size, max_length=MAX_LENGTH):\n",
    "    terminal_sentences, prev_top_sentences, next_top_sentences = [], [], []\n",
    "    prev_top_sentences.append(Sentence(decoder_hidden))\n",
    "    for i in range(max_length):\n",
    "        \n",
    "        for sentence in prev_top_sentences:\n",
    "            decoder_input = torch.LongTensor([[sentence.last_idx]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "\n",
    "            decoder_hidden = sentence.decoder_hidden\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(beam_size)\n",
    "            term, top = sentence.addTopk(topi, topv, decoder_hidden, beam_size, voc)\n",
    "            terminal_sentences.extend(term)\n",
    "            next_top_sentences.extend(top)\n",
    "           \n",
    "        \n",
    "        next_top_sentences.sort(key=lambda s: s.avgScore(), reverse=True)\n",
    "        prev_top_sentences = next_top_sentences[:beam_size]\n",
    "        next_top_sentences = []\n",
    "        \n",
    "\n",
    "    terminal_sentences += [sentence.toWordScore(voc) for sentence in prev_top_sentences]\n",
    "    terminal_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    n = min(len(terminal_sentences), 15)\n",
    "    return terminal_sentences[:n]\n",
    "\n",
    "\n",
    "class BeamSearchDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, voc, beam_size=10):\n",
    "        super(BeamSearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.voc = voc\n",
    "        self.beam_size = beam_size\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden[:self.decoder.n_layers]\n",
    "        \n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        sentences = beam_decode(self.decoder, decoder_hidden, encoder_outputs, self.voc, self.beam_size, max_length)\n",
    "        \n",
    "        \n",
    "        all_tokens = [torch.tensor(self.voc.word2index.get(w, 0)) for w in sentences[0][0]]\n",
    "        return all_tokens, None\n",
    "\n",
    "    def __str__(self):\n",
    "        res = f\"BeamSearchDecoder with beam size {self.beam_size}\"\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "\n",
    "hidden_size = 512\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 4\n",
    "dropout_p = 0.5\n",
    "batch_size = 256 \n",
    "loadFilename = None\n",
    "\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout_p)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout_p)\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.2.1 Run Model Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n",
      "Iteration: 100; Percent complete: 1.7%; Average loss: 3.4290\n",
      "Iteration: 200; Percent complete: 3.3%; Average loss: 1.8739\n",
      "Iteration: 300; Percent complete: 5.0%; Average loss: 0.7766\n",
      "Iteration: 400; Percent complete: 6.7%; Average loss: 0.2634\n",
      "Iteration: 500; Percent complete: 8.3%; Average loss: 0.1071\n",
      "Iteration: 600; Percent complete: 10.0%; Average loss: 0.0519\n",
      "Iteration: 700; Percent complete: 11.7%; Average loss: 0.0335\n",
      "Iteration: 800; Percent complete: 13.3%; Average loss: 0.0257\n",
      "Iteration: 900; Percent complete: 15.0%; Average loss: 0.0221\n",
      "Iteration: 1000; Percent complete: 16.7%; Average loss: 0.0272\n",
      "Iteration: 1100; Percent complete: 18.3%; Average loss: 0.0172\n",
      "Iteration: 1200; Percent complete: 20.0%; Average loss: 0.0151\n",
      "Iteration: 1300; Percent complete: 21.7%; Average loss: 0.0136\n",
      "Iteration: 1400; Percent complete: 23.3%; Average loss: 0.0145\n",
      "Iteration: 1500; Percent complete: 25.0%; Average loss: 0.0177\n",
      "Iteration: 1600; Percent complete: 26.7%; Average loss: 0.0129\n",
      "Iteration: 1700; Percent complete: 28.3%; Average loss: 0.0121\n",
      "Iteration: 1800; Percent complete: 30.0%; Average loss: 0.0118\n",
      "Iteration: 1900; Percent complete: 31.7%; Average loss: 0.0114\n",
      "Iteration: 2000; Percent complete: 33.3%; Average loss: 0.0124\n",
      "content/cb_model/Chat/2-4_512\n",
      "Iteration: 2100; Percent complete: 35.0%; Average loss: 0.0118\n",
      "Iteration: 2200; Percent complete: 36.7%; Average loss: 0.0107\n",
      "Iteration: 2300; Percent complete: 38.3%; Average loss: 0.0111\n",
      "Iteration: 2400; Percent complete: 40.0%; Average loss: 0.0115\n",
      "Iteration: 2500; Percent complete: 41.7%; Average loss: 0.0121\n",
      "Iteration: 2600; Percent complete: 43.3%; Average loss: 0.0114\n",
      "Iteration: 2700; Percent complete: 45.0%; Average loss: 0.0151\n",
      "Iteration: 2800; Percent complete: 46.7%; Average loss: 0.0168\n",
      "Iteration: 2900; Percent complete: 48.3%; Average loss: 0.0103\n",
      "Iteration: 3000; Percent complete: 50.0%; Average loss: 0.0170\n",
      "Iteration: 3100; Percent complete: 51.7%; Average loss: 0.0136\n",
      "Iteration: 3200; Percent complete: 53.3%; Average loss: 0.0104\n",
      "Iteration: 3300; Percent complete: 55.0%; Average loss: 0.0118\n",
      "Iteration: 3400; Percent complete: 56.7%; Average loss: 0.0118\n",
      "Iteration: 3500; Percent complete: 58.3%; Average loss: 0.0107\n",
      "Iteration: 3600; Percent complete: 60.0%; Average loss: 0.0104\n",
      "Iteration: 3700; Percent complete: 61.7%; Average loss: 0.0097\n",
      "Iteration: 3800; Percent complete: 63.3%; Average loss: 0.0101\n",
      "Iteration: 3900; Percent complete: 65.0%; Average loss: 0.0096\n",
      "Iteration: 4000; Percent complete: 66.7%; Average loss: 0.0097\n",
      "content/cb_model/Chat/2-4_512\n",
      "Iteration: 4100; Percent complete: 68.3%; Average loss: 0.0097\n",
      "Iteration: 4200; Percent complete: 70.0%; Average loss: 0.0098\n",
      "Iteration: 4300; Percent complete: 71.7%; Average loss: 0.0100\n",
      "Iteration: 4400; Percent complete: 73.3%; Average loss: 0.0102\n",
      "Iteration: 4500; Percent complete: 75.0%; Average loss: 0.0095\n",
      "Iteration: 4600; Percent complete: 76.7%; Average loss: 0.0123\n",
      "Iteration: 4700; Percent complete: 78.3%; Average loss: 0.0128\n",
      "Iteration: 4800; Percent complete: 80.0%; Average loss: 0.0100\n",
      "Iteration: 4900; Percent complete: 81.7%; Average loss: 0.0100\n",
      "Iteration: 5000; Percent complete: 83.3%; Average loss: 0.0096\n",
      "Iteration: 5100; Percent complete: 85.0%; Average loss: 0.0098\n",
      "Iteration: 5200; Percent complete: 86.7%; Average loss: 0.0096\n",
      "Iteration: 5300; Percent complete: 88.3%; Average loss: 0.0094\n",
      "Iteration: 5400; Percent complete: 90.0%; Average loss: 0.0092\n",
      "Iteration: 5500; Percent complete: 91.7%; Average loss: 0.0096\n",
      "Iteration: 5600; Percent complete: 93.3%; Average loss: 0.0097\n",
      "Iteration: 5700; Percent complete: 95.0%; Average loss: 0.0098\n",
      "Iteration: 5800; Percent complete: 96.7%; Average loss: 0.0093\n",
      "Iteration: 5900; Percent complete: 98.3%; Average loss: 0.0098\n",
      "Iteration: 6000; Percent complete: 100.0%; Average loss: 0.0097\n",
      "content/cb_model/Chat/2-4_512\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'content/'\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 6000\n",
    "print_every = 100\n",
    "save_every = 2000\n",
    "loadFilename = None\n",
    "corpus_name=\"Chat\"\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "print(\"Starting Training!\")\n",
    "lossvalues = trainIters(model, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(lossvalues)\n",
    "plt.ylabel('losses')\n",
    "plt.xlabel('epochs')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU score calculation\n",
    "\n",
    "Let's calculate the BLEU score with the beam search decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0 0.0\n",
      "Total Bleu Score for 1 grams on testing pairs:  0.35400078957937203\n",
      "Total Bleu Score for 2 grams on testing pairs:  0.2196769646729017\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu,corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "# Set dropout layers to eval mode\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "\n",
    "############################################################################\n",
    "# Difference between greedy search and beam search is here\n",
    "\n",
    "# greedy search\n",
    "# searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# beam search\n",
    "searcher = BeamSearchDecoder(encoder, decoder, voc, 3)\n",
    "############################################################################\n",
    "gram1_bleu_score = []\n",
    "gram2_bleu_score = []\n",
    "\n",
    "for i in range(0,len(testpairs),1):\n",
    "    input_sentence = testpairs[i][0]\n",
    "  \n",
    "    reference = testpairs[i][1:]\n",
    "    templist = []\n",
    "    for k in range(len(reference)):\n",
    "        if(reference[k]!=''):\n",
    "            temp = reference[k].split(' ')\n",
    "            templist.append(temp)\n",
    "  \n",
    "    input_sentence = normalizeString(input_sentence)\n",
    "    output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "    output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "    chencherry = SmoothingFunction()\n",
    "    score1 = sentence_bleu(templist,output_words,weights=(1, 0, 0, 0) ,smoothing_function=chencherry.method1)\n",
    "    score2 = sentence_bleu(templist,output_words,weights=(0.5, 0.5, 0, 0),smoothing_function=chencherry.method1) \n",
    "    gram1_bleu_score.append(score1)\n",
    "    gram2_bleu_score.append(score2)\n",
    "    if i%1000 == 0:\n",
    "        print(i,sum(gram1_bleu_score)/len(gram1_bleu_score),sum(gram2_bleu_score)/len(gram2_bleu_score))\n",
    "        \n",
    "print(\"Total Bleu Score for 1 grams on testing pairs: \", sum(gram1_bleu_score)/len(gram1_bleu_score))  \n",
    "print(\"Total Bleu Score for 2 grams on testing pairs: \", sum(gram2_bleu_score)/len(gram2_bleu_score))  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chat with the beam search bot\n",
    "\n",
    "Let's chat with the new chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "searcher = BeamSearchDecoder(encoder, decoder, voc, 3)\n",
    "evaluateInput(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this lab we learned about LSTM and all of its equations in action.\n",
    "\n",
    "Then we use LSTM cells in a model called seq2seq. The seq2seq model consists of encoder and decoder network both of which includes LSTM cells. The encoder encode sequence of words and embed them as a representation then the decoder takes the embedded representation and decode another sequence of words. We use this architecture to make a chat bot where it takes a sentence as input and output a response or answer.\n",
    "\n",
    "There are 2 search algorithm we can use in the decoder part. First is the greedy search which choose the word with maximum probability at every step but might not end up with the optimum sentence. Second is beam search which adds the k candidate to the greedy search.\n",
    "\n",
    "The first part we trained a chatbot from a movie dialogue corpus and for the homework I trained a chatbot with real a human-human conversations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Replace the LSTM encoder/decoder with a Transformer. \n",
    "Check out the [PyTorch Transformer module documentation](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random, math, time\n",
    "\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "P_{i,j}= \n",
    "\\begin{cases}\n",
    "    sin(\\frac{1}{1000^\\frac{j}{d_{embed}}}),& \\text{if j is even}\\\\\n",
    "    cos(\\frac{1}{1000^\\frac{j}{d_{embed}}}),& \\text{if j is odd}\n",
    "\\end{cases}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len = 100):\n",
    "        super().__init__()\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "\n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "  \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    def __init__(self, decoder_hidden, last_idx=SOS_token, sentence_idxes=[], sentence_scores=[]):\n",
    "        if(len(sentence_idxes) != len(sentence_scores)):\n",
    "            raise ValueError(\"length of indexes and scores should be the same\")\n",
    "        self.decoder_hidden = decoder_hidden\n",
    "        self.last_idx = last_idx\n",
    "        self.sentence_idxes =  sentence_idxes\n",
    "        self.sentence_scores = sentence_scores\n",
    "\n",
    "    def avgScore(self):\n",
    "        if len(self.sentence_scores) == 0:\n",
    "            raise ValueError(\"Calculate average score of sentence, but got no word\")\n",
    "        # return mean of sentence_score\n",
    "        return sum(self.sentence_scores) / len(self.sentence_scores)\n",
    "\n",
    "    def addTopk(self, topi, topv, decoder_hidden, beam_size, voc):\n",
    "        topv = torch.log(topv)\n",
    "        terminates, sentences = [], []\n",
    "        for i in range(beam_size):\n",
    "            if topi[0][i] == EOS_token:\n",
    "                terminates.append(([voc.index2word[idx.item()] for idx in self.sentence_idxes] + ['<EOS>'],\n",
    "                                   self.avgScore())) \n",
    "                continue\n",
    "            idxes = self.sentence_idxes[:] \n",
    "            scores = self.sentence_scores[:] \n",
    "            idxes.append(topi[0][i])\n",
    "            scores.append(topv[0][i])\n",
    "            sentences.append(Sentence(decoder_hidden, topi[0][i], idxes, scores))\n",
    "        return terminates, sentences\n",
    "\n",
    "    def toWordScore(self, voc):\n",
    "        \n",
    "        words = []\n",
    "        for i in range(len(self.sentence_idxes)):\n",
    "            if self.sentence_idxes[i] == EOS_token:\n",
    "                words.append('<EOS>')\n",
    "            else:\n",
    "                words.append(voc.index2word[self.sentence_idxes[i].item()])\n",
    "       \n",
    "        if self.sentence_idxes[-1] != EOS_token:\n",
    "            words.append('<EOS>')\n",
    "        return (words, self.avgScore())\n",
    "\n",
    "    def __repr__(self):\n",
    "        res = f\"Sentence with indices {self.sentence_idxes} \"\n",
    "        res += f\"and scores {self.sentence_scores}\"\n",
    "        return res\n",
    "\n",
    "\n",
    "def beam_decode(decoder, decoder_hidden, encoder_outputs, voc, beam_size, max_length=MAX_LENGTH):\n",
    "    terminal_sentences, prev_top_sentences, next_top_sentences = [], [], []\n",
    "    prev_top_sentences.append(Sentence(decoder_hidden))\n",
    "    for i in range(max_length):\n",
    "        \n",
    "        for sentence in prev_top_sentences:\n",
    "            decoder_input = torch.LongTensor([[sentence.last_idx]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "\n",
    "            decoder_hidden = sentence.decoder_hidden\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(beam_size)\n",
    "            term, top = sentence.addTopk(topi, topv, decoder_hidden, beam_size, voc)\n",
    "            terminal_sentences.extend(term)\n",
    "            next_top_sentences.extend(top)\n",
    "           \n",
    "        \n",
    "        next_top_sentences.sort(key=lambda s: s.avgScore(), reverse=True)\n",
    "        prev_top_sentences = next_top_sentences[:beam_size]\n",
    "        next_top_sentences = []\n",
    "        \n",
    "\n",
    "    terminal_sentences += [sentence.toWordScore(voc) for sentence in prev_top_sentences]\n",
    "    terminal_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    n = min(len(terminal_sentences), 15)\n",
    "    return terminal_sentences[:n]\n",
    "\n",
    "\n",
    "class BeamSearchDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, voc, beam_size=10):\n",
    "        super(BeamSearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.voc = voc\n",
    "        self.beam_size = beam_size\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden[:self.decoder.n_layers]\n",
    "        \n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        sentences = beam_decode(self.decoder, decoder_hidden, encoder_outputs, self.voc, self.beam_size, max_length)\n",
    "        \n",
    "        \n",
    "        all_tokens = [torch.tensor(self.voc.word2index.get(w, 0)) for w in sentences[0][0]]\n",
    "        return all_tokens, None\n",
    "\n",
    "    def __str__(self):\n",
    "        res = f\"BeamSearchDecoder with beam size {self.beam_size}\"\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
    "    def __init__(self, input_dim, num_heads, hid_dim, num_layers, dropout_p=0.5):\n",
    "        super(Transformer, self).__init__()\n",
    "        try:\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer, TransformerDecoder, TransformerDecoderLayer\n",
    "        except BaseException as e:\n",
    "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.') from e\n",
    "        \n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(input_dim, dropout_p)\n",
    "\n",
    "        encoder_layers = TransformerEncoderLayer(input_dim, num_heads, hid_dim, dropout_p)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers)\n",
    "\n",
    "        decoder_layers = TransformerDecoderLayer(input_dim, num_heads, hid_dim, dropout_p)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layers, num_layers)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        # self.decoder = nn.Linear(input_dim, num_tokens)\n",
    "\n",
    "        # self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    # def init_weights(self):\n",
    "    #     initrange = 0.1\n",
    "    #     nn.init.uniform_(self.embedding.weight, -initrange, initrange)\n",
    "    #     nn.init.zeros_(self.decoder.bias)\n",
    "    #     nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, src, trg, has_mask=True):\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "\n",
    "        src = self.embedding(src) * math.sqrt(self.input_dim)\n",
    "        trg = self.embedding(trg) * math.sqrt(self.input_dim)\n",
    "        src = self.pos_encoder(src)\n",
    "        trg = self.pos_encoder(trg)\n",
    "        \n",
    "        encoder_output = self.transformer_encoder(src, self.src_mask)\n",
    "\n",
    "        decoder_output = self.transformer_decoder(trg, encoder_output)\n",
    "        # decoder_output = self.decoder(decoder_output)\n",
    "\n",
    "        return F.log_softmax(decoder_output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (119) must match the existing size (120) at non-singleton dimension 1.  Target sizes: [100, 119].  Tensor sizes: [100, 120]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\DSAI-AIT-2022\\Course\\Recent Trends in Machine Learning\\Labolatory\\10-LSTMs\\st123012-10-LSTMS.ipynb Cell 80\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Recent%20Trends%20in%20Machine%20Learning/Labolatory/10-LSTMs/st123012-10-LSTMS.ipynb#Y142sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m hidden_size \u001b[39m=\u001b[39m \u001b[39m256\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Recent%20Trends%20in%20Machine%20Learning/Labolatory/10-LSTMs/st123012-10-LSTMS.ipynb#Y142sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m embedding \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mEmbedding(voc\u001b[39m.\u001b[39mnum_words, hidden_size)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Recent%20Trends%20in%20Machine%20Learning/Labolatory/10-LSTMs/st123012-10-LSTMS.ipynb#Y142sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m model \u001b[39m=\u001b[39m Transformer(input_dim \u001b[39m=\u001b[39;49m voc\u001b[39m.\u001b[39;49mnum_words, num_heads\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, hid_dim\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m, num_layers\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, dropout_p\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Recent%20Trends%20in%20Machine%20Learning/Labolatory/10-LSTMs/st123012-10-LSTMS.ipynb#Y142sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m learning_rate \u001b[39m=\u001b[39m \u001b[39m0.0001\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Recent%20Trends%20in%20Machine%20Learning/Labolatory/10-LSTMs/st123012-10-LSTMS.ipynb#Y142sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate)\n",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\DSAI-AIT-2022\\Course\\Recent Trends in Machine Learning\\Labolatory\\10-LSTMs\\st123012-10-LSTMS.ipynb Cell 80\u001b[0m in \u001b[0;36mTransformer.__init__\u001b[1;34m(self, input_dim, num_heads, hid_dim, num_layers, dropout_p)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Recent%20Trends%20in%20Machine%20Learning/Labolatory/10-LSTMs/st123012-10-LSTMS.ipynb#Y142sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_type \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mTransformer\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Recent%20Trends%20in%20Machine%20Learning/Labolatory/10-LSTMs/st123012-10-LSTMS.ipynb#Y142sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msrc_mask \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Recent%20Trends%20in%20Machine%20Learning/Labolatory/10-LSTMs/st123012-10-LSTMS.ipynb#Y142sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoder \u001b[39m=\u001b[39m PositionalEncoding(input_dim, dropout_p)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Recent%20Trends%20in%20Machine%20Learning/Labolatory/10-LSTMs/st123012-10-LSTMS.ipynb#Y142sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m encoder_layers \u001b[39m=\u001b[39m TransformerEncoderLayer(input_dim, num_heads, hid_dim, dropout_p)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Recent%20Trends%20in%20Machine%20Learning/Labolatory/10-LSTMs/st123012-10-LSTMS.ipynb#Y142sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_encoder \u001b[39m=\u001b[39m TransformerEncoder(encoder_layers, num_layers)\n",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\DSAI-AIT-2022\\Course\\Recent Trends in Machine Learning\\Labolatory\\10-LSTMs\\st123012-10-LSTMS.ipynb Cell 80\u001b[0m in \u001b[0;36mPositionalEncoding.__init__\u001b[1;34m(self, dim_model, dropout_p, max_len)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Recent%20Trends%20in%20Machine%20Learning/Labolatory/10-LSTMs/st123012-10-LSTMS.ipynb#Y142sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m pos_encoding[:, \u001b[39m0\u001b[39m::\u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msin(positions_list \u001b[39m*\u001b[39m division_term)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Recent%20Trends%20in%20Machine%20Learning/Labolatory/10-LSTMs/st123012-10-LSTMS.ipynb#Y142sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Recent%20Trends%20in%20Machine%20Learning/Labolatory/10-LSTMs/st123012-10-LSTMS.ipynb#Y142sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m pos_encoding[:, \u001b[39m1\u001b[39m::\u001b[39m2\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcos(positions_list \u001b[39m*\u001b[39m division_term)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Recent%20Trends%20in%20Machine%20Learning/Labolatory/10-LSTMs/st123012-10-LSTMS.ipynb#Y142sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Saving buffer (same as parameter without gradients needed)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Course/Recent%20Trends%20in%20Machine%20Learning/Labolatory/10-LSTMs/st123012-10-LSTMS.ipynb#Y142sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m pos_encoding \u001b[39m=\u001b[39m pos_encoding\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The expanded size of the tensor (119) must match the existing size (120) at non-singleton dimension 1.  Target sizes: [100, 119].  Tensor sizes: [100, 120]"
     ]
    }
   ],
   "source": [
    "save_dir = 'content/'\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 2000\n",
    "print_every = 100\n",
    "save_every = 2000\n",
    "loadFilename = None\n",
    "corpus_name = \"ChatTransformer\"\n",
    "batch_size  = 256 \n",
    "hidden_size = 256\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "model = Transformer(input_dim = voc.num_words, num_heads=2, hid_dim=256, num_layers=3, dropout_p=0.1).to(device)\n",
    "learning_rate = 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = optim.Adam(model.transformer_encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(model.transformer_decoder.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def trainIters_transformer(model, voc, pairs, optimizer, embedding, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    losslist = []\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        \n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "        \n",
    "        output = model(input_variable, target_variable)\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg    = [batch size, trg len]\n",
    "        print(output.shape)\n",
    "        print(target_variable.shape)\n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.reshape(-1, output_dim)\n",
    "        target_variable = target_variable[:,:].reshape(-1)\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg    = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, target_variable)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        print_loss += loss\n",
    "\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "            losslist.append(print_loss_avg)\n",
    "        \n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir,'{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': model.transformer_encoder.state_dict(),\n",
    "                'de': model.transformer_decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n",
    "    return losslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Training!\")\n",
    "lossvalues = trainIters_transformer(model, voc, pairs, optimizer,\n",
    "           embedding, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEGCAYAAAB7DNKzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAnxElEQVR4nO3dd3iUZd728e8vCQSB0HtHQKRIDUgvggiItEUULIjuKiJV3X3c1X3W5XGLZVGaAoKKa8FKERBpUkIP0nuXTgDpUgLX+8cMu3kxiYFk5p5Mzs9xzJGZe+7MnAyZnJm7XJc55xAREUlJhNcBREQktKkoREQkVSoKERFJlYpCRERSpaIQEZFURXkdIBAKFSrkypUr53UMEZFMY9WqVcecc4WTuy8si6JcuXLEx8d7HUNEJNMws70p3efppicza2tmW81sh5m9kMz90Wb2mf/+5WZWzoOYIiJZmmdFYWaRwCigHVAV6GFmVa9b7QngJ+dcReBN4NXgphQRES8/UdQHdjjndjnnLgETgU7XrdMJmOC//iXQyswsiBlFRLI8L4uiJLAvye39/mXJruOcSwROAQWTezAze9LM4s0sPiEhIQBxRUSyprA5PNY5N9Y5F+uciy1cONkd9yIichO8LIoDQOkkt0v5lyW7jplFAXmB40FJJyIigLdFsRKoZGblzSw78CAw9bp1pgK9/Ne7AfOchrsVEQkqz4rCv8+hH/AdsBn43Dm30cyGmFlH/2rjgYJmtgN4FvjFIbQZafjc7azffyqQTyEikulYOP6BHhsb6270hLuT5y/Rbtgijp29yO/vqcxvm9xKRIQOsBKRrMHMVjnnYpO7L2x2ZqdXvpzZ+XZgU+66vQh/n7GFXu+v4OjpC17HEhHxnIoiiXw5szP64br8vcsdrNxzgrbDFjFvyxGvY4mIeEpFcR0zo+edZfimXxOKxETz+AfxvDx1IxcuX/E6moiIJ1QUKahUNIbJzzSmd+NyfLBkD51HLWb7kTNexxIRCToVRSpyZIvkL/dV4/3H6pFw5iIdRsTx0bK9hOMBACIiKVFRpEHL24vw7aCm1C9fgJcmb+Cpf6/ip3OXvI4lIhIUKoo0KhKTgwm96/PSvVX4futR2g5byJKdx7yOJSIScCqKGxARYfy26a1M6tuYXNmjeGjccl6buYXLV656HU1EJGBUFDehesm8fNO/Cd3rlubt+TvpNnope4+f8zqWiEhAqChuUq7oKF7tVoNRPeuwO+Es9w6PY9Lq/V7HEhHJcCqKdLq3RnG+HdSMKsVjGPzZWnq/v0LjRYlIWFFRZICS+W7h09814I/tbueHH09y38g4fjshng0HVBgikvlpUMAMdubCZT5YvId3F+3i9IVE2lQtyqDWt1G1RB5P8oiIpEVqgwKqKALk1M+XeX/xbsbH7ebMhUTaVS/GoNa3UblYjKe5RESSo6Lw0Knzlxkft4v3Fu/h3KVE2t9RnEGtKlGpqApDREKHiiIEnDx/iXGLdvP+4t2cv3yF+2qUYECrSlQsktvraCIiKopQcuLcJd5dtIsJS/Zw4fIVOtUqyYBWlShfKJfX0UQkC1NRhKDjZy8yduEuPly6l4uJV+hSuxQDWlWkbEEVhogEn4oihB07e5ExC3by4dK9JF51dK1dkv53VaJMwZxeRxORLERFkQkcPXOB0fN38fHyvVy56uhWtxTPtKxI6QIqDBEJPBVFJnLk9AXemb+TT1b8iHOO+2NL80zLipTMd4vX0UQkjKkoMqHDpy7w9vwdTFyxD4fjgXq+wiieV4UhIhlPRZGJHTz5M6O+38Hn8fswjB71S9O3ZUWK5snhdTQRCSMqijCw/6fzjPp+B1/E7yciwuhZvwx9W1SgiApDRDKAiiKM7DtxnpHzdvDlD/uJijAeblCWPs0rUDgm2utoIpKJqSjC0N7j5xgxbweTVh8gW6TxSIOyPNW8AoVyqzBE5MapKMLYnmPnGD5vO5NXHyA6KpLHGpejT7MK5M2ZzetoIpKJqCiygJ0JZxk+dztT1x4kJjqKPi0q0LtReW7JHul1NBHJBFQUWcjmQ6d547utzN1ylCIx0fRvVYkH65UmW6TmqBKRlKVWFPrtEWaqFM/D+Mfq8UWfhpQtmJM/T95A66ELmLLmAFevht8fBSISeCqKMFWvXAE+f6oh7z9Wj1uyRTJw4hruHRHH91uOEo6fIkUkcFQUYczMaHl7EWYMaMqwB2tx7mIivT9YyQNjlhG/54TX8UQkk/CkKMysgJnNNrPt/q/5U1jvipmt8V+mBjtnuIiIMDrVKsmcZ5vzf52rs/v4ObqNXsoTH6xk86HTXscTkRDnyc5sM3sNOOGc+6eZvQDkd879TzLrnXXO3fAUcFl5Z3ZanL+UyPuL9zB6wU7OXkykc62SDG59m4Y2F8nCQu6oJzPbCrRwzh0ys+LAfOdc5WTWU1EE0Mnzlxi9YBfvL97NVefoUb8M/e6qSJEYDQsiktWEYlGcdM7l81834Kdrt69bLxFYAyQC/3TOTU7lMZ8EngQoU6ZM3b1792Z47nB1+NQFhs/bzmcr95EjKoI+zSvw26a36hwMkSzEk6IwszlAsWTuehGYkLQYzOwn59wv9lOYWUnn3AEzuxWYB7Ryzu38tefWJ4qbsyvhLK/N3MrMjYcpmiea59tUpmudUkRGmNfRRCTAQvETRZo2PV33PR8A05xzX/7a46so0mfF7hP8bfom1u4/RdXieXjx3io0rljI61giEkCheMLdVKCX/3ovYMr1K5hZfjOL9l8vBDQGNgUtYRZWv3wBJvVtzPAetTn182UeGrec3u+vYNuRM15HExEPePWJoiDwOVAG2At0d86dMLNYoI9z7rdm1ggYA1zFV2hvOefGp+Xx9Yki41y4fIUPl+5hxLwdnLuYyAP1yjD47kra4S0SZkJu01OgqSgy3k/nLjFs7nY+WraXaO3wFgk7objpSTKZ/Lmy83LHasx+tjlNKxXmX7O30eKN7/kifh9XNIaUSFhTUcgNKV8oF6MfqcsXfRpSLO8t/P7Lddw3Io7FO455HU1EAkRFITelXrkCTO7biBE9anP6wn93eG/XDm+RsKOikJtmZtxXswRznm3On9rfTvzen2g7bBEvT93IyfOXvI4nIhlERSHpliNbJE82q8CC37ekR/3SfLh0Dy3emM+/l+4h8cpVr+OJSDqpKCTDFMiVnVc638H0AU2pUiwPf56ykXuHa/+FSGanopAMV6V4Hj753Z2MfrgO5y8n8tC45Tz173h+PH7e62gichNUFBIQZkbb6sWZPbg5v7+nMou2H6P10AW8OnMLZy8meh1PRG6AikICKke2SJ5pWZHvn29Bh5rFeWf+Tlq+MZ8vV+3XHN4imYSKQoKiaJ4cDO1ei0l9G1Ey3y08/8Vaury9mFV7f/I6moj8ChWFBFXtMvn5+ulGvPlATQ6fvsBv3lnCoImrOXTqZ6+jiUgKVBQSdBERRpfapZj3XAv6tazIjA2HueuNBQyfu50Ll694HU9ErqOiEM/kio7i+XsqM/fZ5rSoXJihs7fReugCZm44TDgOVimSWakoxHOlC+TknYfr8snv7iRX9ij6fLSKR99bwc6Es15HExFUFBJCGlUoxPQBTfjLfVVZs+8kbd9ayD9mbNbhtCIeU1FISImKjKB34/J8/3wLutQuyZiFu7jrjflMXn1Am6NEPKKikJBUKHc0r3WryaS+jSiWNweDPltD9zFL2XjwlNfRRLIcFYWEtNpl8jO5b2P+2fUOdiac474Rcfx58gaNTisSRCoKCXkREcaD9cvw/XMteKRBWT5evpeWb8znk+U/anY9kSBQUUimkTdnNv7aqTrT+jelUpEY/jRpPZ1H6exukUBTUUimU7VEHj57qgHDHqzF0TO+s7uf/2ItCWcueh1NJCypKCRTMjM61SrJ3Oda0Kd5BaasOcBdb8znvbjdmixJJIOpKCRTyx0dxQvtbmfmoGbULpufIdM20WnUYtbsO+l1NJGwoaKQsFChcG4m9K7HqJ51SDhzkS5vL+Z/p2zg9IXLXkcTyfRUFBI2zIx7axRn7nPN6dWwHB8t20urfy1g6tqDOllPJB1UFBJ2YnJk4+WO1ZjyTBOK583BgE9X8+h7K9hz7JzX0UQyJRWFhK07SuVlUt/G/LVjNVb/eJI2by1k2JztXEzUUOYiN0JFIWEtMsLo1agcc59rTpuqRXlzzjbavbWIJTuOeR1NJNNQUUiWUDRPDkb2rMOEx+uTeNXRc9xyBn+2hmNnde6FyK9RUUiW0vy2wswa3Iz+d1Vk2rqD3OUfCuSqhgIRSZGKQrKcHNkiea5NZb4d2IyqJfLwp0nr6TZ6CZsOnvY6mkhIUlFIllWxSG4+/V0Dhnavyd7j57lvZBx/n7GZ85c0UZJIUp4UhZndb2YbzeyqmcWmsl5bM9tqZjvM7IVgZpSswczoWqcUc59rTvfYUoxduIs2by5kwbYEr6OJhAyvPlFsALoCC1NawcwigVFAO6Aq0MPMqgYnnmQ1+XJm5x9da/D5Uw2Jjoqg13srGDhxtXZ2i+BRUTjnNjvntv7KavWBHc65Xc65S8BEoFPg00lWVr98AWYMbMqg1pX4dv1hWv1rAZ/H79OZ3ZKlhfI+ipLAviS39/uXJcvMnjSzeDOLT0jQZgO5edFRkQxqfRszBjbhtqK5+cOX6+j57nJ2JZz1OpqIJwJWFGY2x8w2JHMJyKcC59xY51yscy62cOHCgXgKyWIqFonhsycb8vcud7Dh4CnaDlvEyHnbuZSoYcwla4kK1AM751qn8yEOAKWT3C7lXyYSNBERRs87y9C6ShH++s0m3pi1jalrD/KPrjWoWza/1/FEgiKUNz2tBCqZWXkzyw48CEz1OJNkUUXy5GDUQ3UY92gsZy8k0m30Ev48WcOYS9bg1eGxXcxsP9AQmG5m3/mXlzCzGQDOuUSgH/AdsBn43Dm30Yu8Ite0rlqUWc8257FG5fho+V7uHrqAmRsOex1LJKAsHI/miI2NdfHx8V7HkDC3dt9JXvh6PZsPnaZN1aIM6VSdYnlzeB1L5KaY2SrnXLLntYXypieRkFazdD6m9mvMC+1uZ+H2BFoPXcCHS/do3CgJOyoKkXTIFhlBn+YVmDWoObXL5ON/p2zk/jFL2XH0jNfRRDKMikIkA5QpmJMPH6/Pv+6vyc6Es7QfFsewOTqUVsLDDReFmeU3sxqBCCOSmZkZv6lbijnPNuee6sV4c842OoxYxA8//uR1NJF0SVNRmNl8M8tjZgWAH4B3zWxoYKOJZE6FckczokdtxveK5cyFRH7zzhL++s1Gzl3UqLSSOaX1E0Ve59xpfAP5feicuxNI7wl1ImGtVZWizBrcjEcalOWDJXto8+ZC5m896nUskRuW1qKIMrPiQHdgWgDziISVmBzZGNKpOl881ZAc2SJ47P2VDJq4mhPnLnkdTSTN0loUQ/Cd+LbTObfSzG4Ftgculkh4iS3nG5V2QKtKTF9/iNZDFzB59QGNSiuZgk64EwmyrYfP8D9frWPNvpO0qFyYVzpXp1T+nF7Hkiwu3SfcmdltZjbXzDb4b9cws5cyMqRIVlG5WAxfPd2Iv9xXlRW7T9DmzYW8v3g3V3SinoSotG56ehf4I3AZwDm3Dt8gfSJyEyIjjN6Ny/PdoGbElivAX7/ZRLfRS9h2RCfqSehJa1HkdM6tuG6ZjvUTSafSBXIyoXc93nqgFnuOnaPD8DhGfb+DxCs6UU9CR1qL4piZVQAcgJl1Aw4FLJVIFmJmdK5dkjnPNufuakV5/butdHl7CVsOn/Y6mgiQ9qJ4BhgD3G5mB4BBwNOBCiWSFRXMHc2onnV4+6E6HDz5M/eNiGPE3O1c1qcL8dgNHfVkZrmACOdcSG9I1VFPktkdP3uRv0zdyLR1h6heMg+vd6tJleJ5vI4lYSwjjnoaaGZ5gPPAm2b2g5m1yciQIvJfBXNHM7JnHd55qA6HT12g48g4huvThXgkrZueHvcP4dEGKAg8AvwzYKlEBIB2dxRn1uDmtKtenKGzt9Fp5GI2HdS+CwmutBaF+b+2xzfW08Yky0QkgArkys7wHrUZ/XBdjp65SMeRcbw1Z5uGMJegSWtRrDKzWfiK4jsziwH0UyoSRG2rF2P24GbcW6M4b83ZTqdRi9l48JTXsSQLSGtRPAG8ANRzzp0HsgG9A5ZKRJKVP1d2hj1YmzGP1CXhzEU6jVzMm7P16UICK61F0RDY6pw7aWYPAy8B+lNGxCP3VCvGnGebcV/NEgybu52OI+PYcEBvSQmMtBbFO8B5M6sJPAfsBD4MWCoR+VX5cmbnzQdq8e6jsRw/d4nOoxYzdNZWfbqQDJfWokh0vhMuOgEjnXOjgJjAxRKRtLq7alFmD25Gx5olGD5vB51HLdZZ3ZKh0loUZ8zsj/gOi51uZhH49lOISAjIlzM7Qx+oxdhH6nL0zAXuGxHH2/M1ZpRkjLQWxQPARXznUxwGSgGvByyViNyUNtWKMWtwc+6uWpTXZm7l/jFL2ZVw1utYksmlqSj85fAxkNfMOgAXnHPaRyESggrkys6onnUY3qM2uxLO0X74It6L281VzXchNymtQ3h0B1YA9+ObN3u5fwRZEQlBZkbHmiWYPbgZjSoUYsi0TfQct4x9J857HU0yoTQNCmhma4G7nXNH/bcLA3OcczUDnO+maFBAkf9yzvHFqv0M+WYTzjle6lCVB+uVxkyDK8h/pXtQQHwjxh5Ncvv4DXyviHjIzOgeW5qZg5pSs3Q+/vj1eh57fyWHT13wOppkEmn9ZT/TzL4zs8fM7DFgOjAjcLFEJKOVyp+Tj564kyGdqvnn6l7ApNX7uZGpBiRrSvN8FGb2G6Cx/+Yi59ykgKVKJ216EkndnmPneP6LtcTv/Yl7qhXlb13uoFDuaK9jiYdS2/R0QxMXZRYqCpFfd+WqY3zcLt6YtY3c0VH8rXN12t1R3OtY4pGb3kdhZmfM7HQylzNmdtOnfprZ/Wa20cyumlmywfzr7TGz9Wa2xsz0m18kA0VGGE82q8D0/k0ome8Wnv74BwZOXM2p85e9jiYhJtWicM7FOOfyJHOJcc6lZ17GDUBXYGEa1m3pnKuVUtOJSPpUKhrD130bMbj1bUxfd4h73lrIwm0JXseSEOLJkUvOuc3Oua1ePLeI/FK2yAgGtq7EpL6NickRxaPvreClyes5fynR62gSAkL9EFcHzDKzVWb2ZGormtmTZhZvZvEJCfprSORm3FEqL9/0b8Lvmpbn4+U/0n7YIlbtPeF1LPFYwIrCzOaY2YZkLp1u4GGaOOfqAO2AZ8ysWUorOufGOudinXOxhQsXTnd+kawqR7ZIXry3Kp/+rgGJVx33j17KazO3cDHxitfRxCNRgXpg51zrDHiMA/6vR81sElCftO3XEJF0anBrQWYOasYr0zbx9vydzNtylDcfqEWV4unZPSmZUchuejKzXP65uTGzXEAbfDvBRSRIckdH8c/f1GB8r1iOnb1Ex5FxvDN/J1c0wGCW4klRmFkXM9uPb4rV6Wb2nX95CTO7dsZ3USDOP87UCmC6c26mF3lFsrpWVYoya3Az7q5alFdnbqH7mKXsOXbO61gSJDrhTkTSzDnH1LUH+fPkDVy+4njx3io8dGcZDTAYBjJiUEAREcyMTrVK8t3gZsSWy89LkzfQSwMMhj0VhYjcsOJ5b+HDx+vzf52rs9I/wOCUNQc0wGCYUlGIyE0xMx5pUJYZA5tSsUhuBk5cQ79PV3Py/CWvo0kGU1GISLqUL5SLL/o04vf3VGbWxsPc89ZC4rYf8zqWZCAVhYikW2SE8UzLikzq25jc0VE8PH45Q77ZxIXLOkkvHKgoRCTDVC+Zl2n9m9KrYVneW7ybTiMXs+ngTQ80LSFCRSEiGeqW7JH8tVN1PuhdjxPnL9F51GLGLtzJVZ2kl2mpKEQkIFpULsJ3g5rRonJh/j5jCw+NW87Bkz97HUtugopCRAKmQK7sjHmkLq/9pgbr9p+k7VsLmbr2oNex5AapKEQkoMyM7vVK/+cw2gGfrvbNpPezZtLLLFQUIhIUZQvm4vOnGvLs3bcxbd0h2r21kKU7j3sdS9JARSEiQRMVGcGAVpX46ulGRGeLpOe4ZfxjxmbNdRHiVBQiEnS1Sudj+oAm9KhfhjELd9F51BK2HTnjdSxJgYpCRDyRM3sUf+9yB+MejeXo6Qt0GBHH+4t3a7yoEKSiEBFPta5alJmDmtGkYiH++s0men+wkmNnL3odS5JQUYiI5wrHRDO+VyxDOlVjyc7jtH1rEQu2JXgdS/xUFCISEsyMRxuWY2q/xhTIlY1e763glWmbtKM7BKgoRCSk3F4sD1P7NeHRhmUZF7ebrm8vYWfCWa9jZWkqChEJOTmyRTKkU3XefTSWgyd/psPwOCau+FE7uj2iohCRkHW3f0d3nbL5eOHr9TzzyQ+cOq8zuoNNRSEiIa1onhz8+/E7eaHd7czaeIR2wxayYvcJr2NlKSoKEQl5ERFGn+YV+OrpRmSPiuDBsUsZOmsriVeueh0tS1BRiEimUbN0PqYNaErXOqUYPm8H3ccsZd+J817HCnsqChHJVHJHR/HG/TUZ3qM224+cpf2wRUxZc8DrWGFNRSEimVLHmiWYMbAptxWLYeDENTz3+VrOXkz0OlZYUlGISKZVukBOPnuyAQNaVWLS6v10GL6I9ftPeR0r7KgoRCRTi4qM4Nm7b2Pikw25mHiVru8sZtyiXZqjOwOpKEQkLNQvX4BvBzalZeUivDJ9M09MWMlxDS6YIVQUIhI28uX0zdE9pFM1Fu88Trthi1iy45jXsTI9FYWIhJVrgwtO7tuYmBxRPDR+Oa9/t0XnXKSDikJEwlLVEnn4pn8Tutctzajvd/LA2GXs/0nnXNwMFYWIhK2c2aN4tVsNhveozdbDZ2g/bBHfrj/kdaxMx5OiMLPXzWyLma0zs0lmli+F9dqa2VYz22FmLwQ5poiEiY41SzBjQFPKF87N0x//wJ8mrefCZc1zkVZefaKYDVR3ztUAtgF/vH4FM4sERgHtgKpADzOrGtSUIhI2yhTMyRdPNeSp5rfyyfIf6Tgyjm1HzngdK1PwpCicc7Occ9dOoVwGlEpmtfrADufcLufcJWAi0ClYGUUk/GSPiuCP7aow4fH6nDh3iftGxPHx8r2a5+JXhMI+iseBb5NZXhLYl+T2fv+yZJnZk2YWb2bxCQmaa1dEUtb8tsLMGNiU+uUL8OKkDb55Ln7WPBcpCVhRmNkcM9uQzKVTknVeBBKBj9P7fM65sc65WOdcbOHChdP7cCIS5orE5GBC7/r/meei/bBFrNr7k9exQlJUoB7YOdc6tfvN7DGgA9DKJf+57wBQOsntUv5lIiIZ4to8F3eWL0D/T1fTfcxSnm9Tmaea3UpEhHkdL2R4ddRTW+APQEfnXEoHNq8EKplZeTPLDjwITA1WRhHJOmqXyc+MgU25p1pRXp25hcc+WMkxDf/xH17toxgJxACzzWyNmY0GMLMSZjYDwL+zux/wHbAZ+Nw5t9GjvCIS5vLkyMaonnV4pXN1lu06Tvthi1i687jXsUKChePe/tjYWBcfH+91DBHJpDYdPE2/T35gz/FzDGhVif53VSIyzDdFmdkq51xscveFwlFPIiIh5drwH51qleStOdt5eNxyjpy+4HUsz6goRESSkSs6iqHda/J6txqs2XeS9sMWsWBb1jz0XkUhIpICM+P+2NJM7deYQrmj6fXeCl6duYXLWWwkWhWFiMivqFQ0hsnPNKZH/dK8M38nD45dxoGTP3sdK2hUFCIiaXBL9kj+0fX/H4l29qYjXscKChWFiMgN6FizBNP6N6F0gVv43YfxDPlmE5cSw3tTlIpCROQGlSuUi6+ebsRjjcrx3uLddBu9hL3Hz3kdK2BUFCIiNyE6KpKXO1ZjzCN12XPsHB2GxzFt3UGvYwWEikJEJB3uqVaMGQObUrFobvp9spq/TNnAxcTwmhRJRSEikk6l8ufk86ca8kST8kxYupfuo5ey70T4zM+tohARyQDZIiP4c4eqjH64LrsSztFhRBxzN4fHUVEqChGRDNS2ejGmDWhCqfy38MSEeP757RYSM/kJeioKEZEMVrag76ioHvXLMHrBTnqOW87RTDxWlIpCRCQAcmSL5B9d7+DNB2qyfv8p2g9fxJIdx7yOdVNUFCIiAdSldimm9mtMvpzZeXj8ckbM3c7Vq5lregcVhYhIgFUqGsOUZxpzX80S/Gv2Nnp/sJIT5y55HSvNVBQiIkGQKzqKtx6oxd+6VGfpzuPcO3wRq/b+5HWsNFFRiIgEiZnx0J1l+bpvI6IijQfGLGV83G5CfaZRFYWISJBVL5mXaf2bctftRfi/aZt4+qMfOH3hstexUqSiEBHxQN5bsjHmkbq8dG8V5mw+wn0j4thw4JTXsZKlohAR8YiZ8dumtzLxyQZcvHyVru8s4fOV+7yO9QsqChERj8WWK8D0AU2oX64Af/hqHS98tY4Ll0NnYEEVhYhICCiYO5oJj9enX8uKTFy5j26jl4TMwIIqChGREBEZYTx/T2XG94rlx+PnuXf4IuZt8X5gQRWFiEiIaVWlKNP6N6VU/pw8/kE8Q2dt5YqHZ3OrKEREQlCZgjn5um8j7q9biuHzdvDY+ys8O5tbRSEiEqJyZIvk9ftr8upv7mD57hN0GL6I1T8G/2xuFYWISIh7oF4Zvn66ERERRvcxS/n30j1BPZtbRSEikgn4zuZuQpOKhfjzlI0M/mwN5y8lBuW5VRQiIplEvpzZGd+rHs/dfRtT1h6k86jF7Eo4G/DnVVGIiGQiERFG/1aVmNC7PglnLtJx5GJmbjgU2OcM6KOLiEhANLutMNMGNKVCkdz0+egH/j5jc8Dm5vakKMzsdTPbYmbrzGySmeVLYb09ZrbezNaYWXyQY4qIhLSS+W7h86ca8HCDMoxduIue45Zz7mLG77fw6hPFbKC6c64GsA34YyrrtnTO1XLOxQYnmohI5hEdFckrnX1zc5cvmIuc2SMz/DmiMvwR08A5NyvJzWVANy9yiIiEiy61S9GldqmAPHYo7KN4HPg2hfscMMvMVpnZk6k9iJk9aWbxZhafkJCQ4SFFRLKqgH2iMLM5QLFk7nrROTfFv86LQCLwcQoP08Q5d8DMigCzzWyLc25hcis658YCYwFiY2NDe15BEZFMJGBF4Zxrndr9ZvYY0AFo5VI4xdA5d8D/9aiZTQLqA8kWhYiIBIZXRz21Bf4AdHTOJTvgupnlMrOYa9eBNsCG4KUUERHwbh/FSCAG3+akNWY2GsDMSpjZDP86RYE4M1sLrACmO+dmehNXRCTr8uqop4opLD8ItPdf3wXUDGYuERH5pVA46klEREKYikJERFJlwRzTPFjMLAHYe5PfXgg4loFxMprypY/ypY/ypU8o5yvrnCuc3B1hWRTpYWbxoTxciPKlj/Klj/KlT6jnS4k2PYmISKpUFCIikioVxS+N9TrAr1C+9FG+9FG+9An1fMnSPgoREUmVPlGIiEiqVBQiIpKqLFsUZtbWzLaa2Q4zeyGZ+6PN7DP//cvNrFwQs5U2s+/NbJOZbTSzgcms08LMTvnHylpjZv8brHz+5091mlrzGe5//daZWZ0gZquc5HVZY2anzWzQdesE9fUzs/fM7KiZbUiyrICZzTaz7f6v+VP43l7+dbabWa8g5guZKYtTyPeymR1I8n/YPoXvTfW9HsB8nyXJtsfM1qTwvaE/5bNzLstdgEhgJ3ArkB1YC1S9bp2+wGj/9QeBz4KYrzhQx389Bt90sdfnawFM8/A13AMUSuX+9vgmpDKgAbDcw//rw/hOJvLs9QOaAXWADUmWvQa84L/+AvBqMt9XANjl/5rffz1/kPK1AaL8119NLl9afhYCmO9l4Pk0/P+n+l4PVL7r7v8X8L9evX7pvWTVTxT1gR3OuV3OuUvARKDTdet0Aib4r38JtDIzC0Y459wh59wP/utngM1AyWA8dwbqBHzofJYB+cysuAc5WgE7nXM3e6Z+hnC+CbdOXLc46c/YBKBzMt96DzDbOXfCOfcTvvnm2wYjn3NulnMu0X9zGRCYeTbTIIXXLy3S8l5Pt9Ty+X9vdAc+zejnDZasWhQlgX1Jbu/nl7+I/7OO/81yCigYlHRJ+Dd51QaWJ3N3QzNba2bfmlm14Cb71Wlq0/IaB8ODpPwG9fL1AyjqnDvkv34Y39D61wuV1zFDpiwOgH7+TWPvpbDpLhRev6bAEefc9hTu9/L1S5OsWhSZgpnlBr4CBjnnTl939w/4NqfUBEYAk4Mcr4lzrg7QDnjGzJoF+fl/lZllBzoCXyRzt9ev3//H+bZBhOSx6pa2KYu9+Fl4B6gA1AIO4du8E4p6kPqniZB/L2XVojgAlE5yu5R/WbLrmFkUkBc4HpR0vufMhq8kPnbOfX39/c650865s/7rM4BsZlYoWPlckmlqgWvT1CaVltc40NoBPzjnjlx/h9evn9+Ra5vj/F+PJrOOp6+j/XfK4of8ZfYLafhZCAjn3BHn3BXn3FXg3RSe1+vXLwroCnyW0jpevX43IqsWxUqgkpmV9//V+SAw9bp1pgLXjjDpBsxL6Y2S0fzbNMcDm51zQ1NYp9i1fSZmVh/f/2VQiszSNk3tVOBR/9FPDYBTSTazBEuKf8l5+folkfRnrBcwJZl1vgPamFl+/6aVNv5lAWchPmXxdfu8uqTwvGl5rwdSa2CLc25/cnd6+frdEK/3pnt1wXdUzjZ8R0S86F82BN+bAiAHvk0WO/BNxXprELM1wbcZYh2wxn9pD/QB+vjX6QdsxHcUxzKgURDz3ep/3rX+DNdev6T5DBjlf33XA7FB/v/Nhe8Xf94kyzx7/fAV1iHgMr7t5E/g2+c1F9gOzAEK+NeNBcYl+d7H/T+HO4DeQcy3A9/2/Ws/g9eOAiwBzEjtZyFI+f7t/9lah++Xf/Hr8/lv/+K9Hox8/uUfXPuZS7Ju0F+/9F40hIeIiKQqq256EhGRNFJRiIhIqlQUIiKSKhWFiIikSkUhIiKpUlGIhAD/aLbTvM4hkhwVhYiIpEpFIXIDzOxhM1vhnztgjJlFmtlZM3vTfHOHzDWzwv51a5nZsiTzOeT3L69oZnP8AxL+YGYV/A+f28y+9M8B8XGSM8f/ab65SdaZ2Rse/dMlC1NRiKSRmVUBHgAaO+dqAVeAh/CdBR7vnKsGLAD+4v+WD4H/cc7VwHcG8bXlHwOjnG9Awkb4zugF3yjBg4Cq+M7YbWxmBfENT1HN/zivBPLfKJIcFYVI2rUC6gIr/bOVtcL3C/0q/x307SOgiZnlBfI55xb4l08AmvnH9SnpnJsE4Jy74P47jtIK59x+5xvkbg1QDt/w9heA8WbWFUh2zCWRQFJRiKSdAROcc7X8l8rOuZeTWe9mx8W5mOT6FXyzyyXiG030S3yjuM68yccWuWkqCpG0mwt0M7Mi8J85r8viex9186/TE4hzzp0CfjKzpv7ljwALnG/Gwv1m1tn/GNFmljOlJ/TPSZLX+YZCHwzUDMC/SyRVUV4HEMksnHObzOwlfLORReAbKfQZ4BxQ33/fUXz7McA3dPhofxHsAnr7lz8CjDGzIf7HuD+Vp40BpphZDnyfaJ7N4H+WyK/S6LEi6WRmZ51zub3OIRIo2vQkIiKp0icKERFJlT5RiIhIqlQUIiKSKhWFiIikSkUhIiKpUlGIiEiq/h+CbJbqQT34CwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([loss.detach().numpy() for loss in lossvalues])\n",
    "plt.ylabel('losses')\n",
    "plt.xlabel('epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLEU score calculation\n",
    "\n",
    "Let's calculate the BLEU score with the beam search decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_decode(decoder, decoder_hidden, encoder_outputs, voc, beam_size, max_length=MAX_LENGTH):\n",
    "    terminal_sentences, prev_top_sentences, next_top_sentences = [], [], []\n",
    "    prev_top_sentences.append(Sentence(decoder_hidden))\n",
    "    for i in range(max_length):\n",
    "        \n",
    "        for sentence in prev_top_sentences:\n",
    "            decoder_input = torch.LongTensor([[sentence.last_idx]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "\n",
    "            decoder_hidden = sentence.decoder_hidden\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(beam_size)\n",
    "            term, top = sentence.addTopk(topi, topv, decoder_hidden, beam_size, voc)\n",
    "            terminal_sentences.extend(term)\n",
    "            next_top_sentences.extend(top)\n",
    "           \n",
    "        \n",
    "        next_top_sentences.sort(key=lambda s: s.avgScore(), reverse=True)\n",
    "        prev_top_sentences = next_top_sentences[:beam_size]\n",
    "        next_top_sentences = []\n",
    "        \n",
    "\n",
    "    terminal_sentences += [sentence.toWordScore(voc) for sentence in prev_top_sentences]\n",
    "    terminal_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    n = min(len(terminal_sentences), 15)\n",
    "    return terminal_sentences[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, voc, beam_size=10):\n",
    "        super(BeamSearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.voc = voc\n",
    "        self.beam_size = beam_size\n",
    "        self.src_mask = None\n",
    "        \n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(input_seq):\n",
    "            mask = self._generate_square_subsequent_mask(len(input_seq)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        # encoder_outputs, encoder_hidden = self.encoder(input_seq, self.src_mask)\n",
    "\n",
    "        encoder_outputs = self.encoder(input_seq)\n",
    "        \n",
    "        decoder_hidden = encoder_outputs[:self.decoder.n_layers]\n",
    "        \n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        sentences = beam_decode(self.decoder, decoder_hidden, encoder_outputs, self.voc, self.beam_size, max_length)\n",
    "        \n",
    "        \n",
    "        all_tokens = [torch.tensor(self.voc.word2index.get(w, 0)) for w in sentences[0][0]]\n",
    "        return all_tokens, None\n",
    "    \n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def __str__(self):\n",
    "        res = f\"BeamSearchDecoder with beam size {self.beam_size}\"\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "  \n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    \n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    \n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    \n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    \n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    \n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "# Set dropout layers to eval mode\n",
    "\n",
    "# model_name.eval()\n",
    "model.transformer_encoder.eval()\n",
    "model.transformer_decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "\n",
    "############################################################################\n",
    "# Difference between greedy search and beam search is here\n",
    "\n",
    "# greedy search\n",
    "# searcher = GreedySearchDecoder(model_name.transformer_encoder, model_name.transformer_decoder)\n",
    "\n",
    "# beam search\n",
    "searcher = BeamSearchDecoder(model.transformer_encoder, model.transformer_decoder, voc, 3)\n",
    "############################################################################\n",
    "gram1_bleu_score = []\n",
    "gram2_bleu_score = []\n",
    "\n",
    "for i in range(0,len(testpairs),1):\n",
    "    input_sentence = testpairs[i][0]\n",
    "    reference = testpairs[i][1:]\n",
    "    print(f'input_sentence : {input_sentence}')\n",
    "    # print(f'reference : {reference}')\n",
    "    templist = []\n",
    "    for k in range(len(reference)):\n",
    "        if(reference[k]!=''):\n",
    "            temp = reference[k].split(' ')\n",
    "            templist.append(temp)\n",
    "  \n",
    "    input_sentence = normalizeString(input_sentence)\n",
    "    output_words = evaluate(searcher, voc, input_sentence)\n",
    "    output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "    chencherry = SmoothingFunction()\n",
    "    score1 = sentence_bleu(templist,output_words,weights=(1, 0, 0, 0) ,smoothing_function=chencherry.method1)\n",
    "    score2 = sentence_bleu(templist,output_words,weights=(0.5, 0.5, 0, 0),smoothing_function=chencherry.method1) \n",
    "    gram1_bleu_score.append(score1)\n",
    "    gram2_bleu_score.append(score2)\n",
    "    if i%1000 == 0:\n",
    "        print(i,sum(gram1_bleu_score)/len(gram1_bleu_score),sum(gram2_bleu_score)/len(gram2_bleu_score))\n",
    "        \n",
    "print(\"Total Bleu Score for 1 grams on testing pairs: \", sum(gram1_bleu_score)/len(gram1_bleu_score))  \n",
    "print(\"Total Bleu Score for 2 grams on testing pairs: \", sum(gram2_bleu_score)/len(gram2_bleu_score))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
