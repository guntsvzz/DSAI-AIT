{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. \n",
    "Develop your own GAN to model data generated as follows:\n",
    "\n",
    "$$\\begin{eqnarray} \\theta & \\sim & {\\cal U}(0,2\\pi) \\\\\n",
    "                      r      & \\sim & {\\cal N}(0, 1) \\\\\n",
    "                      \\mathbf{x} & \\leftarrow & \\begin{cases} \\begin{bmatrix} (10+r)\\cos\\theta \\\\ (10+r)\\sin\\theta + 10\\end{bmatrix} & \\frac{1}{2}\\pi \\le \\theta \\le \\frac{3}{2}\\pi \\\\ \\begin{bmatrix} (10+r)\\cos\\theta \\\\ (10+r)\\sin\\theta - 10\\end{bmatrix} & \\mathrm{otherwise} \\end{cases} \\end{eqnarray} $$\n",
    "\n",
    "You should create a PyTorch DataSet that generates the 2D data in the `__init__()` method, outputs a sample in the `__getitem__()` method, and returns the dataset size in the `__len__()` method. \n",
    "\n",
    "Use the vanilla GAN approach above with an appropriate structure for the generator. \n",
    "\n",
    "Can your GAN generate a convincing facsimile of a set of samples from the actual distribution?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On SNAKE data,\n",
    "\n",
    "### The optimizers used are:\n",
    "\n",
    "        d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))<br>\n",
    "        g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "### loss function:\n",
    "\n",
    "        loss = nn.BCELoss()\n",
    "\n",
    "NOTE: Tanh() activation function in Generator is removed due to the data range of SNAKE dataset. \n",
    "\n",
    "Tanh limits the range to be in between [-1,1] which is not suitable for the dataset of SNAKE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import errno\n",
    "import torchvision.utils as vutils\n",
    "from tensorboardX import SummaryWriter\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "\n",
    "'''\n",
    "    TensorBoard Data will be stored in './runs' path\n",
    "'''\n",
    "\n",
    "\n",
    "class Logger:\n",
    "\n",
    "    def __init__(self, model_name, data_name):\n",
    "        self.model_name = model_name\n",
    "        self.data_name = data_name\n",
    "\n",
    "        self.comment = '{}_{}'.format(model_name, data_name)\n",
    "        self.data_subdir = '{}/{}'.format(model_name, data_name)\n",
    "\n",
    "        # TensorBoard\n",
    "        self.writer = SummaryWriter(comment=self.comment)\n",
    "\n",
    "    def log(self, d_error, g_error, epoch, n_batch, num_batches):\n",
    "\n",
    "        # var_class = torch.autograd.variable.Variable\n",
    "        if isinstance(d_error, torch.autograd.Variable):\n",
    "            d_error = d_error.data.cpu().numpy()\n",
    "        if isinstance(g_error, torch.autograd.Variable):\n",
    "            g_error = g_error.data.cpu().numpy()\n",
    "\n",
    "        step = Logger._step(epoch, n_batch, num_batches)\n",
    "        self.writer.add_scalar(\n",
    "            '{}/D_error'.format(self.comment), d_error, step)\n",
    "        self.writer.add_scalar(\n",
    "            '{}/G_error'.format(self.comment), g_error, step)\n",
    "\n",
    "    def log_images(self, images, num_images, epoch, n_batch, num_batches, format='NCHW', normalize=True):\n",
    "        '''\n",
    "        input images are expected in format (NCHW)\n",
    "        '''\n",
    "        if type(images) == np.ndarray:\n",
    "            images = torch.from_numpy(images)\n",
    "        \n",
    "        if format=='NHWC':\n",
    "            images = images.transpose(1,3)\n",
    "        \n",
    "\n",
    "        step = Logger._step(epoch, n_batch, num_batches)\n",
    "        img_name = '{}/images{}'.format(self.comment, '')\n",
    "\n",
    "        # Make horizontal grid from image tensor\n",
    "        horizontal_grid = vutils.make_grid(\n",
    "            images, normalize=normalize, scale_each=True)\n",
    "        # Make vertical grid from image tensor\n",
    "        nrows = int(np.sqrt(num_images))\n",
    "        grid = vutils.make_grid(\n",
    "            images, nrow=nrows, normalize=True, scale_each=True)\n",
    "\n",
    "        # Add horizontal images to tensorboard\n",
    "        self.writer.add_image(img_name, horizontal_grid, step)\n",
    "\n",
    "        # Save plots\n",
    "        self.save_torch_images(horizontal_grid, grid, epoch, n_batch)\n",
    "\n",
    "    def save_torch_images(self, horizontal_grid, grid, epoch, n_batch, plot_horizontal=True):\n",
    "        out_dir = './data/images/{}'.format(self.data_subdir)\n",
    "        Logger._make_dir(out_dir)\n",
    "\n",
    "        # Plot and save horizontal\n",
    "        fig = plt.figure(figsize=(16, 16))\n",
    "        plt.imshow(np.moveaxis(horizontal_grid.numpy(), 0, -1))\n",
    "        plt.axis('off')\n",
    "        if plot_horizontal:\n",
    "            display.display(plt.gcf()) #get current axis\n",
    "        self._save_images(fig, epoch, n_batch, 'hori')\n",
    "        plt.close()\n",
    "\n",
    "        # Save squared\n",
    "        fig = plt.figure()\n",
    "        plt.imshow(np.moveaxis(grid.numpy(), 0, -1))\n",
    "        plt.axis('off')\n",
    "        self._save_images(fig, epoch, n_batch)\n",
    "        plt.close()\n",
    "\n",
    "    def _save_images(self, fig, epoch, n_batch, comment=''):\n",
    "        out_dir = './data/images/{}'.format(self.data_subdir)\n",
    "        Logger._make_dir(out_dir)\n",
    "        fig.savefig('{}/{}_epoch_{}_batch_{}.png'.format(out_dir,\n",
    "                                                         comment, epoch, n_batch))\n",
    "\n",
    "    def display_status(self, epoch, num_epochs, n_batch, num_batches, d_error, g_error, d_pred_real, d_pred_fake):\n",
    "        \n",
    "        # var_class = torch.autograd.variable.Variable\n",
    "        if isinstance(d_error, torch.autograd.Variable):\n",
    "            d_error = d_error.data.cpu().numpy()\n",
    "        if isinstance(g_error, torch.autograd.Variable):\n",
    "            g_error = g_error.data.cpu().numpy()\n",
    "        if isinstance(d_pred_real, torch.autograd.Variable):\n",
    "            d_pred_real = d_pred_real.data\n",
    "        if isinstance(d_pred_fake, torch.autograd.Variable):\n",
    "            d_pred_fake = d_pred_fake.data\n",
    "        \n",
    "        \n",
    "        print('Epoch: [{}/{}], Batch Num: [{}/{}]'.format(\n",
    "            epoch,num_epochs, n_batch, num_batches)\n",
    "             )\n",
    "        print('Discriminator Loss: {:.4f}, Generator Loss: {:.4f}'.format(d_error, g_error))\n",
    "        print('D(x): {:.4f}, D(G(z)): {:.4f}'.format(d_pred_real.mean(), d_pred_fake.mean()))\n",
    "\n",
    "    def save_models(self, generator, discriminator, epoch):\n",
    "        out_dir = './data/models/{}'.format(self.data_subdir)\n",
    "        Logger._make_dir(out_dir)\n",
    "        torch.save(generator.state_dict(),\n",
    "                   '{}/G_epoch_{}'.format(out_dir, epoch))\n",
    "        torch.save(discriminator.state_dict(),\n",
    "                   '{}/D_epoch_{}'.format(out_dir, epoch))\n",
    "\n",
    "    def close(self):\n",
    "        self.writer.close()\n",
    "\n",
    "    # Private Functionality\n",
    "\n",
    "    @staticmethod\n",
    "    def _step(epoch, n_batch, num_batches):\n",
    "        return epoch * num_batches + n_batch\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_dir(directory):\n",
    "        try:\n",
    "            os.makedirs(directory)\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla GAN for SNAKE dataset\n",
    "\n",
    "Next we'll download the MNIST dataset as a small dataset we can get things running on quickly:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 SNAKE dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "class snake_dataset():\n",
    "    \n",
    "    def __init__(self, num_sample = 1000):\n",
    "        self.pi = np.pi\n",
    "        self.num_sample = num_sample\n",
    "        self.r = torch.randn(self.num_sample,1)\n",
    "        self.theta = torch.FloatTensor(self.num_sample,1).uniform_(0,2*self.pi)\n",
    "        self.a = 10 + self.r\n",
    "        self.data = torch.empty(self.num_sample,2)\n",
    "        #self.label = 0\n",
    "        # self.Y = torch.empty(self.num_sample,1)\n",
    "        for i in range(self.num_sample):\n",
    "            if  0.5* self.pi <= self.theta[i] and self.theta[i] <= (3/2) * self.pi :\n",
    "                self.a = 10 + self.r[i]\n",
    "                self.x_data = self.a * torch.cos(self.theta[i])\n",
    "                self.y_data = (self.a * torch.sin(self.theta[i])) + 10\n",
    "                self.data[i,0] = self.x_data\n",
    "                self.data[i,1] = self.y_data\n",
    "\n",
    "            else:\n",
    "                self.a = 10 + self.r[i]\n",
    "                self.x_data = self.a * torch.cos(self.theta[i])\n",
    "                self.y_data = (self.a * torch.sin(self.theta[i])) - 10\n",
    "                self.data[i,0] = self.x_data\n",
    "                self.data[i,1] = self.y_data\n",
    "\n",
    "        self.len = self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # return (self.X[index], self.Y[index])\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Generate snake dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Load Dataset and attach a DataLoader\n",
    "\n",
    "from dataset_snake import snake_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "num_sample = 100000\n",
    "dataset = snake_dataset(num_sample)\n",
    "plt.scatter(dataset.data[:,0],dataset.data[:,1])\n",
    "\n",
    "#data = mnist_data()\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=True)\n",
    "num_batches = len(data_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Generator\n",
    "\n",
    "This \"vanilla\" GAN is the simplest GAN network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A three hidden-layer generative neural network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GeneratorNet, self).__init__()\n",
    "        n_features = 2\n",
    "        n_out = 2\n",
    "        \n",
    "        self.hidden0 = nn.Sequential(\n",
    "            nn.Linear(n_features, 256),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.hidden1 = nn.Sequential(            \n",
    "            nn.Linear(256, 256),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(256, n_out),\n",
    "            #nn.Tanh() #Tanh because output image = [0,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"generator\")\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Discriminator\n",
    "\n",
    "The discriminator has the responsibity to classify its input as\n",
    "real or fake. When a fake sample from the generator is given, it should ouptut 0 for fake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A three hidden-layer discriminative neural network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorNet, self).__init__()\n",
    "        n_features = 2\n",
    "        n_out = 1\n",
    "        \n",
    "        self.hidden0 = nn.Sequential( \n",
    "            nn.Linear(n_features, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.hidden1 = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            torch.nn.Linear(256, n_out),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"discriminator\")\n",
    "        x = self.hidden0(x)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "# def images_to_vectors(images):\n",
    "#     return images.view(images.size(0), 784)\n",
    "\n",
    "# def vectors_to_images(vectors):\n",
    "#     return vectors.view(vectors.size(0), 1, 28, 28)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Create the modules\n",
    "\n",
    "Let's create an instance of the generator and discriminator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = DiscriminatorNet()\n",
    "generator = GeneratorNet()\n",
    "\n",
    "from chosen_gpu import get_freer_gpu\n",
    "device = torch.device(get_freer_gpu()) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Configured device: \", device)\n",
    "#device = 1\n",
    "if torch.cuda.is_available():\n",
    "    discriminator.cuda(device)\n",
    "    generator.cuda(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Set up the optimizers and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Loss function\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "# How many epochs to train for\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "# Number of steps to apply to the discriminator for each step of the generator (1 in Goodfellow et al.)\n",
    "\n",
    "d_steps = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 Training\n",
    "\n",
    "The targets for the discriminator may be 0 or 1 depending on whether we're giving it\n",
    "real or fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_data_target(size):\n",
    "    '''\n",
    "    Tensor containing ones, with shape = size\n",
    "    '''\n",
    "    data = torch.ones(size, 1)\n",
    "    if torch.cuda.is_available(): return data.cuda(device)\n",
    "    return data\n",
    "\n",
    "def fake_data_target(size):\n",
    "    '''\n",
    "    Tensor containing zeros, with shape = size\n",
    "    '''\n",
    "    data = torch.zeros(size, 1)\n",
    "    if torch.cuda.is_available(): return data.cuda(device)\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a function for a single step for the discriminator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(optimizer, real_data, fake_data):\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Propagate real data\n",
    "    prediction_real = discriminator(real_data)\n",
    "    error_real = loss(prediction_real, real_data_target(real_data.size(0)))\n",
    "    error_real.backward()\n",
    "\n",
    "    # Propagate fake data\n",
    "    prediction_fake = discriminator(fake_data)\n",
    "    error_fake = loss(prediction_fake, fake_data_target(real_data.size(0)))\n",
    "    error_fake.backward()\n",
    "    \n",
    "    # Take a step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Return error\n",
    "    return error_real + error_fake, prediction_real, prediction_fake"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's a function for a single step of the generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(optimizer, fake_data):\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Propagate the fake data through the discriminator and backpropagate.\n",
    "    # Note that since we want the generator to output something that gets\n",
    "    # the discriminator to output a 1, we use the real data target here.\n",
    "    prediction = discriminator(fake_data)\n",
    "    error = loss(prediction, real_data_target(prediction.size(0)))\n",
    "    error.backward()\n",
    "    \n",
    "    # Update weights with gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Return error\n",
    "    return error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.8 Generate test noise samples\n",
    "\n",
    "Let's generate some noise vectors to use as inputs to the generator.\n",
    "We'll use these samples repeatedly to see the evolution of the generator\n",
    "over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create noise samples for the generator's input\n",
    "\n",
    "def noise(size):\n",
    "    n = torch.randn(size, 2) #[size vector of length 100]\n",
    "    if torch.cuda.is_available(): return n.cuda(device) \n",
    "    return n\n",
    "\n",
    "num_test_samples = 100\n",
    "test_noise = noise(num_test_samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.9 Start training\n",
    "\n",
    "Now let's train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_output(fake_data):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.xlim(-20,20)\n",
    "    plt.ylim(-20,20)\n",
    "    plt.scatter(fake_data[:,0], fake_data[:,1])\n",
    "    plt.show()\n",
    "\n",
    "logger = Logger(model_name='VGAN', data_name='SNAKE')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for n_batch, real_batch in enumerate(data_loader):\n",
    "        # Train discriminator on a real batch and a fake batch\n",
    "        \n",
    "        #real_data = images_to_vectors(real_batch)\n",
    "        real_data = real_batch\n",
    "        real_data = real_data.cuda(device)\n",
    "        #if torch.cuda.is_available(): real_data = real_data.cuda(device)\n",
    "        fake_data = generator(noise(real_data.size(0))).detach()\n",
    "        d_error, d_pred_real, d_pred_fake = train_discriminator(d_optimizer,\n",
    "                                                                real_data, fake_data)\n",
    "        \n",
    "        # Train generator\n",
    "\n",
    "        fake_data = generator(noise(real_batch.size(0)))\n",
    "        g_error = train_generator(g_optimizer, fake_data)\n",
    "        \n",
    "        # Log errors and display progress\n",
    "\n",
    "        logger.log(d_error, g_error, epoch, n_batch, num_batches)\n",
    "        if (n_batch) % 10 == 0:\n",
    "            display.clear_output(True)\n",
    "            # Display Images\n",
    "            #test_images = vectors_to_images(generator(test_noise)).data.cpu()\n",
    "            #test_images = generator(test_noise).data.cpu()\n",
    "            # print(test_images.shape)\n",
    "            test_plot = plt_output(generator(test_noise).cpu().detach().numpy())\n",
    "            #logger.log_images(test_images, num_test_samples, epoch, n_batch, num_batches);\n",
    "            # Display status Logs\n",
    "            logger.display_status(\n",
    "                epoch, num_epochs, n_batch, num_batches,\n",
    "                d_error, g_error, d_pred_real, d_pred_fake\n",
    "            )\n",
    "            \n",
    "        # Save model checkpoints\n",
    "        logger.save_models(generator, discriminator, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<img src=\"SNAKE_.png/\" title=\"MINIST on VGAN\" width=\"800\" height=\"1000\">\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "img = mpimg.imread('plots_tensorboard/SNAKE_.png')\n",
    "\n",
    "#<img src=\"MNIST.png/\" title=\"MINIST on VGAN\" width=\"800\" height=\"1000\">\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation on SNAKE\n",
    "\n",
    "Traing GANs on SNAKE dataset was relatively faster than when on MINIST. That is due to the less complexity of the data. D(x) value when compared to that of the training on MNIST is also better (closer to 0.5) which in turns better reproduction of the input. One thing to note is that due to the different nature of the input, the following functions are not needed: \n",
    "\n",
    "        def images_to_vectors(images):\n",
    "            return images.view(images.size(0), 784)\n",
    "\n",
    "        def vectors_to_images(vectors):\n",
    "            return vectors.view(vectors.size(0), 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
