{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTML Lab 2: AlexNet vs. GoogLeNet\n",
    "\n",
    "## Todsavad Tangortan st123012\n",
    "\n",
    "In this lab, we will develop PyTorch implementations of AlexNet and GoogleLeNet from scratch and compare them on CIFAR-10."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Create VSCode projects for each of these three networks. Be sure to properly define your Python classes, with one class per file and a main module that sets up your objects, runs the training process, and saves the necessary data.\n",
    "2. Note that the AlexNet implementation here does not have the local response normalization feature described in the paper. Take a look at the [PyTorch implementation of LRN](https://pytorch.org/docs/stable/generated/torch.nn.LocalResponseNorm.html) and incorporate it into your AlexNet implementation as it is described in the paper. Compare your test set results with and without LRN.\n",
    "3. Note that the backbone of the GoogLeNet implemented thus far does not correspond exactly to the description. Modify the architecture to\n",
    "   1. Use the same backbone (input image size, convolutions, etc.) before the first Inception module\n",
    "   2. Add the two side classifiers\n",
    "\n",
    "4. Compare your GoogLeNet and AlexNet implementations on CIFAR-10. Comment on the number of parameters, speed of training, and accuracy of the two models on this dataset when trained from scratch.\n",
    "\n",
    "5. Experiment with the pretrained GoogLeNet from the torchvision repository. Does it give better results on CIFAR-10 similar to what we found with AlexNet last week? Comment\n",
    "   on what we can glean from the results about the capacity and generalization ability of these two models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The report\n",
    "\n",
    "Use the same format as last week. Describe your experiments and their results. The report should be turned in on Google Classroom before next week's lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_module import train_model\n",
    "from test_module import test_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. AlexNet Implementation\n",
    "- Note that the AlexNet implementation here does not have the local response normalization feature described in the paper. \n",
    "- Take a look at the [PyTorch implementation of LRN](https://pytorch.org/docs/stable/generated/torch.nn.LocalResponseNorm.html) and incorporate it into your AlexNet implementation as it is described in the paper. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet using nn.Module with and without LRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetModule_LRN(nn.Module):\n",
    "    '''\n",
    "    An AlexNet-like CNN\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    num_classes : int\n",
    "        Number of classes in the final multinomial output layer\n",
    "    features : Sequential\n",
    "        The feature extraction portion of the network\n",
    "    avgpool : AdaptiveAvgPool2d\n",
    "        Convert the final feature layer to 6x6 feature maps by average pooling if they are not already 6x6\n",
    "    classifier : Sequential\n",
    "        Classify the feature maps into num_classes classes\n",
    "    '''\n",
    "    def __init__(self, num_classes: int = 10) -> None:\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(size=5,alpha=1e-4, beta=0.75, k=2), # added LRN\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(size=5,alpha=1e-4, beta=0.75, k=2), # added LRN\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNetModule(nn.Module):\n",
    "    '''\n",
    "    An AlexNet-like CNN\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    num_classes : int\n",
    "        Number of classes in the final multinomial output layer\n",
    "    features : Sequential\n",
    "        The feature extraction portion of the network\n",
    "    avgpool : AdaptiveAvgPool2d\n",
    "        Convert the final feature layer to 6x6 feature maps by average pooling if they are not already 6x6\n",
    "    classifier : Sequential\n",
    "        Classify the feature maps into num_classes classes\n",
    "    '''\n",
    "    def __init__(self, num_classes: int = 10) -> None:\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compare your test set results with and without LRN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Set\n",
    "##AlexNet With LRN\n",
    "'''\n",
    "Epoch 9/9\n",
    "----------\n",
    "train Loss: 0.3815 Acc: 0.8657\n",
    "Epoch time taken:  146.6789107322693\n",
    "val Loss: 0.5868 Acc: 0.8014\n",
    "Epoch time taken:  162.75838804244995\n",
    "Training complete in 26m 2s\n",
    "Best val Acc: 0.8014\n",
    "'''\n",
    "##AlexNet Withouts LRN\n",
    "'''\n",
    "Epoch 9/9\n",
    "----------\n",
    "train Loss: 0.4255 Acc: 0.8528\n",
    "Epoch time taken:  132.84145975112915\n",
    "val Loss: 0.6705 Acc: 0.7764\n",
    "Epoch time taken:  149.2431561946869\n",
    "Training complete in 25m 5s\n",
    "Best val Acc: 0.7764\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Set\n",
    "##AlexNet With LRN\n",
    "'''\n",
    "test Loss: 0.6008 Acc: 0.8029\n",
    "Epoch time taken:  0.07869291305541992\n",
    "'''\n",
    "##AlexNet Withouts LRN\n",
    "'''\n",
    "test Loss: 0.6659 Acc: 0.7741\n",
    "Epoch time taken:  0.08170104026794434\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GoogLeNet Implementation\n",
    "- Note that the backbone of the GoogLeNet implemented thus far does not correspond exactly to the description.\n",
    "- Modify the architecture to\n",
    "   1. Use the same backbone (input image size, convolutions, etc.) before the first Inception module\n",
    "   2. Add the two side classifiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        return x.view(batch_size, -1)\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    '''\n",
    "    Inception block for a GoogLeNet-like CNN\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    in_planes : int\n",
    "        Number of input feature maps\n",
    "    n1x1 : int\n",
    "        Number of direct 1x1 convolutions\n",
    "    n3x3red : int\n",
    "        Number of 1x1 reductions before the 3x3 convolutions\n",
    "    n3x3 : int\n",
    "        Number of 3x3 convolutions\n",
    "    n5x5red : int\n",
    "        Number of 1x1 reductions before the 5x5 convolutions\n",
    "    n5x5 : int\n",
    "        Number of 5x5 convolutions\n",
    "    pool_planes : int\n",
    "        Number of 1x1 convolutions after 3x3 max pooling\n",
    "    b1 : Sequential\n",
    "        First branch (direct 1x1 convolutions)\n",
    "    b2 : Sequential\n",
    "        Second branch (reduction then 3x3 convolutions)\n",
    "    b3 : Sequential\n",
    "        Third branch (reduction then 5x5 convolutions)\n",
    "    b4 : Sequential\n",
    "        Fourth branch (max pooling then reduction)\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n",
    "        super(Inception, self).__init__()\n",
    "        self.in_planes = in_planes\n",
    "        self.n1x1 = n1x1\n",
    "        self.n3x3red = n3x3red\n",
    "        self.n3x3 = n3x3\n",
    "        self.n5x5red = n5x5red\n",
    "        self.n5x5 = n5x5\n",
    "        self.pool_planes = pool_planes\n",
    "        \n",
    "        # 1x1 conv branch\n",
    "        self.b1 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, n1x1, kernel_size=1),\n",
    "            nn.BatchNorm2d(n1x1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # 1x1 conv -> 3x3 conv branch\n",
    "        self.b2 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, n3x3red, kernel_size=1),\n",
    "            nn.BatchNorm2d(n3x3red),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n3x3),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # 1x1 conv -> 5x5 conv branch\n",
    "        self.b3 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, n5x5red, kernel_size=1),\n",
    "            nn.BatchNorm2d(n5x5red),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n5x5red, n5x5, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n5x5),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n5x5),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # 3x3 pool -> 1x1 conv branch\n",
    "        self.b4 = nn.Sequential(\n",
    "            nn.MaxPool2d(3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n",
    "            nn.BatchNorm2d(pool_planes),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.b1(x)\n",
    "        y2 = self.b2(x)\n",
    "        y3 = self.b3(x)\n",
    "        y4 = self.b4(x)\n",
    "        return torch.cat([y1, y2, y3, y4], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from inception import Inception\n",
    "\n",
    "class GoogLeNet(nn.Module):\n",
    "    '''\n",
    "    GoogLeNet-like CNN\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    pre_layers : Sequential\n",
    "        Initial convolutional layer\n",
    "    a3 : Inception\n",
    "        First inception block\n",
    "    b3 : Inception\n",
    "        Second inception block\n",
    "    maxpool : MaxPool2d\n",
    "        Pooling layer after second inception block\n",
    "    a4 : Inception\n",
    "        Third inception block\n",
    "    b4 : Inception\n",
    "        Fourth inception block\n",
    "    c4 : Inception\n",
    "        Fifth inception block\n",
    "    d4 : Inception\n",
    "        Sixth inception block\n",
    "    e4 : Inception\n",
    "        Seventh inception block\n",
    "    a5 : Inception\n",
    "        Eighth inception block\n",
    "    b5 : Inception\n",
    "        Ninth inception block\n",
    "    avgpool : AvgPool2d\n",
    "        Average pool layer after final inception block\n",
    "    linear : Linear\n",
    "        Fully connected layer\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        self.pre_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(3, stride=2, padding=1),\n",
    "            nn.LocalResponseNorm(size=5,alpha=1e-4, beta=0.75, k=2),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=1),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Conv2d(64, 192, kernel_size=3,padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.LocalResponseNorm(size=5,alpha=1e-4, beta=0.75, k=2)\n",
    "        )        \n",
    "\n",
    "        self.a3 = Inception(192,  64,  96, 128, 16, 32, 32)\n",
    "        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "        self.a4 = Inception(480, 192,  96, 208, 16,  48,  64)\n",
    "        self.aux1 = nn.Sequential(\n",
    "            nn.AvgPool2d(5, stride=3),\n",
    "            nn.Conv2d(512,128, kernel_size=1),\n",
    "            nn.ReLU(True),\n",
    "            Flatten(),\n",
    "            nn.Linear(2048,1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.7),\n",
    "            nn.Linear(1024,10)\n",
    "        )\n",
    "        \n",
    "        self.b4 = Inception(512, 160, 112, 224, 24,  64,  64)\n",
    "        self.c4 = Inception(512, 128, 128, 256, 24,  64,  64)\n",
    "        self.d4 = Inception(512, 112, 144, 288, 32,  64,  64)\n",
    "        self.aux2 = nn.Sequential(\n",
    "            nn.AvgPool2d(5, stride=3),\n",
    "            nn.Conv2d(528,128, kernel_size=1),\n",
    "            nn.ReLU(True),\n",
    "            Flatten(),\n",
    "            nn.Linear(2048,1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.7),\n",
    "            nn.Linear(1024,10)\n",
    "        )\n",
    "        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "\n",
    "        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.linear = nn.Linear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pre_layers(x)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.a3(out)\n",
    "        out = self.b3(out)\n",
    "\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.a4(out)\n",
    "        aux_out1 = self.aux1(out)\n",
    "\n",
    "        out = self.b4(out)\n",
    "        out = self.c4(out)\n",
    "\n",
    "        out = self.d4(out)\n",
    "        aux_out2 = self.aux2(out)\n",
    "\n",
    "        out = self.e4(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        out = self.a5(out)\n",
    "        out = self.b5(out)\n",
    "\n",
    "        out = self.avgpool(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out, aux_out1, aux_out2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  CIFAR-10 with GoogLeNet and AlexNet\n",
    "- Compare your GoogLeNet and AlexNet implementations on CIFAR-10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GoogleNet\n",
    "#Training Set\n",
    "'''\n",
    "Epoch 9/9\n",
    "----------\n",
    "train Loss: 0.6107 Acc: 0.8964\n",
    "Epoch time taken:  648.7454090118408\n",
    "val Loss: 0.4725 Acc: 0.8507\n",
    "Epoch time taken:  691.7330749034882\n",
    "Training complete in 117m 23s\n",
    "Best val Acc: 0.850700\n",
    "'''\n",
    "#Testing Set\n",
    "'''\n",
    "test Loss: 0.4734 Acc: 0.8501\n",
    "Epoch time taken:  0.11796164512634277\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comment on the number of parameters, speed of training, and accuracy of the two models on this dataset when trained from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for counting the number of parameters in a model\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNetModule has 57,044,810 trainable parameters\n",
      "GoogLeNet has 10,635,134 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "alexnet = AlexNetModule()\n",
    "googlenet = GoogLeNet()\n",
    "\n",
    "models = [alexnet, googlenet]\n",
    "\n",
    "for model in models:\n",
    "    print(f'{type(model).__name__} has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. \n",
    "- Experiment with the pretrained GoogLeNet from the torchvision repository. \n",
    "- Does it give better results on CIFAR-10 similar to what we found with AlexNet last week? \n",
    "- Comment on what we can glean from the results about the capacity and generalization ability of these two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "googlenet_pre = torch.hub.load('pytorch/vision:v0.6.0', 'googlenet', pretrained=True, aux_logits = True)\n",
    "googlenet_pre.aux1.fc2 = nn.Linear(1024,10)\n",
    "googlenet_pre.aux2.fc2 = nn.Linear(1024,10)\n",
    "googlenet_pre.fc = nn.Linear(1024,10)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AlexNet\n",
    "#Training Set\n",
    "'''\n",
    "Epoch 9/9\n",
    "----------\n",
    "train Loss: 0.3163 Acc: 0.8951\n",
    "Epoch time taken:  119.86784219741821\n",
    "val Loss: 0.6379 Acc: 0.7931\n",
    "Epoch time taken:  134.3369767665863\n",
    "'''\n",
    "#Testing Set\n",
    "'''\n",
    "test Loss: 0.5410 Acc: 0.8301\n",
    "Epoch time taken:  0.09445977210998535\n",
    "'''\n",
    "#GoogLeNet\n",
    "#Training Set\n",
    "'''\n",
    "Epoch 9/9\n",
    "----------\n",
    "train Loss: 0.2292 Acc: 0.9775\n",
    "Epoch time taken:  499.1913321018219\n",
    "val Loss: 0.2488 Acc: 0.9299\n",
    "Epoch time taken:  531.9703154563904\n",
    "Training complete in 91m 57s\n",
    "Best val Acc: 0.929900 \n",
    "'''\n",
    "#Testing Set\n",
    "'''\n",
    "test Loss: 0.2375 Acc: 0.9316\n",
    "Epoch time taken:  0.08515691757202148\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 26 2021, 20:14:08) \n[GCC 9.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
