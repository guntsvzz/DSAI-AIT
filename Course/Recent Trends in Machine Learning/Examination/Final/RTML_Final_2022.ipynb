{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "drDWFoYxQVqf"
      },
      "source": [
        "# RTML Final 2022\n",
        "\n",
        "Welcome to the RTML final exam, version 2022!\n",
        "\n",
        "Prepare your answer to each question, writing your answers directly in this notebook, print as PDF, and turn in via Google Classroom by the deadline.\n",
        "\n",
        "You have 2.5 hours to complete the exam. Good luck!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2EJp7FDhQVq4"
      },
      "source": [
        "## Question 1 (20 points)\n",
        "\n",
        "Suppose you have a dataset consisting of 500 essays on the assigned topic,\n",
        "\"Why is it so difficult to produce a computer program that can pass the Turing Test?\"\n",
        "The essays are by Data Science and AI students from all over Asia and are each 250-350 words long.\n",
        "\n",
        "Suppose further that having taken RTML, you know a lot about GANs and RNNs, so you decide to build a recurrent\n",
        "GAN to generate fresh essays on the same topic.\n",
        "\n",
        "Explain in detail how you could use a LSTM-based RNN as the generator in a GAN with this goal.\n",
        "Be sure to indicate the detailed structure of the generator and discriminator, the loss functions,\n",
        "how the models are trained, how the cell/hidden state is initialized, what is the input to the model\n",
        "during training, and how the resulting model is used for inference.\n",
        "\n",
        "*Write your answer here.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TLbiIw45QVrC"
      },
      "source": [
        "## Question 2 (10 points)\n",
        "\n",
        "Explain how could BERT be fine tuned on the task of Question 1 and how the resulting model would be used for inference.\n",
        "\n",
        "*Write your answer here.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "d6lhrSeOQVrK"
      },
      "source": [
        "## Question 3 (10 points)\n",
        "\n",
        "In Lab 12, we implemented the basic REINFORCE algorithm on CartPole.\n",
        "Run your trained REINFORCE model on CartPole. Show a screenshot of your trained\n",
        "REINFORCE model playing the game here.\n",
        "\n",
        "*Put your screenshot here*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "475.0\n",
            "Episode 10\tLast reward: 9.00\tAverage reward: 15.14\n",
            "Episode 20\tLast reward: 16.00\tAverage reward: 17.23\n",
            "Episode 30\tLast reward: 11.00\tAverage reward: 18.63\n",
            "Episode 40\tLast reward: 15.00\tAverage reward: 20.65\n",
            "Episode 50\tLast reward: 31.00\tAverage reward: 30.82\n",
            "Episode 60\tLast reward: 45.00\tAverage reward: 30.54\n",
            "Episode 70\tLast reward: 234.00\tAverage reward: 51.10\n",
            "Episode 80\tLast reward: 127.00\tAverage reward: 86.97\n",
            "Episode 90\tLast reward: 63.00\tAverage reward: 102.29\n",
            "Episode 100\tLast reward: 229.00\tAverage reward: 105.88\n",
            "Episode 110\tLast reward: 326.00\tAverage reward: 204.60\n",
            "Episode 120\tLast reward: 193.00\tAverage reward: 235.82\n",
            "Episode 130\tLast reward: 305.00\tAverage reward: 236.45\n",
            "Episode 140\tLast reward: 300.00\tAverage reward: 247.11\n",
            "Episode 150\tLast reward: 428.00\tAverage reward: 293.05\n",
            "Episode 160\tLast reward: 352.00\tAverage reward: 351.60\n",
            "Episode 170\tLast reward: 223.00\tAverage reward: 304.66\n",
            "Episode 180\tLast reward: 636.00\tAverage reward: 314.75\n",
            "Solved! Running reward is now 710.3551424899367 and the last episode runs to 6093 time steps!\n"
          ]
        }
      ],
      "source": [
        "# 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/.mujoco/mujoco210/bin' \n",
        "# mkdir ~/.mujoco\n",
        "# wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O mujoco.tar.gz\n",
        "# rm mujoco.tar.gz\n",
        "# pip install mujoco-py\n",
        "# pip install gymnasium[mujoco]\n",
        "# pip install gymnasium[classic-control]\n",
        "# apt-get install libglew-dev patchelf libosmesa6-dev libgl1-mesa-glx\n",
        "# apt-get install -y xvfb python-opengl \n",
        "# xvfb-run -a -s \"-screen 0 1400x900x24\" bash\n",
        "\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch import optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super(Policy, self).__init__()\n",
        "        self.n_inputs = env.observation_space.shape[0]\n",
        "        self.n_outputs = env.action_space.n\n",
        "        \n",
        "        self.affine1 = nn.Linear(self.n_inputs, 128)\n",
        "        self.dropout = nn.Dropout(p=0.6)\n",
        "        self.affine2 = nn.Linear(128, self.n_outputs)\n",
        "\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.affine1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(x)\n",
        "        action_scores = self.affine2(x)\n",
        "        return F.softmax(action_scores, dim=1)\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        state = torch.from_numpy(np.array(state)).float().unsqueeze(0)\n",
        "        probs = self.forward(state)\n",
        "        m = torch.distributions.Categorical(probs)\n",
        "        action = m.sample()\n",
        "        self.saved_log_probs.append(m.log_prob(action))\n",
        "        return action.item()\n",
        "\n",
        "#RL environment parameters\n",
        "gamma = 0.95\n",
        "seed = 0\n",
        "render = False\n",
        "log_interval = 10\n",
        "\n",
        "#Set up \n",
        "\n",
        "# env = gym.make(\"ALE/SpaceInvaders-ram-v5\")\n",
        "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
        "reward_threshold = env.spec.reward_threshold\n",
        "print(reward_threshold)\n",
        "env.reset(seed=seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "#Create out policy Network\n",
        "policy = Policy(env)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
        "eps = np.finfo(np.float32).eps.item()\n",
        "\n",
        "# env.reset()\n",
        "# x = env.render()\n",
        "# print(x)\n",
        "\n",
        "def finish_episode():\n",
        "    R = 0\n",
        "    policy_loss = []\n",
        "    returns = []\n",
        "    for r in policy.rewards[::-1]:\n",
        "        R = r + gamma * R\n",
        "        returns.insert(0, R)\n",
        "    returns = torch.tensor(returns)\n",
        "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
        "        policy_loss.append(-log_prob * R)\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss = torch.cat(policy_loss).sum()\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "    del policy.rewards[:]\n",
        "    del policy.saved_log_probs[:]\n",
        "\n",
        "from itertools import count\n",
        "def reinforce():\n",
        "    running_reward = 10\n",
        "    for i_episode in count(1):\n",
        "        (state, info), ep_reward = env.reset(), 0\n",
        "        # print('Initial State', state)\n",
        "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
        "            action = policy.select_action(state)\n",
        "            state, reward, done, truncated, info = env.step(action)\n",
        "            # print('New State', state)\n",
        "            if render:\n",
        "                env.render()\n",
        "            policy.rewards.append(reward)\n",
        "            ep_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # calculate reward\n",
        "        # It accepts a list of rewards for the whole episode and needs to calculate \n",
        "        # the discounted total reward for every step. To do this efficiently,\n",
        "        # we calculate the reward from the end of the local reward list.\n",
        "        # The last step of the episode will have the total reward equal to its local reward.\n",
        "        # The step before the last will have the total reward of ep_reward + gamma * running_reward\n",
        "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
        "        finish_episode()\n",
        "        if i_episode % log_interval == 0:\n",
        "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
        "                  i_episode, ep_reward, running_reward))\n",
        "            \n",
        "        if running_reward > env.spec.reward_threshold: #475\n",
        "            print(\"Solved! Running reward is now {} and \"\n",
        "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
        "            break\n",
        "        \n",
        "reinforce()\n",
        "env.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sm3a9RC3QVrR"
      },
      "source": [
        "## Question 4 (20 points)\n",
        "\n",
        "Next, let's replace the policy network that is currently working with the fully observed MDP with a POMDP using only the image\n",
        "of the environment as the observation.\n",
        "\n",
        "If you completed Lab 12, you should already have an implementation of REINFORCE on Space Invaders that you can reuse.\n",
        "\n",
        "By default, CartPole will render at 600x400 resolution. We will want to downscale that, perhaps to 150x100, and stack subsequent\n",
        "frames, perhaps 4 of them, in order to provide some history information.\n",
        "\n",
        "Below, show a revision of your REINFORCE policy model that takes as input a stack of the four most recent downscaled grayscale\n",
        "images and outputs an action. The model should have an appropriate series of convolutions, one or more fully connected layers,\n",
        "and a linear/softmax layer that ouptuts an action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gym is an OpenAI toolkit for RL\n",
        "from gymnasium.spaces import Box\n",
        "from gymnasium.wrappers import FrameStack\n",
        "import gymnasium as gym\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.distributions import Categorical\n",
        "import torch.autograd as autograd \n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from collections import namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "from utils import GrayScaleObservation, ResizeObservation, SkipFrame\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super(Policy, self).__init__()\n",
        "        # self.n_inputs = env.observation_space.shape[2]\n",
        "        self.n_outputs = env.action_space.n\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=2, stride=1)\n",
        "        \n",
        "        self.affine1 = nn.Linear(10240, 128)\n",
        "        self.dropout = nn.Dropout(p=0.6)\n",
        "        self.affine2 = nn.Linear(128, self.n_outputs)\n",
        "\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x / 255.0  # normalize pixel values\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        x = self.affine1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(x)\n",
        "        action_scores = self.affine2(x)\n",
        "        return F.softmax(action_scores, dim=1)\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        state = torch.from_numpy(np.array(state)).float().unsqueeze(0)\n",
        "        probs = self.forward(state)\n",
        "        m = torch.distributions.Categorical(probs)\n",
        "        action = m.sample()\n",
        "        self.saved_log_probs.append(m.log_prob(action))\n",
        "        return action.item(), probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "4FLyLTaNQVrb"
      },
      "outputs": [],
      "source": [
        "#RL environment parameters\n",
        "gamma = 0.95\n",
        "seed = 0\n",
        "render = False\n",
        "log_interval = 10\n",
        "\n",
        "env = gym.make(\"ALE/SpaceInvaders-v5\", render_mode=\"rgb_array\")\n",
        "# define a reward threshold\n",
        "reward_threshold = 250\n",
        "# register the reward threshold with the environment\n",
        "env.spec.reward_threshold = reward_threshold\n",
        "# env = gym.make(\"SpaceInvaders-v0\")\n",
        "env = FrameStack(ResizeObservation(GrayScaleObservation(SkipFrame(env, skip=4)), shape=(150,100)), num_stack=4)\n",
        "env.reset(seed=seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "#Create out policy Network\n",
        "policy = Policy(env)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
        "eps = np.finfo(np.float32).eps.item()    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 10\tLast reward: 285.00\tAverage reward: 119.22\n",
            "Episode 20\tLast reward: 285.00\tAverage reward: 180.98\n",
            "Episode 30\tLast reward: 180.00\tAverage reward: 210.65\n",
            "Episode 40\tLast reward: 285.00\tAverage reward: 240.48\n",
            "Solved! Running reward is now 250.55427543651658 and the last episode runs to 182 time steps!\n"
          ]
        }
      ],
      "source": [
        "def finish_episode():\n",
        "    R = 0\n",
        "    policy_loss = []\n",
        "    returns = []\n",
        "    for r in policy.rewards[::-1]:\n",
        "        R = r + gamma * R\n",
        "        returns.insert(0, R)\n",
        "    returns = torch.tensor(returns)\n",
        "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
        "        policy_loss.append(-log_prob * R)\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss = torch.cat(policy_loss).sum()\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "    del policy.rewards[:]\n",
        "    del policy.saved_log_probs[:]\n",
        "\n",
        "from itertools import count\n",
        "def reinforce():\n",
        "    running_reward = 10\n",
        "    for i_episode in count(1):\n",
        "        (state, info), ep_reward = env.reset(), 0\n",
        "        # print('Initial State', state)\n",
        "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
        "            action, _ = policy.select_action(state)\n",
        "            state, reward, done, truncated, info = env.step(action)\n",
        "            # print('New State', state)\n",
        "            if render:\n",
        "                env.render()\n",
        "            policy.rewards.append(reward)\n",
        "            ep_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # calculate reward\n",
        "        # It accepts a list of rewards for the whole episode and needs to calculate \n",
        "        # the discounted total reward for every step. To do this efficiently,\n",
        "        # we calculate the reward from the end of the local reward list.\n",
        "        # The last step of the episode will have the total reward equal to its local reward.\n",
        "        # The step before the last will have the total reward of ep_reward + gamma * running_reward\n",
        "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
        "        finish_episode()\n",
        "        if i_episode % log_interval == 0:\n",
        "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
        "                  i_episode, ep_reward, running_reward))\n",
        "\n",
        "        if running_reward >= env.spec.reward_threshold:\n",
        "            print(\"Solved! Running reward is now {} and \"\n",
        "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
        "            break\n",
        "\n",
        "reinforce()\n",
        "env.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vkPSsufQQVrg"
      },
      "source": [
        "Next, demonstrate that your policy model when given a 4x100x150 tensor of zeros outputs, the policy model outputs an appropriate\n",
        "shaped vector representing a multinomial distribution over the action space."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K2mN9tvKQVrj"
      },
      "source": [
        "## Question 5 (20 points)\n",
        "\n",
        "Modify the `reinforce()` function to generate the visual input to the Policy model rather than the fully\n",
        "observed state.\n",
        "\n",
        "In your code, after each of the following lines\n",
        "\n",
        "    state, ep_reward = env.reset(), 0\n",
        "    \n",
        "    ...\n",
        "    \n",
        "        state, reward, done, _ = env.step(action)\n",
        "    \n",
        "please add code to replace the fully observed state with the observation:\n",
        "\n",
        "    obs_t = env.render(mode=\"rgb_array\")\n",
        "    obs_seq.append(obs_t)\n",
        "    state = make_observation(obs_seq)\n",
        "\n",
        "You'll have to add some code to initialize the `obs_seq` array appropriately and write the `make_observation()` function\n",
        "to convert the four most recent observations to grayscale and stack them in a tensor that your policy\n",
        "network can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_observation():\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVSQ573dQVrl"
      },
      "outputs": [],
      "source": [
        "# Place revised reinforce() code here\n",
        "def reinforce():\n",
        "    running_reward = 10\n",
        "    obs_seq = list()\n",
        "    for i_episode in count(1):\n",
        "        (state, info), ep_reward = env.reset(), 0\n",
        "        # print('Initial State', state)\n",
        "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
        "            action, _ = policy.select_action(state)\n",
        "            state, reward, done, truncated, info = env.step(action)\n",
        "            # print('New State', state)\n",
        "            if render:\n",
        "                obs_t = env.render(mode=\"rgb_array\")\n",
        "                obs_seq.append(obs_t)\n",
        "                state = make_observation(obs_seq)\n",
        "            policy.rewards.append(reward)\n",
        "            ep_reward += reward\n",
        "            if done:\n",
        "                break"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GEHYFO_QQVrm"
      },
      "source": [
        "Show that the resulting policy model can be trained for a few episodes. You don't have to train the model to perfection -- you can do it on your own PC in CPU mode and just\n",
        "show that policy model is learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxX76wgwQVrm"
      },
      "outputs": [],
      "source": [
        "# Code to train for a few episodes goes here\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hRbMMkIcQVrn"
      },
      "source": [
        "## Question 6 (10 points)\n",
        "\n",
        "What are the major differences between this visual REINFORCE method and Mnih et al.'s DQN method?\n",
        "\n",
        "*Your answer goes here.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k3W_1BVoQVro"
      },
      "source": [
        "## Question 7 (10 points)\n",
        "\n",
        "What are the major differences between this visual REINFORCE method and the A2C (Advantage-Actor-Critic) method? In your answer, assume we make the\n",
        "same modification to A2C to use visual observations instead of full state observations.\n",
        "\n",
        "*Your answer goes here.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txgzPlKIQVro"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
