{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "drDWFoYxQVqf"
      },
      "source": [
        "# RTML Final 2022\n",
        "\n",
        "Welcome to the RTML final exam, version 2022!\n",
        "\n",
        "Prepare your answer to each question, writing your answers directly in this notebook, print as PDF, and turn in via Google Classroom by the deadline.\n",
        "\n",
        "You have 2.5 hours to complete the exam. Good luck!"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2EJp7FDhQVq4"
      },
      "source": [
        "## Question 1 (20 points)\n",
        "\n",
        "Suppose you have a dataset consisting of 500 essays on the assigned topic,\n",
        "\"Why is it so difficult to produce a computer program that can pass the Turing Test?\"\n",
        "The essays are by Data Science and AI students from all over Asia and are each 250-350 words long.\n",
        "\n",
        "Suppose further that having taken RTML, you know a lot about GANs and RNNs, so you decide to build a recurrent\n",
        "GAN to generate fresh essays on the same topic.\n",
        "\n",
        "Explain in detail how you could use a LSTM-based RNN as the generator in a GAN with this goal.\n",
        "Be sure to indicate the detailed structure of the generator and discriminator, the loss functions,\n",
        "how the models are trained, how the cell/hidden state is initialized, what is the input to the model\n",
        "during training, and how the resulting model is used for inference.\n",
        "\n",
        "*Write your answer here.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To use a LSTM-based RNN as the generator in a GAN for generating fresh essays on the assigned topic, we can follow the following steps:\n",
        "\n",
        "1. Data preprocessing: We preprocess the dataset by tokenizing each essay into words, and creating a vocabulary of all the unique words in the dataset. We also assign an integer ID to each word in the vocabulary, and represent each essay as a sequence of word IDs.\n",
        "\n",
        "2. Generator architecture: The generator is an LSTM-based RNN that takes as input a sequence of word IDs sampled from a noise distribution, and generates a sequence of word IDs that form a new essay. The generator is conditioned on a binary label indicating whether the generated essay is real or fake. The generator has the following components:\n",
        "\n",
        "  - Embedding layer: This layer converts the word IDs into dense embeddings that capture the semantic meaning of each word.\n",
        "  - LSTM layer(s): This layer processes the input sequence of embeddings and generates a new sequence of embeddings representing the generated essay.\n",
        "  - Dense output layer: This layer maps the output sequence of embeddings to a sequence of word IDs.\n",
        "\n",
        "3. Discriminator architecture: The discriminator is a binary classifier that takes as input a sequence of word IDs representing an essay, and predicts whether the essay is real or fake. The discriminator has the following components:\n",
        "\n",
        "  - Embedding layer: This layer converts the word IDs into dense embeddings that capture the semantic meaning of each word.\n",
        "  - LSTM layer(s): This layer processes the input sequence of embeddings and generates a fixed-length feature vector that summarizes the content of the essay.\n",
        "  - Dense output layer: This layer maps the feature vector to a binary classification output indicating whether the essay is real or fake.\n",
        "\n",
        "4. Loss functions: The generator and discriminator are trained using the following loss functions:\n",
        "\n",
        "  - Generator loss: The generator is trained to minimize the binary cross-entropy loss between the discriminator's predictions for the generated essays and the target label of \"real\" (i.e., fool the discriminator into thinking the generated essay is real).\n",
        "  - Discriminator loss: The discriminator is trained to minimize the binary cross-entropy loss between its predictions for real essays and the target label of \"real\", and to maximize the binary cross-entropy loss between its predictions for fake essays and the target label of \"fake\".\n",
        "\n",
        "5. Training procedure: During training, we alternate between training the generator and discriminator. For each batch of training data, we sample a batch of noise vectors from a noise distribution, and use the generator to generate a batch of fake essays. We then use the discriminator to classify the batch of real and fake essays, and update the parameters of the generator and discriminator based on their respective loss functions.\n",
        "\n",
        "6. Cell/hidden state initialization: The cell/hidden state of the LSTM in the generator is initialized to zero at the start of each training iteration.\n",
        "\n",
        "7. Input to the model during training: During training, the generator takes as input a sequence of word IDs sampled from a noise distribution, and the discriminator takes as input a sequence of word IDs representing an essay.\n",
        "\n",
        "8. Inference: To generate new essays using the trained generator, we sample a noise vector from the noise distribution and use the generator to generate a sequence of word IDs representing the new essay. We can then decode the sequence of word IDs back into human-readable text using the vocabulary mapping. The quality of the generated essays can be evaluated using metrics such as perplexity, coherence, and grammaticality."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TLbiIw45QVrC"
      },
      "source": [
        "## Question 2 (10 points)\n",
        "\n",
        "Explain how could BERT be fine tuned on the task of Question 1 and how the resulting model would be used for inference.\n",
        "\n",
        "*Write your answer here.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model that can be fine-tuned on specific downstream tasks, including text classification, question answering, and language generation. Here, we will explain how BERT can be fine-tuned on the task of generating fresh essays on the assigned topic of \"Why is it so difficult to produce a computer program that can pass the Turing Test?\" and how the resulting model would be used for inference.\n",
        "\n",
        "1. Data preprocessing: We preprocess the dataset by tokenizing each essay into subwords, and creating a vocabulary of all the unique subwords in the dataset. We also assign an integer ID to each subword in the vocabulary, and represent each essay as a sequence of subword IDs. We split the dataset into training, validation, and test sets.\n",
        "\n",
        "2. Fine-tuning BERT: We fine-tune BERT on the task of generating fresh essays on the assigned topic using the following steps:\n",
        "\n",
        "   - Input representation: We represent each essay as a sequence of subword IDs, and prepend a special [CLS] token to the start of the sequence to indicate the task of essay generation.\n",
        "   \n",
        "   - Fine-tuning procedure: We train BERT end-to-end on the training set using the cross-entropy loss between the predicted subword logits and the target subword IDs. We use the validation set to tune the hyperparameters, such as learning rate and batch size, and to monitor the model's performance.\n",
        "   \n",
        "   - Output generation: To generate a new essay, we first encode the assigned topic as a sequence of subword IDs using BERT's tokenizer, and concatenate it with the [CLS] token. We then feed the concatenated sequence to the fine-tuned BERT model, and generate new subwords autoregressively by sampling from the conditional probability distribution over subwords at each time step. We stop generating subwords when we reach a special [SEP] token or a maximum length limit. We decode the generated sequence of subword IDs back into human-readable text using the vocabulary mapping.\n",
        "\n",
        "3. Evaluation: We evaluate the quality of the generated essays using metrics such as perplexity, coherence, and grammaticality. We also compare the generated essays with the original essays in the test set to measure their similarity and diversity.\n",
        "\n",
        "In summary, fine-tuning BERT on the task of generating fresh essays on the assigned topic involves representing the input essays as subword sequences, adding a special [CLS] token to the start of the sequences, and fine-tuning BERT end-to-end using the cross-entropy loss between the predicted subword logits and the target subword IDs. To generate new essays, we encode the assigned topic as a subword sequence, concatenate it with the [CLS] token, and feed it to the fine-tuned BERT model to generate new subwords autoregressively. The resulting model can be used for inference by generating new essays on the assigned topic based on user inputs or prompts."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "d6lhrSeOQVrK"
      },
      "source": [
        "## Question 3 (10 points)\n",
        "\n",
        "In Lab 12, we implemented the basic REINFORCE algorithm on CartPole.\n",
        "Run your trained REINFORCE model on CartPole. Show a screenshot of your trained\n",
        "REINFORCE model playing the game here.\n",
        "\n",
        "*Put your screenshot here*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "475.0\n",
            "Episode 10\tLast reward: 9.00\tAverage reward: 15.14\n",
            "Episode 20\tLast reward: 16.00\tAverage reward: 17.23\n",
            "Episode 30\tLast reward: 11.00\tAverage reward: 18.63\n",
            "Episode 40\tLast reward: 15.00\tAverage reward: 20.65\n",
            "Episode 50\tLast reward: 31.00\tAverage reward: 30.82\n",
            "Episode 60\tLast reward: 45.00\tAverage reward: 30.54\n",
            "Episode 70\tLast reward: 234.00\tAverage reward: 51.10\n",
            "Episode 80\tLast reward: 127.00\tAverage reward: 86.97\n",
            "Episode 90\tLast reward: 63.00\tAverage reward: 102.29\n",
            "Episode 100\tLast reward: 229.00\tAverage reward: 105.88\n",
            "Episode 110\tLast reward: 326.00\tAverage reward: 204.60\n",
            "Episode 120\tLast reward: 193.00\tAverage reward: 235.82\n",
            "Episode 130\tLast reward: 305.00\tAverage reward: 236.45\n",
            "Episode 140\tLast reward: 300.00\tAverage reward: 247.11\n",
            "Episode 150\tLast reward: 428.00\tAverage reward: 293.05\n",
            "Episode 160\tLast reward: 352.00\tAverage reward: 351.60\n",
            "Episode 170\tLast reward: 223.00\tAverage reward: 304.66\n",
            "Episode 180\tLast reward: 636.00\tAverage reward: 314.75\n",
            "Solved! Running reward is now 710.3551424899367 and the last episode runs to 6093 time steps!\n"
          ]
        }
      ],
      "source": [
        "# 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/.mujoco/mujoco210/bin' \n",
        "# mkdir ~/.mujoco\n",
        "# wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O mujoco.tar.gz\n",
        "# rm mujoco.tar.gz\n",
        "# pip install mujoco-py\n",
        "# pip install gymnasium[mujoco]\n",
        "# pip install gymnasium[classic-control]\n",
        "# apt-get install libglew-dev patchelf libosmesa6-dev libgl1-mesa-glx\n",
        "# apt-get install -y xvfb python-opengl \n",
        "# xvfb-run -a -s \"-screen 0 1400x900x24\" bash\n",
        "\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch import optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super(Policy, self).__init__()\n",
        "        self.n_inputs = env.observation_space.shape[0]\n",
        "        self.n_outputs = env.action_space.n\n",
        "        \n",
        "        self.affine1 = nn.Linear(self.n_inputs, 128)\n",
        "        self.dropout = nn.Dropout(p=0.6)\n",
        "        self.affine2 = nn.Linear(128, self.n_outputs)\n",
        "\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.affine1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(x)\n",
        "        action_scores = self.affine2(x)\n",
        "        return F.softmax(action_scores, dim=1)\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        state = torch.from_numpy(np.array(state)).float().unsqueeze(0)\n",
        "        probs = self.forward(state)\n",
        "        m = torch.distributions.Categorical(probs)\n",
        "        action = m.sample()\n",
        "        self.saved_log_probs.append(m.log_prob(action))\n",
        "        return action.item()\n",
        "\n",
        "#RL environment parameters\n",
        "gamma = 0.95\n",
        "seed = 0\n",
        "render = False\n",
        "log_interval = 10\n",
        "\n",
        "#Set up \n",
        "\n",
        "# env = gym.make(\"ALE/SpaceInvaders-ram-v5\")\n",
        "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
        "reward_threshold = env.spec.reward_threshold\n",
        "print(reward_threshold)\n",
        "env.reset(seed=seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "#Create out policy Network\n",
        "policy = Policy(env)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
        "eps = np.finfo(np.float32).eps.item()\n",
        "\n",
        "# env.reset()\n",
        "# x = env.render()\n",
        "# print(x)\n",
        "\n",
        "def finish_episode():\n",
        "    R = 0\n",
        "    policy_loss = []\n",
        "    returns = []\n",
        "    for r in policy.rewards[::-1]:\n",
        "        R = r + gamma * R\n",
        "        returns.insert(0, R)\n",
        "    returns = torch.tensor(returns)\n",
        "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
        "        policy_loss.append(-log_prob * R)\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss = torch.cat(policy_loss).sum()\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "    del policy.rewards[:]\n",
        "    del policy.saved_log_probs[:]\n",
        "\n",
        "from itertools import count\n",
        "def reinforce():\n",
        "    running_reward = 10\n",
        "    for i_episode in count(1):\n",
        "        (state, info), ep_reward = env.reset(), 0\n",
        "        # print('Initial State', state)\n",
        "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
        "            action = policy.select_action(state)\n",
        "            state, reward, done, truncated, info = env.step(action)\n",
        "            # print('New State', state)\n",
        "            if render:\n",
        "                env.render()\n",
        "            policy.rewards.append(reward)\n",
        "            ep_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # calculate reward\n",
        "        # It accepts a list of rewards for the whole episode and needs to calculate \n",
        "        # the discounted total reward for every step. To do this efficiently,\n",
        "        # we calculate the reward from the end of the local reward list.\n",
        "        # The last step of the episode will have the total reward equal to its local reward.\n",
        "        # The step before the last will have the total reward of ep_reward + gamma * running_reward\n",
        "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
        "        finish_episode()\n",
        "        if i_episode % log_interval == 0:\n",
        "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
        "                  i_episode, ep_reward, running_reward))\n",
        "            \n",
        "        if running_reward > env.spec.reward_threshold: #475\n",
        "            print(\"Solved! Running reward is now {} and \"\n",
        "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
        "            break\n",
        "        \n",
        "reinforce()\n",
        "env.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sm3a9RC3QVrR"
      },
      "source": [
        "## Question 4 (20 points)\n",
        "\n",
        "Next, let's replace the policy network that is currently working with the fully observed MDP with a POMDP using only the image\n",
        "of the environment as the observation.\n",
        "\n",
        "If you completed Lab 12, you should already have an implementation of REINFORCE on Space Invaders that you can reuse.\n",
        "\n",
        "By default, CartPole will render at 600x400 resolution. We will want to downscale that, perhaps to 150x100, and stack subsequent\n",
        "frames, perhaps 4 of them, in order to provide some history information.\n",
        "\n",
        "Below, show a revision of your REINFORCE policy model that takes as input a stack of the four most recent downscaled grayscale\n",
        "images and outputs an action. The model should have an appropriate series of convolutions, one or more fully connected layers,\n",
        "and a linear/softmax layer that ouptuts an action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gym is an OpenAI toolkit for RL\n",
        "from gymnasium.spaces import Box\n",
        "from gymnasium.wrappers import FrameStack\n",
        "import gymnasium as gym\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.distributions import Categorical\n",
        "import torch.autograd as autograd \n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "from collections import namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "from utils import GrayScaleObservation, ResizeObservation, SkipFrame\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super(Policy, self).__init__()\n",
        "        # self.n_inputs = env.observation_space.shape[2]\n",
        "        self.n_outputs = env.action_space.n\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=2, stride=1)\n",
        "        \n",
        "        self.affine1 = nn.Linear(10240, 128)\n",
        "        self.dropout = nn.Dropout(p=0.6)\n",
        "        self.affine2 = nn.Linear(128, self.n_outputs)\n",
        "\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x / 255.0  # normalize pixel values\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        x = self.affine1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(x)\n",
        "        action_scores = self.affine2(x)\n",
        "        return F.softmax(action_scores, dim=1)\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        state = torch.from_numpy(np.array(state)).float().unsqueeze(0)\n",
        "        probs = self.forward(state)\n",
        "        m = torch.distributions.Categorical(probs)\n",
        "        action = m.sample()\n",
        "        self.saved_log_probs.append(m.log_prob(action))\n",
        "        return action.item(), probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "4FLyLTaNQVrb"
      },
      "outputs": [],
      "source": [
        "#RL environment parameters\n",
        "gamma = 0.95\n",
        "seed = 0\n",
        "render = False\n",
        "log_interval = 10\n",
        "\n",
        "env = gym.make(\"ALE/SpaceInvaders-v5\", render_mode=\"rgb_array\")\n",
        "# define a reward threshold\n",
        "reward_threshold = 250\n",
        "# register the reward threshold with the environment\n",
        "env.spec.reward_threshold = reward_threshold\n",
        "# env = gym.make(\"SpaceInvaders-v0\")\n",
        "env = FrameStack(ResizeObservation(GrayScaleObservation(SkipFrame(env, skip=4)), shape=(150,100)), num_stack=4)\n",
        "env.reset(seed=seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "#Create out policy Network\n",
        "policy = Policy(env)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
        "eps = np.finfo(np.float32).eps.item()    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vkPSsufQQVrg"
      },
      "source": [
        "Next, demonstrate that your policy model when given a 4x100x150 tensor of zeros outputs, the policy model outputs an appropriate\n",
        "shaped vector representing a multinomial distribution over the action space."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K2mN9tvKQVrj"
      },
      "source": [
        "## Question 5 (20 points)\n",
        "\n",
        "Modify the `reinforce()` function to generate the visual input to the Policy model rather than the fully\n",
        "observed state.\n",
        "\n",
        "In your code, after each of the following lines\n",
        "\n",
        "    state, ep_reward = env.reset(), 0\n",
        "    \n",
        "    ...\n",
        "    \n",
        "        state, reward, done, _ = env.step(action)\n",
        "    \n",
        "please add code to replace the fully observed state with the observation:\n",
        "\n",
        "    obs_t = env.render(mode=\"rgb_array\")\n",
        "    obs_seq.append(obs_t)\n",
        "    state = make_observation(obs_seq)\n",
        "\n",
        "You'll have to add some code to initialize the `obs_seq` array appropriately and write the `make_observation()` function\n",
        "to convert the four most recent observations to grayscale and stack them in a tensor that your policy\n",
        "network can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def finish_episode():\n",
        "    R = 0\n",
        "    policy_loss = []\n",
        "    returns = []\n",
        "    for r in policy.rewards[::-1]:\n",
        "        R = r + gamma * R\n",
        "        returns.insert(0, R)\n",
        "    returns = torch.tensor(returns)\n",
        "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
        "        policy_loss.append(-log_prob * R)\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss = torch.cat(policy_loss).sum()\n",
        "    policy_loss.backward()\n",
        "    optimizer.step()\n",
        "    del policy.rewards[:]\n",
        "    del policy.saved_log_probs[:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 10\tLast reward: 285.00\tAverage reward: 119.22\n",
            "Episode 20\tLast reward: 285.00\tAverage reward: 180.98\n",
            "Episode 30\tLast reward: 180.00\tAverage reward: 210.65\n",
            "Episode 40\tLast reward: 285.00\tAverage reward: 240.48\n",
            "Solved! Running reward is now 250.55427543651658 and the last episode runs to 182 time steps!\n"
          ]
        }
      ],
      "source": [
        "# Place revised reinforce() code here\n",
        "from itertools import count\n",
        "def reinforce():\n",
        "    running_reward = 10\n",
        "    for i_episode in count(1):\n",
        "        (state, info), ep_reward = env.reset(), 0\n",
        "        # print('Initial State', state)\n",
        "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
        "            action, _ = policy.select_action(state)\n",
        "            state, reward, done, truncated, info = env.step(action)\n",
        "            # print('New State', state)\n",
        "            if render:\n",
        "                env.render()\n",
        "            policy.rewards.append(reward)\n",
        "            ep_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # calculate reward\n",
        "        # It accepts a list of rewards for the whole episode and needs to calculate \n",
        "        # the discounted total reward for every step. To do this efficiently,\n",
        "        # we calculate the reward from the end of the local reward list.\n",
        "        # The last step of the episode will have the total reward equal to its local reward.\n",
        "        # The step before the last will have the total reward of ep_reward + gamma * running_reward\n",
        "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
        "        finish_episode()\n",
        "        if i_episode % log_interval == 0:\n",
        "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
        "                  i_episode, ep_reward, running_reward))\n",
        "\n",
        "        if running_reward >= env.spec.reward_threshold:\n",
        "            print(\"Solved! Running reward is now {} and \"\n",
        "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
        "            break\n",
        "\n",
        "reinforce()\n",
        "env.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GEHYFO_QQVrm"
      },
      "source": [
        "Show that the resulting policy model can be trained for a few episodes. You don't have to train the model to perfection -- you can do it on your own PC in CPU mode and just\n",
        "show that policy model is learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxX76wgwQVrm"
      },
      "outputs": [],
      "source": [
        "# Code to train for a few episodes goes here\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hRbMMkIcQVrn"
      },
      "source": [
        "## Question 6 (10 points)\n",
        "\n",
        "What are the major differences between this visual REINFORCE method and Mnih et al.'s DQN method?\n",
        "\n",
        "*Your answer goes here.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The main differences between visual REINFORCE and Mnih et al.'s DQN (Deep Q-Network) method are the use of a Q-network and experience replay.\n",
        "\n",
        "In visual REINFORCE, the policy is directly optimized using the REINFORCE algorithm with stochastic gradient ascent on the policy parameters. The agent takes actions based on the current policy and observes the corresponding rewards. There is no notion of a value function, and the agent updates the policy directly based on the observed rewards. In contrast, in DQN, the agent learns a Q-function that estimates the expected cumulative reward of taking a particular action in a particular state. The agent uses an epsilon-greedy policy to select actions based on the Q-function estimates, and updates the Q-function using temporal-difference (TD) learning.\n",
        "\n",
        "Another key difference is the use of experience replay in DQN. Experience replay allows the agent to learn from a batch of randomly sampled experiences, rather than learning from experiences as they are encountered. This can improve the efficiency and stability of the learning process, and reduce the correlations between subsequent updates. In visual REINFORCE, there is no explicit use of experience replay, as the agent updates the policy parameters based on the current trajectory.\n",
        "\n",
        "Furthermore, DQN is typically used in discrete action spaces, where the agent selects an action from a finite set of options, whereas visual REINFORCE is often used in continuous action spaces, where the agent must select an action from a continuous range of options. To handle continuous action spaces, visual REINFORCE often uses a Gaussian policy that outputs mean and variance parameters for a normal distribution over the action space, which are sampled to select actions.\n",
        "\n",
        "In summary, the main differences between visual REINFORCE and DQN are the use of a Q-network and experience replay in DQN, and the direct optimization of the policy using REINFORCE in visual REINFORCE. Visual REINFORCE is often used in continuous action spaces and uses a Gaussian policy, while DQN is typically used in discrete action spaces and uses an epsilon-greedy policy."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k3W_1BVoQVro"
      },
      "source": [
        "## Question 7 (10 points)\n",
        "\n",
        "What are the major differences between this visual REINFORCE method and the A2C (Advantage-Actor-Critic) method? In your answer, assume we make the same modification to A2C to use visual observations instead of full state observations.\n",
        "\n",
        "*Your answer goes here.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txgzPlKIQVro"
      },
      "source": [
        "The main difference between visual REINFORCE and A2C (Advantage-Actor-Critic) when using visual observations is the use of a critic network to estimate the value function. \n",
        "\n",
        "In visual REINFORCE, the policy network directly learns to maximize the expected return by adjusting its parameters using the REINFORCE algorithm. It does not use a value function to estimate the expected reward of being in a particular state. Instead, it uses the observed rewards as a baseline to adjust the policy gradient. This method is simpler and easier to implement, but it may be less efficient in terms of convergence speed and stability.\n",
        "\n",
        "On the other hand, A2C combines the REINFORCE algorithm with a critic network that estimates the value function. The critic network provides an estimate of the expected reward of being in a particular state, which is used to calculate the advantage function. The advantage function is then added to the log-probability of the selected action in the policy gradient update to reduce the variance of the gradient estimates. The use of the critic network makes the A2C algorithm more sample-efficient and less sensitive to the choice of hyperparameters. \n",
        "\n",
        "When using visual observations, the main modification to A2C is to replace the input state with a visual input that is processed by a convolutional neural network (CNN) to extract relevant features. The CNN is typically shared between the policy and the value networks to reduce computation and improve generalization.\n",
        "\n",
        "In summary, the main difference between visual REINFORCE and A2C is the use of a critic network to estimate the value function in A2C. The use of a value function makes A2C more sample-efficient and stable, but also more complex to implement and tune."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
