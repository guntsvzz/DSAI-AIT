{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTML Final 2021\n",
    "\n",
    "In this exam, we'll have some practical exercises using RNNs and some short answer questions regarding the Transformer/attention\n",
    "and reinforcement learning.\n",
    "\n",
    "Consider the AGNews text classification dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label frequencies: {3: 30000, 4: 30000, 2: 30000, 1: 30000}\n",
      "A few token frequencies: [('.', 225971), ('the', 205040), (',', 165685), ('to', 119817), ('a', 110942)]\n",
      "Label meanings: 1: World news, 2: Sports news, 3: Business news, 4: Sci/Tech news\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "counter = Counter()\n",
    "\n",
    "def clean(line):\n",
    "    line = line.replace('\\\\', ' ')\n",
    "    return line\n",
    "\n",
    "labels = {}\n",
    "for (label, line) in train_iter:\n",
    "    if label in labels:\n",
    "        labels[label] += 1\n",
    "    else:\n",
    "        labels[label] = 1\n",
    "    counter.update(tokenizer(clean(line)))\n",
    "\n",
    "vocab = Vocab(counter, min_freq=1)\n",
    "\n",
    "print('Label frequencies:', labels)\n",
    "print('A few token frequencies:', vocab.freqs.most_common(5))\n",
    "print('Label meanings: 1: World news, 2: Sports news, 3: Business news, 4: Sci/Tech news')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we can get a sequence of tokens for a sentence with the cleaner, tokenizer, and vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4248, 4, 116, 3, 244, 46857, 4, 23, 62, 7, 3, 812, 2009, 7, 989]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab[token] for token in tokenizer(clean('Bangkok, or The Big Mango, is one of the great cities of Asia'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make pipelines for processing a news story and a label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(clean(x))]\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to create dataloaders for the training and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, length_list = [], [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        length_list.append(processed_text.shape[0])\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = pad_sequence(text_list, padding_value=0)\n",
    "    length_list = torch.tensor(length_list, dtype=torch.int64)\n",
    "    return label_list.to(device), text_list.to(device), length_list.to(device)\n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "train_dataset = list(train_iter)\n",
    "test_iter = AG_NEWS(split='test')\n",
    "test_dataset = list(test_iter)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to get a batch from one of these dataloaders. The first entry is a 1D tensor of labels for the batch\n",
    "(8 values between 0 and 3), then a 2D tensor representing the stories with dimension T x B (number of tokens x batch size). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, (tensor([2, 0, 2, 1, 3, 3, 3, 0]), tensor([[ 1391,   408,  8399, 16990,   571,    24,   245, 62687],\n",
      "        [  296,   931,   779,  1090, 29701,  1847,   645,  3263],\n",
      "        [ 1391,   400,     7,    48, 12503,  6257,   590,    23],\n",
      "        [  545,   921,  1936,    20,  1627,  6034,  9597,  3153],\n",
      "        [  835,     8, 10911, 22148, 39312,  5560,   965, 20425],\n",
      "        [   30,   322,   988,  2462,   180,  2923,     4,   401],\n",
      "        [34522,     4,   229, 43097,  1092,  1847,   522,   186],\n",
      "        [  122,  1093,   439, 16667,     4,    88,    85,   768],\n",
      "        [ 1048,     2,    14,     8,  2611,     2,  2043,   502],\n",
      "        [  296,   192,    28,  6253,  2902,    13,   116,  1557],\n",
      "        [ 1048,  3652,    15,  1701,     2,    10,    47,    77],\n",
      "        [  136, 12917,    16,    42, 20602,    24,    98,  5871],\n",
      "        [    6,     4,  1931,   167,  3116, 25746,     7,     2],\n",
      "        [ 4696,   322,   462,     2,    12,  4736,   245,  2606],\n",
      "        [    2,  1073,     6,     2,     3,  6257,   286,  1636],\n",
      "        [ 9173,    14,  9410,     2,   428,     4,   382,     4],\n",
      "        [   14,    28,  1936,    42,   371,  1648,  2153,  1460],\n",
      "        [  734,    15,  8177,   166,  2355,    11,     3,   477],\n",
      "        [    2,    16,   383,  5031,     4,    58,   965,     4],\n",
      "        [10089,   324,    64, 22148,  6587,     4,     7, 28898],\n",
      "        [   15,   353,  1064,  2462,   706,    43,  1700,     5],\n",
      "        [  122,   143,   621,   215,   587,   905, 27618,     6],\n",
      "        [   75,   365,    27,   579,    31,  1174, 29092,  7814],\n",
      "        [    5,   921,    11,   611,   298,    18,     4,    99],\n",
      "        [  277,   333,    58,  6985,   180,    37,     6,    21],\n",
      "        [ 4162,    95,    68,  1159,   194,   852, 10494,     3],\n",
      "        [ 1783,  7852,   923, 27730,  4321,   237,  9597,  9761],\n",
      "        [    4,    11,     6, 16990,    84,    40,    11,     9],\n",
      "        [ 1103,    57,   988,     3,   801,   717,     3,  6590],\n",
      "        [    3,    20,    64,    48,     5,     5, 25361,  3728],\n",
      "        [ 6621,    68,  8399,   448, 34983,  5597,  8677,     8],\n",
      "        [   55,  9754,  4116,  4550,    39,     3,     4,   239],\n",
      "        [    6,  2167,    88,    65,   104, 10380,   210,     4],\n",
      "        [  751,    67,     2,     8, 39312,  1310,     5,   474],\n",
      "        [    8,   322,    14,     3,   471,    25,     6,    92],\n",
      "        [    3,     5,   795,   197,     5,   319,   522,     4],\n",
      "        [ 6681,  8992,     2,   176,  3876,    88,   346,  1318],\n",
      "        [  127,  1183,   374,   918,     2,     2,    11,     2],\n",
      "        [   12,   727,   491,  1701,   298,     0,    57,   249],\n",
      "        [  879,    67,     4,  2051,  4321,     0,    25,     4],\n",
      "        [   18,   408,   563,     2,    10,     0,  3000,   141],\n",
      "        [ 8342,     9,     4,     0, 10323,     0,    13,     2],\n",
      "        [21157,  5839,   208,     0, 32233,     0,    10,     0],\n",
      "        [    2,    49,    15,     0,  1958,     0, 26264,     0],\n",
      "        [    0,   332,     4,     0,    14,     0,  3022,     0],\n",
      "        [    0,   192,  2478,     0,  4458,     0,     2,     0],\n",
      "        [    0,  2009,     3,     0,  3327,     0,     0,     0],\n",
      "        [    0,     8,    55,     0,    19,     0,     0,     0],\n",
      "        [    0,   177,  1441,     0,    24,     0,     0,     0],\n",
      "        [    0,     7,   510,     0, 12302,     0,     0,     0],\n",
      "        [    0,  1337,    61,     0,    15,     0,     0,     0],\n",
      "        [    0,   535,     7,     0,    23,     0,     0,     0],\n",
      "        [    0,     2,   132,     0,   131,     0,     0,     0],\n",
      "        [    0,     0,     2,     0,    68,     0,     0,     0],\n",
      "        [    0,     0,     2,     0,  4321,     0,     0,     0],\n",
      "        [    0,     0,     2,     0,  1693,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,   801,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     2,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,   334,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,   123,     0,     0,     0],\n",
      "        [    0,     0,     0,     0, 17192,     0,     0,     0],\n",
      "        [    0,     0,     0,     0, 29701,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     8,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     3,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,  1958,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,  4321,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,    10,     0,     0,     0],\n",
      "        [    0,     0,     0,     0, 22304,     0,     0,     0],\n",
      "        [    0,     0,     0,     0, 45724,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,   471,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,   116,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,   284,     0,     0,     0],\n",
      "        [    0,     0,     0,     0, 29701,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,    30,     0,     0,     0],\n",
      "        [    0,     0,     0,     0, 37333,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,    67,     0,     0,     0],\n",
      "        [    0,     0,     0,     0, 45724,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,    12,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,  5543,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,  2581,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     6,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,  2942,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     2,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,    12,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,    47,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,  3304,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     4,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     3,     0,     0,     0],\n",
      "        [    0,     0,     0,     0, 67829,     0,     0,     0],\n",
      "        [    0,     0,     0,     0, 27228,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,   108,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,   123,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,  1842,     0,     0,     0],\n",
      "        [    0,     0,     0,     0, 74478,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     9,     0,     0,     0],\n",
      "        [    0,     0,     0,     0, 34316,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,  2318,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,     2,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,  6588,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,  1154,     0,     0,     0],\n",
      "        [    0,     0,     0,     0,   788,     0,     0,     0]]), tensor([ 44,  53,  56,  41, 101,  38,  46,  42])))\n"
     ]
    }
   ],
   "source": [
    "batch = next(enumerate(train_dataloader))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1, 10 points\n",
    "\n",
    "The vocabulary currently is too large for a simple one-hot embedding. Let's reduce the vocabulary size\n",
    "so that we can use one-hot. First, add a step that removes tokens from a list of \"stop words\" to the `text_pipeline` function.\n",
    "You probably want to remove punctuation ('.', ',', '-', etc.) and articles (\"a\", \"the\").\n",
    "\n",
    "Once you've removed stop words, modify the vocabulary to include only the most frequent 1000 tokens (including 0 for an unknown/infrequent word).\n",
    "\n",
    "Write your revised code in the cell below and output the 999 top words with their frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place code for Question 1 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2, 30 points\n",
    "\n",
    "Next, let's build a simple RNN for classification of the AGNews dataset. Use a one-hot embedding of the vocabulary\n",
    "entries and the basic RNN from Lab 10. Use the lengths tensor (the third element in the batch returned by the dataloaders)\n",
    "to determine which output to apply the loss to.\n",
    "\n",
    "Place your training code below, and plot the training and test accuracy as a\n",
    "function of epoch. Finally, output a confusion matrix for the test set.\n",
    "\n",
    "*Do not spend a lot of time on the training! A few minutes is enough. The point is to show that the model is\n",
    "learning, not to get the best possible performance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place code for Question 2 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3, 10 points\n",
    "\n",
    "Next, replace the SRNN from Question 2 with a single-layer LSTM. Give the same output (training and testing accuracy as a function of epoch, as well as confusion\n",
    "matrix for the test set). Comment on the differences you observe between the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place code for Question 3 here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4, 10 points\n",
    "\n",
    "Explain how you could use the Transformer model to perform the same task you explored in Questions 2 and 3.\n",
    "How would attention be useful for this text classification task? Give a precise and detailed answer. Be sure to discuss what\n",
    "parts of the original Transformer you would use and what you would have to remove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5, 10 points\n",
    "\n",
    "In Lab 13, you implemented a DQN model for tic-tac-toe. You method learned to play against a fairly dumb `expert_action` opponent, however.  Also,\n",
    "DQN has proven to be less stable than other methods such as Double DQN, also discussed in Lab 13.\n",
    "\n",
    "Explain below how you would apply double DQN and self-play to improve your tic-tac-toe agent.\n",
    "Provide pseudocode for the algorithm below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your explanation and pseudocode here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6, 30 points\n",
    "\n",
    "Based on your existing DQN implementation, implement the double DQN and self-play training method\n",
    "you just described. After some training (don't spend too much time on training -- again, we just want to see that the model can\n",
    "learn), show the result you playing a game against your learned agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for training and playing goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
