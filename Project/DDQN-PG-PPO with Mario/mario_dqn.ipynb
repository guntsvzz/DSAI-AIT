{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mario with Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.26.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from tqdm import trange\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import random, datetime, os, copy\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python3.10.4\\lib\\site-packages\\gym\\envs\\registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action : 7\n"
     ]
    }
   ],
   "source": [
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT,COMPLEX_MOVEMENT \n",
    "# env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v3\", render_mode='rgb_array', apply_api_compatibility=True)\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "n_actions = env.action_space.n\n",
    "print(\"Action :\",n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((240, 256, 3), {})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs,info = env.reset()\n",
    "obs.shape, info #3 dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Gym is worked or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# done = True\n",
    "# for step in range(500):\n",
    "#     if done:\n",
    "#         next_state = env.reset()\n",
    "#     next_state, reward, done,_, info = env.step(env.action_space.sample())\n",
    "#     env.render()\n",
    "#     #img=env.render(mode=\"rgb_array\")\n",
    "    \n",
    "# env.close()\n",
    "# print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grey Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NES Emulator for OpenAI Gym\n",
    "# from nes_py.wrappers import JoypadSpace\n",
    "# # Super Mario environment for OpenAI Gym\n",
    "# import gym_super_mario_bros\n",
    "# from gym_super_mario_bros.actions import SIMPLE_MOVEMENT,COMPLEX_MOVEMENT \n",
    "# # env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "# env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v3\", render_mode='rgb_array', apply_api_compatibility=True)\n",
    "# env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "# n_actions = env.action_space.n\n",
    "# print(\"Action :\",n_actions)\n",
    "\n",
    "# obs,info = env.reset()\n",
    "# obs.shape, info #3 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 84, 84), {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# env = gym_super_mario_bros.make('SuperMarioBros-1-1-v3')\n",
    "# env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "env = FrameStack(ResizeObservation(GrayScaleObservation(SkipFrame(env, skip=4)), shape=84), num_stack=4)\n",
    "# env.seed(42)\n",
    "# env.action_space.seed(42)\n",
    "# torch.manual_seed(42)\n",
    "# torch.random.manual_seed(42)\n",
    "# np.random.seed(42)\n",
    "\n",
    "obs,info = env.reset()\n",
    "obs.shape, info #3 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "\n",
    "def gen_epsilon_greedy_policy(epsilon):\n",
    "    def policy_function(state, Q, available_actions):\n",
    "        probs = torch.ones(len(available_actions)) * epsilon / len(available_actions)\n",
    "        best_action = Q(state).squeeze()[available_actions].argmax().item()\n",
    "        probs[best_action] += 1.0 - epsilon\n",
    "        action = torch.multinomial(probs,1).item()\n",
    "        return available_actions[action]\n",
    "    return policy_function\n",
    "\n",
    "class Q_network(nn.Module):\n",
    "    def __init__(self, n_action):\n",
    "        super(Q_network, self).__init__()        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(4, 16, 3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d((12,9)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, n_action)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return torch.stack(state).squeeze(),  torch.tensor(action), torch.tensor(reward), torch.stack(next_state).squeeze(), torch.tensor(done).squeeze()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def gen_eps_by_episode(epsilon_start, epsilon_final, epsilon_decay):\n",
    "    def eps_by_episode(episode):\n",
    "        return epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * episode / epsilon_decay)\n",
    "    return eps_by_episode\n",
    "\n",
    "class Agent(object):\n",
    "\n",
    "    def __init__(self, env, eps_by_episode):\n",
    "        self.env = env\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.eps_by_episode = eps_by_episode\n",
    "        self.episodes = 0\n",
    "        self.stack_size = 4\n",
    "        self.skip_frames = 4\n",
    "        self.restart_episode()\n",
    "\n",
    "    def buffer(self,obs):\n",
    "        obs = torch.tensor(obs).max(2).values / 255.0\n",
    "        self.obs_buffer.append(obs)\n",
    "\n",
    "    def restart_episode(self):\n",
    "        obs, info = self.env.reset()\n",
    "        self.policy = gen_epsilon_greedy_policy(self.eps_by_episode(self.episodes))\n",
    "        self.obs_buffer = deque(maxlen=((self.stack_size-1)*self.skip_frames+1))\n",
    "        self.buffer(obs.copy())\n",
    "        self.episodes += 1\n",
    "\n",
    "    def collect_state(self):\n",
    "        frame_inds = [-1-n*self.skip_frames for n in range(self.stack_size)]\n",
    "        frames = [self.obs_buffer[max(f,-len(self.obs_buffer))] for f in frame_inds]\n",
    "        return torch.stack(frames)\n",
    "\n",
    "    def act(self,Q):\n",
    "        state = self.collect_state().unsqueeze(0)\n",
    "        action = self.policy(state,Q,range(self.n_actions))\n",
    "        obs, reward, terminated , truncated, info = self.env.step(action)\n",
    "        self.buffer(obs.copy())\n",
    "        new_state = self.collect_state().squeeze(0)\n",
    "        if terminated or truncated:\n",
    "            self.restart_episode()\n",
    "        return state, action, reward, new_state, terminated or truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.548066\n",
      "Loss 0.547704\n",
      "Loss 0.509695\n",
      "Loss 0.521695\n",
      "Loss 0.509414\n",
      "Loss 0.463629\n",
      "Loss 0.439879\n",
      "Loss 0.371639\n",
      "Loss 0.315486\n",
      "Loss 0.322882\n",
      "Loss 0.394075\n",
      "Loss 0.576082\n",
      "Loss 0.676571\n",
      "Loss 0.722980\n",
      "Loss 0.548906\n",
      "Loss 0.377518\n",
      "Loss 0.287506\n",
      "Loss 0.331154\n",
      "Loss 0.351134\n",
      "Loss 0.387616\n",
      "Loss 0.515805\n",
      "Loss 0.558465\n",
      "Loss 0.562184\n",
      "Loss 0.706646\n",
      "Loss 0.720281\n",
      "Loss 0.697624\n",
      "Loss 0.742142\n",
      "Loss 0.690922\n",
      "Loss 0.730727\n",
      "Loss 0.857787\n",
      "Loss 0.790777\n",
      "Loss 0.790850\n",
      "Loss 0.754285\n",
      "Loss 0.782667\n",
      "Loss 0.909529\n",
      "Loss 0.881262\n",
      "Loss 0.952283\n",
      "Loss 0.712549\n",
      "Loss 0.773670\n",
      "Loss 0.805689\n",
      "Loss 0.791226\n",
      "Loss 0.807567\n",
      "Loss 0.877517\n",
      "Loss 0.835518\n",
      "Loss 1.063218\n",
      "Loss 0.957944\n",
      "Loss 0.964997\n",
      "Loss 1.056380\n",
      "Loss 0.914605\n",
      "Loss 0.889779\n",
      "Loss 0.903740\n",
      "Loss 0.801558\n",
      "Loss 1.049876\n",
      "Loss 0.988252\n",
      "Loss 0.980865\n",
      "Loss 0.954669\n",
      "Loss 0.879528\n",
      "Loss 1.110577\n",
      "Loss 0.878751\n",
      "Loss 0.922700\n",
      "Loss 0.819126\n",
      "Loss 0.847833\n",
      "Loss 1.028753\n",
      "Loss 0.907391\n",
      "Loss 0.917524\n",
      "Loss 0.828879\n",
      "Loss 0.841741\n",
      "Loss 0.905755\n",
      "Loss 0.956233\n",
      "Loss 0.809621\n",
      "Loss 0.671502\n",
      "Loss 0.797778\n",
      "Loss 0.631400\n",
      "Loss 0.632031\n",
      "Loss 0.660491\n",
      "Loss 0.583964\n",
      "Loss 0.653023\n",
      "Loss 0.440338\n",
      "Loss 0.508218\n",
      "Loss 0.511009\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\ML_Project\\mario_dqn.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_dqn.ipynb#X12sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs,targets)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_dqn.ipynb#X12sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mLoss \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m loss)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_dqn.ipynb#X12sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_dqn.ipynb#X12sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m optimzer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_dqn.ipynb#X12sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mif\u001b[39;00m (step\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m C \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m :\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import copy\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "bufsize = 10000\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 1000\n",
    "batch_size = 64\n",
    "gamma = 0.99\n",
    "\n",
    "Q = Q_network(n_actions).to(device)\n",
    "Qhat = Q_network(n_actions).to(device)\n",
    "D = ReplayBuffer(bufsize)\n",
    "eps_by_episode = gen_eps_by_episode(epsilon_start,epsilon_final,epsilon_decay)\n",
    "A = Agent(env, eps_by_episode)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimzer = torch.optim.Adam(Q.parameters())\n",
    "\n",
    "Q.train()\n",
    "Qhat.eval()\n",
    "step = 0\n",
    "C = 1000\n",
    "episode_rewards = 0\n",
    "episodes_rewards = []\n",
    "\n",
    "while True:\n",
    "    state, aciton, reward, next_state, done = A.act(Q)\n",
    "    # print(next_state.shape)\n",
    "    # plt.imshow(torch.permute(next_state,(1,2,0)))\n",
    "    # display.display(plt.gcf())    \n",
    "    # display.clear_output(wait=True)\n",
    "    episode_rewards += reward\n",
    "    if done:\n",
    "        print('Done :',done)\n",
    "        episodes_rewards.append(episode_rewards)\n",
    "        episode_rewards = 0\n",
    "        display.clear_output(wait=True)\n",
    "        plt.plot(episodes_rewards)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "    D.push(state, aciton, reward, next_state, done)\n",
    "    if len(D) >= batch_size:\n",
    "        states, acitons, rewards, next_states, dones = D.sample(batch_size)\n",
    "        outputs = Q(states).gather(-1,acitons.unsqueeze(1)).squeeze()\n",
    "        targets = rewards + gamma*Qhat(next_states).max(1).values.detach() * dones\n",
    "        loss = loss_fn(outputs,targets)\n",
    "        print('Loss %f' % loss)\n",
    "        loss.backward()\n",
    "        optimzer.step()\n",
    "        if (step+1) % C == 0 :\n",
    "            Qhat = copy.deepcopy(Q)\n",
    "            Qhat.eval()\n",
    "        step += 1\n",
    "    # break #delete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
