{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lillicrap, Timothy P., et al. \"Continuous control with deep reinforcement learning.\" arXiv preprint arXiv:1509.02971 (2015).\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import random, datetime, os, copy\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from collections import deque\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT,COMPLEX_MOVEMENT \n",
    "# env = gym_super_mario_bros.make('SuperMarioBros-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size=100000):\n",
    "        super(ReplayBuffer, self).__init__()\n",
    "        self.max_size = max_size\n",
    "        self.memory = deque(maxlen=self.max_size)\n",
    "        \n",
    "    # Add the replay memory\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # Sample the replay memory\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.memory, min(batch_size, len(self.memory)))\n",
    "        states, actions, rewards, next_states, dones = map(np.stack, zip(*batch))\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, state_num, action_num, min_action, max_action, bn=False):\n",
    "        super(ActorNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, action_num),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.00025, eps=1e-4)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "\n",
    "        action = self.output(x)\n",
    "        action = torch.clamp(action, self.min_action, self.max_action)\n",
    "        return action\n",
    "        \n",
    "        # self.input = nn.Linear(state_num, 256)\n",
    "        # self.fc = nn.Linear(256, 512)\n",
    "        # self.output = nn.Linear(512, action_num)\n",
    "        \n",
    "        # # Batch normalization\n",
    "        # self.bn = bn\n",
    "        # self.bn1 = nn.BatchNorm1d(256)\n",
    "        # self.bn2 = nn.BatchNorm1d(512)\n",
    "        \n",
    "        # # Get the action interval for clipping\n",
    "        # self.min_action = min_action\n",
    "        # self.max_action = max_action\n",
    "    \n",
    "    # def forward(self, x):\n",
    "    #     if self.bn:\n",
    "    #         x = F.relu(self.bn1(self.input(x)))\n",
    "    #         x = F.relu(self.bn2(self.fc(x)))\n",
    "    #     else:\n",
    "    #         x = F.relu(self.input(x))\n",
    "    #         x = F.relu(self.fc(x))\n",
    "    \n",
    "        action = self.output(x)\n",
    "        action = torch.clamp(action, self.min_action, self.max_action)\n",
    "        return action\n",
    "\n",
    "class CriticNet(nn.Module):\n",
    "    def __init__(self, state_num, action_num, bn=False):\n",
    "        super(CriticNet, self).__init__()\n",
    "        self.input = nn.Linear(state_num + action_num, 256)\n",
    "        self.fc = nn.Linear(256, 512)\n",
    "        self.output = nn.Linear(512, 1)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.bn = bn\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "    \n",
    "    def forward(self, x, u):\n",
    "        x = torch.cat([x, u], 1)\n",
    "        \n",
    "        if self.bn:\n",
    "            x = F.relu(self.bn1(self.input(x)))\n",
    "            x = F.relu(self.bn2(self.fc(x)))\n",
    "        else:\n",
    "            x = F.relu(self.input(x))\n",
    "            x = F.relu(self.fc(x))\n",
    "        \n",
    "        value = self.output(x)\n",
    "        return value\n",
    "    \n",
    "class DDPG():\n",
    "    def __init__(self, env, memory_size=10000000, batch_size=64, tau=0.01, gamma=0.95, learning_rate=1e-3, eps_min=0.05, eps_period=10000, bn=False):\n",
    "        super(DDPG, self).__init__()\n",
    "        self.env = env\n",
    "        self.state_num = self.env.observation_space.shape[0]\n",
    "        self.action_num = self.env.action_space.n\n",
    "        self.action_max = float(6)\n",
    "        self.action_min = float(0)\n",
    "        #self.action_max = float(env.action_space.high[0])\n",
    "        #self.action_min = float(env.action_space.low[0])\n",
    "                \n",
    "        # Torch\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Actor\n",
    "        self.actor_net = ActorNet(self.state_num, self.action_num, self.action_min, self.action_max, bn).to(self.device)\n",
    "        self.actor_opt = optim.Adam(self.actor_net.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Target Actor\n",
    "        self.actor_target_net = ActorNet(self.state_num, self.action_num, self.action_min, self.action_max, bn).to(self.device)\n",
    "        self.actor_target_net.load_state_dict(self.actor_net.state_dict())\n",
    "        \n",
    "        # Critic\n",
    "        self.critic_net = CriticNet(self.state_num, self.action_num, bn).to(self.device)\n",
    "        self.critic_opt = optim.Adam(self.critic_net.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Target Critic\n",
    "        self.critic_target_net = CriticNet(self.state_num, self.action_num, bn).to(self.device)\n",
    "        self.critic_target_net.load_state_dict(self.critic_net.state_dict())\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(memory_size)\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Learning setting\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        # Noise setting\n",
    "        self.epsilon = 1\n",
    "        self.eps_min = eps_min\n",
    "        self.eps_period = eps_period\n",
    "\n",
    "    # Get the action\n",
    "    def get_action(self, state, exploration=True):\n",
    "        self.actor_net.eval()\n",
    "        # print(type(state))\n",
    "        # state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        # print(state.shape)\n",
    "        # action = self.actor_net(state).cpu().detach().numpy().flatten()\n",
    "        action_values = self.actor_net(torch.tensor(state.__array__()).unsqueeze(0).to(self.device)) \n",
    "        action = torch.argmax(action_values, dim=1).item()\n",
    "        print(action.shape)\n",
    "        self.actor_net.train()\n",
    "        \n",
    "        if exploration:\n",
    "            # Get noise (gaussian distribution with epsilon greedy)\n",
    "            action_mean = (self.action_max + self.action_min) / 2\n",
    "            action_std = (self.action_max - self.action_min) / 2\n",
    "            action_noise = np.random.normal(action_mean, action_std, 1)[0]\n",
    "            action_noise *= self.epsilon\n",
    "            self.epsilon = self.epsilon - (1 - self.eps_min) / self.eps_period if self.epsilon > self.eps_min else self.eps_min\n",
    "            \n",
    "            # Final action\n",
    "            action = action + action_noise\n",
    "            action = np.clip(action, self.action_min, self.action_max)\n",
    "            return action\n",
    "        \n",
    "        else:\n",
    "            return action\n",
    "\n",
    "    # Soft update a target network\n",
    "    def soft_update(self, net, target_net):\n",
    "        for param, target_param in zip(net.parameters(), target_net.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "    \n",
    "    # Learn the policy\n",
    "    def learn(self):\n",
    "        # Replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.FloatTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        # Target Q values\n",
    "        next_actions = self.actor_target_net(next_states)\n",
    "        target_q = self.critic_target_net(next_states, next_actions).view(1, -1)\n",
    "        target_q = (rewards + self.gamma * target_q * (1-dones))\n",
    "\n",
    "        # Current Q values\n",
    "        values = self.critic_net(states, actions).view(1, -1)\n",
    "        \n",
    "        # Calculate the critic loss and optimize the critic network\n",
    "        critic_loss = F.mse_loss(values, target_q)\n",
    "        self.critic_opt.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_opt.step()\n",
    "        \n",
    "        # Calculate the actor loss and optimize the actor network\n",
    "        actor_loss = -self.critic_net(states, self.actor_net(states)).mean()\n",
    "        self.actor_opt.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_opt.step()\n",
    "\n",
    "        # Soft update the target networks\n",
    "        self.soft_update(self.critic_net, self.critic_target_net)\n",
    "        self.soft_update(self.actor_net, self.actor_target_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((240, 256, 3), {})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v3\", render_mode='rgb-array', apply_api_compatibility=True)\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "\n",
    "obs,info = env.reset()\n",
    "obs.shape, info #3 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 84, 84), {})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = FrameStack(ResizeObservation(GrayScaleObservation(SkipFrame(env, skip=4)), shape=84), num_stack=4)\n",
    "obs,info = env.reset()\n",
    "obs.shape, info #3 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDPG(env, memory_size=100000, batch_size=64, tau=0.01, gamma=0.95, learning_rate=1e-3, eps_min=0.00001, eps_period=100000, bn=True)\n",
    "ep_rewards = deque(maxlen=1)\n",
    "total_episode = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (336x84 and 4x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\ML_Project\\mario_ddpg.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddpg.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m ep_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddpg.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddpg.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mget_action(state, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddpg.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     next_state, reward , done, _, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddpg.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     ep_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\ML_Project\\mario_ddpg.ipynb Cell 7\u001b[0m in \u001b[0;36mDDPG.get_action\u001b[1;34m(self, state, exploration)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddpg.ipynb#X11sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor_net\u001b[39m.\u001b[39meval()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddpg.ipynb#X11sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m \u001b[39m# print(type(state))\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddpg.ipynb#X11sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m \u001b[39m# state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddpg.ipynb#X11sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m \u001b[39m# print(state.shape)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddpg.ipynb#X11sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m \u001b[39m# action = self.actor_net(state).cpu().detach().numpy().flatten()\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddpg.ipynb#X11sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m action_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor_net(torch\u001b[39m.\u001b[39;49mtensor(state\u001b[39m.\u001b[39;49m__array__())\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)) \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddpg.ipynb#X11sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m action \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(action_values, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mitem()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddpg.ipynb#X11sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m \u001b[39mprint\u001b[39m(action\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\ML_Project\\mario_ddpg.ipynb Cell 7\u001b[0m in \u001b[0;36mActorNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddpg.ipynb#X11sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddpg.ipynb#X11sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddpg.ipynb#X11sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m         x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput(x)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddpg.ipynb#X11sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m         x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(x)))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/ML_Project/mario_ddpg.ipynb#X11sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (336x84 and 4x256)"
     ]
    }
   ],
   "source": [
    "for i in range(total_episode):\n",
    "    state, info = env.reset()\n",
    "    ep_reward = 0\n",
    "    while True:\n",
    "        action = agent.get_action(state, True)\n",
    "        next_state, reward , done, _, info = env.step(action)\n",
    "        ep_reward += reward\n",
    "\n",
    "        agent.replay_buffer.add(state, action, reward, next_state, done)\n",
    "        if i > 2:\n",
    "            agent.learn()\n",
    "        \n",
    "        if done:\n",
    "            ep_rewards.append(ep_reward)\n",
    "            if i % 1 == 0:\n",
    "                print(\"episode: {}\\treward: {}\".format(i, round(np.mean(ep_rewards), 3)))\n",
    "            break\n",
    "\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
