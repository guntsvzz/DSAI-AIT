{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips : Some of the text preprocessing techniques,\n",
    "\n",
    "- Tokenization\n",
    "- Lemmatization\n",
    "- Removing Punctuations and Stopwords\n",
    "- Part of Speech Tagging\n",
    "- Entity Recognition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Embracing', 'and', 'analyzing', 'self', 'failures', '(', 'of', 'however', 'multitude', ')', 'is', 'a', 'virtue', 'of', 'nobelmen', '.']\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import spacy\n",
    "#instantiating English module\n",
    "nlp = spacy.load(\"../Dependency Parsing Project/en_core_web_sm/en_core_web_sm-3.4.1\")\n",
    "#sample\n",
    "x = \"Embracing and analyzing self failures (of however multitude) is a virtue of nobelmen.\"\n",
    "#creating doc object containing our token features\n",
    "doc = nlp(x)\n",
    "#Creating and updating our list of tokens using list comprehension \n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <spacy.pipeline.sentencizer.Sentencizer object at 0x0000020665DEBA00> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Guntsv\\Documents\\GitHub\\DSAI-AIT-2022\\Project\\NLP\\Sentiment Analysis.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Project/NLP/Sentiment%20Analysis.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m sbd \u001b[39m=\u001b[39m nlp\u001b[39m.\u001b[39mcreate_pipe(\u001b[39m'\u001b[39m\u001b[39msentencizer\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Project/NLP/Sentiment%20Analysis.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Adding the component to the pipeline\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Project/NLP/Sentiment%20Analysis.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m nlp\u001b[39m.\u001b[39;49madd_pipe(sbd)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Project/NLP/Sentiment%20Analysis.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mEmbracing and analyzing self failures (of however multitude) is a virtue of nobelmen. And nobility is a treasure few possess.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Guntsv/Documents/GitHub/DSAI-AIT-2022/Project/NLP/Sentiment%20Analysis.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#creating doc object carring our sentence tokens\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python3.10.4\\lib\\site-packages\\spacy\\language.py:779\u001b[0m, in \u001b[0;36mLanguage.add_pipe\u001b[1;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[0;32m    777\u001b[0m     bad_val \u001b[39m=\u001b[39m \u001b[39mrepr\u001b[39m(factory_name)\n\u001b[0;32m    778\u001b[0m     err \u001b[39m=\u001b[39m Errors\u001b[39m.\u001b[39mE966\u001b[39m.\u001b[39mformat(component\u001b[39m=\u001b[39mbad_val, name\u001b[39m=\u001b[39mname)\n\u001b[1;32m--> 779\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(err)\n\u001b[0;32m    780\u001b[0m name \u001b[39m=\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m factory_name\n\u001b[0;32m    781\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcomponent_names:\n",
      "\u001b[1;31mValueError\u001b[0m: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <spacy.pipeline.sentencizer.Sentencizer object at 0x0000020665DEBA00> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"../Dependency Parsing Project/en_core_web_sm/en_core_web_sm-3.4.1\")\n",
    "#Creating the pipeline 'sentencizer' component\n",
    "sbd = nlp.create_pipe('sentencizer')\n",
    "# Adding the component to the pipeline\n",
    "nlp.add_pipe(sbd)\n",
    "x = \"Embracing and analyzing self failures (of however multitude) is a virtue of nobelmen. And nobility is a treasure few possess.\"\n",
    "#creating doc object carring our sentence tokens\n",
    "doc = nlp(x)\n",
    "#Creating and updating our list of tokens using list comprehension \n",
    "tokens = [token for token in doc.sents]\n",
    "print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Running', 'run'], ['down', 'down'], ['the', 'the'], ['street', 'street'], ['with', 'with'], ['my', 'my'], ['best', 'good'], ['buddy', 'buddy'], ['.', '.']]\n"
     ]
    }
   ],
   "source": [
    "#sample\n",
    "x = \"Running down the street with my best buddy.\"\n",
    "#creating doc object containing our token features\n",
    "doc = nlp(x)\n",
    "#Creating and updating our list of tokens using list comprehension \n",
    "tokens = [[token.text,token.lemma_] for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'once', 'enough', 'be', 'first', 'herself', 'against', 'many', 'whence', 'here', 'nowhere', 'most', 'hundred', 'somewhere', 'out', 'she', 'of', 'via', '’m', 'amount', 'latter', 'or', 'i', 'empty', \"'d\", 'not', 'just', 'everything', 'several', 'whereby', 'less', 'two', 'those', 'this', 'an', 'you', 'every', 'both', 'everywhere', 'thereupon', 'well', 'her', 'hers', 'then', 'forty', 'until', 'seeming', 'is', 'do', 'mine', 'themselves', 'whom', 'seems', 'can', 'few', 'yourself', \"n't\", 'itself', 'could', 'either', 'top', 'rather', 'moreover', 'whatever', 'after', 'although', 'which', 'them', 'again', 'anyway', 'since', '‘s', 'where', 'alone', 'however', 'by', \"'re\", 'bottom', 'does', 'along', 'without', 'something', 'from', 'because', 'therein', 'n‘t', 'part', 'used', 'why', 'has', 'thence', 'thereafter', 'twelve', 'therefore', 'four', 'name', 'was', 'show', 'whereas', 'seem', 'in', 'unless', 'more', 'throughout', 'up', 'one', 'really', 'these', 'whole', 'between', 'eight', 'everyone', 'became', 'some', 'only', 'under', '‘m', 'will', 'namely', 'whose', 'nobody', 'somehow', 'mostly', 'no', 'former', 'otherwise', 'who', 'using', 'among', 'what', 'about', 'made', 'toward', 'done', 'may', 'thereby', 'a', 'upon', 'ten', 'he', 'anyone', 'were', 'him', 'call', 'besides', 'himself', 'neither', 'sixty', 'should', 'someone', 'own', 'such', 'hereafter', 'my', 'due', 'whoever', 'together', 'down', 'fifteen', 'move', 'please', 'others', 'almost', 'wherein', 'have', \"'ll\", 'me', 'sometime', 'nothing', 'afterwards', 'elsewhere', 'all', 'various', 'often', 'being', 'beside', 'twenty', 'yet', 'eleven', 'around', 'say', 'anything', 'within', 'much', 'ourselves', 'would', 'must', 'ours', 'their', 'if', 'thus', '‘ve', 'your', '’s', 'am', 'meanwhile', 'into', 'behind', 'that', \"'m\", 'keep', 'hence', 'we', 'anyhow', 'quite', 'and', 'n’t', 'always', 'same', 'except', 'perhaps', 'whether', 'before', 'so', 'over', 'sometimes', 'other', 'become', '’re', 'when', 'none', 'amongst', 'last', 'did', 'latterly', 'further', 'take', 'get', 'give', 'at', 'herein', 'with', 'anywhere', 'least', 'myself', 'serious', 'becomes', 'side', 'five', 'across', 'full', 'though', 'during', 'beyond', 'next', 'hereupon', 'hereby', 'see', '’ll', 'whither', 'still', \"'ve\", 'six', 'as', 'formerly', 'already', 'wherever', 'go', 'even', 'very', 'doing', 'too', 'each', 'but', '‘d', '’d', 'they', 'whereupon', 'there', 'while', 'below', 'now', 'yours', 'whenever', 'noone', 'to', 'us', 'also', 'nor', 'never', 're', 'through', 'thru', '‘ll', 'had', 'else', 'regarding', \"'s\", 'his', 'put', 'becoming', 'ca', 'beforehand', 'are', 'yourselves', 'cannot', 'on', 'it', 'fifty', 'nevertheless', 'third', 'whereafter', 'off', 'our', 'onto', 'than', 'three', 'another', 'nine', 'might', 'back', '‘re', 'the', 'front', 'how', 'been', 'make', 'its', '’ve', 'for', 'above', 'towards', 'seemed', 'indeed', 'ever', 'any', 'per'}\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "stop = STOP_WORDS\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Running', 'down', 'the', 'street', 'with', 'my', 'best', 'buddy', '.']\n",
      "['Running', 'street', 'best', 'buddy', '.']\n"
     ]
    }
   ],
   "source": [
    "#sample\n",
    "x = \"Running down the street with my best buddy.\"\n",
    "#creation of doc object containing our token features\n",
    "doc = nlp(x)\n",
    "#Creating and updating our list of tokens using list comprehension \n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)\n",
    "#Creating and updating our list of filtered tokens using list comprehension \n",
    "filtered = [token.text for token in doc if token.is_stop == False]\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BLIMEY', '!', '!', 'Such', 'an', 'exhausting', 'day', ',', 'I', 'ca', \"n't\", 'even', 'describe', '.']\n",
      "['BLIMEY', 'exhausting', 'day', 'describe']\n"
     ]
    }
   ],
   "source": [
    "#sample \n",
    "x = \"BLIMEY!! Such an exhausting day, I can't even describe.\"\n",
    "#creation of doc object containing our token features\n",
    "doc = nlp(x)\n",
    "#Unfiltered tokens \n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)\n",
    "#Filtering our tokens\n",
    "filtered = [token.text for token in doc if token.is_stop == False and       \n",
    "token.text.isalpha() == True]\n",
    "print(filtered)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-Speech Tagging (POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Robin', 'PROPN'], ['is', 'AUX'], ['an', 'DET'], ['astute', 'ADJ'], ['programmer', 'NOUN']]\n"
     ]
    }
   ],
   "source": [
    "#sample\n",
    "x = \"Robin is an astute programmer\"\n",
    "#Creating doc object\n",
    "doc = nlp(x)\n",
    "#Extracting POS\n",
    "pos = [[token.text,token.pos_] for token in doc]\n",
    "print (pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c81d839d3c4227cd770621df97fe8191838af02e7eef185a922d8250cb33d344"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
